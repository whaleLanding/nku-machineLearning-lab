{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89cdf3a6",
   "metadata": {},
   "source": [
    "# Lab 4: Mindspore实现手写数字识别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2508ee0",
   "metadata": {},
   "source": [
    "## Before we start\n",
    "\n",
    "我们创建了[实验课的github仓库](https://github.com/Yujie-G/ML-2024Spring)，你可以在这里找到所有的实验指导书和相关资源。\n",
    "\n",
    "由于众所周知的原因，我们会在智慧树平台上上传一份实验资源的拷贝，不使用git仓库**不会**影响你完成实验。\n",
    "\n",
    "为什么使用git?\n",
    "\n",
    "1. 你可以第一时间获取实验指导代码的更新，代码框架的修改等。\n",
    "2. 你可以方便的在本地查看代码的变更和历史。\n",
    "3. 你可以在issue中提出关于实验代码的问题，可以帮助到有相同问题的同学。\n",
    "\n",
    "\n",
    "How to start:\n",
    "\n",
    "初始化：\n",
    "```bash\n",
    "git clone git@github.com:Yujie-G/ML-2024Spring.git\n",
    "```\n",
    "\n",
    "之后，每次新实验发布，你可以通过以下命令来更新本地仓库：\n",
    "```bash\n",
    "git pull\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69083059",
   "metadata": {},
   "source": [
    "\n",
    "## TODO\n",
    "\n",
    "1. 安装Mindspore\n",
    "2. 阅读并理解全连接网络的实现代码\n",
    "3. 实现LeNet5网络(部分代码已给出)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e4916e",
   "metadata": {},
   "source": [
    "## 1. 安装Mindspore\n",
    "\n",
    "[进入官网获取下载命令](https://www.mindspore.cn/install)， 建议选择2.1.1版本的Mindspore, 安装方式选择pip/conda安装均可\n",
    "\n",
    "不会Mindspore?可以点击[这里](https://www.mindspore.cn/tutorials/zh-CN/r2.2/index.html)学习官方教程\n",
    "\n",
    "你也可以参考[官方的API文档](https://www.mindspore.cn/docs/zh-CN/r2.1/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aecfe6",
   "metadata": {},
   "source": [
    "## 2. 利用Mindspore实现全连接网络手写数字识别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb11fe11",
   "metadata": {},
   "source": [
    "你可以参考[这份华为官方的指导手册](./assets/MindsporeTutorial.pdf),查看Mindspore的教程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76076af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "from mindspore import ops\n",
    "from mindspore import nn\n",
    "from mindspore.dataset import vision, transforms\n",
    "from mindspore.dataset import MnistDataset\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f18c5382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image', 'label']\n"
     ]
    }
   ],
   "source": [
    "# 加载MNIST数据集\n",
    "train_dataset_dir = \"./MNIST/train\"\n",
    "train_dataset = MnistDataset(dataset_dir=train_dataset_dir)\n",
    "test_dataset_dir = \"./MNIST/test\"\n",
    "test_dataset = MnistDataset(dataset_dir=test_dataset_dir)\n",
    "\n",
    "print(train_dataset.get_col_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8df62e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapipe(dataset, batch_size):\n",
    "    image_transforms = [\n",
    "        vision.Rescale(1.0 / 255.0, 0),\n",
    "        vision.Normalize(mean=(0.1307,), std=(0.3081,)),\n",
    "        vision.HWC2CHW()\n",
    "    ]\n",
    "    label_transform = transforms.TypeCast(mindspore.int32)\n",
    "\n",
    "    dataset = dataset.map(image_transforms, 'image')\n",
    "    dataset = dataset.map(label_transform, 'label')\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = datapipe(train_dataset, 64)\n",
    "test_dataset = datapipe(test_dataset, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88a06ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image [N, C, H, W]: (64, 1, 28, 28) Float32\n",
      "Shape of label: (64,) Int32\n"
     ]
    }
   ],
   "source": [
    "for image, label in test_dataset.create_tuple_iterator():\n",
    "    print(f\"Shape of image [N, C, H, W]: {image.shape} {image.dtype}\")\n",
    "    print(f\"Shape of label: {label.shape} {label.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62ff2b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image [N, C, H, W]: (64, 1, 28, 28) Float32\n",
      "Shape of label: (64,) Int32\n"
     ]
    }
   ],
   "source": [
    "for image, label in test_dataset.create_tuple_iterator():\n",
    "    print(f\"Shape of image [N, C, H, W]: {image.shape} {image.dtype}\")\n",
    "    print(f\"Shape of label: {label.shape} {label.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c7f4880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network<\n",
      "  (flatten): Flatten<>\n",
      "  (dense_relu_sequential): SequentialCell<\n",
      "    (0): Dense<input_channels=784, output_channels=512, has_bias=True>\n",
      "    (1): ReLU<>\n",
      "    (2): Dense<input_channels=512, output_channels=512, has_bias=True>\n",
      "    (3): ReLU<>\n",
      "    (4): Dense<input_channels=512, output_channels=10, has_bias=True>\n",
      "    >\n",
      "  >\n"
     ]
    }
   ],
   "source": [
    "# 网络构建\n",
    "class Network(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense_relu_sequential = nn.SequentialCell(\n",
    "            nn.Dense(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(512, 10)\n",
    "        )\n",
    "    \n",
    "    def construct(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.dense_relu_sequential(x)\n",
    "        return logits\n",
    "    \n",
    "model = Network()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44b1ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = nn.SGD(model.trainable_params(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f409878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, loss_fn, optimizer):\n",
    "    def forward_fn(data, label):\n",
    "        logits = model(data)\n",
    "        loss = loss_fn(logits, label)\n",
    "        return loss, logits\n",
    "    \n",
    "    grad_fn = ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "    \n",
    "    def train_step(data, label):\n",
    "        (loss, _), grads = grad_fn(data, label)\n",
    "        loss = ops.depend(loss, optimizer(grads))\n",
    "        return loss\n",
    "    \n",
    "    size = dataset.get_dataset_size()\n",
    "    model.set_train()\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        loss = train_step(data, label)\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.asnumpy(), batch\n",
    "            print(f\"loss: {loss:>7f}  [{current:>3d}/{size:>3d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77575ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataset, loss_fn):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for data, label in dataset.create_tuple_iterator():\n",
    "        pred = model(data)\n",
    "        total += len(data)\n",
    "        test_loss += loss_fn(pred, label).asnumpy()\n",
    "        correct += (pred.argmax(1) == label).asnumpy().sum()\n",
    "    test_loss /= num_batches\n",
    "    correct /= total\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5defa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.055614  [  0/938]\n",
      "loss: 0.074898  [100/938]\n",
      "loss: 0.090094  [200/938]\n",
      "loss: 0.073458  [300/938]\n",
      "loss: 0.036597  [400/938]\n",
      "loss: 0.113472  [500/938]\n",
      "loss: 0.090123  [600/938]\n",
      "loss: 0.112469  [700/938]\n",
      "loss: 0.081419  [800/938]\n",
      "loss: 0.035383  [900/938]\n",
      "Test: \n",
      " Accuracy: 96.9%, Avg loss: 0.099123 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.177960  [  0/938]\n",
      "loss: 0.140007  [100/938]\n",
      "loss: 0.079025  [200/938]\n",
      "loss: 0.087178  [300/938]\n",
      "loss: 0.230235  [400/938]\n",
      "loss: 0.110756  [500/938]\n",
      "loss: 0.053181  [600/938]\n",
      "loss: 0.084230  [700/938]\n",
      "loss: 0.024468  [800/938]\n",
      "loss: 0.062206  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.1%, Avg loss: 0.091054 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.062178  [  0/938]\n",
      "loss: 0.052646  [100/938]\n",
      "loss: 0.132047  [200/938]\n",
      "loss: 0.173868  [300/938]\n",
      "loss: 0.158869  [400/938]\n",
      "loss: 0.075314  [500/938]\n",
      "loss: 0.180499  [600/938]\n",
      "loss: 0.065313  [700/938]\n",
      "loss: 0.088117  [800/938]\n",
      "loss: 0.060853  [900/938]\n",
      "Test: \n",
      " Accuracy: 97.2%, Avg loss: 0.091232 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(model, train_dataset, loss_fn, optimizer)\n",
    "    test(model, test_dataset, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fb5c4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model to model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Save checkpoint\n",
    "mindspore.save_checkpoint(model, \"model.ckpt\")\n",
    "print(\"Saved Model to model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bcaafe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a random initialized model\n",
    "model = Network()\n",
    "# Load checkpoint and load parameter to model\n",
    "param_dict = mindspore.load_checkpoint(\"model.ckpt\")\n",
    "param_not_load, _ = mindspore.load_param_into_net(model, param_dict)\n",
    "print(param_not_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0992bfec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network<\n",
       "  (flatten): Flatten<>\n",
       "  (dense_relu_sequential): SequentialCell<\n",
       "    (0): Dense<input_channels=784, output_channels=512, has_bias=True>\n",
       "    (1): ReLU<>\n",
       "    (2): Dense<input_channels=512, output_channels=512, has_bias=True>\n",
       "    (3): ReLU<>\n",
       "    (4): Dense<input_channels=512, output_channels=10, has_bias=True>\n",
       "    >\n",
       "  >"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"[7 1 1 3 8 0 4 0 9 7]\", Actual: \"[7 1 1 3 8 0 4 0 9 7]\"\n"
     ]
    }
   ],
   "source": [
    "model.set_train(False)\n",
    "for data, label in test_dataset:\n",
    "    pred = model(data)\n",
    "    predicted = pred.argmax(1)\n",
    "    print(f'Predicted: \"{predicted[:10]}\", Actual: \"{label[:10]}\"')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f0695",
   "metadata": {},
   "source": [
    "## 3. 实现LeNet5网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a190dbbc",
   "metadata": {},
   "source": [
    "![](assets/images/2024-04-07-22-36-11.png)\n",
    "\n",
    "\n",
    "根据上图说明的参数，实现LeNet5网络，完成手写数字识别任务，部分代码已给出，你需要补全代码\n",
    "\n",
    "你可能会用到的[MindSpore卷积神经网络API](https://www.mindspore.cn/docs/zh-CN/r2.1/api_python/mindspore.nn.html#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B1%82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "468d3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary pkgs\n",
    "import os\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Model\n",
    "import mindspore.dataset as ds\n",
    "from mindspore import ops\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f34092c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !设置全局种子，这里改成你的学号后四位\n",
    "np.random.seed(2195)\n",
    "ms.set_seed(2195)\n",
    "\n",
    "\n",
    "## hyperparameters\n",
    "batch_size = 32\n",
    "epoch_size = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "dataset_dir = \"./MNIST\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10cdaf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入你需要的包\n",
    "## pkgs import\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.vision as CV\n",
    "import mindspore.dataset.transforms as C\n",
    "from mindspore import dtype as mstype\n",
    "from mindspore.dataset.vision import Inter \n",
    "\n",
    "resize_height, resize_width = 32, 32\n",
    "rescale = 1.0 / 255.0\n",
    "shift = 0.0\n",
    "rescale_nml = 1 / 0.3081\n",
    "shift_nml = -1 * 0.1307 / 0.3081\n",
    "\n",
    "\n",
    "def create_dataset(data_path, batch_size=32, repeat_size=1, num_parallel_workers=1):\n",
    "\n",
    "    # 创建数据集\n",
    "    mnist_ds = ds.MnistDataset(data_path)\n",
    "\n",
    "    # 实现数据增强和处理(不要忘记处理label)\n",
    "    # 1. 将图像缩放到模型需要的输入，比如32x32, 插值方式为线性插值。\n",
    "    # [附加题] 将图像的对比度和亮度做适当的调整，调整幅度任意。 [提示：需要先对色彩空间进行转化]\n",
    "    \"\"\"\n",
    "    ==========================修改这部分代码=======================\n",
    "    \"\"\"\n",
    "    image_trans = [\n",
    "        CV.Resize((32, 32), interpolation=Inter.LINEAR),\n",
    "        CV.Rescale(1, shift_nml),\n",
    "        CV.Rescale(1, shift),\n",
    "        CV.HWC2CHW(),\n",
    "    ]\n",
    "    label_trans = C.TypeCast(mstype.int32)\n",
    "    mnist_ds = mnist_ds.map(operations=image_trans,input_columns=[\"image\"], num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(operations=label_trans,input_columns=[\"label\"], num_parallel_workers=num_parallel_workers)\n",
    "    \"\"\"\n",
    "    ===============================================================\n",
    "    \"\"\"\n",
    "\n",
    "    # 当需要对**指定标签**的数据进行增强时，可以使用类似下面的双变量迭代方式, 例如：\n",
    "    # mnist_ds = mnist_ds.map(operations=(lambda img, lb: ... if lb == ... else ...),input_columns=[\"image\", \"label\"], num_parallel_workers=num_parallel_workers)\n",
    "    # 处理生成的数据集\n",
    "    buffer_size = 10000\n",
    "    mnist_ds = mnist_ds.shuffle(buffer_size=buffer_size)\n",
    "    mnist_ds = mnist_ds.batch(batch_size, drop_remainder=True)\n",
    "    mnist_ds = mnist_ds.repeat(repeat_size)\n",
    "\n",
    "    return mnist_ds\n",
    "\n",
    "\n",
    "train_dataset = create_dataset(os.path.join(dataset_dir, \"train\"), batch_size=batch_size)\n",
    "test_dataset = create_dataset(os.path.join(dataset_dir, \"test\"), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0668ff8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875\n",
      "(32, 1, 32, 32) Float32 , the result should be (32, 1, 32, 32) Float32\n",
      "(32,) Int32 , the result should be (32,) Int32\n"
     ]
    }
   ],
   "source": [
    "# this part is for debug use\n",
    "image, label = next(train_dataset.create_tuple_iterator())\n",
    "print(train_dataset.get_dataset_size())\n",
    "print(image.shape, image.dtype, f\", the result should be ({batch_size}, 1, {resize_height}, {resize_width}) Float32\")\n",
    "print(label.shape, label.dtype, f\", the result should be ({batch_size},) Int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81043964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense_relu_sequential = nn.SequentialCell(\n",
    "            nn.Dense(32*32, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(84, 10)\n",
    "        )\n",
    "    \n",
    "    def construct(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.dense_relu_sequential(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "852fdd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LeNet5()\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "optim = nn.Momentum(params=net.trainable_params(), learning_rate=learning_rate, momentum=0.9)\n",
    "model = Model(network = net, loss_fn=loss, optimizer=optim, metrics={\"Accuracy\": nn.Accuracy()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2068f4a9",
   "metadata": {},
   "source": [
    "#### 训练遇到报错？先在[Q&A](https://github.com/Yujie-G/ML-2024Spring/tree/main/Q%26A)中看看有没类似情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "790ff1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现训练部分代码，并打印训练过程中的loss值，[建议]可视化查看loss值的变化\n",
    "model.train(epoch=10, train_dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96261510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 10.503235  [  0/1875]\n",
      "loss: 2.293808  [100/1875]\n",
      "loss: 2.303506  [200/1875]\n",
      "loss: 2.306923  [300/1875]\n",
      "loss: 2.301941  [400/1875]\n",
      "loss: 2.295349  [500/1875]\n",
      "loss: 2.313744  [600/1875]\n",
      "loss: 2.302676  [700/1875]\n",
      "loss: 2.291261  [800/1875]\n",
      "loss: 2.292457  [900/1875]\n",
      "loss: 2.296773  [1000/1875]\n",
      "loss: 2.307923  [1100/1875]\n",
      "loss: 2.310169  [1200/1875]\n",
      "loss: 2.293696  [1300/1875]\n",
      "loss: 2.308665  [1400/1875]\n",
      "loss: 2.294751  [1500/1875]\n",
      "loss: 2.299837  [1600/1875]\n",
      "loss: 2.301261  [1700/1875]\n",
      "loss: 2.299310  [1800/1875]\n",
      "Test: \n",
      " Accuracy: 11.4%, Avg loss: 2.301121 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.312440  [  0/1875]\n",
      "loss: 2.301395  [100/1875]\n",
      "loss: 2.300680  [200/1875]\n",
      "loss: 2.308075  [300/1875]\n",
      "loss: 2.293785  [400/1875]\n",
      "loss: 2.291690  [500/1875]\n",
      "loss: 2.302833  [600/1875]\n",
      "loss: 2.294736  [700/1875]\n",
      "loss: 2.292112  [800/1875]\n",
      "loss: 2.288865  [900/1875]\n",
      "loss: 2.298661  [1000/1875]\n",
      "loss: 2.302379  [1100/1875]\n",
      "loss: 2.305946  [1200/1875]\n",
      "loss: 2.286235  [1300/1875]\n",
      "loss: 2.306633  [1400/1875]\n",
      "loss: 2.296978  [1500/1875]\n",
      "loss: 2.299304  [1600/1875]\n",
      "loss: 2.299091  [1700/1875]\n",
      "loss: 2.296580  [1800/1875]\n",
      "Test: \n",
      " Accuracy: 11.4%, Avg loss: 2.301086 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.313462  [  0/1875]\n",
      "loss: 2.303677  [100/1875]\n",
      "loss: 2.300092  [200/1875]\n",
      "loss: 2.309451  [300/1875]\n",
      "loss: 2.292397  [400/1875]\n",
      "loss: 2.293254  [500/1875]\n",
      "loss: 2.301315  [600/1875]\n",
      "loss: 2.293229  [700/1875]\n",
      "loss: 2.292583  [800/1875]\n",
      "loss: 2.287852  [900/1875]\n",
      "loss: 2.298239  [1000/1875]\n",
      "loss: 2.301799  [1100/1875]\n",
      "loss: 2.305774  [1200/1875]\n",
      "loss: 2.285495  [1300/1875]\n",
      "loss: 2.306510  [1400/1875]\n",
      "loss: 2.297040  [1500/1875]\n",
      "loss: 2.299043  [1600/1875]\n",
      "loss: 2.298996  [1700/1875]\n",
      "loss: 2.296423  [1800/1875]\n",
      "Test: \n",
      " Accuracy: 11.4%, Avg loss: 2.301091 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model = LeNet5()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = nn.SGD(model.trainable_params(), 1e-2)\n",
    "def train(model, dataset, loss_fn, optimizer):\n",
    "    def forward_fn(data, label):\n",
    "        logits = model(data)\n",
    "        loss = loss_fn(logits, label)\n",
    "        return loss, logits\n",
    "    \n",
    "    grad_fn = ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "    \n",
    "    def train_step(data, label):\n",
    "        (loss, _), grads = grad_fn(data, label)\n",
    "        loss = ops.depend(loss, optimizer(grads))\n",
    "        return loss\n",
    "    \n",
    "    size = dataset.get_dataset_size()\n",
    "    model.set_train()\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        loss = train_step(data, label)\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.asnumpy(), batch\n",
    "            print(f\"loss: {loss:>7f}  [{current:>3d}/{size:>3d}]\")\n",
    "def test(model, dataset, loss_fn):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for data, label in dataset.create_tuple_iterator():\n",
    "        pred = model(data)\n",
    "        total += len(data)\n",
    "        test_loss += loss_fn(pred, label).asnumpy()\n",
    "        correct += (pred.argmax(1) == label).asnumpy().sum()\n",
    "    test_loss /= num_batches\n",
    "    correct /= total\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "epochs = 3\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(model, train_dataset, loss_fn, optimizer)\n",
    "    test(model, test_dataset, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a45123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.save_checkpoint(net, \"net.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c1fa409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Accuracy:{'Accuracy': 0.11358173076923077} ==============\n"
     ]
    }
   ],
   "source": [
    "## test\n",
    "def test_net(network, model,path):\n",
    "    \"\"\"Define the evaluation method.\"\"\"\n",
    "    # 加载已保存的模型\n",
    "    param_dict = ms.load_checkpoint(path)\n",
    "    # load parameter to the network\n",
    "    ms.load_param_into_net(network, param_dict)\n",
    "    # evaluation\n",
    "    acc = model.eval(test_dataset, dataset_sink_mode=False)\n",
    "    print(\"============== Accuracy:{} ==============\".format(acc))\n",
    "\n",
    "# 修改为你的checkpoint路径\n",
    "test_net(net, model,r\"D:\\machine learning\\Lab4-LeNet(1)\\net.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5cb9523a3ce612da594b1721626e95f90edbb30067f86e1c6d953bc531f0cdd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
