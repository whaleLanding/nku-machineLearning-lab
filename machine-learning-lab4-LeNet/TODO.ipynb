{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89cdf3a6",
   "metadata": {},
   "source": [
    "# Lab 4: Mindspore实现手写数字识别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2508ee0",
   "metadata": {},
   "source": [
    "## Before we start\n",
    "\n",
    "我们创建了[实验课的github仓库](https://github.com/Yujie-G/ML-2024Spring)，你可以在这里找到所有的实验指导书和相关资源。\n",
    "\n",
    "由于众所周知的原因，我们会在智慧树平台上上传一份实验资源的拷贝，不使用git仓库**不会**影响你完成实验。\n",
    "\n",
    "为什么使用git?\n",
    "\n",
    "1. 你可以第一时间获取实验指导代码的更新，代码框架的修改等。\n",
    "2. 你可以方便的在本地查看代码的变更和历史。\n",
    "3. 你可以在issue中提出关于实验代码的问题，可以帮助到有相同问题的同学。\n",
    "\n",
    "\n",
    "How to start:\n",
    "\n",
    "初始化：\n",
    "```bash\n",
    "git clone git@github.com:Yujie-G/ML-2024Spring.git\n",
    "```\n",
    "\n",
    "之后，每次新实验发布，你可以通过以下命令来更新本地仓库：\n",
    "```bash\n",
    "git pull\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69083059",
   "metadata": {},
   "source": [
    "\n",
    "## TODO\n",
    "\n",
    "1. 安装Mindspore\n",
    "2. 阅读并理解全连接网络的实现代码\n",
    "3. 实现LeNet5网络(部分代码已给出)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e4916e",
   "metadata": {},
   "source": [
    "## 1. 安装Mindspore\n",
    "\n",
    "[进入官网获取下载命令](https://www.mindspore.cn/install)， 建议选择2.1.1版本的Mindspore, 安装方式选择pip/conda安装均可\n",
    "\n",
    "不会Mindspore?可以点击[这里](https://www.mindspore.cn/tutorials/zh-CN/r2.2/index.html)学习官方教程\n",
    "\n",
    "你也可以参考[官方的API文档](https://www.mindspore.cn/docs/zh-CN/r2.1/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aecfe6",
   "metadata": {},
   "source": [
    "## 2. 利用Mindspore实现全连接网络手写数字识别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb11fe11",
   "metadata": {},
   "source": [
    "你可以参考[这份华为官方的指导手册](./assets/MindsporeTutorial.pdf),查看Mindspore的教程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76076af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "from mindspore import ops\n",
    "from mindspore import nn\n",
    "from mindspore.dataset import vision, transforms\n",
    "from mindspore.dataset import MnistDataset\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f18c5382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image', 'label']\n"
     ]
    }
   ],
   "source": [
    "# 加载MNIST数据集\n",
    "train_dataset_dir = \"./MNIST/train\"\n",
    "train_dataset = MnistDataset(dataset_dir=train_dataset_dir)\n",
    "test_dataset_dir = \"./MNIST/test\"\n",
    "test_dataset = MnistDataset(dataset_dir=test_dataset_dir)\n",
    "\n",
    "print(train_dataset.get_col_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8df62e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapipe(dataset, batch_size):\n",
    "    image_transforms = [\n",
    "        vision.Rescale(1.0 / 255.0, 0),\n",
    "        vision.Normalize(mean=(0.1307,), std=(0.3081,)),\n",
    "        vision.HWC2CHW()\n",
    "    ]\n",
    "    label_transform = transforms.TypeCast(mindspore.int32)\n",
    "\n",
    "    dataset = dataset.map(image_transforms, 'image')\n",
    "    dataset = dataset.map(label_transform, 'label')\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = datapipe(train_dataset, 64)\n",
    "test_dataset = datapipe(test_dataset, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88a06ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image [N, C, H, W]: (64, 1, 28, 28) Float32\n",
      "Shape of label: (64,) Int32\n"
     ]
    }
   ],
   "source": [
    "for image, label in test_dataset.create_tuple_iterator():\n",
    "    print(f\"Shape of image [N, C, H, W]: {image.shape} {image.dtype}\")\n",
    "    print(f\"Shape of label: {label.shape} {label.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62ff2b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image [N, C, H, W]: (64, 1, 28, 28) Float32\n",
      "Shape of label: (64,) Int32\n"
     ]
    }
   ],
   "source": [
    "for image, label in test_dataset.create_tuple_iterator():\n",
    "    print(f\"Shape of image [N, C, H, W]: {image.shape} {image.dtype}\")\n",
    "    print(f\"Shape of label: {label.shape} {label.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c7f4880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network<\n",
      "  (flatten): Flatten<>\n",
      "  (dense_relu_sequential): SequentialCell<\n",
      "    (0): Dense<input_channels=784, output_channels=512, has_bias=True>\n",
      "    (1): ReLU<>\n",
      "    (2): Dense<input_channels=512, output_channels=512, has_bias=True>\n",
      "    (3): ReLU<>\n",
      "    (4): Dense<input_channels=512, output_channels=10, has_bias=True>\n",
      "    >\n",
      "  >\n"
     ]
    }
   ],
   "source": [
    "# 网络构建\n",
    "class Network(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense_relu_sequential = nn.SequentialCell(\n",
    "            nn.Dense(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(512, 10)\n",
    "        )\n",
    "    \n",
    "    def construct(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.dense_relu_sequential(x)\n",
    "        return logits\n",
    "    \n",
    "model = Network()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44b1ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = nn.SGD(model.trainable_params(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f409878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, loss_fn, optimizer):\n",
    "    def forward_fn(data, label):\n",
    "        logits = model(data)\n",
    "        loss = loss_fn(logits, label)\n",
    "        return loss, logits\n",
    "    \n",
    "    grad_fn = ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "    \n",
    "    def train_step(data, label):\n",
    "        (loss, _), grads = grad_fn(data, label)\n",
    "        loss = ops.depend(loss, optimizer(grads))\n",
    "        return loss\n",
    "    \n",
    "    size = dataset.get_dataset_size()\n",
    "    model.set_train()\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        loss = train_step(data, label)\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.asnumpy(), batch\n",
    "            print(f\"loss: {loss:>7f}  [{current:>3d}/{size:>3d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77575ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataset, loss_fn):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for data, label in dataset.create_tuple_iterator():\n",
    "        pred = model(data)\n",
    "        total += len(data)\n",
    "        test_loss += loss_fn(pred, label).asnumpy()\n",
    "        correct += (pred.argmax(1) == label).asnumpy().sum()\n",
    "    test_loss /= num_batches\n",
    "    correct /= total\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5defa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.206138  [  0/938]\n",
      "loss: 0.152164  [100/938]\n",
      "loss: 0.212019  [200/938]\n",
      "loss: 0.213164  [300/938]\n",
      "loss: 0.132117  [400/938]\n",
      "loss: 0.346765  [500/938]\n",
      "loss: 0.123721  [600/938]\n",
      "loss: 0.167727  [700/938]\n",
      "loss: 0.293147  [800/938]\n",
      "loss: 0.449689  [900/938]\n",
      "Test: \n",
      " Accuracy: 94.2%, Avg loss: 0.202366 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.199232  [  0/938]\n",
      "loss: 0.114885  [100/938]\n",
      "loss: 0.192825  [200/938]\n",
      "loss: 0.177965  [300/938]\n",
      "loss: 0.110029  [400/938]\n",
      "loss: 0.286768  [500/938]\n",
      "loss: 0.092223  [600/938]\n",
      "loss: 0.123744  [700/938]\n",
      "loss: 0.256621  [800/938]\n",
      "loss: 0.410659  [900/938]\n",
      "Test: \n",
      " Accuracy: 95.0%, Avg loss: 0.176041 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.190286  [  0/938]\n",
      "loss: 0.094568  [100/938]\n",
      "loss: 0.171633  [200/938]\n",
      "loss: 0.152394  [300/938]\n",
      "loss: 0.092784  [400/938]\n",
      "loss: 0.240326  [500/938]\n",
      "loss: 0.070049  [600/938]\n",
      "loss: 0.093074  [700/938]\n",
      "loss: 0.224874  [800/938]\n",
      "loss: 0.376854  [900/938]\n",
      "Test: \n",
      " Accuracy: 95.4%, Avg loss: 0.156217 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(model, train_dataset, loss_fn, optimizer)\n",
    "    test(model, test_dataset, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fb5c4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model to model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Save checkpoint\n",
    "mindspore.save_checkpoint(model, \"model.ckpt\")\n",
    "print(\"Saved Model to model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bcaafe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a random initialized model\n",
    "model = Network()\n",
    "# Load checkpoint and load parameter to model\n",
    "param_dict = mindspore.load_checkpoint(\"model.ckpt\")\n",
    "param_not_load, _ = mindspore.load_param_into_net(model, param_dict)\n",
    "print(param_not_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0992bfec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network<\n",
       "  (flatten): Flatten<>\n",
       "  (dense_relu_sequential): SequentialCell<\n",
       "    (0): Dense<input_channels=784, output_channels=512, has_bias=True>\n",
       "    (1): ReLU<>\n",
       "    (2): Dense<input_channels=512, output_channels=512, has_bias=True>\n",
       "    (3): ReLU<>\n",
       "    (4): Dense<input_channels=512, output_channels=10, has_bias=True>\n",
       "    >\n",
       "  >"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"[1 8 9 7 6 9 3 0 2 2]\", Actual: \"[1 8 9 7 6 9 3 5 2 2]\"\n"
     ]
    }
   ],
   "source": [
    "model.set_train(False)\n",
    "for data, label in test_dataset:\n",
    "    pred = model(data)\n",
    "    predicted = pred.argmax(1)\n",
    "    print(f'Predicted: \"{predicted[:10]}\", Actual: \"{label[:10]}\"')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f0695",
   "metadata": {},
   "source": [
    "## 3. 实现LeNet5网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a190dbbc",
   "metadata": {},
   "source": [
    "![](assets/images/2024-04-07-22-36-11.png)\n",
    "\n",
    "\n",
    "根据上图说明的参数，实现LeNet5网络，完成手写数字识别任务，部分代码已给出，你需要补全代码\n",
    "\n",
    "你可能会用到的[MindSpore卷积神经网络API](https://www.mindspore.cn/docs/zh-CN/r2.1/api_python/mindspore.nn.html#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B1%82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "468d3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary pkgs\n",
    "import os\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Model\n",
    "import mindspore.dataset as ds\n",
    "from mindspore import ops\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f34092c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !设置全局种子，这里改成你的学号后四位\n",
    "np.random.seed(2195)\n",
    "ms.set_seed(2195)\n",
    "\n",
    "\n",
    "## hyperparameters\n",
    "batch_size = 32\n",
    "epoch_size = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "dataset_dir = \"./MNIST\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10cdaf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 导入你需要的包\n",
    "## pkgs import\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.vision as CV\n",
    "import mindspore.dataset.transforms as C\n",
    "from mindspore import dtype as mstype\n",
    "from mindspore.dataset.vision import Inter \n",
    "\n",
    "resize_height, resize_width = 32, 32\n",
    "rescale = 1.0 / 255.0\n",
    "shift = 0.0\n",
    "rescale_nml = 1 / 0.3081\n",
    "shift_nml = -1 * 0.1307 / 0.3081\n",
    "\n",
    "\n",
    "def create_dataset(data_path, batch_size=32, repeat_size=1, num_parallel_workers=1):\n",
    "\n",
    "    # 创建数据集\n",
    "    mnist_ds = ds.MnistDataset(data_path)\n",
    "\n",
    "    # 实现数据增强和处理(不要忘记处理label)\n",
    "    # 1. 将图像缩放到模型需要的输入，比如32x32, 插值方式为线性插值。\n",
    "    # [附加题] 将图像的对比度和亮度做适当的调整，调整幅度任意。 [提示：需要先对色彩空间进行转化]\n",
    "    \"\"\"\n",
    "    ==========================修改这部分代码=======================\n",
    "    \"\"\"\n",
    "    image_trans = [\n",
    "        CV.Resize((32, 32), interpolation=Inter.LINEAR),\n",
    "        CV.Rescale(rescale_nml, shift_nml),\n",
    "        CV.Rescale(rescale, shift),\n",
    "        CV.HWC2CHW(),\n",
    "    ]\n",
    "    label_trans = C.TypeCast(mstype.int32)\n",
    "    mnist_ds = mnist_ds.map(operations=image_trans,input_columns=[\"image\"], num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(operations=label_trans,input_columns=[\"label\"], num_parallel_workers=num_parallel_workers)\n",
    "    \"\"\"\n",
    "    ===============================================================\n",
    "    \"\"\"\n",
    "\n",
    "    # 当需要对**指定标签**的数据进行增强时，可以使用类似下面的双变量迭代方式, 例如：\n",
    "    # mnist_ds = mnist_ds.map(operations=(lambda img, lb: ... if lb == ... else ...),input_columns=[\"image\", \"label\"], num_parallel_workers=num_parallel_workers)\n",
    "    # 处理生成的数据集\n",
    "    buffer_size = 10000\n",
    "    mnist_ds = mnist_ds.shuffle(buffer_size=buffer_size)\n",
    "    mnist_ds = mnist_ds.batch(batch_size, drop_remainder=True)\n",
    "    mnist_ds = mnist_ds.repeat(repeat_size)\n",
    "\n",
    "    return mnist_ds\n",
    "\n",
    "\n",
    "train_dataset = create_dataset(os.path.join(dataset_dir, \"train\"), batch_size=batch_size)\n",
    "test_dataset = create_dataset(os.path.join(dataset_dir, \"test\"), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0668ff8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875\n",
      "(32, 1, 32, 32) Float32 , the result should be (32, 1, 32, 32) Float32\n",
      "(32,) Int32 , the result should be (32,) Int32\n"
     ]
    }
   ],
   "source": [
    "# this part is for debug use\n",
    "image, label = next(train_dataset.create_tuple_iterator())\n",
    "print(train_dataset.get_dataset_size())\n",
    "print(image.shape, image.dtype, f\", the result should be ({batch_size}, 1, {resize_height}, {resize_width}) Float32\")\n",
    "print(label.shape, label.dtype, f\", the result should be ({batch_size},) Int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81043964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense_relu_sequential = nn.SequentialCell(\n",
    "            nn.Dense(32*32, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(84, 10)\n",
    "        )\n",
    "    \n",
    "    def construct(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.dense_relu_sequential(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "852fdd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LeNet5()\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "optim = nn.Momentum(params=net.trainable_params(), learning_rate=learning_rate, momentum=0.9)\n",
    "model = Model(network = net, loss_fn=loss, optimizer=optim, metrics={\"Accuracy\": nn.Accuracy()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2068f4a9",
   "metadata": {},
   "source": [
    "#### 训练遇到报错？先在[Q&A](https://github.com/Yujie-G/ML-2024Spring/tree/main/Q%26A)中看看有没类似情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "790ff1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1, loss is 2.328641176223755\n",
      "epoch: 1 step: 2, loss is 2.2912392616271973\n",
      "epoch: 1 step: 3, loss is 2.3088839054107666\n",
      "epoch: 1 step: 4, loss is 2.2783544063568115\n",
      "epoch: 1 step: 5, loss is 2.257840633392334\n",
      "epoch: 1 step: 6, loss is 2.231645345687866\n",
      "epoch: 1 step: 7, loss is 2.2120602130889893\n",
      "epoch: 1 step: 8, loss is 2.2625298500061035\n",
      "epoch: 1 step: 9, loss is 2.2399661540985107\n",
      "epoch: 1 step: 10, loss is 2.172929525375366\n",
      "epoch: 1 step: 11, loss is 2.2031679153442383\n",
      "epoch: 1 step: 12, loss is 2.2674520015716553\n",
      "epoch: 1 step: 13, loss is 2.0931060314178467\n",
      "epoch: 1 step: 14, loss is 2.1739842891693115\n",
      "epoch: 1 step: 15, loss is 2.0627968311309814\n",
      "epoch: 1 step: 16, loss is 2.0448203086853027\n",
      "epoch: 1 step: 17, loss is 2.0410077571868896\n",
      "epoch: 1 step: 18, loss is 2.0329537391662598\n",
      "epoch: 1 step: 19, loss is 2.0289602279663086\n",
      "epoch: 1 step: 20, loss is 1.8521307706832886\n",
      "epoch: 1 step: 21, loss is 1.924983263015747\n",
      "epoch: 1 step: 22, loss is 1.7925680875778198\n",
      "epoch: 1 step: 23, loss is 1.868318796157837\n",
      "epoch: 1 step: 24, loss is 1.855181336402893\n",
      "epoch: 1 step: 25, loss is 1.7737627029418945\n",
      "epoch: 1 step: 26, loss is 1.7214345932006836\n",
      "epoch: 1 step: 27, loss is 1.7540639638900757\n",
      "epoch: 1 step: 28, loss is 1.4340100288391113\n",
      "epoch: 1 step: 29, loss is 1.4354572296142578\n",
      "epoch: 1 step: 30, loss is 1.3077287673950195\n",
      "epoch: 1 step: 31, loss is 1.27420973777771\n",
      "epoch: 1 step: 32, loss is 1.3036638498306274\n",
      "epoch: 1 step: 33, loss is 1.2838122844696045\n",
      "epoch: 1 step: 34, loss is 1.0687888860702515\n",
      "epoch: 1 step: 35, loss is 1.0306085348129272\n",
      "epoch: 1 step: 36, loss is 1.2092891931533813\n",
      "epoch: 1 step: 37, loss is 1.022841453552246\n",
      "epoch: 1 step: 38, loss is 1.1309982538223267\n",
      "epoch: 1 step: 39, loss is 0.8599117398262024\n",
      "epoch: 1 step: 40, loss is 0.9708631038665771\n",
      "epoch: 1 step: 41, loss is 0.7857924103736877\n",
      "epoch: 1 step: 42, loss is 0.7175737023353577\n",
      "epoch: 1 step: 43, loss is 0.8110746145248413\n",
      "epoch: 1 step: 44, loss is 0.8601720333099365\n",
      "epoch: 1 step: 45, loss is 0.8910666108131409\n",
      "epoch: 1 step: 46, loss is 0.7588391304016113\n",
      "epoch: 1 step: 47, loss is 0.6296431422233582\n",
      "epoch: 1 step: 48, loss is 0.6557418704032898\n",
      "epoch: 1 step: 49, loss is 0.6618818044662476\n",
      "epoch: 1 step: 50, loss is 0.6677024364471436\n",
      "epoch: 1 step: 51, loss is 0.9423997402191162\n",
      "epoch: 1 step: 52, loss is 0.5930416584014893\n",
      "epoch: 1 step: 53, loss is 0.5490713119506836\n",
      "epoch: 1 step: 54, loss is 0.7617806792259216\n",
      "epoch: 1 step: 55, loss is 0.921291708946228\n",
      "epoch: 1 step: 56, loss is 0.6465532183647156\n",
      "epoch: 1 step: 57, loss is 0.6302387714385986\n",
      "epoch: 1 step: 58, loss is 0.6563233137130737\n",
      "epoch: 1 step: 59, loss is 0.5645406246185303\n",
      "epoch: 1 step: 60, loss is 0.9827278256416321\n",
      "epoch: 1 step: 61, loss is 0.9365037083625793\n",
      "epoch: 1 step: 62, loss is 0.5687028169631958\n",
      "epoch: 1 step: 63, loss is 0.36860549449920654\n",
      "epoch: 1 step: 64, loss is 1.0701063871383667\n",
      "epoch: 1 step: 65, loss is 0.7188141942024231\n",
      "epoch: 1 step: 66, loss is 0.6447762846946716\n",
      "epoch: 1 step: 67, loss is 0.6498141884803772\n",
      "epoch: 1 step: 68, loss is 0.5181456208229065\n",
      "epoch: 1 step: 69, loss is 0.870455801486969\n",
      "epoch: 1 step: 70, loss is 0.3396866023540497\n",
      "epoch: 1 step: 71, loss is 0.4104086458683014\n",
      "epoch: 1 step: 72, loss is 0.5070021152496338\n",
      "epoch: 1 step: 73, loss is 0.577485978603363\n",
      "epoch: 1 step: 74, loss is 0.6786502599716187\n",
      "epoch: 1 step: 75, loss is 0.5584572553634644\n",
      "epoch: 1 step: 76, loss is 0.5055744647979736\n",
      "epoch: 1 step: 77, loss is 0.8041029572486877\n",
      "epoch: 1 step: 78, loss is 0.8708775043487549\n",
      "epoch: 1 step: 79, loss is 0.3893227279186249\n",
      "epoch: 1 step: 80, loss is 0.920956015586853\n",
      "epoch: 1 step: 81, loss is 0.4309768080711365\n",
      "epoch: 1 step: 82, loss is 0.5270434617996216\n",
      "epoch: 1 step: 83, loss is 0.4921281933784485\n",
      "epoch: 1 step: 84, loss is 0.4899239242076874\n",
      "epoch: 1 step: 85, loss is 0.45869457721710205\n",
      "epoch: 1 step: 86, loss is 0.5625591278076172\n",
      "epoch: 1 step: 87, loss is 0.4798189401626587\n",
      "epoch: 1 step: 88, loss is 0.41897356510162354\n",
      "epoch: 1 step: 89, loss is 0.4182506501674652\n",
      "epoch: 1 step: 90, loss is 0.6726500391960144\n",
      "epoch: 1 step: 91, loss is 0.5417277812957764\n",
      "epoch: 1 step: 92, loss is 0.40914273262023926\n",
      "epoch: 1 step: 93, loss is 0.6146524548530579\n",
      "epoch: 1 step: 94, loss is 0.5445880889892578\n",
      "epoch: 1 step: 95, loss is 0.33407390117645264\n",
      "epoch: 1 step: 96, loss is 1.1800216436386108\n",
      "epoch: 1 step: 97, loss is 0.6201011538505554\n",
      "epoch: 1 step: 98, loss is 0.5075730085372925\n",
      "epoch: 1 step: 99, loss is 0.5666825175285339\n",
      "epoch: 1 step: 100, loss is 0.5212236046791077\n",
      "epoch: 1 step: 101, loss is 0.3330176770687103\n",
      "epoch: 1 step: 102, loss is 0.26764416694641113\n",
      "epoch: 1 step: 103, loss is 0.3728887438774109\n",
      "epoch: 1 step: 104, loss is 0.5343887805938721\n",
      "epoch: 1 step: 105, loss is 0.6630244851112366\n",
      "epoch: 1 step: 106, loss is 0.48280107975006104\n",
      "epoch: 1 step: 107, loss is 0.25543123483657837\n",
      "epoch: 1 step: 108, loss is 0.31082379817962646\n",
      "epoch: 1 step: 109, loss is 0.6423013806343079\n",
      "epoch: 1 step: 110, loss is 0.5224090218544006\n",
      "epoch: 1 step: 111, loss is 0.5025906562805176\n",
      "epoch: 1 step: 112, loss is 0.6804065108299255\n",
      "epoch: 1 step: 113, loss is 0.6208733320236206\n",
      "epoch: 1 step: 114, loss is 0.5618929266929626\n",
      "epoch: 1 step: 115, loss is 0.5569959878921509\n",
      "epoch: 1 step: 116, loss is 0.3123694956302643\n",
      "epoch: 1 step: 117, loss is 0.6709920167922974\n",
      "epoch: 1 step: 118, loss is 0.4243225157260895\n",
      "epoch: 1 step: 119, loss is 0.7687031626701355\n",
      "epoch: 1 step: 120, loss is 0.2942235469818115\n",
      "epoch: 1 step: 121, loss is 0.5067504644393921\n",
      "epoch: 1 step: 122, loss is 0.37472498416900635\n",
      "epoch: 1 step: 123, loss is 0.49972254037857056\n",
      "epoch: 1 step: 124, loss is 0.4617435932159424\n",
      "epoch: 1 step: 125, loss is 0.2625187337398529\n",
      "epoch: 1 step: 126, loss is 0.7441151142120361\n",
      "epoch: 1 step: 127, loss is 1.0752360820770264\n",
      "epoch: 1 step: 128, loss is 0.6615446209907532\n",
      "epoch: 1 step: 129, loss is 0.3794419765472412\n",
      "epoch: 1 step: 130, loss is 0.41070276498794556\n",
      "epoch: 1 step: 131, loss is 0.35245075821876526\n",
      "epoch: 1 step: 132, loss is 0.6587339639663696\n",
      "epoch: 1 step: 133, loss is 0.38610097765922546\n",
      "epoch: 1 step: 134, loss is 0.5115320682525635\n",
      "epoch: 1 step: 135, loss is 0.8224985003471375\n",
      "epoch: 1 step: 136, loss is 0.39380407333374023\n",
      "epoch: 1 step: 137, loss is 0.2791556715965271\n",
      "epoch: 1 step: 138, loss is 0.46848371624946594\n",
      "epoch: 1 step: 139, loss is 0.5503857731819153\n",
      "epoch: 1 step: 140, loss is 0.6432432532310486\n",
      "epoch: 1 step: 141, loss is 0.21796450018882751\n",
      "epoch: 1 step: 142, loss is 0.6745225787162781\n",
      "epoch: 1 step: 143, loss is 0.5339213013648987\n",
      "epoch: 1 step: 144, loss is 0.584515392780304\n",
      "epoch: 1 step: 145, loss is 0.34088072180747986\n",
      "epoch: 1 step: 146, loss is 0.48709508776664734\n",
      "epoch: 1 step: 147, loss is 0.2119722217321396\n",
      "epoch: 1 step: 148, loss is 0.2299865186214447\n",
      "epoch: 1 step: 149, loss is 0.28658798336982727\n",
      "epoch: 1 step: 150, loss is 0.5416690707206726\n",
      "epoch: 1 step: 151, loss is 0.4712958335876465\n",
      "epoch: 1 step: 152, loss is 0.5333128571510315\n",
      "epoch: 1 step: 153, loss is 0.28383252024650574\n",
      "epoch: 1 step: 154, loss is 0.6001711487770081\n",
      "epoch: 1 step: 155, loss is 0.4598442316055298\n",
      "epoch: 1 step: 156, loss is 0.6410384178161621\n",
      "epoch: 1 step: 157, loss is 0.5641520023345947\n",
      "epoch: 1 step: 158, loss is 0.5553198456764221\n",
      "epoch: 1 step: 159, loss is 0.5629441142082214\n",
      "epoch: 1 step: 160, loss is 0.2485392540693283\n",
      "epoch: 1 step: 161, loss is 0.3711053431034088\n",
      "epoch: 1 step: 162, loss is 0.35143306851387024\n",
      "epoch: 1 step: 163, loss is 0.4559025764465332\n",
      "epoch: 1 step: 164, loss is 0.5610764026641846\n",
      "epoch: 1 step: 165, loss is 0.6348872780799866\n",
      "epoch: 1 step: 166, loss is 0.6837005019187927\n",
      "epoch: 1 step: 167, loss is 0.42610564827919006\n",
      "epoch: 1 step: 168, loss is 0.6987156867980957\n",
      "epoch: 1 step: 169, loss is 0.3030933737754822\n",
      "epoch: 1 step: 170, loss is 0.3068540692329407\n",
      "epoch: 1 step: 171, loss is 0.47820547223091125\n",
      "epoch: 1 step: 172, loss is 0.21236954629421234\n",
      "epoch: 1 step: 173, loss is 0.3490406572818756\n",
      "epoch: 1 step: 174, loss is 0.4992113709449768\n",
      "epoch: 1 step: 175, loss is 0.18845784664154053\n",
      "epoch: 1 step: 176, loss is 0.5017621517181396\n",
      "epoch: 1 step: 177, loss is 0.23815038800239563\n",
      "epoch: 1 step: 178, loss is 0.17199929058551788\n",
      "epoch: 1 step: 179, loss is 0.3041631579399109\n",
      "epoch: 1 step: 180, loss is 0.2459709197282791\n",
      "epoch: 1 step: 181, loss is 0.4121359586715698\n",
      "epoch: 1 step: 182, loss is 0.2595936357975006\n",
      "epoch: 1 step: 183, loss is 0.5908839702606201\n",
      "epoch: 1 step: 184, loss is 0.3838328719139099\n",
      "epoch: 1 step: 185, loss is 0.35073474049568176\n",
      "epoch: 1 step: 186, loss is 0.8576313257217407\n",
      "epoch: 1 step: 187, loss is 0.4010274112224579\n",
      "epoch: 1 step: 188, loss is 0.3081238567829132\n",
      "epoch: 1 step: 189, loss is 0.3033381402492523\n",
      "epoch: 1 step: 190, loss is 0.1882927566766739\n",
      "epoch: 1 step: 191, loss is 0.20720644295215607\n",
      "epoch: 1 step: 192, loss is 0.34237098693847656\n",
      "epoch: 1 step: 193, loss is 0.6917860507965088\n",
      "epoch: 1 step: 194, loss is 0.16397587954998016\n",
      "epoch: 1 step: 195, loss is 0.24809543788433075\n",
      "epoch: 1 step: 196, loss is 0.28093066811561584\n",
      "epoch: 1 step: 197, loss is 0.5077140927314758\n",
      "epoch: 1 step: 198, loss is 0.2395823895931244\n",
      "epoch: 1 step: 199, loss is 0.4128381311893463\n",
      "epoch: 1 step: 200, loss is 0.618989884853363\n",
      "epoch: 1 step: 201, loss is 0.2184911072254181\n",
      "epoch: 1 step: 202, loss is 0.32575252652168274\n",
      "epoch: 1 step: 203, loss is 0.6317174434661865\n",
      "epoch: 1 step: 204, loss is 0.4618127644062042\n",
      "epoch: 1 step: 205, loss is 0.08153989166021347\n",
      "epoch: 1 step: 206, loss is 0.3417881429195404\n",
      "epoch: 1 step: 207, loss is 0.9828659296035767\n",
      "epoch: 1 step: 208, loss is 0.294238805770874\n",
      "epoch: 1 step: 209, loss is 0.499510794878006\n",
      "epoch: 1 step: 210, loss is 0.2659354507923126\n",
      "epoch: 1 step: 211, loss is 0.3915490508079529\n",
      "epoch: 1 step: 212, loss is 0.5504657030105591\n",
      "epoch: 1 step: 213, loss is 0.43043777346611023\n",
      "epoch: 1 step: 214, loss is 0.5126013159751892\n",
      "epoch: 1 step: 215, loss is 0.20956362783908844\n",
      "epoch: 1 step: 216, loss is 0.2411932647228241\n",
      "epoch: 1 step: 217, loss is 0.2326129525899887\n",
      "epoch: 1 step: 218, loss is 0.5193835496902466\n",
      "epoch: 1 step: 219, loss is 0.251606822013855\n",
      "epoch: 1 step: 220, loss is 0.32812270522117615\n",
      "epoch: 1 step: 221, loss is 0.37841418385505676\n",
      "epoch: 1 step: 222, loss is 0.35366395115852356\n",
      "epoch: 1 step: 223, loss is 0.32968050241470337\n",
      "epoch: 1 step: 224, loss is 0.509105920791626\n",
      "epoch: 1 step: 225, loss is 0.266359806060791\n",
      "epoch: 1 step: 226, loss is 0.2009977102279663\n",
      "epoch: 1 step: 227, loss is 0.3315992057323456\n",
      "epoch: 1 step: 228, loss is 0.3314293920993805\n",
      "epoch: 1 step: 229, loss is 0.4883885085582733\n",
      "epoch: 1 step: 230, loss is 0.41711336374282837\n",
      "epoch: 1 step: 231, loss is 0.15891030430793762\n",
      "epoch: 1 step: 232, loss is 0.37686705589294434\n",
      "epoch: 1 step: 233, loss is 0.45806610584259033\n",
      "epoch: 1 step: 234, loss is 0.31180185079574585\n",
      "epoch: 1 step: 235, loss is 0.38591310381889343\n",
      "epoch: 1 step: 236, loss is 0.23365230858325958\n",
      "epoch: 1 step: 237, loss is 0.20303083956241608\n",
      "epoch: 1 step: 238, loss is 0.6704831123352051\n",
      "epoch: 1 step: 239, loss is 0.3203129768371582\n",
      "epoch: 1 step: 240, loss is 0.22136330604553223\n",
      "epoch: 1 step: 241, loss is 0.2959470748901367\n",
      "epoch: 1 step: 242, loss is 0.201779305934906\n",
      "epoch: 1 step: 243, loss is 0.1865154653787613\n",
      "epoch: 1 step: 244, loss is 0.16851218044757843\n",
      "epoch: 1 step: 245, loss is 0.32192420959472656\n",
      "epoch: 1 step: 246, loss is 0.19674517214298248\n",
      "epoch: 1 step: 247, loss is 0.262901246547699\n",
      "epoch: 1 step: 248, loss is 0.47606295347213745\n",
      "epoch: 1 step: 249, loss is 0.3315514028072357\n",
      "epoch: 1 step: 250, loss is 0.3813324272632599\n",
      "epoch: 1 step: 251, loss is 0.3057110905647278\n",
      "epoch: 1 step: 252, loss is 0.24000445008277893\n",
      "epoch: 1 step: 253, loss is 0.461175799369812\n",
      "epoch: 1 step: 254, loss is 0.2726042568683624\n",
      "epoch: 1 step: 255, loss is 0.6068434715270996\n",
      "epoch: 1 step: 256, loss is 0.5331230163574219\n",
      "epoch: 1 step: 257, loss is 0.33852121233940125\n",
      "epoch: 1 step: 258, loss is 0.6271265745162964\n",
      "epoch: 1 step: 259, loss is 0.1954127848148346\n",
      "epoch: 1 step: 260, loss is 0.800854504108429\n",
      "epoch: 1 step: 261, loss is 0.3997022807598114\n",
      "epoch: 1 step: 262, loss is 0.3433431088924408\n",
      "epoch: 1 step: 263, loss is 0.2580587565898895\n",
      "epoch: 1 step: 264, loss is 0.2358092963695526\n",
      "epoch: 1 step: 265, loss is 0.4304300844669342\n",
      "epoch: 1 step: 266, loss is 0.1212727352976799\n",
      "epoch: 1 step: 267, loss is 0.2751935124397278\n",
      "epoch: 1 step: 268, loss is 0.7199463248252869\n",
      "epoch: 1 step: 269, loss is 0.4624928832054138\n",
      "epoch: 1 step: 270, loss is 0.5885364413261414\n",
      "epoch: 1 step: 271, loss is 0.4170573949813843\n",
      "epoch: 1 step: 272, loss is 0.29945746064186096\n",
      "epoch: 1 step: 273, loss is 0.18329481780529022\n",
      "epoch: 1 step: 274, loss is 0.24135853350162506\n",
      "epoch: 1 step: 275, loss is 0.9278224110603333\n",
      "epoch: 1 step: 276, loss is 0.8037844896316528\n",
      "epoch: 1 step: 277, loss is 0.47193461656570435\n",
      "epoch: 1 step: 278, loss is 0.38510480523109436\n",
      "epoch: 1 step: 279, loss is 0.3586123287677765\n",
      "epoch: 1 step: 280, loss is 0.4026640057563782\n",
      "epoch: 1 step: 281, loss is 0.17016370594501495\n",
      "epoch: 1 step: 282, loss is 0.8849694728851318\n",
      "epoch: 1 step: 283, loss is 0.5344718098640442\n",
      "epoch: 1 step: 284, loss is 0.2717988193035126\n",
      "epoch: 1 step: 285, loss is 0.5772796869277954\n",
      "epoch: 1 step: 286, loss is 0.42601847648620605\n",
      "epoch: 1 step: 287, loss is 0.3255726099014282\n",
      "epoch: 1 step: 288, loss is 0.6317968964576721\n",
      "epoch: 1 step: 289, loss is 0.43485212326049805\n",
      "epoch: 1 step: 290, loss is 0.545802652835846\n",
      "epoch: 1 step: 291, loss is 0.21092961728572845\n",
      "epoch: 1 step: 292, loss is 0.6821169257164001\n",
      "epoch: 1 step: 293, loss is 0.1774328202009201\n",
      "epoch: 1 step: 294, loss is 0.6511290669441223\n",
      "epoch: 1 step: 295, loss is 0.3299301862716675\n",
      "epoch: 1 step: 296, loss is 0.27466604113578796\n",
      "epoch: 1 step: 297, loss is 0.260576993227005\n",
      "epoch: 1 step: 298, loss is 0.1956065595149994\n",
      "epoch: 1 step: 299, loss is 1.0728839635849\n",
      "epoch: 1 step: 300, loss is 0.44586628675460815\n",
      "epoch: 1 step: 301, loss is 0.5174903869628906\n",
      "epoch: 1 step: 302, loss is 0.27525025606155396\n",
      "epoch: 1 step: 303, loss is 0.2203848659992218\n",
      "epoch: 1 step: 304, loss is 0.4837888777256012\n",
      "epoch: 1 step: 305, loss is 0.26659920811653137\n",
      "epoch: 1 step: 306, loss is 0.43382054567337036\n",
      "epoch: 1 step: 307, loss is 0.3169609010219574\n",
      "epoch: 1 step: 308, loss is 0.622487485408783\n",
      "epoch: 1 step: 309, loss is 0.37259531021118164\n",
      "epoch: 1 step: 310, loss is 0.24917171895503998\n",
      "epoch: 1 step: 311, loss is 0.31215864419937134\n",
      "epoch: 1 step: 312, loss is 0.167557030916214\n",
      "epoch: 1 step: 313, loss is 0.16153189539909363\n",
      "epoch: 1 step: 314, loss is 0.291461706161499\n",
      "epoch: 1 step: 315, loss is 0.1772594451904297\n",
      "epoch: 1 step: 316, loss is 0.1687026470899582\n",
      "epoch: 1 step: 317, loss is 0.31666532158851624\n",
      "epoch: 1 step: 318, loss is 0.10608992725610733\n",
      "epoch: 1 step: 319, loss is 0.4062104821205139\n",
      "epoch: 1 step: 320, loss is 0.2899244725704193\n",
      "epoch: 1 step: 321, loss is 0.29011958837509155\n",
      "epoch: 1 step: 322, loss is 0.15336006879806519\n",
      "epoch: 1 step: 323, loss is 0.3495483994483948\n",
      "epoch: 1 step: 324, loss is 0.6039014458656311\n",
      "epoch: 1 step: 325, loss is 0.7332832217216492\n",
      "epoch: 1 step: 326, loss is 0.3222399950027466\n",
      "epoch: 1 step: 327, loss is 0.3707265257835388\n",
      "epoch: 1 step: 328, loss is 0.6994792819023132\n",
      "epoch: 1 step: 329, loss is 0.09627184271812439\n",
      "epoch: 1 step: 330, loss is 0.1817294955253601\n",
      "epoch: 1 step: 331, loss is 0.24059785902500153\n",
      "epoch: 1 step: 332, loss is 0.2128613293170929\n",
      "epoch: 1 step: 333, loss is 0.18756136298179626\n",
      "epoch: 1 step: 334, loss is 0.22457273304462433\n",
      "epoch: 1 step: 335, loss is 0.3008338510990143\n",
      "epoch: 1 step: 336, loss is 0.2792419195175171\n",
      "epoch: 1 step: 337, loss is 0.1588289439678192\n",
      "epoch: 1 step: 338, loss is 0.2230498194694519\n",
      "epoch: 1 step: 339, loss is 0.1595659703016281\n",
      "epoch: 1 step: 340, loss is 0.14850106835365295\n",
      "epoch: 1 step: 341, loss is 0.28387972712516785\n",
      "epoch: 1 step: 342, loss is 0.16828398406505585\n",
      "epoch: 1 step: 343, loss is 0.2943137586116791\n",
      "epoch: 1 step: 344, loss is 0.44700103998184204\n",
      "epoch: 1 step: 345, loss is 0.19408977031707764\n",
      "epoch: 1 step: 346, loss is 0.32788175344467163\n",
      "epoch: 1 step: 347, loss is 0.2888656556606293\n",
      "epoch: 1 step: 348, loss is 0.2355121225118637\n",
      "epoch: 1 step: 349, loss is 0.4169275462627411\n",
      "epoch: 1 step: 350, loss is 0.1451965719461441\n",
      "epoch: 1 step: 351, loss is 0.4235564172267914\n",
      "epoch: 1 step: 352, loss is 0.38329780101776123\n",
      "epoch: 1 step: 353, loss is 0.1216350719332695\n",
      "epoch: 1 step: 354, loss is 0.3659471571445465\n",
      "epoch: 1 step: 355, loss is 0.13282690942287445\n",
      "epoch: 1 step: 356, loss is 0.07309725135564804\n",
      "epoch: 1 step: 357, loss is 0.34696295857429504\n",
      "epoch: 1 step: 358, loss is 0.24352240562438965\n",
      "epoch: 1 step: 359, loss is 0.32549789547920227\n",
      "epoch: 1 step: 360, loss is 0.3023064434528351\n",
      "epoch: 1 step: 361, loss is 0.2891048789024353\n",
      "epoch: 1 step: 362, loss is 0.2730525732040405\n",
      "epoch: 1 step: 363, loss is 0.22325360774993896\n",
      "epoch: 1 step: 364, loss is 0.373287171125412\n",
      "epoch: 1 step: 365, loss is 0.6757580637931824\n",
      "epoch: 1 step: 366, loss is 0.48554909229278564\n",
      "epoch: 1 step: 367, loss is 0.28266602754592896\n",
      "epoch: 1 step: 368, loss is 0.20820581912994385\n",
      "epoch: 1 step: 369, loss is 0.2042594999074936\n",
      "epoch: 1 step: 370, loss is 0.35706987977027893\n",
      "epoch: 1 step: 371, loss is 0.1471526175737381\n",
      "epoch: 1 step: 372, loss is 0.14991284906864166\n",
      "epoch: 1 step: 373, loss is 0.26357823610305786\n",
      "epoch: 1 step: 374, loss is 0.4314068853855133\n",
      "epoch: 1 step: 375, loss is 0.2964434325695038\n",
      "epoch: 1 step: 376, loss is 0.5283833742141724\n",
      "epoch: 1 step: 377, loss is 0.5357528924942017\n",
      "epoch: 1 step: 378, loss is 0.32965677976608276\n",
      "epoch: 1 step: 379, loss is 0.2972252368927002\n",
      "epoch: 1 step: 380, loss is 0.1704319417476654\n",
      "epoch: 1 step: 381, loss is 0.41481396555900574\n",
      "epoch: 1 step: 382, loss is 0.25539878010749817\n",
      "epoch: 1 step: 383, loss is 0.22984693944454193\n",
      "epoch: 1 step: 384, loss is 0.2793933153152466\n",
      "epoch: 1 step: 385, loss is 0.4238210618495941\n",
      "epoch: 1 step: 386, loss is 0.424215167760849\n",
      "epoch: 1 step: 387, loss is 0.42876508831977844\n",
      "epoch: 1 step: 388, loss is 0.2629978656768799\n",
      "epoch: 1 step: 389, loss is 0.3433096408843994\n",
      "epoch: 1 step: 390, loss is 0.5523409247398376\n",
      "epoch: 1 step: 391, loss is 0.31483030319213867\n",
      "epoch: 1 step: 392, loss is 0.16215918958187103\n",
      "epoch: 1 step: 393, loss is 0.3147450089454651\n",
      "epoch: 1 step: 394, loss is 0.2061239331960678\n",
      "epoch: 1 step: 395, loss is 0.18242523074150085\n",
      "epoch: 1 step: 396, loss is 0.26005133986473083\n",
      "epoch: 1 step: 397, loss is 0.6428454518318176\n",
      "epoch: 1 step: 398, loss is 0.4078516960144043\n",
      "epoch: 1 step: 399, loss is 0.10017133504152298\n",
      "epoch: 1 step: 400, loss is 0.31360888481140137\n",
      "epoch: 1 step: 401, loss is 0.5878106355667114\n",
      "epoch: 1 step: 402, loss is 0.20608748495578766\n",
      "epoch: 1 step: 403, loss is 0.3785291910171509\n",
      "epoch: 1 step: 404, loss is 0.41023629903793335\n",
      "epoch: 1 step: 405, loss is 0.416041761636734\n",
      "epoch: 1 step: 406, loss is 0.37418463826179504\n",
      "epoch: 1 step: 407, loss is 0.3696487545967102\n",
      "epoch: 1 step: 408, loss is 0.05780332162976265\n",
      "epoch: 1 step: 409, loss is 0.22888383269309998\n",
      "epoch: 1 step: 410, loss is 0.2310762107372284\n",
      "epoch: 1 step: 411, loss is 0.4338226020336151\n",
      "epoch: 1 step: 412, loss is 0.07901234179735184\n",
      "epoch: 1 step: 413, loss is 0.8977787494659424\n",
      "epoch: 1 step: 414, loss is 0.2948356866836548\n",
      "epoch: 1 step: 415, loss is 0.1715146005153656\n",
      "epoch: 1 step: 416, loss is 0.1571437418460846\n",
      "epoch: 1 step: 417, loss is 0.09135023504495621\n",
      "epoch: 1 step: 418, loss is 0.5581262111663818\n",
      "epoch: 1 step: 419, loss is 0.3324454128742218\n",
      "epoch: 1 step: 420, loss is 0.24890202283859253\n",
      "epoch: 1 step: 421, loss is 0.12030989676713943\n",
      "epoch: 1 step: 422, loss is 0.2823207676410675\n",
      "epoch: 1 step: 423, loss is 0.4492952227592468\n",
      "epoch: 1 step: 424, loss is 0.8629005551338196\n",
      "epoch: 1 step: 425, loss is 0.3929827809333801\n",
      "epoch: 1 step: 426, loss is 0.19011758267879486\n",
      "epoch: 1 step: 427, loss is 0.46499472856521606\n",
      "epoch: 1 step: 428, loss is 0.18589851260185242\n",
      "epoch: 1 step: 429, loss is 0.24959620833396912\n",
      "epoch: 1 step: 430, loss is 0.5736854076385498\n",
      "epoch: 1 step: 431, loss is 0.3926694393157959\n",
      "epoch: 1 step: 432, loss is 0.1731475591659546\n",
      "epoch: 1 step: 433, loss is 0.5705068111419678\n",
      "epoch: 1 step: 434, loss is 0.19922250509262085\n",
      "epoch: 1 step: 435, loss is 0.2706080973148346\n",
      "epoch: 1 step: 436, loss is 0.32993772625923157\n",
      "epoch: 1 step: 437, loss is 0.21168673038482666\n",
      "epoch: 1 step: 438, loss is 0.4040047526359558\n",
      "epoch: 1 step: 439, loss is 0.2773585617542267\n",
      "epoch: 1 step: 440, loss is 0.3182888925075531\n",
      "epoch: 1 step: 441, loss is 0.39871829748153687\n",
      "epoch: 1 step: 442, loss is 0.29834046959877014\n",
      "epoch: 1 step: 443, loss is 0.16741560399532318\n",
      "epoch: 1 step: 444, loss is 0.35736945271492004\n",
      "epoch: 1 step: 445, loss is 0.12692230939865112\n",
      "epoch: 1 step: 446, loss is 0.12287731468677521\n",
      "epoch: 1 step: 447, loss is 0.2853647470474243\n",
      "epoch: 1 step: 448, loss is 0.15286657214164734\n",
      "epoch: 1 step: 449, loss is 0.14344635605812073\n",
      "epoch: 1 step: 450, loss is 0.15768949687480927\n",
      "epoch: 1 step: 451, loss is 0.20659326016902924\n",
      "epoch: 1 step: 452, loss is 0.314654678106308\n",
      "epoch: 1 step: 453, loss is 0.2972237467765808\n",
      "epoch: 1 step: 454, loss is 0.17868581414222717\n",
      "epoch: 1 step: 455, loss is 0.2625353932380676\n",
      "epoch: 1 step: 456, loss is 0.06606671214103699\n",
      "epoch: 1 step: 457, loss is 0.10665132105350494\n",
      "epoch: 1 step: 458, loss is 0.1030973345041275\n",
      "epoch: 1 step: 459, loss is 0.3512914776802063\n",
      "epoch: 1 step: 460, loss is 0.12406174838542938\n",
      "epoch: 1 step: 461, loss is 0.4434780180454254\n",
      "epoch: 1 step: 462, loss is 0.18444038927555084\n",
      "epoch: 1 step: 463, loss is 0.12865152955055237\n",
      "epoch: 1 step: 464, loss is 0.26173773407936096\n",
      "epoch: 1 step: 465, loss is 0.09877213835716248\n",
      "epoch: 1 step: 466, loss is 0.14172197878360748\n",
      "epoch: 1 step: 467, loss is 0.06836089491844177\n",
      "epoch: 1 step: 468, loss is 0.38709840178489685\n",
      "epoch: 1 step: 469, loss is 0.03183500096201897\n",
      "epoch: 1 step: 470, loss is 0.39445018768310547\n",
      "epoch: 1 step: 471, loss is 0.19519776105880737\n",
      "epoch: 1 step: 472, loss is 0.15207044780254364\n",
      "epoch: 1 step: 473, loss is 0.12596867978572845\n",
      "epoch: 1 step: 474, loss is 0.04699302464723587\n",
      "epoch: 1 step: 475, loss is 0.10027530789375305\n",
      "epoch: 1 step: 476, loss is 0.17554906010627747\n",
      "epoch: 1 step: 477, loss is 0.25028321146965027\n",
      "epoch: 1 step: 478, loss is 0.057919107377529144\n",
      "epoch: 1 step: 479, loss is 0.2972489893436432\n",
      "epoch: 1 step: 480, loss is 0.6819778680801392\n",
      "epoch: 1 step: 481, loss is 0.1148240715265274\n",
      "epoch: 1 step: 482, loss is 0.1902984231710434\n",
      "epoch: 1 step: 483, loss is 0.1576090157032013\n",
      "epoch: 1 step: 484, loss is 0.3706080913543701\n",
      "epoch: 1 step: 485, loss is 0.12352508306503296\n",
      "epoch: 1 step: 486, loss is 0.36596548557281494\n",
      "epoch: 1 step: 487, loss is 0.4056234657764435\n",
      "epoch: 1 step: 488, loss is 0.1951877325773239\n",
      "epoch: 1 step: 489, loss is 0.5171135067939758\n",
      "epoch: 1 step: 490, loss is 0.15194623172283173\n",
      "epoch: 1 step: 491, loss is 0.12166091799736023\n",
      "epoch: 1 step: 492, loss is 0.4652918875217438\n",
      "epoch: 1 step: 493, loss is 0.33276113867759705\n",
      "epoch: 1 step: 494, loss is 0.3687454164028168\n",
      "epoch: 1 step: 495, loss is 0.16625317931175232\n",
      "epoch: 1 step: 496, loss is 0.30316340923309326\n",
      "epoch: 1 step: 497, loss is 0.2667059302330017\n",
      "epoch: 1 step: 498, loss is 0.27116280794143677\n",
      "epoch: 1 step: 499, loss is 0.2340012937784195\n",
      "epoch: 1 step: 500, loss is 0.24431659281253815\n",
      "epoch: 1 step: 501, loss is 0.11783707141876221\n",
      "epoch: 1 step: 502, loss is 0.4184309244155884\n",
      "epoch: 1 step: 503, loss is 0.38218146562576294\n",
      "epoch: 1 step: 504, loss is 0.18262256681919098\n",
      "epoch: 1 step: 505, loss is 0.086296945810318\n",
      "epoch: 1 step: 506, loss is 0.15960736572742462\n",
      "epoch: 1 step: 507, loss is 0.12339118868112564\n",
      "epoch: 1 step: 508, loss is 0.1475251168012619\n",
      "epoch: 1 step: 509, loss is 0.15345795452594757\n",
      "epoch: 1 step: 510, loss is 0.25216278433799744\n",
      "epoch: 1 step: 511, loss is 0.3484044373035431\n",
      "epoch: 1 step: 512, loss is 0.21751472353935242\n",
      "epoch: 1 step: 513, loss is 0.10953457653522491\n",
      "epoch: 1 step: 514, loss is 0.19532717764377594\n",
      "epoch: 1 step: 515, loss is 0.30629876255989075\n",
      "epoch: 1 step: 516, loss is 0.2624628245830536\n",
      "epoch: 1 step: 517, loss is 0.1210184097290039\n",
      "epoch: 1 step: 518, loss is 0.013902470469474792\n",
      "epoch: 1 step: 519, loss is 0.19822604954242706\n",
      "epoch: 1 step: 520, loss is 0.3448401093482971\n",
      "epoch: 1 step: 521, loss is 0.12988542020320892\n",
      "epoch: 1 step: 522, loss is 0.11235961318016052\n",
      "epoch: 1 step: 523, loss is 0.3755907118320465\n",
      "epoch: 1 step: 524, loss is 0.2298864722251892\n",
      "epoch: 1 step: 525, loss is 0.15113945305347443\n",
      "epoch: 1 step: 526, loss is 0.3406807780265808\n",
      "epoch: 1 step: 527, loss is 0.15832142531871796\n",
      "epoch: 1 step: 528, loss is 0.363353967666626\n",
      "epoch: 1 step: 529, loss is 0.3366445004940033\n",
      "epoch: 1 step: 530, loss is 0.42477378249168396\n",
      "epoch: 1 step: 531, loss is 0.4094291925430298\n",
      "epoch: 1 step: 532, loss is 0.26471298933029175\n",
      "epoch: 1 step: 533, loss is 0.16999535262584686\n",
      "epoch: 1 step: 534, loss is 0.10381856560707092\n",
      "epoch: 1 step: 535, loss is 0.06205514073371887\n",
      "epoch: 1 step: 536, loss is 0.4906439483165741\n",
      "epoch: 1 step: 537, loss is 0.04655589535832405\n",
      "epoch: 1 step: 538, loss is 0.3092876076698303\n",
      "epoch: 1 step: 539, loss is 0.7408057451248169\n",
      "epoch: 1 step: 540, loss is 0.3162602484226227\n",
      "epoch: 1 step: 541, loss is 0.33940693736076355\n",
      "epoch: 1 step: 542, loss is 0.10133061558008194\n",
      "epoch: 1 step: 543, loss is 0.22614741325378418\n",
      "epoch: 1 step: 544, loss is 0.3471791744232178\n",
      "epoch: 1 step: 545, loss is 0.1281822919845581\n",
      "epoch: 1 step: 546, loss is 0.34900960326194763\n",
      "epoch: 1 step: 547, loss is 0.6026095747947693\n",
      "epoch: 1 step: 548, loss is 0.27374741435050964\n",
      "epoch: 1 step: 549, loss is 0.28278300166130066\n",
      "epoch: 1 step: 550, loss is 0.07936889678239822\n",
      "epoch: 1 step: 551, loss is 0.4548048675060272\n",
      "epoch: 1 step: 552, loss is 0.36516523361206055\n",
      "epoch: 1 step: 553, loss is 0.6814256906509399\n",
      "epoch: 1 step: 554, loss is 0.10083065181970596\n",
      "epoch: 1 step: 555, loss is 0.2219143509864807\n",
      "epoch: 1 step: 556, loss is 0.14053364098072052\n",
      "epoch: 1 step: 557, loss is 0.43950018286705017\n",
      "epoch: 1 step: 558, loss is 0.28562960028648376\n",
      "epoch: 1 step: 559, loss is 0.2526814937591553\n",
      "epoch: 1 step: 560, loss is 0.6609830856323242\n",
      "epoch: 1 step: 561, loss is 0.10184282064437866\n",
      "epoch: 1 step: 562, loss is 0.20227544009685516\n",
      "epoch: 1 step: 563, loss is 0.15216436982154846\n",
      "epoch: 1 step: 564, loss is 0.2296629101037979\n",
      "epoch: 1 step: 565, loss is 0.2581464946269989\n",
      "epoch: 1 step: 566, loss is 0.35326334834098816\n",
      "epoch: 1 step: 567, loss is 0.3926282823085785\n",
      "epoch: 1 step: 568, loss is 0.3482785224914551\n",
      "epoch: 1 step: 569, loss is 0.21347802877426147\n",
      "epoch: 1 step: 570, loss is 0.06036467105150223\n",
      "epoch: 1 step: 571, loss is 0.1815449744462967\n",
      "epoch: 1 step: 572, loss is 0.20100137591362\n",
      "epoch: 1 step: 573, loss is 0.18326643109321594\n",
      "epoch: 1 step: 574, loss is 0.4793640673160553\n",
      "epoch: 1 step: 575, loss is 0.4043739438056946\n",
      "epoch: 1 step: 576, loss is 0.15595099329948425\n",
      "epoch: 1 step: 577, loss is 0.07541009783744812\n",
      "epoch: 1 step: 578, loss is 0.19668127596378326\n",
      "epoch: 1 step: 579, loss is 0.1999416947364807\n",
      "epoch: 1 step: 580, loss is 0.1724364012479782\n",
      "epoch: 1 step: 581, loss is 0.10301516205072403\n",
      "epoch: 1 step: 582, loss is 0.47430533170700073\n",
      "epoch: 1 step: 583, loss is 0.31828954815864563\n",
      "epoch: 1 step: 584, loss is 0.14019478857517242\n",
      "epoch: 1 step: 585, loss is 0.25519514083862305\n",
      "epoch: 1 step: 586, loss is 0.33111611008644104\n",
      "epoch: 1 step: 587, loss is 0.1295180320739746\n",
      "epoch: 1 step: 588, loss is 0.1325083076953888\n",
      "epoch: 1 step: 589, loss is 0.07093734294176102\n",
      "epoch: 1 step: 590, loss is 0.42629942297935486\n",
      "epoch: 1 step: 591, loss is 0.0824057012796402\n",
      "epoch: 1 step: 592, loss is 0.38207656145095825\n",
      "epoch: 1 step: 593, loss is 0.07200281322002411\n",
      "epoch: 1 step: 594, loss is 0.10782100260257721\n",
      "epoch: 1 step: 595, loss is 0.19480404257774353\n",
      "epoch: 1 step: 596, loss is 0.1518840342760086\n",
      "epoch: 1 step: 597, loss is 0.1917308270931244\n",
      "epoch: 1 step: 598, loss is 0.34018200635910034\n",
      "epoch: 1 step: 599, loss is 0.31497395038604736\n",
      "epoch: 1 step: 600, loss is 0.10534670203924179\n",
      "epoch: 1 step: 601, loss is 0.17307226359844208\n",
      "epoch: 1 step: 602, loss is 0.09668565541505814\n",
      "epoch: 1 step: 603, loss is 0.5125181674957275\n",
      "epoch: 1 step: 604, loss is 0.25390270352363586\n",
      "epoch: 1 step: 605, loss is 0.20150399208068848\n",
      "epoch: 1 step: 606, loss is 0.2183358073234558\n",
      "epoch: 1 step: 607, loss is 0.1618385761976242\n",
      "epoch: 1 step: 608, loss is 0.12005829066038132\n",
      "epoch: 1 step: 609, loss is 0.14139649271965027\n",
      "epoch: 1 step: 610, loss is 0.12368399649858475\n",
      "epoch: 1 step: 611, loss is 0.1997033655643463\n",
      "epoch: 1 step: 612, loss is 0.15140974521636963\n",
      "epoch: 1 step: 613, loss is 0.1312810331583023\n",
      "epoch: 1 step: 614, loss is 0.032082971185445786\n",
      "epoch: 1 step: 615, loss is 0.22717511653900146\n",
      "epoch: 1 step: 616, loss is 0.04122443497180939\n",
      "epoch: 1 step: 617, loss is 0.5025943517684937\n",
      "epoch: 1 step: 618, loss is 0.4387432932853699\n",
      "epoch: 1 step: 619, loss is 0.25665509700775146\n",
      "epoch: 1 step: 620, loss is 0.1239379420876503\n",
      "epoch: 1 step: 621, loss is 0.07389622926712036\n",
      "epoch: 1 step: 622, loss is 0.5216661691665649\n",
      "epoch: 1 step: 623, loss is 0.23873209953308105\n",
      "epoch: 1 step: 624, loss is 0.20541450381278992\n",
      "epoch: 1 step: 625, loss is 0.06161441281437874\n",
      "epoch: 1 step: 626, loss is 0.18381844460964203\n",
      "epoch: 1 step: 627, loss is 0.5985303521156311\n",
      "epoch: 1 step: 628, loss is 0.28500935435295105\n",
      "epoch: 1 step: 629, loss is 0.28901156783103943\n",
      "epoch: 1 step: 630, loss is 0.2500210702419281\n",
      "epoch: 1 step: 631, loss is 0.017218122258782387\n",
      "epoch: 1 step: 632, loss is 0.26900750398635864\n",
      "epoch: 1 step: 633, loss is 0.27790188789367676\n",
      "epoch: 1 step: 634, loss is 0.05360756441950798\n",
      "epoch: 1 step: 635, loss is 0.07381705939769745\n",
      "epoch: 1 step: 636, loss is 0.08500098437070847\n",
      "epoch: 1 step: 637, loss is 0.05121912062168121\n",
      "epoch: 1 step: 638, loss is 0.2613977789878845\n",
      "epoch: 1 step: 639, loss is 0.0642915740609169\n",
      "epoch: 1 step: 640, loss is 0.34103795886039734\n",
      "epoch: 1 step: 641, loss is 0.10272110998630524\n",
      "epoch: 1 step: 642, loss is 0.12934193015098572\n",
      "epoch: 1 step: 643, loss is 0.3628218173980713\n",
      "epoch: 1 step: 644, loss is 0.1481291651725769\n",
      "epoch: 1 step: 645, loss is 0.13028372824192047\n",
      "epoch: 1 step: 646, loss is 0.5104518532752991\n",
      "epoch: 1 step: 647, loss is 0.31430092453956604\n",
      "epoch: 1 step: 648, loss is 0.16983559727668762\n",
      "epoch: 1 step: 649, loss is 0.4577251374721527\n",
      "epoch: 1 step: 650, loss is 0.09454356878995895\n",
      "epoch: 1 step: 651, loss is 0.17648804187774658\n",
      "epoch: 1 step: 652, loss is 0.4166858494281769\n",
      "epoch: 1 step: 653, loss is 0.30916303396224976\n",
      "epoch: 1 step: 654, loss is 0.22460003197193146\n",
      "epoch: 1 step: 655, loss is 0.2935147285461426\n",
      "epoch: 1 step: 656, loss is 0.2067103385925293\n",
      "epoch: 1 step: 657, loss is 0.20636028051376343\n",
      "epoch: 1 step: 658, loss is 0.24552181363105774\n",
      "epoch: 1 step: 659, loss is 0.2730289399623871\n",
      "epoch: 1 step: 660, loss is 0.2197386771440506\n",
      "epoch: 1 step: 661, loss is 0.25224143266677856\n",
      "epoch: 1 step: 662, loss is 0.5203732252120972\n",
      "epoch: 1 step: 663, loss is 0.0701921358704567\n",
      "epoch: 1 step: 664, loss is 0.18149296939373016\n",
      "epoch: 1 step: 665, loss is 0.28973791003227234\n",
      "epoch: 1 step: 666, loss is 0.0638648197054863\n",
      "epoch: 1 step: 667, loss is 0.1749536693096161\n",
      "epoch: 1 step: 668, loss is 0.20914897322654724\n",
      "epoch: 1 step: 669, loss is 0.20984625816345215\n",
      "epoch: 1 step: 670, loss is 0.05136175826191902\n",
      "epoch: 1 step: 671, loss is 0.26709750294685364\n",
      "epoch: 1 step: 672, loss is 0.06846679002046585\n",
      "epoch: 1 step: 673, loss is 0.15638835728168488\n",
      "epoch: 1 step: 674, loss is 0.0888075977563858\n",
      "epoch: 1 step: 675, loss is 0.48227280378341675\n",
      "epoch: 1 step: 676, loss is 0.3181363344192505\n",
      "epoch: 1 step: 677, loss is 0.32441285252571106\n",
      "epoch: 1 step: 678, loss is 0.32675591111183167\n",
      "epoch: 1 step: 679, loss is 0.14752931892871857\n",
      "epoch: 1 step: 680, loss is 0.13740375638008118\n",
      "epoch: 1 step: 681, loss is 0.4527910649776459\n",
      "epoch: 1 step: 682, loss is 0.24146908521652222\n",
      "epoch: 1 step: 683, loss is 0.09628890454769135\n",
      "epoch: 1 step: 684, loss is 0.10001364350318909\n",
      "epoch: 1 step: 685, loss is 0.090247243642807\n",
      "epoch: 1 step: 686, loss is 0.058443307876586914\n",
      "epoch: 1 step: 687, loss is 0.2113467901945114\n",
      "epoch: 1 step: 688, loss is 0.11331045627593994\n",
      "epoch: 1 step: 689, loss is 0.12184582650661469\n",
      "epoch: 1 step: 690, loss is 0.27238383889198303\n",
      "epoch: 1 step: 691, loss is 0.2276255190372467\n",
      "epoch: 1 step: 692, loss is 0.15298300981521606\n",
      "epoch: 1 step: 693, loss is 0.26370540261268616\n",
      "epoch: 1 step: 694, loss is 0.296743780374527\n",
      "epoch: 1 step: 695, loss is 0.17341363430023193\n",
      "epoch: 1 step: 696, loss is 0.13457784056663513\n",
      "epoch: 1 step: 697, loss is 0.28859156370162964\n",
      "epoch: 1 step: 698, loss is 0.18257802724838257\n",
      "epoch: 1 step: 699, loss is 0.27502283453941345\n",
      "epoch: 1 step: 700, loss is 0.01237795501947403\n",
      "epoch: 1 step: 701, loss is 0.2647385001182556\n",
      "epoch: 1 step: 702, loss is 0.11036686599254608\n",
      "epoch: 1 step: 703, loss is 0.2126118242740631\n",
      "epoch: 1 step: 704, loss is 0.10338886082172394\n",
      "epoch: 1 step: 705, loss is 0.06354226917028427\n",
      "epoch: 1 step: 706, loss is 0.3313138782978058\n",
      "epoch: 1 step: 707, loss is 0.15254700183868408\n",
      "epoch: 1 step: 708, loss is 0.19190466403961182\n",
      "epoch: 1 step: 709, loss is 0.15210823714733124\n",
      "epoch: 1 step: 710, loss is 0.11111655831336975\n",
      "epoch: 1 step: 711, loss is 0.12253184616565704\n",
      "epoch: 1 step: 712, loss is 0.17498812079429626\n",
      "epoch: 1 step: 713, loss is 0.21373702585697174\n",
      "epoch: 1 step: 714, loss is 0.2706111967563629\n",
      "epoch: 1 step: 715, loss is 0.225894957780838\n",
      "epoch: 1 step: 716, loss is 0.26078394055366516\n",
      "epoch: 1 step: 717, loss is 0.11629266291856766\n",
      "epoch: 1 step: 718, loss is 0.415431946516037\n",
      "epoch: 1 step: 719, loss is 0.032711852341890335\n",
      "epoch: 1 step: 720, loss is 0.1720155030488968\n",
      "epoch: 1 step: 721, loss is 0.1859886795282364\n",
      "epoch: 1 step: 722, loss is 0.4798327088356018\n",
      "epoch: 1 step: 723, loss is 0.37737730145454407\n",
      "epoch: 1 step: 724, loss is 0.14941108226776123\n",
      "epoch: 1 step: 725, loss is 0.42405471205711365\n",
      "epoch: 1 step: 726, loss is 0.4563649594783783\n",
      "epoch: 1 step: 727, loss is 0.2982642352581024\n",
      "epoch: 1 step: 728, loss is 0.46638911962509155\n",
      "epoch: 1 step: 729, loss is 0.18321987986564636\n",
      "epoch: 1 step: 730, loss is 0.1284097284078598\n",
      "epoch: 1 step: 731, loss is 0.07705603539943695\n",
      "epoch: 1 step: 732, loss is 0.1844213604927063\n",
      "epoch: 1 step: 733, loss is 0.42654290795326233\n",
      "epoch: 1 step: 734, loss is 0.058980200439691544\n",
      "epoch: 1 step: 735, loss is 0.1088080182671547\n",
      "epoch: 1 step: 736, loss is 0.07436735183000565\n",
      "epoch: 1 step: 737, loss is 0.3124755024909973\n",
      "epoch: 1 step: 738, loss is 0.5457102656364441\n",
      "epoch: 1 step: 739, loss is 0.1019153967499733\n",
      "epoch: 1 step: 740, loss is 0.4281822144985199\n",
      "epoch: 1 step: 741, loss is 0.22461913526058197\n",
      "epoch: 1 step: 742, loss is 0.22063544392585754\n",
      "epoch: 1 step: 743, loss is 0.3128701150417328\n",
      "epoch: 1 step: 744, loss is 0.2746873199939728\n",
      "epoch: 1 step: 745, loss is 0.13118743896484375\n",
      "epoch: 1 step: 746, loss is 0.1625356525182724\n",
      "epoch: 1 step: 747, loss is 0.35447004437446594\n",
      "epoch: 1 step: 748, loss is 0.22269019484519958\n",
      "epoch: 1 step: 749, loss is 0.09164653718471527\n",
      "epoch: 1 step: 750, loss is 0.0951930359005928\n",
      "epoch: 1 step: 751, loss is 0.10187719017267227\n",
      "epoch: 1 step: 752, loss is 0.2907949388027191\n",
      "epoch: 1 step: 753, loss is 0.15954619646072388\n",
      "epoch: 1 step: 754, loss is 0.05022628232836723\n",
      "epoch: 1 step: 755, loss is 0.19626453518867493\n",
      "epoch: 1 step: 756, loss is 0.09050296247005463\n",
      "epoch: 1 step: 757, loss is 0.268314391374588\n",
      "epoch: 1 step: 758, loss is 0.0215278472751379\n",
      "epoch: 1 step: 759, loss is 0.4052668809890747\n",
      "epoch: 1 step: 760, loss is 0.18457189202308655\n",
      "epoch: 1 step: 761, loss is 0.20219899713993073\n",
      "epoch: 1 step: 762, loss is 0.2435520440340042\n",
      "epoch: 1 step: 763, loss is 0.052888818085193634\n",
      "epoch: 1 step: 764, loss is 0.3162270188331604\n",
      "epoch: 1 step: 765, loss is 0.17622342705726624\n",
      "epoch: 1 step: 766, loss is 0.17085163295269012\n",
      "epoch: 1 step: 767, loss is 0.26497042179107666\n",
      "epoch: 1 step: 768, loss is 0.5346845388412476\n",
      "epoch: 1 step: 769, loss is 0.12098589539527893\n",
      "epoch: 1 step: 770, loss is 0.328687846660614\n",
      "epoch: 1 step: 771, loss is 0.17066466808319092\n",
      "epoch: 1 step: 772, loss is 0.30693697929382324\n",
      "epoch: 1 step: 773, loss is 0.12761826813220978\n",
      "epoch: 1 step: 774, loss is 0.13759426772594452\n",
      "epoch: 1 step: 775, loss is 0.1268415004014969\n",
      "epoch: 1 step: 776, loss is 0.09798141568899155\n",
      "epoch: 1 step: 777, loss is 0.11697197705507278\n",
      "epoch: 1 step: 778, loss is 0.2205415666103363\n",
      "epoch: 1 step: 779, loss is 0.18476061522960663\n",
      "epoch: 1 step: 780, loss is 0.18063855171203613\n",
      "epoch: 1 step: 781, loss is 0.27789607644081116\n",
      "epoch: 1 step: 782, loss is 0.03989194706082344\n",
      "epoch: 1 step: 783, loss is 0.08806625008583069\n",
      "epoch: 1 step: 784, loss is 0.17165625095367432\n",
      "epoch: 1 step: 785, loss is 0.06962338835000992\n",
      "epoch: 1 step: 786, loss is 0.331237256526947\n",
      "epoch: 1 step: 787, loss is 0.3172200322151184\n",
      "epoch: 1 step: 788, loss is 0.1830383539199829\n",
      "epoch: 1 step: 789, loss is 0.3725338876247406\n",
      "epoch: 1 step: 790, loss is 0.10271165519952774\n",
      "epoch: 1 step: 791, loss is 0.22026820480823517\n",
      "epoch: 1 step: 792, loss is 0.306210458278656\n",
      "epoch: 1 step: 793, loss is 0.12374362349510193\n",
      "epoch: 1 step: 794, loss is 0.1879081130027771\n",
      "epoch: 1 step: 795, loss is 0.14538660645484924\n",
      "epoch: 1 step: 796, loss is 0.19915801286697388\n",
      "epoch: 1 step: 797, loss is 0.13577020168304443\n",
      "epoch: 1 step: 798, loss is 0.17619650065898895\n",
      "epoch: 1 step: 799, loss is 0.16265805065631866\n",
      "epoch: 1 step: 800, loss is 0.05100889131426811\n",
      "epoch: 1 step: 801, loss is 0.07896025478839874\n",
      "epoch: 1 step: 802, loss is 0.13196389377117157\n",
      "epoch: 1 step: 803, loss is 0.33466798067092896\n",
      "epoch: 1 step: 804, loss is 0.05667656287550926\n",
      "epoch: 1 step: 805, loss is 0.18667475879192352\n",
      "epoch: 1 step: 806, loss is 0.08983021974563599\n",
      "epoch: 1 step: 807, loss is 0.13690218329429626\n",
      "epoch: 1 step: 808, loss is 0.1556433141231537\n",
      "epoch: 1 step: 809, loss is 0.025733329355716705\n",
      "epoch: 1 step: 810, loss is 0.12246871739625931\n",
      "epoch: 1 step: 811, loss is 0.10335856676101685\n",
      "epoch: 1 step: 812, loss is 0.2752200961112976\n",
      "epoch: 1 step: 813, loss is 0.22330516576766968\n",
      "epoch: 1 step: 814, loss is 0.16402356326580048\n",
      "epoch: 1 step: 815, loss is 0.06594374775886536\n",
      "epoch: 1 step: 816, loss is 0.32743826508522034\n",
      "epoch: 1 step: 817, loss is 0.08685789257287979\n",
      "epoch: 1 step: 818, loss is 0.22089537978172302\n",
      "epoch: 1 step: 819, loss is 0.1115737184882164\n",
      "epoch: 1 step: 820, loss is 0.122296042740345\n",
      "epoch: 1 step: 821, loss is 0.03699512407183647\n",
      "epoch: 1 step: 822, loss is 0.23537160456180573\n",
      "epoch: 1 step: 823, loss is 0.3413914144039154\n",
      "epoch: 1 step: 824, loss is 0.124457448720932\n",
      "epoch: 1 step: 825, loss is 0.1509103924036026\n",
      "epoch: 1 step: 826, loss is 0.03950910642743111\n",
      "epoch: 1 step: 827, loss is 0.03274541348218918\n",
      "epoch: 1 step: 828, loss is 0.11897037923336029\n",
      "epoch: 1 step: 829, loss is 0.3804588317871094\n",
      "epoch: 1 step: 830, loss is 0.05849245935678482\n",
      "epoch: 1 step: 831, loss is 0.5229988694190979\n",
      "epoch: 1 step: 832, loss is 0.04627423360943794\n",
      "epoch: 1 step: 833, loss is 0.05682457238435745\n",
      "epoch: 1 step: 834, loss is 0.25208359956741333\n",
      "epoch: 1 step: 835, loss is 0.284310907125473\n",
      "epoch: 1 step: 836, loss is 0.326394647359848\n",
      "epoch: 1 step: 837, loss is 0.1858336329460144\n",
      "epoch: 1 step: 838, loss is 0.1059296652674675\n",
      "epoch: 1 step: 839, loss is 0.0599590539932251\n",
      "epoch: 1 step: 840, loss is 0.026768572628498077\n",
      "epoch: 1 step: 841, loss is 0.17644166946411133\n",
      "epoch: 1 step: 842, loss is 0.08035005629062653\n",
      "epoch: 1 step: 843, loss is 0.20587420463562012\n",
      "epoch: 1 step: 844, loss is 0.20160387456417084\n",
      "epoch: 1 step: 845, loss is 0.050831470638513565\n",
      "epoch: 1 step: 846, loss is 0.20106589794158936\n",
      "epoch: 1 step: 847, loss is 0.12102407217025757\n",
      "epoch: 1 step: 848, loss is 0.12179822474718094\n",
      "epoch: 1 step: 849, loss is 0.01222014520317316\n",
      "epoch: 1 step: 850, loss is 0.43462520837783813\n",
      "epoch: 1 step: 851, loss is 0.09341739863157272\n",
      "epoch: 1 step: 852, loss is 0.3188537359237671\n",
      "epoch: 1 step: 853, loss is 0.06459306925535202\n",
      "epoch: 1 step: 854, loss is 0.12000033259391785\n",
      "epoch: 1 step: 855, loss is 0.2661275267601013\n",
      "epoch: 1 step: 856, loss is 0.28910496830940247\n",
      "epoch: 1 step: 857, loss is 0.0851840227842331\n",
      "epoch: 1 step: 858, loss is 0.1758236140012741\n",
      "epoch: 1 step: 859, loss is 0.15387053787708282\n",
      "epoch: 1 step: 860, loss is 0.1425342857837677\n",
      "epoch: 1 step: 861, loss is 0.14410600066184998\n",
      "epoch: 1 step: 862, loss is 0.3847846984863281\n",
      "epoch: 1 step: 863, loss is 0.05079605057835579\n",
      "epoch: 1 step: 864, loss is 0.21348324418067932\n",
      "epoch: 1 step: 865, loss is 0.29791274666786194\n",
      "epoch: 1 step: 866, loss is 0.4194801449775696\n",
      "epoch: 1 step: 867, loss is 0.35016149282455444\n",
      "epoch: 1 step: 868, loss is 0.07096491754055023\n",
      "epoch: 1 step: 869, loss is 0.1235298439860344\n",
      "epoch: 1 step: 870, loss is 0.33690428733825684\n",
      "epoch: 1 step: 871, loss is 0.2059011608362198\n",
      "epoch: 1 step: 872, loss is 0.1978861540555954\n",
      "epoch: 1 step: 873, loss is 0.13465477526187897\n",
      "epoch: 1 step: 874, loss is 0.21950329840183258\n",
      "epoch: 1 step: 875, loss is 0.08224622160196304\n",
      "epoch: 1 step: 876, loss is 0.043574150651693344\n",
      "epoch: 1 step: 877, loss is 0.3359876573085785\n",
      "epoch: 1 step: 878, loss is 0.10545207560062408\n",
      "epoch: 1 step: 879, loss is 0.22403894364833832\n",
      "epoch: 1 step: 880, loss is 0.24132712185382843\n",
      "epoch: 1 step: 881, loss is 0.2700779438018799\n",
      "epoch: 1 step: 882, loss is 0.25571295619010925\n",
      "epoch: 1 step: 883, loss is 0.06245754286646843\n",
      "epoch: 1 step: 884, loss is 0.1349932998418808\n",
      "epoch: 1 step: 885, loss is 0.26318594813346863\n",
      "epoch: 1 step: 886, loss is 0.0954720750451088\n",
      "epoch: 1 step: 887, loss is 0.203741654753685\n",
      "epoch: 1 step: 888, loss is 0.2836858034133911\n",
      "epoch: 1 step: 889, loss is 0.04118446260690689\n",
      "epoch: 1 step: 890, loss is 0.10858812183141708\n",
      "epoch: 1 step: 891, loss is 0.41723623871803284\n",
      "epoch: 1 step: 892, loss is 0.1930398941040039\n",
      "epoch: 1 step: 893, loss is 0.2000131607055664\n",
      "epoch: 1 step: 894, loss is 0.13535815477371216\n",
      "epoch: 1 step: 895, loss is 0.24122846126556396\n",
      "epoch: 1 step: 896, loss is 0.036183491349220276\n",
      "epoch: 1 step: 897, loss is 0.3085959851741791\n",
      "epoch: 1 step: 898, loss is 0.2662905752658844\n",
      "epoch: 1 step: 899, loss is 0.15962816774845123\n",
      "epoch: 1 step: 900, loss is 0.37829872965812683\n",
      "epoch: 1 step: 901, loss is 0.035293448716402054\n",
      "epoch: 1 step: 902, loss is 0.15143842995166779\n",
      "epoch: 1 step: 903, loss is 0.0914493128657341\n",
      "epoch: 1 step: 904, loss is 0.4365135729312897\n",
      "epoch: 1 step: 905, loss is 0.2073994278907776\n",
      "epoch: 1 step: 906, loss is 0.2025153934955597\n",
      "epoch: 1 step: 907, loss is 0.0387599915266037\n",
      "epoch: 1 step: 908, loss is 0.020688999444246292\n",
      "epoch: 1 step: 909, loss is 0.1367536336183548\n",
      "epoch: 1 step: 910, loss is 0.02944035828113556\n",
      "epoch: 1 step: 911, loss is 0.04062348231673241\n",
      "epoch: 1 step: 912, loss is 0.1671651154756546\n",
      "epoch: 1 step: 913, loss is 0.3713797926902771\n",
      "epoch: 1 step: 914, loss is 0.24165686964988708\n",
      "epoch: 1 step: 915, loss is 0.08872367441654205\n",
      "epoch: 1 step: 916, loss is 0.04768506810069084\n",
      "epoch: 1 step: 917, loss is 0.15783125162124634\n",
      "epoch: 1 step: 918, loss is 0.020840153098106384\n",
      "epoch: 1 step: 919, loss is 0.2251972258090973\n",
      "epoch: 1 step: 920, loss is 0.10586802661418915\n",
      "epoch: 1 step: 921, loss is 0.44109460711479187\n",
      "epoch: 1 step: 922, loss is 0.33823832869529724\n",
      "epoch: 1 step: 923, loss is 0.12006241083145142\n",
      "epoch: 1 step: 924, loss is 0.10656336694955826\n",
      "epoch: 1 step: 925, loss is 0.10305476188659668\n",
      "epoch: 1 step: 926, loss is 0.18732425570487976\n",
      "epoch: 1 step: 927, loss is 0.07484108954668045\n",
      "epoch: 1 step: 928, loss is 0.04023867845535278\n",
      "epoch: 1 step: 929, loss is 0.1449556052684784\n",
      "epoch: 1 step: 930, loss is 0.1458112895488739\n",
      "epoch: 1 step: 931, loss is 0.17311300337314606\n",
      "epoch: 1 step: 932, loss is 0.4424506425857544\n",
      "epoch: 1 step: 933, loss is 0.10735224932432175\n",
      "epoch: 1 step: 934, loss is 0.1782386600971222\n",
      "epoch: 1 step: 935, loss is 0.24544097483158112\n",
      "epoch: 1 step: 936, loss is 0.14275667071342468\n",
      "epoch: 1 step: 937, loss is 0.052818406373262405\n",
      "epoch: 1 step: 938, loss is 0.47493165731430054\n",
      "epoch: 1 step: 939, loss is 0.27156588435173035\n",
      "epoch: 1 step: 940, loss is 0.14670050144195557\n",
      "epoch: 1 step: 941, loss is 0.14331060647964478\n",
      "epoch: 1 step: 942, loss is 0.04322487488389015\n",
      "epoch: 1 step: 943, loss is 0.4049482047557831\n",
      "epoch: 1 step: 944, loss is 0.5349263548851013\n",
      "epoch: 1 step: 945, loss is 0.09916932880878448\n",
      "epoch: 1 step: 946, loss is 0.3031213581562042\n",
      "epoch: 1 step: 947, loss is 0.16817337274551392\n",
      "epoch: 1 step: 948, loss is 0.45626363158226013\n",
      "epoch: 1 step: 949, loss is 0.29472455382347107\n",
      "epoch: 1 step: 950, loss is 0.18410202860832214\n",
      "epoch: 1 step: 951, loss is 0.3902116119861603\n",
      "epoch: 1 step: 952, loss is 0.03945669159293175\n",
      "epoch: 1 step: 953, loss is 0.08931934088468552\n",
      "epoch: 1 step: 954, loss is 0.08825049549341202\n",
      "epoch: 1 step: 955, loss is 0.23079556226730347\n",
      "epoch: 1 step: 956, loss is 0.19517667591571808\n",
      "epoch: 1 step: 957, loss is 0.05898214504122734\n",
      "epoch: 1 step: 958, loss is 0.12235738337039948\n",
      "epoch: 1 step: 959, loss is 0.2537310719490051\n",
      "epoch: 1 step: 960, loss is 0.1058439239859581\n",
      "epoch: 1 step: 961, loss is 0.43970465660095215\n",
      "epoch: 1 step: 962, loss is 0.3687363564968109\n",
      "epoch: 1 step: 963, loss is 0.25219058990478516\n",
      "epoch: 1 step: 964, loss is 0.21122515201568604\n",
      "epoch: 1 step: 965, loss is 0.3407687842845917\n",
      "epoch: 1 step: 966, loss is 0.20841167867183685\n",
      "epoch: 1 step: 967, loss is 0.2128763347864151\n",
      "epoch: 1 step: 968, loss is 0.0683627501130104\n",
      "epoch: 1 step: 969, loss is 0.06312714517116547\n",
      "epoch: 1 step: 970, loss is 0.06895408034324646\n",
      "epoch: 1 step: 971, loss is 0.09554125368595123\n",
      "epoch: 1 step: 972, loss is 0.2608858048915863\n",
      "epoch: 1 step: 973, loss is 0.3056064546108246\n",
      "epoch: 1 step: 974, loss is 0.10667718946933746\n",
      "epoch: 1 step: 975, loss is 0.11121433973312378\n",
      "epoch: 1 step: 976, loss is 0.16276347637176514\n",
      "epoch: 1 step: 977, loss is 0.08667894452810287\n",
      "epoch: 1 step: 978, loss is 0.22545140981674194\n",
      "epoch: 1 step: 979, loss is 0.08573047816753387\n",
      "epoch: 1 step: 980, loss is 0.3064030110836029\n",
      "epoch: 1 step: 981, loss is 0.03276672214269638\n",
      "epoch: 1 step: 982, loss is 0.2310067117214203\n",
      "epoch: 1 step: 983, loss is 0.29436248540878296\n",
      "epoch: 1 step: 984, loss is 0.14286959171295166\n",
      "epoch: 1 step: 985, loss is 0.1959068328142166\n",
      "epoch: 1 step: 986, loss is 0.053237736225128174\n",
      "epoch: 1 step: 987, loss is 0.059172939509153366\n",
      "epoch: 1 step: 988, loss is 0.16844329237937927\n",
      "epoch: 1 step: 989, loss is 0.20686830580234528\n",
      "epoch: 1 step: 990, loss is 0.02107716165482998\n",
      "epoch: 1 step: 991, loss is 0.22884595394134521\n",
      "epoch: 1 step: 992, loss is 0.19676515460014343\n",
      "epoch: 1 step: 993, loss is 0.19049452245235443\n",
      "epoch: 1 step: 994, loss is 0.10893287509679794\n",
      "epoch: 1 step: 995, loss is 0.19666732847690582\n",
      "epoch: 1 step: 996, loss is 0.13485179841518402\n",
      "epoch: 1 step: 997, loss is 0.17046979069709778\n",
      "epoch: 1 step: 998, loss is 0.3210572600364685\n",
      "epoch: 1 step: 999, loss is 0.14030134677886963\n",
      "epoch: 1 step: 1000, loss is 0.13641786575317383\n",
      "epoch: 1 step: 1001, loss is 0.49246034026145935\n",
      "epoch: 1 step: 1002, loss is 0.1755676120519638\n",
      "epoch: 1 step: 1003, loss is 0.49981313943862915\n",
      "epoch: 1 step: 1004, loss is 0.20802901685237885\n",
      "epoch: 1 step: 1005, loss is 0.20257580280303955\n",
      "epoch: 1 step: 1006, loss is 0.5748885869979858\n",
      "epoch: 1 step: 1007, loss is 0.1337057650089264\n",
      "epoch: 1 step: 1008, loss is 0.1096097007393837\n",
      "epoch: 1 step: 1009, loss is 0.14081773161888123\n",
      "epoch: 1 step: 1010, loss is 0.15478435158729553\n",
      "epoch: 1 step: 1011, loss is 0.2709653675556183\n",
      "epoch: 1 step: 1012, loss is 0.06039201095700264\n",
      "epoch: 1 step: 1013, loss is 0.35282060503959656\n",
      "epoch: 1 step: 1014, loss is 0.2962591052055359\n",
      "epoch: 1 step: 1015, loss is 0.08310362696647644\n",
      "epoch: 1 step: 1016, loss is 0.11959688365459442\n",
      "epoch: 1 step: 1017, loss is 0.19646801054477692\n",
      "epoch: 1 step: 1018, loss is 0.3216465413570404\n",
      "epoch: 1 step: 1019, loss is 0.0862436294555664\n",
      "epoch: 1 step: 1020, loss is 0.3523494005203247\n",
      "epoch: 1 step: 1021, loss is 0.06937942653894424\n",
      "epoch: 1 step: 1022, loss is 0.05029657483100891\n",
      "epoch: 1 step: 1023, loss is 0.0233040489256382\n",
      "epoch: 1 step: 1024, loss is 0.20116083323955536\n",
      "epoch: 1 step: 1025, loss is 0.13702541589736938\n",
      "epoch: 1 step: 1026, loss is 0.02078389935195446\n",
      "epoch: 1 step: 1027, loss is 0.27834999561309814\n",
      "epoch: 1 step: 1028, loss is 0.28903234004974365\n",
      "epoch: 1 step: 1029, loss is 0.10894912481307983\n",
      "epoch: 1 step: 1030, loss is 0.1606820970773697\n",
      "epoch: 1 step: 1031, loss is 0.04099120944738388\n",
      "epoch: 1 step: 1032, loss is 0.04804347828030586\n",
      "epoch: 1 step: 1033, loss is 0.17778512835502625\n",
      "epoch: 1 step: 1034, loss is 0.09638424962759018\n",
      "epoch: 1 step: 1035, loss is 0.1219421848654747\n",
      "epoch: 1 step: 1036, loss is 0.048753753304481506\n",
      "epoch: 1 step: 1037, loss is 0.15104401111602783\n",
      "epoch: 1 step: 1038, loss is 0.570862352848053\n",
      "epoch: 1 step: 1039, loss is 0.07480587810277939\n",
      "epoch: 1 step: 1040, loss is 0.3202284872531891\n",
      "epoch: 1 step: 1041, loss is 0.1873839646577835\n",
      "epoch: 1 step: 1042, loss is 0.38256898522377014\n",
      "epoch: 1 step: 1043, loss is 0.40211251378059387\n",
      "epoch: 1 step: 1044, loss is 0.15954671800136566\n",
      "epoch: 1 step: 1045, loss is 0.12414312362670898\n",
      "epoch: 1 step: 1046, loss is 0.24700583517551422\n",
      "epoch: 1 step: 1047, loss is 0.29285991191864014\n",
      "epoch: 1 step: 1048, loss is 0.11512483656406403\n",
      "epoch: 1 step: 1049, loss is 0.1699405312538147\n",
      "epoch: 1 step: 1050, loss is 0.13036881387233734\n",
      "epoch: 1 step: 1051, loss is 0.10793161392211914\n",
      "epoch: 1 step: 1052, loss is 0.13370393216609955\n",
      "epoch: 1 step: 1053, loss is 0.19717629253864288\n",
      "epoch: 1 step: 1054, loss is 0.27129238843917847\n",
      "epoch: 1 step: 1055, loss is 0.10248252749443054\n",
      "epoch: 1 step: 1056, loss is 0.03174440190196037\n",
      "epoch: 1 step: 1057, loss is 0.3595917522907257\n",
      "epoch: 1 step: 1058, loss is 0.167716383934021\n",
      "epoch: 1 step: 1059, loss is 0.08458168804645538\n",
      "epoch: 1 step: 1060, loss is 0.06542722880840302\n",
      "epoch: 1 step: 1061, loss is 0.22608862817287445\n",
      "epoch: 1 step: 1062, loss is 0.06054339557886124\n",
      "epoch: 1 step: 1063, loss is 0.32920128107070923\n",
      "epoch: 1 step: 1064, loss is 0.21672233939170837\n",
      "epoch: 1 step: 1065, loss is 0.388225793838501\n",
      "epoch: 1 step: 1066, loss is 0.2553967237472534\n",
      "epoch: 1 step: 1067, loss is 0.16381236910820007\n",
      "epoch: 1 step: 1068, loss is 0.13421620428562164\n",
      "epoch: 1 step: 1069, loss is 0.2724163234233856\n",
      "epoch: 1 step: 1070, loss is 0.23594272136688232\n",
      "epoch: 1 step: 1071, loss is 0.11174648255109787\n",
      "epoch: 1 step: 1072, loss is 0.2926056683063507\n",
      "epoch: 1 step: 1073, loss is 0.09825117886066437\n",
      "epoch: 1 step: 1074, loss is 0.2634306252002716\n",
      "epoch: 1 step: 1075, loss is 0.34675663709640503\n",
      "epoch: 1 step: 1076, loss is 0.07309110462665558\n",
      "epoch: 1 step: 1077, loss is 0.10066097229719162\n",
      "epoch: 1 step: 1078, loss is 0.07095912843942642\n",
      "epoch: 1 step: 1079, loss is 0.16153979301452637\n",
      "epoch: 1 step: 1080, loss is 0.14412200450897217\n",
      "epoch: 1 step: 1081, loss is 0.06479040533304214\n",
      "epoch: 1 step: 1082, loss is 0.0969688892364502\n",
      "epoch: 1 step: 1083, loss is 0.24473345279693604\n",
      "epoch: 1 step: 1084, loss is 0.10273522883653641\n",
      "epoch: 1 step: 1085, loss is 0.042121198028326035\n",
      "epoch: 1 step: 1086, loss is 0.1518542766571045\n",
      "epoch: 1 step: 1087, loss is 0.03751630708575249\n",
      "epoch: 1 step: 1088, loss is 0.05167510360479355\n",
      "epoch: 1 step: 1089, loss is 0.20860768854618073\n",
      "epoch: 1 step: 1090, loss is 0.31888702511787415\n",
      "epoch: 1 step: 1091, loss is 0.3309611678123474\n",
      "epoch: 1 step: 1092, loss is 0.16413520276546478\n",
      "epoch: 1 step: 1093, loss is 0.01993858627974987\n",
      "epoch: 1 step: 1094, loss is 0.10119269043207169\n",
      "epoch: 1 step: 1095, loss is 0.21927842497825623\n",
      "epoch: 1 step: 1096, loss is 0.2768808603286743\n",
      "epoch: 1 step: 1097, loss is 0.07962610572576523\n",
      "epoch: 1 step: 1098, loss is 0.2806445062160492\n",
      "epoch: 1 step: 1099, loss is 0.14839068055152893\n",
      "epoch: 1 step: 1100, loss is 0.13236479461193085\n",
      "epoch: 1 step: 1101, loss is 0.043575581163167953\n",
      "epoch: 1 step: 1102, loss is 0.19471818208694458\n",
      "epoch: 1 step: 1103, loss is 0.10048187524080276\n",
      "epoch: 1 step: 1104, loss is 0.1507563292980194\n",
      "epoch: 1 step: 1105, loss is 0.23511908948421478\n",
      "epoch: 1 step: 1106, loss is 0.2315768152475357\n",
      "epoch: 1 step: 1107, loss is 0.03294704109430313\n",
      "epoch: 1 step: 1108, loss is 0.02894805185496807\n",
      "epoch: 1 step: 1109, loss is 0.2721804082393646\n",
      "epoch: 1 step: 1110, loss is 0.2303796410560608\n",
      "epoch: 1 step: 1111, loss is 0.12187137454748154\n",
      "epoch: 1 step: 1112, loss is 0.20596803724765778\n",
      "epoch: 1 step: 1113, loss is 0.26135972142219543\n",
      "epoch: 1 step: 1114, loss is 0.2543734014034271\n",
      "epoch: 1 step: 1115, loss is 0.07263511419296265\n",
      "epoch: 1 step: 1116, loss is 0.038129013031721115\n",
      "epoch: 1 step: 1117, loss is 0.05466921254992485\n",
      "epoch: 1 step: 1118, loss is 0.08824155479669571\n",
      "epoch: 1 step: 1119, loss is 0.09603885561227798\n",
      "epoch: 1 step: 1120, loss is 0.7123394012451172\n",
      "epoch: 1 step: 1121, loss is 0.2107214629650116\n",
      "epoch: 1 step: 1122, loss is 0.20402540266513824\n",
      "epoch: 1 step: 1123, loss is 0.2603883445262909\n",
      "epoch: 1 step: 1124, loss is 0.0321379080414772\n",
      "epoch: 1 step: 1125, loss is 0.07633443176746368\n",
      "epoch: 1 step: 1126, loss is 0.26002442836761475\n",
      "epoch: 1 step: 1127, loss is 0.10708680003881454\n",
      "epoch: 1 step: 1128, loss is 0.04994716867804527\n",
      "epoch: 1 step: 1129, loss is 0.04100482165813446\n",
      "epoch: 1 step: 1130, loss is 0.2800922989845276\n",
      "epoch: 1 step: 1131, loss is 0.39830130338668823\n",
      "epoch: 1 step: 1132, loss is 0.38905075192451477\n",
      "epoch: 1 step: 1133, loss is 0.16697634756565094\n",
      "epoch: 1 step: 1134, loss is 0.1611613780260086\n",
      "epoch: 1 step: 1135, loss is 0.18469388782978058\n",
      "epoch: 1 step: 1136, loss is 0.19359199702739716\n",
      "epoch: 1 step: 1137, loss is 0.060978882014751434\n",
      "epoch: 1 step: 1138, loss is 0.09451092034578323\n",
      "epoch: 1 step: 1139, loss is 0.1842234581708908\n",
      "epoch: 1 step: 1140, loss is 0.20035028457641602\n",
      "epoch: 1 step: 1141, loss is 0.09026942402124405\n",
      "epoch: 1 step: 1142, loss is 0.14026537537574768\n",
      "epoch: 1 step: 1143, loss is 0.09000466018915176\n",
      "epoch: 1 step: 1144, loss is 0.15897402167320251\n",
      "epoch: 1 step: 1145, loss is 0.272152841091156\n",
      "epoch: 1 step: 1146, loss is 0.16762807965278625\n",
      "epoch: 1 step: 1147, loss is 0.028966471552848816\n",
      "epoch: 1 step: 1148, loss is 0.08564906567335129\n",
      "epoch: 1 step: 1149, loss is 0.41002827882766724\n",
      "epoch: 1 step: 1150, loss is 0.327820748090744\n",
      "epoch: 1 step: 1151, loss is 0.41788843274116516\n",
      "epoch: 1 step: 1152, loss is 0.10184118151664734\n",
      "epoch: 1 step: 1153, loss is 0.3316754698753357\n",
      "epoch: 1 step: 1154, loss is 0.213255375623703\n",
      "epoch: 1 step: 1155, loss is 0.012475735507905483\n",
      "epoch: 1 step: 1156, loss is 0.05682271718978882\n",
      "epoch: 1 step: 1157, loss is 0.3001551628112793\n",
      "epoch: 1 step: 1158, loss is 0.15027768909931183\n",
      "epoch: 1 step: 1159, loss is 0.030930830165743828\n",
      "epoch: 1 step: 1160, loss is 0.4660467803478241\n",
      "epoch: 1 step: 1161, loss is 0.1142156645655632\n",
      "epoch: 1 step: 1162, loss is 0.2805578410625458\n",
      "epoch: 1 step: 1163, loss is 0.19676105678081512\n",
      "epoch: 1 step: 1164, loss is 0.34023794531822205\n",
      "epoch: 1 step: 1165, loss is 0.12735256552696228\n",
      "epoch: 1 step: 1166, loss is 0.4554877281188965\n",
      "epoch: 1 step: 1167, loss is 0.21604642271995544\n",
      "epoch: 1 step: 1168, loss is 0.18768568336963654\n",
      "epoch: 1 step: 1169, loss is 0.15187658369541168\n",
      "epoch: 1 step: 1170, loss is 0.36178597807884216\n",
      "epoch: 1 step: 1171, loss is 0.05518319457769394\n",
      "epoch: 1 step: 1172, loss is 0.18214482069015503\n",
      "epoch: 1 step: 1173, loss is 0.07076713442802429\n",
      "epoch: 1 step: 1174, loss is 0.18378430604934692\n",
      "epoch: 1 step: 1175, loss is 0.25457602739334106\n",
      "epoch: 1 step: 1176, loss is 0.12073080241680145\n",
      "epoch: 1 step: 1177, loss is 0.04826999828219414\n",
      "epoch: 1 step: 1178, loss is 0.09259700775146484\n",
      "epoch: 1 step: 1179, loss is 0.493910014629364\n",
      "epoch: 1 step: 1180, loss is 0.051313772797584534\n",
      "epoch: 1 step: 1181, loss is 0.08550309389829636\n",
      "epoch: 1 step: 1182, loss is 0.045218974351882935\n",
      "epoch: 1 step: 1183, loss is 0.09331462532281876\n",
      "epoch: 1 step: 1184, loss is 0.20391398668289185\n",
      "epoch: 1 step: 1185, loss is 0.3825812041759491\n",
      "epoch: 1 step: 1186, loss is 0.37608015537261963\n",
      "epoch: 1 step: 1187, loss is 0.13440676033496857\n",
      "epoch: 1 step: 1188, loss is 0.17591749131679535\n",
      "epoch: 1 step: 1189, loss is 0.055369485169649124\n",
      "epoch: 1 step: 1190, loss is 0.14462141692638397\n",
      "epoch: 1 step: 1191, loss is 0.036867331713438034\n",
      "epoch: 1 step: 1192, loss is 0.26511478424072266\n",
      "epoch: 1 step: 1193, loss is 0.056728944182395935\n",
      "epoch: 1 step: 1194, loss is 0.19700194895267487\n",
      "epoch: 1 step: 1195, loss is 0.1413871943950653\n",
      "epoch: 1 step: 1196, loss is 0.10564176738262177\n",
      "epoch: 1 step: 1197, loss is 0.18731454014778137\n",
      "epoch: 1 step: 1198, loss is 0.05486394837498665\n",
      "epoch: 1 step: 1199, loss is 0.12512610852718353\n",
      "epoch: 1 step: 1200, loss is 0.3998338580131531\n",
      "epoch: 1 step: 1201, loss is 0.07856076210737228\n",
      "epoch: 1 step: 1202, loss is 0.05128395929932594\n",
      "epoch: 1 step: 1203, loss is 0.32766279578208923\n",
      "epoch: 1 step: 1204, loss is 0.02587680146098137\n",
      "epoch: 1 step: 1205, loss is 0.2766498625278473\n",
      "epoch: 1 step: 1206, loss is 0.4307974576950073\n",
      "epoch: 1 step: 1207, loss is 0.33888980746269226\n",
      "epoch: 1 step: 1208, loss is 0.2621230483055115\n",
      "epoch: 1 step: 1209, loss is 0.11810006946325302\n",
      "epoch: 1 step: 1210, loss is 0.1518665850162506\n",
      "epoch: 1 step: 1211, loss is 0.31351423263549805\n",
      "epoch: 1 step: 1212, loss is 0.10922154039144516\n",
      "epoch: 1 step: 1213, loss is 0.059875015169382095\n",
      "epoch: 1 step: 1214, loss is 0.01670418307185173\n",
      "epoch: 1 step: 1215, loss is 0.3165767788887024\n",
      "epoch: 1 step: 1216, loss is 0.18089990317821503\n",
      "epoch: 1 step: 1217, loss is 0.057939402759075165\n",
      "epoch: 1 step: 1218, loss is 0.026289423927664757\n",
      "epoch: 1 step: 1219, loss is 0.15906922519207\n",
      "epoch: 1 step: 1220, loss is 0.15489575266838074\n",
      "epoch: 1 step: 1221, loss is 0.34869152307510376\n",
      "epoch: 1 step: 1222, loss is 0.16511037945747375\n",
      "epoch: 1 step: 1223, loss is 0.11528299748897552\n",
      "epoch: 1 step: 1224, loss is 0.384554922580719\n",
      "epoch: 1 step: 1225, loss is 0.18906845152378082\n",
      "epoch: 1 step: 1226, loss is 0.07814209908246994\n",
      "epoch: 1 step: 1227, loss is 0.14178287982940674\n",
      "epoch: 1 step: 1228, loss is 0.24216005206108093\n",
      "epoch: 1 step: 1229, loss is 0.42920371890068054\n",
      "epoch: 1 step: 1230, loss is 0.18243801593780518\n",
      "epoch: 1 step: 1231, loss is 0.23362180590629578\n",
      "epoch: 1 step: 1232, loss is 0.0347885824739933\n",
      "epoch: 1 step: 1233, loss is 0.166263610124588\n",
      "epoch: 1 step: 1234, loss is 0.18562710285186768\n",
      "epoch: 1 step: 1235, loss is 0.21621903777122498\n",
      "epoch: 1 step: 1236, loss is 0.10602231323719025\n",
      "epoch: 1 step: 1237, loss is 0.20568937063217163\n",
      "epoch: 1 step: 1238, loss is 0.15811964869499207\n",
      "epoch: 1 step: 1239, loss is 0.24519389867782593\n",
      "epoch: 1 step: 1240, loss is 0.22192217409610748\n",
      "epoch: 1 step: 1241, loss is 0.136091947555542\n",
      "epoch: 1 step: 1242, loss is 0.21001273393630981\n",
      "epoch: 1 step: 1243, loss is 0.3586433529853821\n",
      "epoch: 1 step: 1244, loss is 0.29120299220085144\n",
      "epoch: 1 step: 1245, loss is 0.027917373925447464\n",
      "epoch: 1 step: 1246, loss is 0.019760802388191223\n",
      "epoch: 1 step: 1247, loss is 0.25922736525535583\n",
      "epoch: 1 step: 1248, loss is 0.5127217173576355\n",
      "epoch: 1 step: 1249, loss is 0.2824016213417053\n",
      "epoch: 1 step: 1250, loss is 0.22653597593307495\n",
      "epoch: 1 step: 1251, loss is 0.20354615151882172\n",
      "epoch: 1 step: 1252, loss is 0.09959834814071655\n",
      "epoch: 1 step: 1253, loss is 0.35960105061531067\n",
      "epoch: 1 step: 1254, loss is 0.10965608805418015\n",
      "epoch: 1 step: 1255, loss is 0.15882009267807007\n",
      "epoch: 1 step: 1256, loss is 0.027677608653903008\n",
      "epoch: 1 step: 1257, loss is 0.06055564433336258\n",
      "epoch: 1 step: 1258, loss is 0.2109365165233612\n",
      "epoch: 1 step: 1259, loss is 0.0818093791604042\n",
      "epoch: 1 step: 1260, loss is 0.00740800378844142\n",
      "epoch: 1 step: 1261, loss is 0.07204905897378922\n",
      "epoch: 1 step: 1262, loss is 0.05091622844338417\n",
      "epoch: 1 step: 1263, loss is 0.4085613191127777\n",
      "epoch: 1 step: 1264, loss is 0.07474414259195328\n",
      "epoch: 1 step: 1265, loss is 0.15411657094955444\n",
      "epoch: 1 step: 1266, loss is 0.16454032063484192\n",
      "epoch: 1 step: 1267, loss is 0.1783442497253418\n",
      "epoch: 1 step: 1268, loss is 0.152299165725708\n",
      "epoch: 1 step: 1269, loss is 0.07566424459218979\n",
      "epoch: 1 step: 1270, loss is 0.005583545193076134\n",
      "epoch: 1 step: 1271, loss is 0.27557897567749023\n",
      "epoch: 1 step: 1272, loss is 0.3295058012008667\n",
      "epoch: 1 step: 1273, loss is 0.319347620010376\n",
      "epoch: 1 step: 1274, loss is 0.3089075982570648\n",
      "epoch: 1 step: 1275, loss is 0.151347354054451\n",
      "epoch: 1 step: 1276, loss is 0.16745851933956146\n",
      "epoch: 1 step: 1277, loss is 0.03682810068130493\n",
      "epoch: 1 step: 1278, loss is 0.2520650625228882\n",
      "epoch: 1 step: 1279, loss is 0.20321908593177795\n",
      "epoch: 1 step: 1280, loss is 0.16402170062065125\n",
      "epoch: 1 step: 1281, loss is 0.10626283288002014\n",
      "epoch: 1 step: 1282, loss is 0.578721821308136\n",
      "epoch: 1 step: 1283, loss is 0.05976833030581474\n",
      "epoch: 1 step: 1284, loss is 0.05190204456448555\n",
      "epoch: 1 step: 1285, loss is 0.19361534714698792\n",
      "epoch: 1 step: 1286, loss is 0.2527710795402527\n",
      "epoch: 1 step: 1287, loss is 0.3279419541358948\n",
      "epoch: 1 step: 1288, loss is 0.0618942454457283\n",
      "epoch: 1 step: 1289, loss is 0.015975214540958405\n",
      "epoch: 1 step: 1290, loss is 0.3929286003112793\n",
      "epoch: 1 step: 1291, loss is 0.038591139018535614\n",
      "epoch: 1 step: 1292, loss is 0.2712016701698303\n",
      "epoch: 1 step: 1293, loss is 0.1567707508802414\n",
      "epoch: 1 step: 1294, loss is 0.09038460999727249\n",
      "epoch: 1 step: 1295, loss is 0.20053835213184357\n",
      "epoch: 1 step: 1296, loss is 0.15019172430038452\n",
      "epoch: 1 step: 1297, loss is 0.04739601910114288\n",
      "epoch: 1 step: 1298, loss is 0.28230398893356323\n",
      "epoch: 1 step: 1299, loss is 0.039192937314510345\n",
      "epoch: 1 step: 1300, loss is 0.060748543590307236\n",
      "epoch: 1 step: 1301, loss is 0.10647395253181458\n",
      "epoch: 1 step: 1302, loss is 0.04978349804878235\n",
      "epoch: 1 step: 1303, loss is 0.08028610050678253\n",
      "epoch: 1 step: 1304, loss is 0.06031034514307976\n",
      "epoch: 1 step: 1305, loss is 0.06823135167360306\n",
      "epoch: 1 step: 1306, loss is 0.27711907029151917\n",
      "epoch: 1 step: 1307, loss is 0.147018164396286\n",
      "epoch: 1 step: 1308, loss is 0.14093820750713348\n",
      "epoch: 1 step: 1309, loss is 0.10831864178180695\n",
      "epoch: 1 step: 1310, loss is 0.1406836211681366\n",
      "epoch: 1 step: 1311, loss is 0.23132264614105225\n",
      "epoch: 1 step: 1312, loss is 0.05451401323080063\n",
      "epoch: 1 step: 1313, loss is 0.3519256114959717\n",
      "epoch: 1 step: 1314, loss is 0.17822900414466858\n",
      "epoch: 1 step: 1315, loss is 0.173985093832016\n",
      "epoch: 1 step: 1316, loss is 0.08221163600683212\n",
      "epoch: 1 step: 1317, loss is 0.040093790739774704\n",
      "epoch: 1 step: 1318, loss is 0.2077743113040924\n",
      "epoch: 1 step: 1319, loss is 0.448810875415802\n",
      "epoch: 1 step: 1320, loss is 0.2963606119155884\n",
      "epoch: 1 step: 1321, loss is 0.1600358933210373\n",
      "epoch: 1 step: 1322, loss is 0.08513748645782471\n",
      "epoch: 1 step: 1323, loss is 0.35683998465538025\n",
      "epoch: 1 step: 1324, loss is 0.0709904208779335\n",
      "epoch: 1 step: 1325, loss is 0.06360562145709991\n",
      "epoch: 1 step: 1326, loss is 0.21964825689792633\n",
      "epoch: 1 step: 1327, loss is 0.1702766716480255\n",
      "epoch: 1 step: 1328, loss is 0.13795018196105957\n",
      "epoch: 1 step: 1329, loss is 0.04309026524424553\n",
      "epoch: 1 step: 1330, loss is 0.06021259352564812\n",
      "epoch: 1 step: 1331, loss is 0.07778091728687286\n",
      "epoch: 1 step: 1332, loss is 0.15103894472122192\n",
      "epoch: 1 step: 1333, loss is 0.8733566999435425\n",
      "epoch: 1 step: 1334, loss is 0.05439220741391182\n",
      "epoch: 1 step: 1335, loss is 0.14016394317150116\n",
      "epoch: 1 step: 1336, loss is 0.051850371062755585\n",
      "epoch: 1 step: 1337, loss is 0.2948475480079651\n",
      "epoch: 1 step: 1338, loss is 0.049837663769721985\n",
      "epoch: 1 step: 1339, loss is 0.3681553602218628\n",
      "epoch: 1 step: 1340, loss is 0.15960144996643066\n",
      "epoch: 1 step: 1341, loss is 0.08076190203428268\n",
      "epoch: 1 step: 1342, loss is 0.07055655866861343\n",
      "epoch: 1 step: 1343, loss is 0.17207936942577362\n",
      "epoch: 1 step: 1344, loss is 0.3138083815574646\n",
      "epoch: 1 step: 1345, loss is 0.20425324141979218\n",
      "epoch: 1 step: 1346, loss is 0.15772207081317902\n",
      "epoch: 1 step: 1347, loss is 0.24400314688682556\n",
      "epoch: 1 step: 1348, loss is 0.11124557256698608\n",
      "epoch: 1 step: 1349, loss is 0.39985963702201843\n",
      "epoch: 1 step: 1350, loss is 0.213069349527359\n",
      "epoch: 1 step: 1351, loss is 0.1510474979877472\n",
      "epoch: 1 step: 1352, loss is 0.031941693276166916\n",
      "epoch: 1 step: 1353, loss is 0.03946373611688614\n",
      "epoch: 1 step: 1354, loss is 0.04827829822897911\n",
      "epoch: 1 step: 1355, loss is 0.0646931603550911\n",
      "epoch: 1 step: 1356, loss is 0.3997267186641693\n",
      "epoch: 1 step: 1357, loss is 0.12638714909553528\n",
      "epoch: 1 step: 1358, loss is 0.046473246067762375\n",
      "epoch: 1 step: 1359, loss is 0.20700965821743011\n",
      "epoch: 1 step: 1360, loss is 0.10965190082788467\n",
      "epoch: 1 step: 1361, loss is 0.24859744310379028\n",
      "epoch: 1 step: 1362, loss is 0.32535138726234436\n",
      "epoch: 1 step: 1363, loss is 0.31435641646385193\n",
      "epoch: 1 step: 1364, loss is 0.139017254114151\n",
      "epoch: 1 step: 1365, loss is 0.1304929405450821\n",
      "epoch: 1 step: 1366, loss is 0.03211628645658493\n",
      "epoch: 1 step: 1367, loss is 0.0880834236741066\n",
      "epoch: 1 step: 1368, loss is 0.09587930142879486\n",
      "epoch: 1 step: 1369, loss is 0.25684651732444763\n",
      "epoch: 1 step: 1370, loss is 0.2109978348016739\n",
      "epoch: 1 step: 1371, loss is 0.07125001400709152\n",
      "epoch: 1 step: 1372, loss is 0.20313572883605957\n",
      "epoch: 1 step: 1373, loss is 0.3827408254146576\n",
      "epoch: 1 step: 1374, loss is 0.028872007504105568\n",
      "epoch: 1 step: 1375, loss is 0.03990524262189865\n",
      "epoch: 1 step: 1376, loss is 0.4778856933116913\n",
      "epoch: 1 step: 1377, loss is 0.06949764490127563\n",
      "epoch: 1 step: 1378, loss is 0.0997084230184555\n",
      "epoch: 1 step: 1379, loss is 0.1065654531121254\n",
      "epoch: 1 step: 1380, loss is 0.2137695550918579\n",
      "epoch: 1 step: 1381, loss is 0.07364626973867416\n",
      "epoch: 1 step: 1382, loss is 0.045919690281152725\n",
      "epoch: 1 step: 1383, loss is 0.3866468071937561\n",
      "epoch: 1 step: 1384, loss is 0.3156454265117645\n",
      "epoch: 1 step: 1385, loss is 0.09845136106014252\n",
      "epoch: 1 step: 1386, loss is 0.030478648841381073\n",
      "epoch: 1 step: 1387, loss is 0.044455062597990036\n",
      "epoch: 1 step: 1388, loss is 0.19444409012794495\n",
      "epoch: 1 step: 1389, loss is 0.21921920776367188\n",
      "epoch: 1 step: 1390, loss is 0.16403426229953766\n",
      "epoch: 1 step: 1391, loss is 0.14020183682441711\n",
      "epoch: 1 step: 1392, loss is 0.09293826669454575\n",
      "epoch: 1 step: 1393, loss is 0.2506922483444214\n",
      "epoch: 1 step: 1394, loss is 0.19580648839473724\n",
      "epoch: 1 step: 1395, loss is 0.11442874372005463\n",
      "epoch: 1 step: 1396, loss is 0.08007136732339859\n",
      "epoch: 1 step: 1397, loss is 0.3383832573890686\n",
      "epoch: 1 step: 1398, loss is 0.11823853850364685\n",
      "epoch: 1 step: 1399, loss is 0.25554415583610535\n",
      "epoch: 1 step: 1400, loss is 0.018663810566067696\n",
      "epoch: 1 step: 1401, loss is 0.18324550986289978\n",
      "epoch: 1 step: 1402, loss is 0.22432786226272583\n",
      "epoch: 1 step: 1403, loss is 0.20587050914764404\n",
      "epoch: 1 step: 1404, loss is 0.2202630639076233\n",
      "epoch: 1 step: 1405, loss is 0.03472018614411354\n",
      "epoch: 1 step: 1406, loss is 0.48799633979797363\n",
      "epoch: 1 step: 1407, loss is 0.11735905706882477\n",
      "epoch: 1 step: 1408, loss is 0.0580708347260952\n",
      "epoch: 1 step: 1409, loss is 0.21262694895267487\n",
      "epoch: 1 step: 1410, loss is 0.28545117378234863\n",
      "epoch: 1 step: 1411, loss is 0.21831104159355164\n",
      "epoch: 1 step: 1412, loss is 0.1678885668516159\n",
      "epoch: 1 step: 1413, loss is 0.07401823997497559\n",
      "epoch: 1 step: 1414, loss is 0.09464775025844574\n",
      "epoch: 1 step: 1415, loss is 0.07892615348100662\n",
      "epoch: 1 step: 1416, loss is 0.048115212470293045\n",
      "epoch: 1 step: 1417, loss is 0.615670919418335\n",
      "epoch: 1 step: 1418, loss is 0.18509505689144135\n",
      "epoch: 1 step: 1419, loss is 0.24306222796440125\n",
      "epoch: 1 step: 1420, loss is 0.07426945120096207\n",
      "epoch: 1 step: 1421, loss is 0.29277661442756653\n",
      "epoch: 1 step: 1422, loss is 0.09243541955947876\n",
      "epoch: 1 step: 1423, loss is 0.17202916741371155\n",
      "epoch: 1 step: 1424, loss is 0.11961140483617783\n",
      "epoch: 1 step: 1425, loss is 0.07394850999116898\n",
      "epoch: 1 step: 1426, loss is 0.03517787903547287\n",
      "epoch: 1 step: 1427, loss is 0.16049644351005554\n",
      "epoch: 1 step: 1428, loss is 0.08059512823820114\n",
      "epoch: 1 step: 1429, loss is 0.15168322622776031\n",
      "epoch: 1 step: 1430, loss is 0.4130779802799225\n",
      "epoch: 1 step: 1431, loss is 0.2810891270637512\n",
      "epoch: 1 step: 1432, loss is 0.44949668645858765\n",
      "epoch: 1 step: 1433, loss is 0.04898516461253166\n",
      "epoch: 1 step: 1434, loss is 0.13747243583202362\n",
      "epoch: 1 step: 1435, loss is 0.23676560819149017\n",
      "epoch: 1 step: 1436, loss is 0.17721186578273773\n",
      "epoch: 1 step: 1437, loss is 0.24907286465168\n",
      "epoch: 1 step: 1438, loss is 0.13032956421375275\n",
      "epoch: 1 step: 1439, loss is 0.24837207794189453\n",
      "epoch: 1 step: 1440, loss is 0.27219945192337036\n",
      "epoch: 1 step: 1441, loss is 0.19685325026512146\n",
      "epoch: 1 step: 1442, loss is 0.18891341984272003\n",
      "epoch: 1 step: 1443, loss is 0.08719826489686966\n",
      "epoch: 1 step: 1444, loss is 0.14925743639469147\n",
      "epoch: 1 step: 1445, loss is 0.02774878218770027\n",
      "epoch: 1 step: 1446, loss is 0.03450053185224533\n",
      "epoch: 1 step: 1447, loss is 0.05251474678516388\n",
      "epoch: 1 step: 1448, loss is 0.23944757878780365\n",
      "epoch: 1 step: 1449, loss is 0.013844465836882591\n",
      "epoch: 1 step: 1450, loss is 0.036098413169384\n",
      "epoch: 1 step: 1451, loss is 0.1022646352648735\n",
      "epoch: 1 step: 1452, loss is 0.27816084027290344\n",
      "epoch: 1 step: 1453, loss is 0.21135009825229645\n",
      "epoch: 1 step: 1454, loss is 0.1469840258359909\n",
      "epoch: 1 step: 1455, loss is 0.09163526445627213\n",
      "epoch: 1 step: 1456, loss is 0.025584330782294273\n",
      "epoch: 1 step: 1457, loss is 0.07954338937997818\n",
      "epoch: 1 step: 1458, loss is 0.39585334062576294\n",
      "epoch: 1 step: 1459, loss is 0.10139016062021255\n",
      "epoch: 1 step: 1460, loss is 0.04943360388278961\n",
      "epoch: 1 step: 1461, loss is 0.17213885486125946\n",
      "epoch: 1 step: 1462, loss is 0.14103832840919495\n",
      "epoch: 1 step: 1463, loss is 0.18677794933319092\n",
      "epoch: 1 step: 1464, loss is 0.16301749646663666\n",
      "epoch: 1 step: 1465, loss is 0.4358553886413574\n",
      "epoch: 1 step: 1466, loss is 0.06392429023981094\n",
      "epoch: 1 step: 1467, loss is 0.15337012708187103\n",
      "epoch: 1 step: 1468, loss is 0.3032326400279999\n",
      "epoch: 1 step: 1469, loss is 0.38001012802124023\n",
      "epoch: 1 step: 1470, loss is 0.03112499788403511\n",
      "epoch: 1 step: 1471, loss is 0.0842464417219162\n",
      "epoch: 1 step: 1472, loss is 0.17432664334774017\n",
      "epoch: 1 step: 1473, loss is 0.05009794235229492\n",
      "epoch: 1 step: 1474, loss is 0.03706638887524605\n",
      "epoch: 1 step: 1475, loss is 0.10870563983917236\n",
      "epoch: 1 step: 1476, loss is 0.06123843789100647\n",
      "epoch: 1 step: 1477, loss is 0.09383492171764374\n",
      "epoch: 1 step: 1478, loss is 0.07999490201473236\n",
      "epoch: 1 step: 1479, loss is 0.17465458810329437\n",
      "epoch: 1 step: 1480, loss is 0.20002254843711853\n",
      "epoch: 1 step: 1481, loss is 0.19399282336235046\n",
      "epoch: 1 step: 1482, loss is 0.033620480448007584\n",
      "epoch: 1 step: 1483, loss is 0.048613764345645905\n",
      "epoch: 1 step: 1484, loss is 0.24451003968715668\n",
      "epoch: 1 step: 1485, loss is 0.05745634064078331\n",
      "epoch: 1 step: 1486, loss is 0.11718114465475082\n",
      "epoch: 1 step: 1487, loss is 0.3891620934009552\n",
      "epoch: 1 step: 1488, loss is 0.060690443962812424\n",
      "epoch: 1 step: 1489, loss is 0.011895042844116688\n",
      "epoch: 1 step: 1490, loss is 0.019696209579706192\n",
      "epoch: 1 step: 1491, loss is 0.3241405189037323\n",
      "epoch: 1 step: 1492, loss is 0.13812755048274994\n",
      "epoch: 1 step: 1493, loss is 0.10573700815439224\n",
      "epoch: 1 step: 1494, loss is 0.06548114120960236\n",
      "epoch: 1 step: 1495, loss is 0.26317068934440613\n",
      "epoch: 1 step: 1496, loss is 0.08093730360269547\n",
      "epoch: 1 step: 1497, loss is 0.04609822481870651\n",
      "epoch: 1 step: 1498, loss is 0.06596745550632477\n",
      "epoch: 1 step: 1499, loss is 0.10061030089855194\n",
      "epoch: 1 step: 1500, loss is 0.24873781204223633\n",
      "epoch: 1 step: 1501, loss is 0.11868834495544434\n",
      "epoch: 1 step: 1502, loss is 0.3682125210762024\n",
      "epoch: 1 step: 1503, loss is 0.3615245223045349\n",
      "epoch: 1 step: 1504, loss is 0.07818327844142914\n",
      "epoch: 1 step: 1505, loss is 0.38266217708587646\n",
      "epoch: 1 step: 1506, loss is 0.10325536131858826\n",
      "epoch: 1 step: 1507, loss is 0.13076113164424896\n",
      "epoch: 1 step: 1508, loss is 0.060051724314689636\n",
      "epoch: 1 step: 1509, loss is 0.08922640234231949\n",
      "epoch: 1 step: 1510, loss is 0.12858229875564575\n",
      "epoch: 1 step: 1511, loss is 0.5744521617889404\n",
      "epoch: 1 step: 1512, loss is 0.10063335299491882\n",
      "epoch: 1 step: 1513, loss is 0.0431063137948513\n",
      "epoch: 1 step: 1514, loss is 0.13926534354686737\n",
      "epoch: 1 step: 1515, loss is 0.13304780423641205\n",
      "epoch: 1 step: 1516, loss is 0.6204537153244019\n",
      "epoch: 1 step: 1517, loss is 0.053736694157123566\n",
      "epoch: 1 step: 1518, loss is 0.16477571427822113\n",
      "epoch: 1 step: 1519, loss is 0.1804092973470688\n",
      "epoch: 1 step: 1520, loss is 0.04661119729280472\n",
      "epoch: 1 step: 1521, loss is 0.1623445600271225\n",
      "epoch: 1 step: 1522, loss is 0.058611951768398285\n",
      "epoch: 1 step: 1523, loss is 0.40736591815948486\n",
      "epoch: 1 step: 1524, loss is 0.052050989121198654\n",
      "epoch: 1 step: 1525, loss is 0.36651381850242615\n",
      "epoch: 1 step: 1526, loss is 0.281326025724411\n",
      "epoch: 1 step: 1527, loss is 0.05121795833110809\n",
      "epoch: 1 step: 1528, loss is 0.2880340814590454\n",
      "epoch: 1 step: 1529, loss is 0.07792165875434875\n",
      "epoch: 1 step: 1530, loss is 0.052682921290397644\n",
      "epoch: 1 step: 1531, loss is 0.2937200367450714\n",
      "epoch: 1 step: 1532, loss is 0.199903205037117\n",
      "epoch: 1 step: 1533, loss is 0.08887168765068054\n",
      "epoch: 1 step: 1534, loss is 0.2074023187160492\n",
      "epoch: 1 step: 1535, loss is 0.09308173507452011\n",
      "epoch: 1 step: 1536, loss is 0.11404916644096375\n",
      "epoch: 1 step: 1537, loss is 0.21846728026866913\n",
      "epoch: 1 step: 1538, loss is 0.4549376964569092\n",
      "epoch: 1 step: 1539, loss is 0.38473981618881226\n",
      "epoch: 1 step: 1540, loss is 0.11928500235080719\n",
      "epoch: 1 step: 1541, loss is 0.3710281252861023\n",
      "epoch: 1 step: 1542, loss is 0.08581244945526123\n",
      "epoch: 1 step: 1543, loss is 0.07995615154504776\n",
      "epoch: 1 step: 1544, loss is 0.10715612024068832\n",
      "epoch: 1 step: 1545, loss is 0.08410971611738205\n",
      "epoch: 1 step: 1546, loss is 0.09354480355978012\n",
      "epoch: 1 step: 1547, loss is 0.18317902088165283\n",
      "epoch: 1 step: 1548, loss is 0.32371413707733154\n",
      "epoch: 1 step: 1549, loss is 0.07470976561307907\n",
      "epoch: 1 step: 1550, loss is 0.19951528310775757\n",
      "epoch: 1 step: 1551, loss is 0.14636310935020447\n",
      "epoch: 1 step: 1552, loss is 0.2385491132736206\n",
      "epoch: 1 step: 1553, loss is 0.07092300057411194\n",
      "epoch: 1 step: 1554, loss is 0.17623157799243927\n",
      "epoch: 1 step: 1555, loss is 0.14845170080661774\n",
      "epoch: 1 step: 1556, loss is 0.3505270481109619\n",
      "epoch: 1 step: 1557, loss is 0.22889244556427002\n",
      "epoch: 1 step: 1558, loss is 0.03628456965088844\n",
      "epoch: 1 step: 1559, loss is 0.10199642926454544\n",
      "epoch: 1 step: 1560, loss is 0.12738177180290222\n",
      "epoch: 1 step: 1561, loss is 0.2191261351108551\n",
      "epoch: 1 step: 1562, loss is 0.04611659795045853\n",
      "epoch: 1 step: 1563, loss is 0.05013728514313698\n",
      "epoch: 1 step: 1564, loss is 0.029099034145474434\n",
      "epoch: 1 step: 1565, loss is 0.2412513792514801\n",
      "epoch: 1 step: 1566, loss is 0.3017022907733917\n",
      "epoch: 1 step: 1567, loss is 0.13836203515529633\n",
      "epoch: 1 step: 1568, loss is 0.2700529396533966\n",
      "epoch: 1 step: 1569, loss is 0.23208478093147278\n",
      "epoch: 1 step: 1570, loss is 0.17918844521045685\n",
      "epoch: 1 step: 1571, loss is 0.2524435818195343\n",
      "epoch: 1 step: 1572, loss is 0.3330783247947693\n",
      "epoch: 1 step: 1573, loss is 0.1571064591407776\n",
      "epoch: 1 step: 1574, loss is 0.20012041926383972\n",
      "epoch: 1 step: 1575, loss is 0.06056151166558266\n",
      "epoch: 1 step: 1576, loss is 0.24830704927444458\n",
      "epoch: 1 step: 1577, loss is 0.11315940320491791\n",
      "epoch: 1 step: 1578, loss is 0.24965234100818634\n",
      "epoch: 1 step: 1579, loss is 0.1577041894197464\n",
      "epoch: 1 step: 1580, loss is 0.11503809690475464\n",
      "epoch: 1 step: 1581, loss is 0.04370304197072983\n",
      "epoch: 1 step: 1582, loss is 0.07274902611970901\n",
      "epoch: 1 step: 1583, loss is 0.3151722550392151\n",
      "epoch: 1 step: 1584, loss is 0.09414063394069672\n",
      "epoch: 1 step: 1585, loss is 0.3280828297138214\n",
      "epoch: 1 step: 1586, loss is 0.34103062748908997\n",
      "epoch: 1 step: 1587, loss is 0.03199516609311104\n",
      "epoch: 1 step: 1588, loss is 0.10282374173402786\n",
      "epoch: 1 step: 1589, loss is 0.1478780210018158\n",
      "epoch: 1 step: 1590, loss is 0.31980806589126587\n",
      "epoch: 1 step: 1591, loss is 0.34986400604248047\n",
      "epoch: 1 step: 1592, loss is 0.19308613240718842\n",
      "epoch: 1 step: 1593, loss is 0.2230016142129898\n",
      "epoch: 1 step: 1594, loss is 0.18661096692085266\n",
      "epoch: 1 step: 1595, loss is 0.2637235224246979\n",
      "epoch: 1 step: 1596, loss is 0.0970032811164856\n",
      "epoch: 1 step: 1597, loss is 0.052466392517089844\n",
      "epoch: 1 step: 1598, loss is 0.462720662355423\n",
      "epoch: 1 step: 1599, loss is 0.140384241938591\n",
      "epoch: 1 step: 1600, loss is 0.4327220916748047\n",
      "epoch: 1 step: 1601, loss is 0.10594667494297028\n",
      "epoch: 1 step: 1602, loss is 0.12817832827568054\n",
      "epoch: 1 step: 1603, loss is 0.2527652084827423\n",
      "epoch: 1 step: 1604, loss is 0.1351541131734848\n",
      "epoch: 1 step: 1605, loss is 0.18869724869728088\n",
      "epoch: 1 step: 1606, loss is 0.0898277536034584\n",
      "epoch: 1 step: 1607, loss is 0.20681186020374298\n",
      "epoch: 1 step: 1608, loss is 0.012985260225832462\n",
      "epoch: 1 step: 1609, loss is 0.06406866014003754\n",
      "epoch: 1 step: 1610, loss is 0.03494703397154808\n",
      "epoch: 1 step: 1611, loss is 0.30565348267555237\n",
      "epoch: 1 step: 1612, loss is 0.28378212451934814\n",
      "epoch: 1 step: 1613, loss is 0.16825883090496063\n",
      "epoch: 1 step: 1614, loss is 0.11488714814186096\n",
      "epoch: 1 step: 1615, loss is 0.1723809540271759\n",
      "epoch: 1 step: 1616, loss is 0.06547962874174118\n",
      "epoch: 1 step: 1617, loss is 0.1574101746082306\n",
      "epoch: 1 step: 1618, loss is 0.08680538833141327\n",
      "epoch: 1 step: 1619, loss is 0.3069218695163727\n",
      "epoch: 1 step: 1620, loss is 0.24684454500675201\n",
      "epoch: 1 step: 1621, loss is 0.28682518005371094\n",
      "epoch: 1 step: 1622, loss is 0.06190963834524155\n",
      "epoch: 1 step: 1623, loss is 0.09351413697004318\n",
      "epoch: 1 step: 1624, loss is 0.14825543761253357\n",
      "epoch: 1 step: 1625, loss is 0.1747228503227234\n",
      "epoch: 1 step: 1626, loss is 0.12138155102729797\n",
      "epoch: 1 step: 1627, loss is 0.025815119966864586\n",
      "epoch: 1 step: 1628, loss is 0.42671042680740356\n",
      "epoch: 1 step: 1629, loss is 0.06713326275348663\n",
      "epoch: 1 step: 1630, loss is 0.04490356519818306\n",
      "epoch: 1 step: 1631, loss is 0.018872521817684174\n",
      "epoch: 1 step: 1632, loss is 0.23160016536712646\n",
      "epoch: 1 step: 1633, loss is 0.08570452779531479\n",
      "epoch: 1 step: 1634, loss is 0.4339243769645691\n",
      "epoch: 1 step: 1635, loss is 0.07354117184877396\n",
      "epoch: 1 step: 1636, loss is 0.03502310439944267\n",
      "epoch: 1 step: 1637, loss is 0.02866353653371334\n",
      "epoch: 1 step: 1638, loss is 0.11706551909446716\n",
      "epoch: 1 step: 1639, loss is 0.05841216444969177\n",
      "epoch: 1 step: 1640, loss is 0.03739994764328003\n",
      "epoch: 1 step: 1641, loss is 0.05996498838067055\n",
      "epoch: 1 step: 1642, loss is 0.07526236772537231\n",
      "epoch: 1 step: 1643, loss is 0.026041097939014435\n",
      "epoch: 1 step: 1644, loss is 0.3075632154941559\n",
      "epoch: 1 step: 1645, loss is 0.10370097309350967\n",
      "epoch: 1 step: 1646, loss is 0.2700999081134796\n",
      "epoch: 1 step: 1647, loss is 0.20572179555892944\n",
      "epoch: 1 step: 1648, loss is 0.02769140899181366\n",
      "epoch: 1 step: 1649, loss is 0.133492574095726\n",
      "epoch: 1 step: 1650, loss is 0.03821537271142006\n",
      "epoch: 1 step: 1651, loss is 0.05831072852015495\n",
      "epoch: 1 step: 1652, loss is 0.1124938353896141\n",
      "epoch: 1 step: 1653, loss is 0.17914997041225433\n",
      "epoch: 1 step: 1654, loss is 0.07979666441679001\n",
      "epoch: 1 step: 1655, loss is 0.08159682154655457\n",
      "epoch: 1 step: 1656, loss is 0.159079447388649\n",
      "epoch: 1 step: 1657, loss is 0.25998654961586\n",
      "epoch: 1 step: 1658, loss is 0.24623587727546692\n",
      "epoch: 1 step: 1659, loss is 0.07254887372255325\n",
      "epoch: 1 step: 1660, loss is 0.08658942580223083\n",
      "epoch: 1 step: 1661, loss is 0.13012303411960602\n",
      "epoch: 1 step: 1662, loss is 0.12030701339244843\n",
      "epoch: 1 step: 1663, loss is 0.02613566443324089\n",
      "epoch: 1 step: 1664, loss is 0.023996368050575256\n",
      "epoch: 1 step: 1665, loss is 0.035695817321538925\n",
      "epoch: 1 step: 1666, loss is 0.33635586500167847\n",
      "epoch: 1 step: 1667, loss is 0.05374031141400337\n",
      "epoch: 1 step: 1668, loss is 0.14557188749313354\n",
      "epoch: 1 step: 1669, loss is 0.5133905410766602\n",
      "epoch: 1 step: 1670, loss is 0.4544713497161865\n",
      "epoch: 1 step: 1671, loss is 0.0656244233250618\n",
      "epoch: 1 step: 1672, loss is 0.022678451612591743\n",
      "epoch: 1 step: 1673, loss is 0.26488006114959717\n",
      "epoch: 1 step: 1674, loss is 0.031991809606552124\n",
      "epoch: 1 step: 1675, loss is 0.383568674325943\n",
      "epoch: 1 step: 1676, loss is 0.07070465385913849\n",
      "epoch: 1 step: 1677, loss is 0.11592376977205276\n",
      "epoch: 1 step: 1678, loss is 0.08843336999416351\n",
      "epoch: 1 step: 1679, loss is 0.13196143507957458\n",
      "epoch: 1 step: 1680, loss is 0.0834246352314949\n",
      "epoch: 1 step: 1681, loss is 0.03181351348757744\n",
      "epoch: 1 step: 1682, loss is 0.10477960109710693\n",
      "epoch: 1 step: 1683, loss is 0.11453251540660858\n",
      "epoch: 1 step: 1684, loss is 0.36490923166275024\n",
      "epoch: 1 step: 1685, loss is 0.166168212890625\n",
      "epoch: 1 step: 1686, loss is 0.411632776260376\n",
      "epoch: 1 step: 1687, loss is 0.07351271063089371\n",
      "epoch: 1 step: 1688, loss is 0.0404619462788105\n",
      "epoch: 1 step: 1689, loss is 0.08195903897285461\n",
      "epoch: 1 step: 1690, loss is 0.3396468758583069\n",
      "epoch: 1 step: 1691, loss is 0.2450057715177536\n",
      "epoch: 1 step: 1692, loss is 0.026778046041727066\n",
      "epoch: 1 step: 1693, loss is 0.0964922234416008\n",
      "epoch: 1 step: 1694, loss is 0.17786060273647308\n",
      "epoch: 1 step: 1695, loss is 0.32199394702911377\n",
      "epoch: 1 step: 1696, loss is 0.09136945754289627\n",
      "epoch: 1 step: 1697, loss is 0.13085083663463593\n",
      "epoch: 1 step: 1698, loss is 0.11846065521240234\n",
      "epoch: 1 step: 1699, loss is 0.1685158610343933\n",
      "epoch: 1 step: 1700, loss is 0.04575805366039276\n",
      "epoch: 1 step: 1701, loss is 0.14485332369804382\n",
      "epoch: 1 step: 1702, loss is 0.09840986132621765\n",
      "epoch: 1 step: 1703, loss is 0.08097246289253235\n",
      "epoch: 1 step: 1704, loss is 0.38343095779418945\n",
      "epoch: 1 step: 1705, loss is 0.1524266004562378\n",
      "epoch: 1 step: 1706, loss is 0.20235441625118256\n",
      "epoch: 1 step: 1707, loss is 0.19162946939468384\n",
      "epoch: 1 step: 1708, loss is 0.2472964972257614\n",
      "epoch: 1 step: 1709, loss is 0.19470810890197754\n",
      "epoch: 1 step: 1710, loss is 0.1662956029176712\n",
      "epoch: 1 step: 1711, loss is 0.028446299955248833\n",
      "epoch: 1 step: 1712, loss is 0.018022548407316208\n",
      "epoch: 1 step: 1713, loss is 0.10169928520917892\n",
      "epoch: 1 step: 1714, loss is 0.2083432972431183\n",
      "epoch: 1 step: 1715, loss is 0.13593968749046326\n",
      "epoch: 1 step: 1716, loss is 0.07302803546190262\n",
      "epoch: 1 step: 1717, loss is 0.07012233883142471\n",
      "epoch: 1 step: 1718, loss is 0.1459612250328064\n",
      "epoch: 1 step: 1719, loss is 0.21893097460269928\n",
      "epoch: 1 step: 1720, loss is 0.017978588119149208\n",
      "epoch: 1 step: 1721, loss is 0.053903933614492416\n",
      "epoch: 1 step: 1722, loss is 0.04539259895682335\n",
      "epoch: 1 step: 1723, loss is 0.05089345946907997\n",
      "epoch: 1 step: 1724, loss is 0.08476398885250092\n",
      "epoch: 1 step: 1725, loss is 0.383515864610672\n",
      "epoch: 1 step: 1726, loss is 0.19808942079544067\n",
      "epoch: 1 step: 1727, loss is 0.24235504865646362\n",
      "epoch: 1 step: 1728, loss is 0.18094171583652496\n",
      "epoch: 1 step: 1729, loss is 0.10921759158372879\n",
      "epoch: 1 step: 1730, loss is 0.26350218057632446\n",
      "epoch: 1 step: 1731, loss is 0.03926611319184303\n",
      "epoch: 1 step: 1732, loss is 0.4852282702922821\n",
      "epoch: 1 step: 1733, loss is 0.16520541906356812\n",
      "epoch: 1 step: 1734, loss is 0.1839708834886551\n",
      "epoch: 1 step: 1735, loss is 0.09501729160547256\n",
      "epoch: 1 step: 1736, loss is 0.028853889554739\n",
      "epoch: 1 step: 1737, loss is 0.3679567575454712\n",
      "epoch: 1 step: 1738, loss is 0.009257431142032146\n",
      "epoch: 1 step: 1739, loss is 0.047796010971069336\n",
      "epoch: 1 step: 1740, loss is 0.18651489913463593\n",
      "epoch: 1 step: 1741, loss is 0.07713324576616287\n",
      "epoch: 1 step: 1742, loss is 0.09243126958608627\n",
      "epoch: 1 step: 1743, loss is 0.11920934170484543\n",
      "epoch: 1 step: 1744, loss is 0.16138134896755219\n",
      "epoch: 1 step: 1745, loss is 0.034892488270998\n",
      "epoch: 1 step: 1746, loss is 0.026283681392669678\n",
      "epoch: 1 step: 1747, loss is 0.11808976531028748\n",
      "epoch: 1 step: 1748, loss is 0.11064405739307404\n",
      "epoch: 1 step: 1749, loss is 0.2162652313709259\n",
      "epoch: 1 step: 1750, loss is 0.22888357937335968\n",
      "epoch: 1 step: 1751, loss is 0.036805011332035065\n",
      "epoch: 1 step: 1752, loss is 0.04989498481154442\n",
      "epoch: 1 step: 1753, loss is 0.10511470586061478\n",
      "epoch: 1 step: 1754, loss is 0.3651694059371948\n",
      "epoch: 1 step: 1755, loss is 0.30571213364601135\n",
      "epoch: 1 step: 1756, loss is 0.017214329913258553\n",
      "epoch: 1 step: 1757, loss is 0.1071985512971878\n",
      "epoch: 1 step: 1758, loss is 0.44040170311927795\n",
      "epoch: 1 step: 1759, loss is 0.2537999153137207\n",
      "epoch: 1 step: 1760, loss is 0.13303931057453156\n",
      "epoch: 1 step: 1761, loss is 0.01323216874152422\n",
      "epoch: 1 step: 1762, loss is 0.1842753142118454\n",
      "epoch: 1 step: 1763, loss is 0.18742375075817108\n",
      "epoch: 1 step: 1764, loss is 0.2344660758972168\n",
      "epoch: 1 step: 1765, loss is 0.026880521327257156\n",
      "epoch: 1 step: 1766, loss is 0.16184234619140625\n",
      "epoch: 1 step: 1767, loss is 0.15816465020179749\n",
      "epoch: 1 step: 1768, loss is 0.056429702788591385\n",
      "epoch: 1 step: 1769, loss is 0.502747654914856\n",
      "epoch: 1 step: 1770, loss is 0.2825222313404083\n",
      "epoch: 1 step: 1771, loss is 0.0844331681728363\n",
      "epoch: 1 step: 1772, loss is 0.13334283232688904\n",
      "epoch: 1 step: 1773, loss is 0.21577417850494385\n",
      "epoch: 1 step: 1774, loss is 0.34159788489341736\n",
      "epoch: 1 step: 1775, loss is 0.14025713503360748\n",
      "epoch: 1 step: 1776, loss is 0.29607322812080383\n",
      "epoch: 1 step: 1777, loss is 0.09622752666473389\n",
      "epoch: 1 step: 1778, loss is 0.05186329409480095\n",
      "epoch: 1 step: 1779, loss is 0.08678816258907318\n",
      "epoch: 1 step: 1780, loss is 0.029022015631198883\n",
      "epoch: 1 step: 1781, loss is 0.06292606890201569\n",
      "epoch: 1 step: 1782, loss is 0.20689187943935394\n",
      "epoch: 1 step: 1783, loss is 0.051712244749069214\n",
      "epoch: 1 step: 1784, loss is 0.23702657222747803\n",
      "epoch: 1 step: 1785, loss is 0.021595211699604988\n",
      "epoch: 1 step: 1786, loss is 0.13773401081562042\n",
      "epoch: 1 step: 1787, loss is 0.038966309279203415\n",
      "epoch: 1 step: 1788, loss is 0.09056778252124786\n",
      "epoch: 1 step: 1789, loss is 0.0453316867351532\n",
      "epoch: 1 step: 1790, loss is 0.03458603471517563\n",
      "epoch: 1 step: 1791, loss is 0.1579301506280899\n",
      "epoch: 1 step: 1792, loss is 0.13722039759159088\n",
      "epoch: 1 step: 1793, loss is 0.0921792984008789\n",
      "epoch: 1 step: 1794, loss is 0.10805123299360275\n",
      "epoch: 1 step: 1795, loss is 0.48564761877059937\n",
      "epoch: 1 step: 1796, loss is 0.2069379985332489\n",
      "epoch: 1 step: 1797, loss is 0.25953251123428345\n",
      "epoch: 1 step: 1798, loss is 0.09295882284641266\n",
      "epoch: 1 step: 1799, loss is 0.16103176772594452\n",
      "epoch: 1 step: 1800, loss is 0.08028297871351242\n",
      "epoch: 1 step: 1801, loss is 0.19241863489151\n",
      "epoch: 1 step: 1802, loss is 0.1279144287109375\n",
      "epoch: 1 step: 1803, loss is 0.022555438801646233\n",
      "epoch: 1 step: 1804, loss is 0.4468143582344055\n",
      "epoch: 1 step: 1805, loss is 0.15506090223789215\n",
      "epoch: 1 step: 1806, loss is 0.08033563196659088\n",
      "epoch: 1 step: 1807, loss is 0.18918095529079437\n",
      "epoch: 1 step: 1808, loss is 0.15320946276187897\n",
      "epoch: 1 step: 1809, loss is 0.15007063746452332\n",
      "epoch: 1 step: 1810, loss is 0.1840062290430069\n",
      "epoch: 1 step: 1811, loss is 0.17569568753242493\n",
      "epoch: 1 step: 1812, loss is 0.03995836153626442\n",
      "epoch: 1 step: 1813, loss is 0.06205599755048752\n",
      "epoch: 1 step: 1814, loss is 0.004147329833358526\n",
      "epoch: 1 step: 1815, loss is 0.1904400736093521\n",
      "epoch: 1 step: 1816, loss is 0.13190732896327972\n",
      "epoch: 1 step: 1817, loss is 0.14528921246528625\n",
      "epoch: 1 step: 1818, loss is 0.3550068438053131\n",
      "epoch: 1 step: 1819, loss is 0.1401543915271759\n",
      "epoch: 1 step: 1820, loss is 0.11315653473138809\n",
      "epoch: 1 step: 1821, loss is 0.09836497902870178\n",
      "epoch: 1 step: 1822, loss is 0.09779535979032516\n",
      "epoch: 1 step: 1823, loss is 0.01743777096271515\n",
      "epoch: 1 step: 1824, loss is 0.06940120458602905\n",
      "epoch: 1 step: 1825, loss is 0.20619042217731476\n",
      "epoch: 1 step: 1826, loss is 0.1388455480337143\n",
      "epoch: 1 step: 1827, loss is 0.06267719715833664\n",
      "epoch: 1 step: 1828, loss is 0.04618866741657257\n",
      "epoch: 1 step: 1829, loss is 0.05030664801597595\n",
      "epoch: 1 step: 1830, loss is 0.30074232816696167\n",
      "epoch: 1 step: 1831, loss is 0.02174433134496212\n",
      "epoch: 1 step: 1832, loss is 0.09359979629516602\n",
      "epoch: 1 step: 1833, loss is 0.2294265776872635\n",
      "epoch: 1 step: 1834, loss is 0.08890695124864578\n",
      "epoch: 1 step: 1835, loss is 0.2083563357591629\n",
      "epoch: 1 step: 1836, loss is 0.12962749600410461\n",
      "epoch: 1 step: 1837, loss is 0.08092599362134933\n",
      "epoch: 1 step: 1838, loss is 0.13411222398281097\n",
      "epoch: 1 step: 1839, loss is 0.00693712942302227\n",
      "epoch: 1 step: 1840, loss is 0.10775548964738846\n",
      "epoch: 1 step: 1841, loss is 0.09015747159719467\n",
      "epoch: 1 step: 1842, loss is 0.020786253735423088\n",
      "epoch: 1 step: 1843, loss is 0.04840793460607529\n",
      "epoch: 1 step: 1844, loss is 0.1573457568883896\n",
      "epoch: 1 step: 1845, loss is 0.05207490175962448\n",
      "epoch: 1 step: 1846, loss is 0.0922837182879448\n",
      "epoch: 1 step: 1847, loss is 0.05586899071931839\n",
      "epoch: 1 step: 1848, loss is 0.23902364075183868\n",
      "epoch: 1 step: 1849, loss is 0.027903061360120773\n",
      "epoch: 1 step: 1850, loss is 0.04814494401216507\n",
      "epoch: 1 step: 1851, loss is 0.22875195741653442\n",
      "epoch: 1 step: 1852, loss is 0.23679591715335846\n",
      "epoch: 1 step: 1853, loss is 0.04136866331100464\n",
      "epoch: 1 step: 1854, loss is 0.11891092360019684\n",
      "epoch: 1 step: 1855, loss is 0.033275067806243896\n",
      "epoch: 1 step: 1856, loss is 0.2112492173910141\n",
      "epoch: 1 step: 1857, loss is 0.22989065945148468\n",
      "epoch: 1 step: 1858, loss is 0.29462501406669617\n",
      "epoch: 1 step: 1859, loss is 0.0806928351521492\n",
      "epoch: 1 step: 1860, loss is 0.1266287863254547\n",
      "epoch: 1 step: 1861, loss is 0.15582707524299622\n",
      "epoch: 1 step: 1862, loss is 0.06858383119106293\n",
      "epoch: 1 step: 1863, loss is 0.03215444087982178\n",
      "epoch: 1 step: 1864, loss is 0.027486376464366913\n",
      "epoch: 1 step: 1865, loss is 0.05907610058784485\n",
      "epoch: 1 step: 1866, loss is 0.12894397974014282\n",
      "epoch: 1 step: 1867, loss is 0.17921839654445648\n",
      "epoch: 1 step: 1868, loss is 0.019235610961914062\n",
      "epoch: 1 step: 1869, loss is 0.08925797790288925\n",
      "epoch: 1 step: 1870, loss is 0.16588912904262543\n",
      "epoch: 1 step: 1871, loss is 0.3586597740650177\n",
      "epoch: 1 step: 1872, loss is 0.05807949975132942\n",
      "epoch: 1 step: 1873, loss is 0.15681734681129456\n",
      "epoch: 1 step: 1874, loss is 0.0248939860612154\n",
      "epoch: 1 step: 1875, loss is 0.19500282406806946\n",
      "epoch: 2 step: 1, loss is 0.20223961770534515\n",
      "epoch: 2 step: 2, loss is 0.09933218359947205\n",
      "epoch: 2 step: 3, loss is 0.0980556309223175\n",
      "epoch: 2 step: 4, loss is 0.020057525485754013\n",
      "epoch: 2 step: 5, loss is 0.007777467370033264\n",
      "epoch: 2 step: 6, loss is 0.2707347273826599\n",
      "epoch: 2 step: 7, loss is 0.19703316688537598\n",
      "epoch: 2 step: 8, loss is 0.21641768515110016\n",
      "epoch: 2 step: 9, loss is 0.1340024620294571\n",
      "epoch: 2 step: 10, loss is 0.09176840633153915\n",
      "epoch: 2 step: 11, loss is 0.106014184653759\n",
      "epoch: 2 step: 12, loss is 0.04076140373945236\n",
      "epoch: 2 step: 13, loss is 0.1534789353609085\n",
      "epoch: 2 step: 14, loss is 0.30180594325065613\n",
      "epoch: 2 step: 15, loss is 0.02402512915432453\n",
      "epoch: 2 step: 16, loss is 0.11947918683290482\n",
      "epoch: 2 step: 17, loss is 0.12177832424640656\n",
      "epoch: 2 step: 18, loss is 0.10096436738967896\n",
      "epoch: 2 step: 19, loss is 0.059547483921051025\n",
      "epoch: 2 step: 20, loss is 0.048523467034101486\n",
      "epoch: 2 step: 21, loss is 0.16700014472007751\n",
      "epoch: 2 step: 22, loss is 0.2029649019241333\n",
      "epoch: 2 step: 23, loss is 0.4432782828807831\n",
      "epoch: 2 step: 24, loss is 0.24806855618953705\n",
      "epoch: 2 step: 25, loss is 0.1886189579963684\n",
      "epoch: 2 step: 26, loss is 0.07459262758493423\n",
      "epoch: 2 step: 27, loss is 0.2510887384414673\n",
      "epoch: 2 step: 28, loss is 0.12976278364658356\n",
      "epoch: 2 step: 29, loss is 0.1932186633348465\n",
      "epoch: 2 step: 30, loss is 0.020013567060232162\n",
      "epoch: 2 step: 31, loss is 0.06256243586540222\n",
      "epoch: 2 step: 32, loss is 0.1451384574174881\n",
      "epoch: 2 step: 33, loss is 0.07454698532819748\n",
      "epoch: 2 step: 34, loss is 0.2578281760215759\n",
      "epoch: 2 step: 35, loss is 0.11494752764701843\n",
      "epoch: 2 step: 36, loss is 0.0259426087141037\n",
      "epoch: 2 step: 37, loss is 0.04542142152786255\n",
      "epoch: 2 step: 38, loss is 0.03844078257679939\n",
      "epoch: 2 step: 39, loss is 0.03412064164876938\n",
      "epoch: 2 step: 40, loss is 0.022289259359240532\n",
      "epoch: 2 step: 41, loss is 0.02551862969994545\n",
      "epoch: 2 step: 42, loss is 0.05658731237053871\n",
      "epoch: 2 step: 43, loss is 0.044559068977832794\n",
      "epoch: 2 step: 44, loss is 0.06710132956504822\n",
      "epoch: 2 step: 45, loss is 0.2818380892276764\n",
      "epoch: 2 step: 46, loss is 0.2036333829164505\n",
      "epoch: 2 step: 47, loss is 0.2594926059246063\n",
      "epoch: 2 step: 48, loss is 0.16307829320430756\n",
      "epoch: 2 step: 49, loss is 0.034667737782001495\n",
      "epoch: 2 step: 50, loss is 0.018032893538475037\n",
      "epoch: 2 step: 51, loss is 0.055438823997974396\n",
      "epoch: 2 step: 52, loss is 0.18529334664344788\n",
      "epoch: 2 step: 53, loss is 0.4104820191860199\n",
      "epoch: 2 step: 54, loss is 0.18175561726093292\n",
      "epoch: 2 step: 55, loss is 0.014220567420125008\n",
      "epoch: 2 step: 56, loss is 0.20359255373477936\n",
      "epoch: 2 step: 57, loss is 0.04023440554738045\n",
      "epoch: 2 step: 58, loss is 0.11543513834476471\n",
      "epoch: 2 step: 59, loss is 0.26092034578323364\n",
      "epoch: 2 step: 60, loss is 0.10466686636209488\n",
      "epoch: 2 step: 61, loss is 0.4181831181049347\n",
      "epoch: 2 step: 62, loss is 0.06319606304168701\n",
      "epoch: 2 step: 63, loss is 0.20159873366355896\n",
      "epoch: 2 step: 64, loss is 0.35375985503196716\n",
      "epoch: 2 step: 65, loss is 0.16610850393772125\n",
      "epoch: 2 step: 66, loss is 0.1132412701845169\n",
      "epoch: 2 step: 67, loss is 0.046500060707330704\n",
      "epoch: 2 step: 68, loss is 0.40761563181877136\n",
      "epoch: 2 step: 69, loss is 0.1900113821029663\n",
      "epoch: 2 step: 70, loss is 0.05563865229487419\n",
      "epoch: 2 step: 71, loss is 0.008634923957288265\n",
      "epoch: 2 step: 72, loss is 0.052367374300956726\n",
      "epoch: 2 step: 73, loss is 0.11325705051422119\n",
      "epoch: 2 step: 74, loss is 0.04640521481633186\n",
      "epoch: 2 step: 75, loss is 0.2819342315196991\n",
      "epoch: 2 step: 76, loss is 0.02808338589966297\n",
      "epoch: 2 step: 77, loss is 0.12272950261831284\n",
      "epoch: 2 step: 78, loss is 0.4147908389568329\n",
      "epoch: 2 step: 79, loss is 0.008381243795156479\n",
      "epoch: 2 step: 80, loss is 0.015124143101274967\n",
      "epoch: 2 step: 81, loss is 0.12130668759346008\n",
      "epoch: 2 step: 82, loss is 0.2527032792568207\n",
      "epoch: 2 step: 83, loss is 0.01923823542892933\n",
      "epoch: 2 step: 84, loss is 0.07508369535207748\n",
      "epoch: 2 step: 85, loss is 0.04373148828744888\n",
      "epoch: 2 step: 86, loss is 0.07630103081464767\n",
      "epoch: 2 step: 87, loss is 0.2047537863254547\n",
      "epoch: 2 step: 88, loss is 0.1408025175333023\n",
      "epoch: 2 step: 89, loss is 0.12813085317611694\n",
      "epoch: 2 step: 90, loss is 0.1358639895915985\n",
      "epoch: 2 step: 91, loss is 0.09706030040979385\n",
      "epoch: 2 step: 92, loss is 0.17056728899478912\n",
      "epoch: 2 step: 93, loss is 0.13186928629875183\n",
      "epoch: 2 step: 94, loss is 0.14937862753868103\n",
      "epoch: 2 step: 95, loss is 0.020531902089715004\n",
      "epoch: 2 step: 96, loss is 0.021514220163226128\n",
      "epoch: 2 step: 97, loss is 0.10022032260894775\n",
      "epoch: 2 step: 98, loss is 0.11589793115854263\n",
      "epoch: 2 step: 99, loss is 0.1572100967168808\n",
      "epoch: 2 step: 100, loss is 0.10623142868280411\n",
      "epoch: 2 step: 101, loss is 0.4121476411819458\n",
      "epoch: 2 step: 102, loss is 0.05603460967540741\n",
      "epoch: 2 step: 103, loss is 0.07028563320636749\n",
      "epoch: 2 step: 104, loss is 0.0477454699575901\n",
      "epoch: 2 step: 105, loss is 0.02101888507604599\n",
      "epoch: 2 step: 106, loss is 0.20051288604736328\n",
      "epoch: 2 step: 107, loss is 0.01507176086306572\n",
      "epoch: 2 step: 108, loss is 0.14693988859653473\n",
      "epoch: 2 step: 109, loss is 0.029542457312345505\n",
      "epoch: 2 step: 110, loss is 0.029322393238544464\n",
      "epoch: 2 step: 111, loss is 0.017515692859888077\n",
      "epoch: 2 step: 112, loss is 0.011481568217277527\n",
      "epoch: 2 step: 113, loss is 0.04500050097703934\n",
      "epoch: 2 step: 114, loss is 0.05515015497803688\n",
      "epoch: 2 step: 115, loss is 0.3297664523124695\n",
      "epoch: 2 step: 116, loss is 0.24420996010303497\n",
      "epoch: 2 step: 117, loss is 0.08034731447696686\n",
      "epoch: 2 step: 118, loss is 0.08911427855491638\n",
      "epoch: 2 step: 119, loss is 0.012067107483744621\n",
      "epoch: 2 step: 120, loss is 0.02673845738172531\n",
      "epoch: 2 step: 121, loss is 0.1911989450454712\n",
      "epoch: 2 step: 122, loss is 0.06795661896467209\n",
      "epoch: 2 step: 123, loss is 0.01773640885949135\n",
      "epoch: 2 step: 124, loss is 0.1467399299144745\n",
      "epoch: 2 step: 125, loss is 0.144692525267601\n",
      "epoch: 2 step: 126, loss is 0.26157718896865845\n",
      "epoch: 2 step: 127, loss is 0.014379056170582771\n",
      "epoch: 2 step: 128, loss is 0.1422663927078247\n",
      "epoch: 2 step: 129, loss is 0.15936492383480072\n",
      "epoch: 2 step: 130, loss is 0.023957492783665657\n",
      "epoch: 2 step: 131, loss is 0.2262880504131317\n",
      "epoch: 2 step: 132, loss is 0.059278495609760284\n",
      "epoch: 2 step: 133, loss is 0.17434538900852203\n",
      "epoch: 2 step: 134, loss is 0.24877119064331055\n",
      "epoch: 2 step: 135, loss is 0.036179568618535995\n",
      "epoch: 2 step: 136, loss is 0.1855139583349228\n",
      "epoch: 2 step: 137, loss is 0.11936163902282715\n",
      "epoch: 2 step: 138, loss is 0.06372981518507004\n",
      "epoch: 2 step: 139, loss is 0.07398898154497147\n",
      "epoch: 2 step: 140, loss is 0.04330703243613243\n",
      "epoch: 2 step: 141, loss is 0.21465061604976654\n",
      "epoch: 2 step: 142, loss is 0.1254560500383377\n",
      "epoch: 2 step: 143, loss is 0.03799910470843315\n",
      "epoch: 2 step: 144, loss is 0.02230357751250267\n",
      "epoch: 2 step: 145, loss is 0.07659083604812622\n",
      "epoch: 2 step: 146, loss is 0.15799684822559357\n",
      "epoch: 2 step: 147, loss is 0.22405679523944855\n",
      "epoch: 2 step: 148, loss is 0.007966465316712856\n",
      "epoch: 2 step: 149, loss is 0.09969405084848404\n",
      "epoch: 2 step: 150, loss is 0.07813799381256104\n",
      "epoch: 2 step: 151, loss is 0.03034355491399765\n",
      "epoch: 2 step: 152, loss is 0.20467516779899597\n",
      "epoch: 2 step: 153, loss is 0.13497039675712585\n",
      "epoch: 2 step: 154, loss is 0.0903325229883194\n",
      "epoch: 2 step: 155, loss is 0.11552984267473221\n",
      "epoch: 2 step: 156, loss is 0.08522233366966248\n",
      "epoch: 2 step: 157, loss is 0.04672294110059738\n",
      "epoch: 2 step: 158, loss is 0.011071819812059402\n",
      "epoch: 2 step: 159, loss is 0.18224342167377472\n",
      "epoch: 2 step: 160, loss is 0.18487516045570374\n",
      "epoch: 2 step: 161, loss is 0.10792192071676254\n",
      "epoch: 2 step: 162, loss is 0.04177340865135193\n",
      "epoch: 2 step: 163, loss is 0.05923487991094589\n",
      "epoch: 2 step: 164, loss is 0.009031268768012524\n",
      "epoch: 2 step: 165, loss is 0.010997790843248367\n",
      "epoch: 2 step: 166, loss is 0.014253349043428898\n",
      "epoch: 2 step: 167, loss is 0.07607264071702957\n",
      "epoch: 2 step: 168, loss is 0.10375544428825378\n",
      "epoch: 2 step: 169, loss is 0.44166454672813416\n",
      "epoch: 2 step: 170, loss is 0.15582579374313354\n",
      "epoch: 2 step: 171, loss is 0.012107530608773232\n",
      "epoch: 2 step: 172, loss is 0.13568229973316193\n",
      "epoch: 2 step: 173, loss is 0.3296489715576172\n",
      "epoch: 2 step: 174, loss is 0.15437646210193634\n",
      "epoch: 2 step: 175, loss is 0.04968123883008957\n",
      "epoch: 2 step: 176, loss is 0.10836643725633621\n",
      "epoch: 2 step: 177, loss is 0.044501133263111115\n",
      "epoch: 2 step: 178, loss is 0.21323378384113312\n",
      "epoch: 2 step: 179, loss is 0.10180208086967468\n",
      "epoch: 2 step: 180, loss is 0.18631747364997864\n",
      "epoch: 2 step: 181, loss is 0.014574122615158558\n",
      "epoch: 2 step: 182, loss is 0.15160568058490753\n",
      "epoch: 2 step: 183, loss is 0.03415747359395027\n",
      "epoch: 2 step: 184, loss is 0.22581607103347778\n",
      "epoch: 2 step: 185, loss is 0.1296413391828537\n",
      "epoch: 2 step: 186, loss is 0.03164128214120865\n",
      "epoch: 2 step: 187, loss is 0.03751753270626068\n",
      "epoch: 2 step: 188, loss is 0.04657471179962158\n",
      "epoch: 2 step: 189, loss is 0.14137758314609528\n",
      "epoch: 2 step: 190, loss is 0.3145788908004761\n",
      "epoch: 2 step: 191, loss is 0.09398289769887924\n",
      "epoch: 2 step: 192, loss is 0.08874567598104477\n",
      "epoch: 2 step: 193, loss is 0.013482559472322464\n",
      "epoch: 2 step: 194, loss is 0.1598615050315857\n",
      "epoch: 2 step: 195, loss is 0.02654171548783779\n",
      "epoch: 2 step: 196, loss is 0.09855302423238754\n",
      "epoch: 2 step: 197, loss is 0.04931648075580597\n",
      "epoch: 2 step: 198, loss is 0.08897464722394943\n",
      "epoch: 2 step: 199, loss is 0.13218437135219574\n",
      "epoch: 2 step: 200, loss is 0.11033961921930313\n",
      "epoch: 2 step: 201, loss is 0.08788211643695831\n",
      "epoch: 2 step: 202, loss is 0.15412728488445282\n",
      "epoch: 2 step: 203, loss is 0.1068972498178482\n",
      "epoch: 2 step: 204, loss is 0.024224048480391502\n",
      "epoch: 2 step: 205, loss is 0.035853948444128036\n",
      "epoch: 2 step: 206, loss is 0.04636027663946152\n",
      "epoch: 2 step: 207, loss is 0.08560686558485031\n",
      "epoch: 2 step: 208, loss is 0.28211379051208496\n",
      "epoch: 2 step: 209, loss is 0.060087233781814575\n",
      "epoch: 2 step: 210, loss is 0.02071935310959816\n",
      "epoch: 2 step: 211, loss is 0.17070485651493073\n",
      "epoch: 2 step: 212, loss is 0.13185106217861176\n",
      "epoch: 2 step: 213, loss is 0.30294671654701233\n",
      "epoch: 2 step: 214, loss is 0.04028952121734619\n",
      "epoch: 2 step: 215, loss is 0.053906895220279694\n",
      "epoch: 2 step: 216, loss is 0.31248971819877625\n",
      "epoch: 2 step: 217, loss is 0.1659487634897232\n",
      "epoch: 2 step: 218, loss is 0.13798384368419647\n",
      "epoch: 2 step: 219, loss is 0.13536672294139862\n",
      "epoch: 2 step: 220, loss is 0.10468518733978271\n",
      "epoch: 2 step: 221, loss is 0.03429318591952324\n",
      "epoch: 2 step: 222, loss is 0.10986549407243729\n",
      "epoch: 2 step: 223, loss is 0.0988883525133133\n",
      "epoch: 2 step: 224, loss is 0.15234029293060303\n",
      "epoch: 2 step: 225, loss is 0.34606093168258667\n",
      "epoch: 2 step: 226, loss is 0.13074330985546112\n",
      "epoch: 2 step: 227, loss is 0.16354863345623016\n",
      "epoch: 2 step: 228, loss is 0.1345645636320114\n",
      "epoch: 2 step: 229, loss is 0.12186216562986374\n",
      "epoch: 2 step: 230, loss is 0.20473173260688782\n",
      "epoch: 2 step: 231, loss is 0.05807791277766228\n",
      "epoch: 2 step: 232, loss is 0.2193807065486908\n",
      "epoch: 2 step: 233, loss is 0.015522850677371025\n",
      "epoch: 2 step: 234, loss is 0.2211129367351532\n",
      "epoch: 2 step: 235, loss is 0.17630165815353394\n",
      "epoch: 2 step: 236, loss is 0.3195796608924866\n",
      "epoch: 2 step: 237, loss is 0.023160671815276146\n",
      "epoch: 2 step: 238, loss is 0.03446119651198387\n",
      "epoch: 2 step: 239, loss is 0.04588167369365692\n",
      "epoch: 2 step: 240, loss is 0.1600085198879242\n",
      "epoch: 2 step: 241, loss is 0.06508588045835495\n",
      "epoch: 2 step: 242, loss is 0.03085598349571228\n",
      "epoch: 2 step: 243, loss is 0.29042157530784607\n",
      "epoch: 2 step: 244, loss is 0.05010104179382324\n",
      "epoch: 2 step: 245, loss is 0.024477310478687286\n",
      "epoch: 2 step: 246, loss is 0.08489444106817245\n",
      "epoch: 2 step: 247, loss is 0.022856060415506363\n",
      "epoch: 2 step: 248, loss is 0.29474425315856934\n",
      "epoch: 2 step: 249, loss is 0.07753358781337738\n",
      "epoch: 2 step: 250, loss is 0.3071655035018921\n",
      "epoch: 2 step: 251, loss is 0.018522480502724648\n",
      "epoch: 2 step: 252, loss is 0.10062777996063232\n",
      "epoch: 2 step: 253, loss is 0.07438291609287262\n",
      "epoch: 2 step: 254, loss is 0.04637496545910835\n",
      "epoch: 2 step: 255, loss is 0.05050981044769287\n",
      "epoch: 2 step: 256, loss is 0.035822391510009766\n",
      "epoch: 2 step: 257, loss is 0.017861099913716316\n",
      "epoch: 2 step: 258, loss is 0.18757738173007965\n",
      "epoch: 2 step: 259, loss is 0.15278270840644836\n",
      "epoch: 2 step: 260, loss is 0.32881343364715576\n",
      "epoch: 2 step: 261, loss is 0.06914398074150085\n",
      "epoch: 2 step: 262, loss is 0.10049180686473846\n",
      "epoch: 2 step: 263, loss is 0.19109366834163666\n",
      "epoch: 2 step: 264, loss is 0.03617408126592636\n",
      "epoch: 2 step: 265, loss is 0.1596839427947998\n",
      "epoch: 2 step: 266, loss is 0.0039099580608308315\n",
      "epoch: 2 step: 267, loss is 0.012802919372916222\n",
      "epoch: 2 step: 268, loss is 0.09154625982046127\n",
      "epoch: 2 step: 269, loss is 0.027527274563908577\n",
      "epoch: 2 step: 270, loss is 0.03723760321736336\n",
      "epoch: 2 step: 271, loss is 0.4799565076828003\n",
      "epoch: 2 step: 272, loss is 0.03348514437675476\n",
      "epoch: 2 step: 273, loss is 0.17703725397586823\n",
      "epoch: 2 step: 274, loss is 0.09735184907913208\n",
      "epoch: 2 step: 275, loss is 0.14013242721557617\n",
      "epoch: 2 step: 276, loss is 0.008094136603176594\n",
      "epoch: 2 step: 277, loss is 0.04860754311084747\n",
      "epoch: 2 step: 278, loss is 0.06117609143257141\n",
      "epoch: 2 step: 279, loss is 0.06438232213258743\n",
      "epoch: 2 step: 280, loss is 0.23380453884601593\n",
      "epoch: 2 step: 281, loss is 0.0526198111474514\n",
      "epoch: 2 step: 282, loss is 0.12354876101016998\n",
      "epoch: 2 step: 283, loss is 0.19653651118278503\n",
      "epoch: 2 step: 284, loss is 0.1250983327627182\n",
      "epoch: 2 step: 285, loss is 0.29579582810401917\n",
      "epoch: 2 step: 286, loss is 0.108185775578022\n",
      "epoch: 2 step: 287, loss is 0.03168641775846481\n",
      "epoch: 2 step: 288, loss is 0.11039090156555176\n",
      "epoch: 2 step: 289, loss is 0.022940954193472862\n",
      "epoch: 2 step: 290, loss is 0.07641864567995071\n",
      "epoch: 2 step: 291, loss is 0.10501746833324432\n",
      "epoch: 2 step: 292, loss is 0.13912688195705414\n",
      "epoch: 2 step: 293, loss is 0.11244678497314453\n",
      "epoch: 2 step: 294, loss is 0.014307878911495209\n",
      "epoch: 2 step: 295, loss is 0.10378804802894592\n",
      "epoch: 2 step: 296, loss is 0.2248178869485855\n",
      "epoch: 2 step: 297, loss is 0.2353697568178177\n",
      "epoch: 2 step: 298, loss is 0.044740717858076096\n",
      "epoch: 2 step: 299, loss is 0.0723142921924591\n",
      "epoch: 2 step: 300, loss is 0.13729824125766754\n",
      "epoch: 2 step: 301, loss is 0.15722894668579102\n",
      "epoch: 2 step: 302, loss is 0.10400346666574478\n",
      "epoch: 2 step: 303, loss is 0.12873348593711853\n",
      "epoch: 2 step: 304, loss is 0.0868016928434372\n",
      "epoch: 2 step: 305, loss is 0.04261630401015282\n",
      "epoch: 2 step: 306, loss is 0.40572959184646606\n",
      "epoch: 2 step: 307, loss is 0.03438205644488335\n",
      "epoch: 2 step: 308, loss is 0.009650520980358124\n",
      "epoch: 2 step: 309, loss is 0.15922018885612488\n",
      "epoch: 2 step: 310, loss is 0.04301047697663307\n",
      "epoch: 2 step: 311, loss is 0.1645478457212448\n",
      "epoch: 2 step: 312, loss is 0.20984899997711182\n",
      "epoch: 2 step: 313, loss is 0.3405027389526367\n",
      "epoch: 2 step: 314, loss is 0.03329093009233475\n",
      "epoch: 2 step: 315, loss is 0.11586856096982956\n",
      "epoch: 2 step: 316, loss is 0.17484377324581146\n",
      "epoch: 2 step: 317, loss is 0.11178073287010193\n",
      "epoch: 2 step: 318, loss is 0.08935020118951797\n",
      "epoch: 2 step: 319, loss is 0.007965866476297379\n",
      "epoch: 2 step: 320, loss is 0.18582645058631897\n",
      "epoch: 2 step: 321, loss is 0.3406814634799957\n",
      "epoch: 2 step: 322, loss is 0.08351694792509079\n",
      "epoch: 2 step: 323, loss is 0.1482483297586441\n",
      "epoch: 2 step: 324, loss is 0.026267286390066147\n",
      "epoch: 2 step: 325, loss is 0.03765339031815529\n",
      "epoch: 2 step: 326, loss is 0.18100936710834503\n",
      "epoch: 2 step: 327, loss is 0.062412943691015244\n",
      "epoch: 2 step: 328, loss is 0.2612391412258148\n",
      "epoch: 2 step: 329, loss is 0.3045694828033447\n",
      "epoch: 2 step: 330, loss is 0.017600039020180702\n",
      "epoch: 2 step: 331, loss is 0.15898561477661133\n",
      "epoch: 2 step: 332, loss is 0.31921422481536865\n",
      "epoch: 2 step: 333, loss is 0.291163831949234\n",
      "epoch: 2 step: 334, loss is 0.1378220170736313\n",
      "epoch: 2 step: 335, loss is 0.09730533510446548\n",
      "epoch: 2 step: 336, loss is 0.0440337099134922\n",
      "epoch: 2 step: 337, loss is 0.11996852606534958\n",
      "epoch: 2 step: 338, loss is 0.412191778421402\n",
      "epoch: 2 step: 339, loss is 0.060244057327508926\n",
      "epoch: 2 step: 340, loss is 0.1116969883441925\n",
      "epoch: 2 step: 341, loss is 0.19902560114860535\n",
      "epoch: 2 step: 342, loss is 0.026161188259720802\n",
      "epoch: 2 step: 343, loss is 0.22415460646152496\n",
      "epoch: 2 step: 344, loss is 0.045570824295282364\n",
      "epoch: 2 step: 345, loss is 0.10617698729038239\n",
      "epoch: 2 step: 346, loss is 0.1659630835056305\n",
      "epoch: 2 step: 347, loss is 0.0419234074652195\n",
      "epoch: 2 step: 348, loss is 0.2787884473800659\n",
      "epoch: 2 step: 349, loss is 0.14077192544937134\n",
      "epoch: 2 step: 350, loss is 0.10294236242771149\n",
      "epoch: 2 step: 351, loss is 0.01966581679880619\n",
      "epoch: 2 step: 352, loss is 0.194919154047966\n",
      "epoch: 2 step: 353, loss is 0.06786472350358963\n",
      "epoch: 2 step: 354, loss is 0.07958530634641647\n",
      "epoch: 2 step: 355, loss is 0.054118260741233826\n",
      "epoch: 2 step: 356, loss is 0.054056987166404724\n",
      "epoch: 2 step: 357, loss is 0.0322253592312336\n",
      "epoch: 2 step: 358, loss is 0.2798544764518738\n",
      "epoch: 2 step: 359, loss is 0.1689768135547638\n",
      "epoch: 2 step: 360, loss is 0.043231066316366196\n",
      "epoch: 2 step: 361, loss is 0.12157939374446869\n",
      "epoch: 2 step: 362, loss is 0.18298807740211487\n",
      "epoch: 2 step: 363, loss is 0.11047293245792389\n",
      "epoch: 2 step: 364, loss is 0.062290292233228683\n",
      "epoch: 2 step: 365, loss is 0.09139332175254822\n",
      "epoch: 2 step: 366, loss is 0.17250178754329681\n",
      "epoch: 2 step: 367, loss is 0.07080329954624176\n",
      "epoch: 2 step: 368, loss is 0.04381389543414116\n",
      "epoch: 2 step: 369, loss is 0.02880244515836239\n",
      "epoch: 2 step: 370, loss is 0.08169352263212204\n",
      "epoch: 2 step: 371, loss is 0.034531354904174805\n",
      "epoch: 2 step: 372, loss is 0.12159595638513565\n",
      "epoch: 2 step: 373, loss is 0.046794891357421875\n",
      "epoch: 2 step: 374, loss is 0.2906893193721771\n",
      "epoch: 2 step: 375, loss is 0.014261607080698013\n",
      "epoch: 2 step: 376, loss is 0.09308638423681259\n",
      "epoch: 2 step: 377, loss is 0.04985350742936134\n",
      "epoch: 2 step: 378, loss is 0.005378636997193098\n",
      "epoch: 2 step: 379, loss is 0.07366333901882172\n",
      "epoch: 2 step: 380, loss is 0.14169953763484955\n",
      "epoch: 2 step: 381, loss is 0.18318213522434235\n",
      "epoch: 2 step: 382, loss is 0.17365612089633942\n",
      "epoch: 2 step: 383, loss is 0.039946991950273514\n",
      "epoch: 2 step: 384, loss is 0.09216254204511642\n",
      "epoch: 2 step: 385, loss is 0.04588628560304642\n",
      "epoch: 2 step: 386, loss is 0.2037990242242813\n",
      "epoch: 2 step: 387, loss is 0.009017507545650005\n",
      "epoch: 2 step: 388, loss is 0.24273847043514252\n",
      "epoch: 2 step: 389, loss is 0.1901378184556961\n",
      "epoch: 2 step: 390, loss is 0.17213401198387146\n",
      "epoch: 2 step: 391, loss is 0.07160967588424683\n",
      "epoch: 2 step: 392, loss is 0.3333122730255127\n",
      "epoch: 2 step: 393, loss is 0.04334000498056412\n",
      "epoch: 2 step: 394, loss is 0.0934235081076622\n",
      "epoch: 2 step: 395, loss is 0.25750693678855896\n",
      "epoch: 2 step: 396, loss is 0.08930402994155884\n",
      "epoch: 2 step: 397, loss is 0.052749618887901306\n",
      "epoch: 2 step: 398, loss is 0.06449193507432938\n",
      "epoch: 2 step: 399, loss is 0.2995859682559967\n",
      "epoch: 2 step: 400, loss is 0.03731000795960426\n",
      "epoch: 2 step: 401, loss is 0.21415621042251587\n",
      "epoch: 2 step: 402, loss is 0.05967295169830322\n",
      "epoch: 2 step: 403, loss is 0.0211647879332304\n",
      "epoch: 2 step: 404, loss is 0.27310705184936523\n",
      "epoch: 2 step: 405, loss is 0.03851911053061485\n",
      "epoch: 2 step: 406, loss is 0.10142742842435837\n",
      "epoch: 2 step: 407, loss is 0.19488166272640228\n",
      "epoch: 2 step: 408, loss is 0.06602394580841064\n",
      "epoch: 2 step: 409, loss is 0.09049251675605774\n",
      "epoch: 2 step: 410, loss is 0.23951339721679688\n",
      "epoch: 2 step: 411, loss is 0.01727426052093506\n",
      "epoch: 2 step: 412, loss is 0.18666142225265503\n",
      "epoch: 2 step: 413, loss is 0.14876249432563782\n",
      "epoch: 2 step: 414, loss is 0.40563851594924927\n",
      "epoch: 2 step: 415, loss is 0.11860380321741104\n",
      "epoch: 2 step: 416, loss is 0.060979075729846954\n",
      "epoch: 2 step: 417, loss is 0.11858964711427689\n",
      "epoch: 2 step: 418, loss is 0.5055170655250549\n",
      "epoch: 2 step: 419, loss is 0.06674797832965851\n",
      "epoch: 2 step: 420, loss is 0.04112277552485466\n",
      "epoch: 2 step: 421, loss is 0.09692872315645218\n",
      "epoch: 2 step: 422, loss is 0.048593439161777496\n",
      "epoch: 2 step: 423, loss is 0.13449092209339142\n",
      "epoch: 2 step: 424, loss is 0.06437703967094421\n",
      "epoch: 2 step: 425, loss is 0.11239216476678848\n",
      "epoch: 2 step: 426, loss is 0.18178991973400116\n",
      "epoch: 2 step: 427, loss is 0.1609516739845276\n",
      "epoch: 2 step: 428, loss is 0.0351809523999691\n",
      "epoch: 2 step: 429, loss is 0.029769208282232285\n",
      "epoch: 2 step: 430, loss is 0.05416065827012062\n",
      "epoch: 2 step: 431, loss is 0.23634734749794006\n",
      "epoch: 2 step: 432, loss is 0.05237497389316559\n",
      "epoch: 2 step: 433, loss is 0.11616728454828262\n",
      "epoch: 2 step: 434, loss is 0.13258996605873108\n",
      "epoch: 2 step: 435, loss is 0.0890420526266098\n",
      "epoch: 2 step: 436, loss is 0.1657559722661972\n",
      "epoch: 2 step: 437, loss is 0.2907560169696808\n",
      "epoch: 2 step: 438, loss is 0.0206281878054142\n",
      "epoch: 2 step: 439, loss is 0.1541195511817932\n",
      "epoch: 2 step: 440, loss is 0.10367889702320099\n",
      "epoch: 2 step: 441, loss is 0.11355964094400406\n",
      "epoch: 2 step: 442, loss is 0.18329457938671112\n",
      "epoch: 2 step: 443, loss is 0.02542933262884617\n",
      "epoch: 2 step: 444, loss is 0.03233123570680618\n",
      "epoch: 2 step: 445, loss is 0.10116160660982132\n",
      "epoch: 2 step: 446, loss is 0.16750894486904144\n",
      "epoch: 2 step: 447, loss is 0.10020240396261215\n",
      "epoch: 2 step: 448, loss is 0.163308247923851\n",
      "epoch: 2 step: 449, loss is 0.08557183295488358\n",
      "epoch: 2 step: 450, loss is 0.19082927703857422\n",
      "epoch: 2 step: 451, loss is 0.04838280752301216\n",
      "epoch: 2 step: 452, loss is 0.21223631501197815\n",
      "epoch: 2 step: 453, loss is 0.19296227395534515\n",
      "epoch: 2 step: 454, loss is 0.10331961512565613\n",
      "epoch: 2 step: 455, loss is 0.10516471415758133\n",
      "epoch: 2 step: 456, loss is 0.1737212985754013\n",
      "epoch: 2 step: 457, loss is 0.05018024146556854\n",
      "epoch: 2 step: 458, loss is 0.3111256957054138\n",
      "epoch: 2 step: 459, loss is 0.23494213819503784\n",
      "epoch: 2 step: 460, loss is 0.01577521115541458\n",
      "epoch: 2 step: 461, loss is 0.041761912405490875\n",
      "epoch: 2 step: 462, loss is 0.14757418632507324\n",
      "epoch: 2 step: 463, loss is 0.16010832786560059\n",
      "epoch: 2 step: 464, loss is 0.07252234220504761\n",
      "epoch: 2 step: 465, loss is 0.13440187275409698\n",
      "epoch: 2 step: 466, loss is 0.02366732433438301\n",
      "epoch: 2 step: 467, loss is 0.053493719547986984\n",
      "epoch: 2 step: 468, loss is 0.3441368043422699\n",
      "epoch: 2 step: 469, loss is 0.3314879834651947\n",
      "epoch: 2 step: 470, loss is 0.063536636531353\n",
      "epoch: 2 step: 471, loss is 0.27071014046669006\n",
      "epoch: 2 step: 472, loss is 0.04273911938071251\n",
      "epoch: 2 step: 473, loss is 0.012857124209403992\n",
      "epoch: 2 step: 474, loss is 0.25777754187583923\n",
      "epoch: 2 step: 475, loss is 0.016458848491311073\n",
      "epoch: 2 step: 476, loss is 0.041278742253780365\n",
      "epoch: 2 step: 477, loss is 0.10846512019634247\n",
      "epoch: 2 step: 478, loss is 0.13833968341350555\n",
      "epoch: 2 step: 479, loss is 0.029030978679656982\n",
      "epoch: 2 step: 480, loss is 0.2121058851480484\n",
      "epoch: 2 step: 481, loss is 0.13177020847797394\n",
      "epoch: 2 step: 482, loss is 0.14195281267166138\n",
      "epoch: 2 step: 483, loss is 0.030111603438854218\n",
      "epoch: 2 step: 484, loss is 0.029216405004262924\n",
      "epoch: 2 step: 485, loss is 0.1417100429534912\n",
      "epoch: 2 step: 486, loss is 0.1358652561903\n",
      "epoch: 2 step: 487, loss is 0.0825091078877449\n",
      "epoch: 2 step: 488, loss is 0.06268757581710815\n",
      "epoch: 2 step: 489, loss is 0.19057776033878326\n",
      "epoch: 2 step: 490, loss is 0.012613129802048206\n",
      "epoch: 2 step: 491, loss is 0.07959900796413422\n",
      "epoch: 2 step: 492, loss is 0.11595228314399719\n",
      "epoch: 2 step: 493, loss is 0.13113951683044434\n",
      "epoch: 2 step: 494, loss is 0.0813467875123024\n",
      "epoch: 2 step: 495, loss is 0.155251145362854\n",
      "epoch: 2 step: 496, loss is 0.13539768755435944\n",
      "epoch: 2 step: 497, loss is 0.16788923740386963\n",
      "epoch: 2 step: 498, loss is 0.16886435449123383\n",
      "epoch: 2 step: 499, loss is 0.017090588808059692\n",
      "epoch: 2 step: 500, loss is 0.021856658160686493\n",
      "epoch: 2 step: 501, loss is 0.056589458137750626\n",
      "epoch: 2 step: 502, loss is 0.15917454659938812\n",
      "epoch: 2 step: 503, loss is 0.16670356690883636\n",
      "epoch: 2 step: 504, loss is 0.06588581949472427\n",
      "epoch: 2 step: 505, loss is 0.03325686976313591\n",
      "epoch: 2 step: 506, loss is 0.19236916303634644\n",
      "epoch: 2 step: 507, loss is 0.12795233726501465\n",
      "epoch: 2 step: 508, loss is 0.2234344482421875\n",
      "epoch: 2 step: 509, loss is 0.5464309453964233\n",
      "epoch: 2 step: 510, loss is 0.08962506800889969\n",
      "epoch: 2 step: 511, loss is 0.33276209235191345\n",
      "epoch: 2 step: 512, loss is 0.20086215436458588\n",
      "epoch: 2 step: 513, loss is 0.038593657314777374\n",
      "epoch: 2 step: 514, loss is 0.16744890809059143\n",
      "epoch: 2 step: 515, loss is 0.06355878710746765\n",
      "epoch: 2 step: 516, loss is 0.1178133562207222\n",
      "epoch: 2 step: 517, loss is 0.03528202325105667\n",
      "epoch: 2 step: 518, loss is 0.22797764837741852\n",
      "epoch: 2 step: 519, loss is 0.06477604061365128\n",
      "epoch: 2 step: 520, loss is 0.025406410917639732\n",
      "epoch: 2 step: 521, loss is 0.13833089172840118\n",
      "epoch: 2 step: 522, loss is 0.02062036097049713\n",
      "epoch: 2 step: 523, loss is 0.05396987870335579\n",
      "epoch: 2 step: 524, loss is 0.024944022297859192\n",
      "epoch: 2 step: 525, loss is 0.21609579026699066\n",
      "epoch: 2 step: 526, loss is 0.0711958184838295\n",
      "epoch: 2 step: 527, loss is 0.03276527300477028\n",
      "epoch: 2 step: 528, loss is 0.049590833485126495\n",
      "epoch: 2 step: 529, loss is 0.0193797517567873\n",
      "epoch: 2 step: 530, loss is 0.10023992508649826\n",
      "epoch: 2 step: 531, loss is 0.0702928975224495\n",
      "epoch: 2 step: 532, loss is 0.12469000369310379\n",
      "epoch: 2 step: 533, loss is 0.25828808546066284\n",
      "epoch: 2 step: 534, loss is 0.21522963047027588\n",
      "epoch: 2 step: 535, loss is 0.10488187521696091\n",
      "epoch: 2 step: 536, loss is 0.25445443391799927\n",
      "epoch: 2 step: 537, loss is 0.3035790026187897\n",
      "epoch: 2 step: 538, loss is 0.01921268180012703\n",
      "epoch: 2 step: 539, loss is 0.03497038036584854\n",
      "epoch: 2 step: 540, loss is 0.13303950428962708\n",
      "epoch: 2 step: 541, loss is 0.03812796249985695\n",
      "epoch: 2 step: 542, loss is 0.0318702794611454\n",
      "epoch: 2 step: 543, loss is 0.23326005041599274\n",
      "epoch: 2 step: 544, loss is 0.027009213343262672\n",
      "epoch: 2 step: 545, loss is 0.04046223685145378\n",
      "epoch: 2 step: 546, loss is 0.108738012611866\n",
      "epoch: 2 step: 547, loss is 0.1681620478630066\n",
      "epoch: 2 step: 548, loss is 0.05096261575818062\n",
      "epoch: 2 step: 549, loss is 0.13230054080486298\n",
      "epoch: 2 step: 550, loss is 0.049543291330337524\n",
      "epoch: 2 step: 551, loss is 0.05479951202869415\n",
      "epoch: 2 step: 552, loss is 0.1686614602804184\n",
      "epoch: 2 step: 553, loss is 0.011538947932422161\n",
      "epoch: 2 step: 554, loss is 0.009904847480356693\n",
      "epoch: 2 step: 555, loss is 0.0940680131316185\n",
      "epoch: 2 step: 556, loss is 0.16990379989147186\n",
      "epoch: 2 step: 557, loss is 0.017709670588374138\n",
      "epoch: 2 step: 558, loss is 0.2946656346321106\n",
      "epoch: 2 step: 559, loss is 0.130338653922081\n",
      "epoch: 2 step: 560, loss is 0.10826487094163895\n",
      "epoch: 2 step: 561, loss is 0.10562501847743988\n",
      "epoch: 2 step: 562, loss is 0.394947350025177\n",
      "epoch: 2 step: 563, loss is 0.04092831909656525\n",
      "epoch: 2 step: 564, loss is 0.2663532495498657\n",
      "epoch: 2 step: 565, loss is 0.004663409199565649\n",
      "epoch: 2 step: 566, loss is 0.07727396488189697\n",
      "epoch: 2 step: 567, loss is 0.021427655592560768\n",
      "epoch: 2 step: 568, loss is 0.0422649160027504\n",
      "epoch: 2 step: 569, loss is 0.1458282470703125\n",
      "epoch: 2 step: 570, loss is 0.2548566460609436\n",
      "epoch: 2 step: 571, loss is 0.057079363614320755\n",
      "epoch: 2 step: 572, loss is 0.1366574466228485\n",
      "epoch: 2 step: 573, loss is 0.029539046809077263\n",
      "epoch: 2 step: 574, loss is 0.18567690253257751\n",
      "epoch: 2 step: 575, loss is 0.07713668793439865\n",
      "epoch: 2 step: 576, loss is 0.03848825767636299\n",
      "epoch: 2 step: 577, loss is 0.06074916571378708\n",
      "epoch: 2 step: 578, loss is 0.16582265496253967\n",
      "epoch: 2 step: 579, loss is 0.00705387257039547\n",
      "epoch: 2 step: 580, loss is 0.023624271154403687\n",
      "epoch: 2 step: 581, loss is 0.2133241593837738\n",
      "epoch: 2 step: 582, loss is 0.02680341713130474\n",
      "epoch: 2 step: 583, loss is 0.06191106140613556\n",
      "epoch: 2 step: 584, loss is 0.07656729221343994\n",
      "epoch: 2 step: 585, loss is 0.06297944486141205\n",
      "epoch: 2 step: 586, loss is 0.16595710813999176\n",
      "epoch: 2 step: 587, loss is 0.06385727971792221\n",
      "epoch: 2 step: 588, loss is 0.024512695148587227\n",
      "epoch: 2 step: 589, loss is 0.2872813940048218\n",
      "epoch: 2 step: 590, loss is 0.20377710461616516\n",
      "epoch: 2 step: 591, loss is 0.08144260197877884\n",
      "epoch: 2 step: 592, loss is 0.23134347796440125\n",
      "epoch: 2 step: 593, loss is 0.003647782374173403\n",
      "epoch: 2 step: 594, loss is 0.2908872067928314\n",
      "epoch: 2 step: 595, loss is 0.020761385560035706\n",
      "epoch: 2 step: 596, loss is 0.06417923420667648\n",
      "epoch: 2 step: 597, loss is 0.10481420904397964\n",
      "epoch: 2 step: 598, loss is 0.12926901876926422\n",
      "epoch: 2 step: 599, loss is 0.23223625123500824\n",
      "epoch: 2 step: 600, loss is 0.016603998839855194\n",
      "epoch: 2 step: 601, loss is 0.024925820529460907\n",
      "epoch: 2 step: 602, loss is 0.09081432968378067\n",
      "epoch: 2 step: 603, loss is 0.09656032919883728\n",
      "epoch: 2 step: 604, loss is 0.013086975552141666\n",
      "epoch: 2 step: 605, loss is 0.008708776906132698\n",
      "epoch: 2 step: 606, loss is 0.0692371129989624\n",
      "epoch: 2 step: 607, loss is 0.23781703412532806\n",
      "epoch: 2 step: 608, loss is 0.0760141983628273\n",
      "epoch: 2 step: 609, loss is 0.1935248225927353\n",
      "epoch: 2 step: 610, loss is 0.004002741072326899\n",
      "epoch: 2 step: 611, loss is 0.06314480304718018\n",
      "epoch: 2 step: 612, loss is 0.013281750492751598\n",
      "epoch: 2 step: 613, loss is 0.0062873768620193005\n",
      "epoch: 2 step: 614, loss is 0.03808315470814705\n",
      "epoch: 2 step: 615, loss is 0.018595296889543533\n",
      "epoch: 2 step: 616, loss is 0.018471023067831993\n",
      "epoch: 2 step: 617, loss is 0.05962691083550453\n",
      "epoch: 2 step: 618, loss is 0.13081154227256775\n",
      "epoch: 2 step: 619, loss is 0.07607842981815338\n",
      "epoch: 2 step: 620, loss is 0.15647238492965698\n",
      "epoch: 2 step: 621, loss is 0.017947999760508537\n",
      "epoch: 2 step: 622, loss is 0.2412201166152954\n",
      "epoch: 2 step: 623, loss is 0.14697617292404175\n",
      "epoch: 2 step: 624, loss is 0.04153931513428688\n",
      "epoch: 2 step: 625, loss is 0.05602661892771721\n",
      "epoch: 2 step: 626, loss is 0.006744076497852802\n",
      "epoch: 2 step: 627, loss is 0.10433104634284973\n",
      "epoch: 2 step: 628, loss is 0.19891685247421265\n",
      "epoch: 2 step: 629, loss is 0.03903317078948021\n",
      "epoch: 2 step: 630, loss is 0.22281086444854736\n",
      "epoch: 2 step: 631, loss is 0.11334456503391266\n",
      "epoch: 2 step: 632, loss is 0.9717084765434265\n",
      "epoch: 2 step: 633, loss is 0.06377775222063065\n",
      "epoch: 2 step: 634, loss is 0.2341335266828537\n",
      "epoch: 2 step: 635, loss is 0.12081305682659149\n",
      "epoch: 2 step: 636, loss is 0.08472683280706406\n",
      "epoch: 2 step: 637, loss is 0.14246347546577454\n",
      "epoch: 2 step: 638, loss is 0.13052116334438324\n",
      "epoch: 2 step: 639, loss is 0.020475585013628006\n",
      "epoch: 2 step: 640, loss is 0.10073324292898178\n",
      "epoch: 2 step: 641, loss is 0.1450182944536209\n",
      "epoch: 2 step: 642, loss is 0.0707612857222557\n",
      "epoch: 2 step: 643, loss is 0.15078973770141602\n",
      "epoch: 2 step: 644, loss is 0.01687372848391533\n",
      "epoch: 2 step: 645, loss is 0.08890543133020401\n",
      "epoch: 2 step: 646, loss is 0.06673932075500488\n",
      "epoch: 2 step: 647, loss is 0.12594617903232574\n",
      "epoch: 2 step: 648, loss is 0.528060793876648\n",
      "epoch: 2 step: 649, loss is 0.28771087527275085\n",
      "epoch: 2 step: 650, loss is 0.08544106781482697\n",
      "epoch: 2 step: 651, loss is 0.07962590456008911\n",
      "epoch: 2 step: 652, loss is 0.03749233856797218\n",
      "epoch: 2 step: 653, loss is 0.44348666071891785\n",
      "epoch: 2 step: 654, loss is 0.2002863883972168\n",
      "epoch: 2 step: 655, loss is 0.08408555388450623\n",
      "epoch: 2 step: 656, loss is 0.09613317251205444\n",
      "epoch: 2 step: 657, loss is 0.4226182699203491\n",
      "epoch: 2 step: 658, loss is 0.056478992104530334\n",
      "epoch: 2 step: 659, loss is 0.03973729908466339\n",
      "epoch: 2 step: 660, loss is 0.19350771605968475\n",
      "epoch: 2 step: 661, loss is 0.34515926241874695\n",
      "epoch: 2 step: 662, loss is 0.178993821144104\n",
      "epoch: 2 step: 663, loss is 0.20504899322986603\n",
      "epoch: 2 step: 664, loss is 0.06671588867902756\n",
      "epoch: 2 step: 665, loss is 0.007818406447768211\n",
      "epoch: 2 step: 666, loss is 0.09226499497890472\n",
      "epoch: 2 step: 667, loss is 0.05078049376606941\n",
      "epoch: 2 step: 668, loss is 0.009836097247898579\n",
      "epoch: 2 step: 669, loss is 0.1412929743528366\n",
      "epoch: 2 step: 670, loss is 0.1423364132642746\n",
      "epoch: 2 step: 671, loss is 0.5907312035560608\n",
      "epoch: 2 step: 672, loss is 0.19184163212776184\n",
      "epoch: 2 step: 673, loss is 0.011653914116322994\n",
      "epoch: 2 step: 674, loss is 0.0580945648252964\n",
      "epoch: 2 step: 675, loss is 0.057787902653217316\n",
      "epoch: 2 step: 676, loss is 0.13189223408699036\n",
      "epoch: 2 step: 677, loss is 0.13088423013687134\n",
      "epoch: 2 step: 678, loss is 0.08927498757839203\n",
      "epoch: 2 step: 679, loss is 0.1193964034318924\n",
      "epoch: 2 step: 680, loss is 0.12337532639503479\n",
      "epoch: 2 step: 681, loss is 0.12341868877410889\n",
      "epoch: 2 step: 682, loss is 0.0661117285490036\n",
      "epoch: 2 step: 683, loss is 0.06327185779809952\n",
      "epoch: 2 step: 684, loss is 0.008751880377531052\n",
      "epoch: 2 step: 685, loss is 0.1621685028076172\n",
      "epoch: 2 step: 686, loss is 0.04252040013670921\n",
      "epoch: 2 step: 687, loss is 0.059915896505117416\n",
      "epoch: 2 step: 688, loss is 0.12815414369106293\n",
      "epoch: 2 step: 689, loss is 0.1039779782295227\n",
      "epoch: 2 step: 690, loss is 0.05083765462040901\n",
      "epoch: 2 step: 691, loss is 0.19075652956962585\n",
      "epoch: 2 step: 692, loss is 0.014302229508757591\n",
      "epoch: 2 step: 693, loss is 0.05224359780550003\n",
      "epoch: 2 step: 694, loss is 0.03617008775472641\n",
      "epoch: 2 step: 695, loss is 0.02998105064034462\n",
      "epoch: 2 step: 696, loss is 0.10849114507436752\n",
      "epoch: 2 step: 697, loss is 0.11920879036188126\n",
      "epoch: 2 step: 698, loss is 0.07373066246509552\n",
      "epoch: 2 step: 699, loss is 0.04314817115664482\n",
      "epoch: 2 step: 700, loss is 0.18817684054374695\n",
      "epoch: 2 step: 701, loss is 0.3824547827243805\n",
      "epoch: 2 step: 702, loss is 0.027940597385168076\n",
      "epoch: 2 step: 703, loss is 0.061851006001234055\n",
      "epoch: 2 step: 704, loss is 0.07219240069389343\n",
      "epoch: 2 step: 705, loss is 0.3077603876590729\n",
      "epoch: 2 step: 706, loss is 0.019287697970867157\n",
      "epoch: 2 step: 707, loss is 0.03021625429391861\n",
      "epoch: 2 step: 708, loss is 0.13054479658603668\n",
      "epoch: 2 step: 709, loss is 0.06936448812484741\n",
      "epoch: 2 step: 710, loss is 0.12171652913093567\n",
      "epoch: 2 step: 711, loss is 0.11614587157964706\n",
      "epoch: 2 step: 712, loss is 0.10018354654312134\n",
      "epoch: 2 step: 713, loss is 0.019099922850728035\n",
      "epoch: 2 step: 714, loss is 0.05503581091761589\n",
      "epoch: 2 step: 715, loss is 0.10400388389825821\n",
      "epoch: 2 step: 716, loss is 0.10008182376623154\n",
      "epoch: 2 step: 717, loss is 0.02243637479841709\n",
      "epoch: 2 step: 718, loss is 0.12613917887210846\n",
      "epoch: 2 step: 719, loss is 0.014005497097969055\n",
      "epoch: 2 step: 720, loss is 0.04143226146697998\n",
      "epoch: 2 step: 721, loss is 0.17996010184288025\n",
      "epoch: 2 step: 722, loss is 0.009862150996923447\n",
      "epoch: 2 step: 723, loss is 0.01536014024168253\n",
      "epoch: 2 step: 724, loss is 0.5468435287475586\n",
      "epoch: 2 step: 725, loss is 0.392148494720459\n",
      "epoch: 2 step: 726, loss is 0.0919504165649414\n",
      "epoch: 2 step: 727, loss is 0.22405874729156494\n",
      "epoch: 2 step: 728, loss is 0.0049904449842870235\n",
      "epoch: 2 step: 729, loss is 0.12639786303043365\n",
      "epoch: 2 step: 730, loss is 0.10589499771595001\n",
      "epoch: 2 step: 731, loss is 0.10222408920526505\n",
      "epoch: 2 step: 732, loss is 0.20651857554912567\n",
      "epoch: 2 step: 733, loss is 0.06885309517383575\n",
      "epoch: 2 step: 734, loss is 0.17046275734901428\n",
      "epoch: 2 step: 735, loss is 0.1425873339176178\n",
      "epoch: 2 step: 736, loss is 0.13546457886695862\n",
      "epoch: 2 step: 737, loss is 0.20499859750270844\n",
      "epoch: 2 step: 738, loss is 0.2149432897567749\n",
      "epoch: 2 step: 739, loss is 0.014547576196491718\n",
      "epoch: 2 step: 740, loss is 0.03819378465414047\n",
      "epoch: 2 step: 741, loss is 0.14488530158996582\n",
      "epoch: 2 step: 742, loss is 0.04766181483864784\n",
      "epoch: 2 step: 743, loss is 0.06534306704998016\n",
      "epoch: 2 step: 744, loss is 0.059511229395866394\n",
      "epoch: 2 step: 745, loss is 0.007905344478785992\n",
      "epoch: 2 step: 746, loss is 0.07158040255308151\n",
      "epoch: 2 step: 747, loss is 0.03344934806227684\n",
      "epoch: 2 step: 748, loss is 0.029138347133994102\n",
      "epoch: 2 step: 749, loss is 0.062191352248191833\n",
      "epoch: 2 step: 750, loss is 0.03709302470088005\n",
      "epoch: 2 step: 751, loss is 0.16775518655776978\n",
      "epoch: 2 step: 752, loss is 0.15449044108390808\n",
      "epoch: 2 step: 753, loss is 0.18771013617515564\n",
      "epoch: 2 step: 754, loss is 0.08671306818723679\n",
      "epoch: 2 step: 755, loss is 0.3256309926509857\n",
      "epoch: 2 step: 756, loss is 0.036290351301431656\n",
      "epoch: 2 step: 757, loss is 0.08977147936820984\n",
      "epoch: 2 step: 758, loss is 0.08979282528162003\n",
      "epoch: 2 step: 759, loss is 0.06394781172275543\n",
      "epoch: 2 step: 760, loss is 0.0479886494576931\n",
      "epoch: 2 step: 761, loss is 0.06897986680269241\n",
      "epoch: 2 step: 762, loss is 0.02843398228287697\n",
      "epoch: 2 step: 763, loss is 0.09676866233348846\n",
      "epoch: 2 step: 764, loss is 0.07296133041381836\n",
      "epoch: 2 step: 765, loss is 0.016504568979144096\n",
      "epoch: 2 step: 766, loss is 0.05781333148479462\n",
      "epoch: 2 step: 767, loss is 0.11657464504241943\n",
      "epoch: 2 step: 768, loss is 0.03368173539638519\n",
      "epoch: 2 step: 769, loss is 0.027213677763938904\n",
      "epoch: 2 step: 770, loss is 0.0979229286313057\n",
      "epoch: 2 step: 771, loss is 0.1197207048535347\n",
      "epoch: 2 step: 772, loss is 0.03653335198760033\n",
      "epoch: 2 step: 773, loss is 0.18129849433898926\n",
      "epoch: 2 step: 774, loss is 0.18818172812461853\n",
      "epoch: 2 step: 775, loss is 0.03264160081744194\n",
      "epoch: 2 step: 776, loss is 0.2366640418767929\n",
      "epoch: 2 step: 777, loss is 0.05718255788087845\n",
      "epoch: 2 step: 778, loss is 0.06075357273221016\n",
      "epoch: 2 step: 779, loss is 0.10592520982027054\n",
      "epoch: 2 step: 780, loss is 0.10529424995183945\n",
      "epoch: 2 step: 781, loss is 0.06346702575683594\n",
      "epoch: 2 step: 782, loss is 0.005209296476095915\n",
      "epoch: 2 step: 783, loss is 0.08223383128643036\n",
      "epoch: 2 step: 784, loss is 0.06350260972976685\n",
      "epoch: 2 step: 785, loss is 0.029971439391374588\n",
      "epoch: 2 step: 786, loss is 0.16208089888095856\n",
      "epoch: 2 step: 787, loss is 0.02900739014148712\n",
      "epoch: 2 step: 788, loss is 0.016075896099209785\n",
      "epoch: 2 step: 789, loss is 0.04600067436695099\n",
      "epoch: 2 step: 790, loss is 0.22750666737556458\n",
      "epoch: 2 step: 791, loss is 0.08256854116916656\n",
      "epoch: 2 step: 792, loss is 0.03839133679866791\n",
      "epoch: 2 step: 793, loss is 0.06320308893918991\n",
      "epoch: 2 step: 794, loss is 0.09929624199867249\n",
      "epoch: 2 step: 795, loss is 0.047599516808986664\n",
      "epoch: 2 step: 796, loss is 0.23743291199207306\n",
      "epoch: 2 step: 797, loss is 0.2983166575431824\n",
      "epoch: 2 step: 798, loss is 0.03577199578285217\n",
      "epoch: 2 step: 799, loss is 0.007524654269218445\n",
      "epoch: 2 step: 800, loss is 0.05180244520306587\n",
      "epoch: 2 step: 801, loss is 0.004823073744773865\n",
      "epoch: 2 step: 802, loss is 0.014407236129045486\n",
      "epoch: 2 step: 803, loss is 0.1992025375366211\n",
      "epoch: 2 step: 804, loss is 0.18156610429286957\n",
      "epoch: 2 step: 805, loss is 0.23852130770683289\n",
      "epoch: 2 step: 806, loss is 0.2151142805814743\n",
      "epoch: 2 step: 807, loss is 0.09150154143571854\n",
      "epoch: 2 step: 808, loss is 0.01345728524029255\n",
      "epoch: 2 step: 809, loss is 0.20464931428432465\n",
      "epoch: 2 step: 810, loss is 0.05289294198155403\n",
      "epoch: 2 step: 811, loss is 0.04029565677046776\n",
      "epoch: 2 step: 812, loss is 0.04526544734835625\n",
      "epoch: 2 step: 813, loss is 0.1483340561389923\n",
      "epoch: 2 step: 814, loss is 0.00995703972876072\n",
      "epoch: 2 step: 815, loss is 0.21493087708950043\n",
      "epoch: 2 step: 816, loss is 0.14079703390598297\n",
      "epoch: 2 step: 817, loss is 0.03542564436793327\n",
      "epoch: 2 step: 818, loss is 0.14861424267292023\n",
      "epoch: 2 step: 819, loss is 0.16702991724014282\n",
      "epoch: 2 step: 820, loss is 0.08821393549442291\n",
      "epoch: 2 step: 821, loss is 0.19826245307922363\n",
      "epoch: 2 step: 822, loss is 0.0400761179625988\n",
      "epoch: 2 step: 823, loss is 0.32735398411750793\n",
      "epoch: 2 step: 824, loss is 0.037788208574056625\n",
      "epoch: 2 step: 825, loss is 0.050387755036354065\n",
      "epoch: 2 step: 826, loss is 0.03344591706991196\n",
      "epoch: 2 step: 827, loss is 0.09134584665298462\n",
      "epoch: 2 step: 828, loss is 0.09915625303983688\n",
      "epoch: 2 step: 829, loss is 0.03187819942831993\n",
      "epoch: 2 step: 830, loss is 0.2785939574241638\n",
      "epoch: 2 step: 831, loss is 0.1649145632982254\n",
      "epoch: 2 step: 832, loss is 0.14337141811847687\n",
      "epoch: 2 step: 833, loss is 0.015551646240055561\n",
      "epoch: 2 step: 834, loss is 0.20096029341220856\n",
      "epoch: 2 step: 835, loss is 0.06481098383665085\n",
      "epoch: 2 step: 836, loss is 0.1273031383752823\n",
      "epoch: 2 step: 837, loss is 0.11454149335622787\n",
      "epoch: 2 step: 838, loss is 0.416748046875\n",
      "epoch: 2 step: 839, loss is 0.08855552971363068\n",
      "epoch: 2 step: 840, loss is 0.33016568422317505\n",
      "epoch: 2 step: 841, loss is 0.0487385056912899\n",
      "epoch: 2 step: 842, loss is 0.05235132575035095\n",
      "epoch: 2 step: 843, loss is 0.1777493953704834\n",
      "epoch: 2 step: 844, loss is 0.092908576130867\n",
      "epoch: 2 step: 845, loss is 0.16519664227962494\n",
      "epoch: 2 step: 846, loss is 0.017379477620124817\n",
      "epoch: 2 step: 847, loss is 0.29453402757644653\n",
      "epoch: 2 step: 848, loss is 0.1524461954832077\n",
      "epoch: 2 step: 849, loss is 0.019773468375205994\n",
      "epoch: 2 step: 850, loss is 0.19751909375190735\n",
      "epoch: 2 step: 851, loss is 0.1483769416809082\n",
      "epoch: 2 step: 852, loss is 0.2252865880727768\n",
      "epoch: 2 step: 853, loss is 0.3294074535369873\n",
      "epoch: 2 step: 854, loss is 0.009732453152537346\n",
      "epoch: 2 step: 855, loss is 0.11410042643547058\n",
      "epoch: 2 step: 856, loss is 0.2997143864631653\n",
      "epoch: 2 step: 857, loss is 0.23016797006130219\n",
      "epoch: 2 step: 858, loss is 0.11514071375131607\n",
      "epoch: 2 step: 859, loss is 0.08798705786466599\n",
      "epoch: 2 step: 860, loss is 0.0606856569647789\n",
      "epoch: 2 step: 861, loss is 0.057285115122795105\n",
      "epoch: 2 step: 862, loss is 0.03232362121343613\n",
      "epoch: 2 step: 863, loss is 0.1812339872121811\n",
      "epoch: 2 step: 864, loss is 0.21831966936588287\n",
      "epoch: 2 step: 865, loss is 0.15675830841064453\n",
      "epoch: 2 step: 866, loss is 0.08955199271440506\n",
      "epoch: 2 step: 867, loss is 0.056495219469070435\n",
      "epoch: 2 step: 868, loss is 0.24078184366226196\n",
      "epoch: 2 step: 869, loss is 0.053845006972551346\n",
      "epoch: 2 step: 870, loss is 0.046143706887960434\n",
      "epoch: 2 step: 871, loss is 0.007375114597380161\n",
      "epoch: 2 step: 872, loss is 0.46189600229263306\n",
      "epoch: 2 step: 873, loss is 0.03719831258058548\n",
      "epoch: 2 step: 874, loss is 0.0479714572429657\n",
      "epoch: 2 step: 875, loss is 0.32427722215652466\n",
      "epoch: 2 step: 876, loss is 0.23774117231369019\n",
      "epoch: 2 step: 877, loss is 0.032807473093271255\n",
      "epoch: 2 step: 878, loss is 0.0801500603556633\n",
      "epoch: 2 step: 879, loss is 0.005546778440475464\n",
      "epoch: 2 step: 880, loss is 0.061958447098731995\n",
      "epoch: 2 step: 881, loss is 0.07229477912187576\n",
      "epoch: 2 step: 882, loss is 0.1557879000902176\n",
      "epoch: 2 step: 883, loss is 0.06365815550088882\n",
      "epoch: 2 step: 884, loss is 0.21717025339603424\n",
      "epoch: 2 step: 885, loss is 0.11760233342647552\n",
      "epoch: 2 step: 886, loss is 0.0697450265288353\n",
      "epoch: 2 step: 887, loss is 0.09268137067556381\n",
      "epoch: 2 step: 888, loss is 0.13098078966140747\n",
      "epoch: 2 step: 889, loss is 0.15363982319831848\n",
      "epoch: 2 step: 890, loss is 0.18514947593212128\n",
      "epoch: 2 step: 891, loss is 0.03164931759238243\n",
      "epoch: 2 step: 892, loss is 0.09558583796024323\n",
      "epoch: 2 step: 893, loss is 0.02209191396832466\n",
      "epoch: 2 step: 894, loss is 0.3298584222793579\n",
      "epoch: 2 step: 895, loss is 0.11900091171264648\n",
      "epoch: 2 step: 896, loss is 0.32456541061401367\n",
      "epoch: 2 step: 897, loss is 0.2564978301525116\n",
      "epoch: 2 step: 898, loss is 0.024979393929243088\n",
      "epoch: 2 step: 899, loss is 0.13441547751426697\n",
      "epoch: 2 step: 900, loss is 0.1853536069393158\n",
      "epoch: 2 step: 901, loss is 0.07897932082414627\n",
      "epoch: 2 step: 902, loss is 0.5312625169754028\n",
      "epoch: 2 step: 903, loss is 0.08740848302841187\n",
      "epoch: 2 step: 904, loss is 0.3912365734577179\n",
      "epoch: 2 step: 905, loss is 0.2361478954553604\n",
      "epoch: 2 step: 906, loss is 0.038755595684051514\n",
      "epoch: 2 step: 907, loss is 0.218589186668396\n",
      "epoch: 2 step: 908, loss is 0.014407425187528133\n",
      "epoch: 2 step: 909, loss is 0.041208185255527496\n",
      "epoch: 2 step: 910, loss is 0.2557061016559601\n",
      "epoch: 2 step: 911, loss is 0.13016009330749512\n",
      "epoch: 2 step: 912, loss is 0.05710037425160408\n",
      "epoch: 2 step: 913, loss is 0.023936264216899872\n",
      "epoch: 2 step: 914, loss is 0.14173784852027893\n",
      "epoch: 2 step: 915, loss is 0.08678527921438217\n",
      "epoch: 2 step: 916, loss is 0.17204920947551727\n",
      "epoch: 2 step: 917, loss is 0.2007930427789688\n",
      "epoch: 2 step: 918, loss is 0.0651990994811058\n",
      "epoch: 2 step: 919, loss is 0.1914404034614563\n",
      "epoch: 2 step: 920, loss is 0.051980555057525635\n",
      "epoch: 2 step: 921, loss is 0.14538808166980743\n",
      "epoch: 2 step: 922, loss is 0.048289649188518524\n",
      "epoch: 2 step: 923, loss is 0.022330231964588165\n",
      "epoch: 2 step: 924, loss is 0.07329001277685165\n",
      "epoch: 2 step: 925, loss is 0.045262955129146576\n",
      "epoch: 2 step: 926, loss is 0.18779143691062927\n",
      "epoch: 2 step: 927, loss is 0.1683872938156128\n",
      "epoch: 2 step: 928, loss is 0.21055768430233002\n",
      "epoch: 2 step: 929, loss is 0.026387425139546394\n",
      "epoch: 2 step: 930, loss is 0.23133662343025208\n",
      "epoch: 2 step: 931, loss is 0.015653979033231735\n",
      "epoch: 2 step: 932, loss is 0.07327695190906525\n",
      "epoch: 2 step: 933, loss is 0.3666047155857086\n",
      "epoch: 2 step: 934, loss is 0.04537707194685936\n",
      "epoch: 2 step: 935, loss is 0.07976828515529633\n",
      "epoch: 2 step: 936, loss is 0.24155911803245544\n",
      "epoch: 2 step: 937, loss is 0.05474031716585159\n",
      "epoch: 2 step: 938, loss is 0.2111523151397705\n",
      "epoch: 2 step: 939, loss is 0.024199331179261208\n",
      "epoch: 2 step: 940, loss is 0.03843281790614128\n",
      "epoch: 2 step: 941, loss is 0.03367159888148308\n",
      "epoch: 2 step: 942, loss is 0.0896352082490921\n",
      "epoch: 2 step: 943, loss is 0.27459272742271423\n",
      "epoch: 2 step: 944, loss is 0.34211426973342896\n",
      "epoch: 2 step: 945, loss is 0.29253697395324707\n",
      "epoch: 2 step: 946, loss is 0.07394774258136749\n",
      "epoch: 2 step: 947, loss is 0.01787310466170311\n",
      "epoch: 2 step: 948, loss is 0.21096119284629822\n",
      "epoch: 2 step: 949, loss is 0.022141151130199432\n",
      "epoch: 2 step: 950, loss is 0.00988885760307312\n",
      "epoch: 2 step: 951, loss is 0.05328790470957756\n",
      "epoch: 2 step: 952, loss is 0.02877599373459816\n",
      "epoch: 2 step: 953, loss is 0.15856564044952393\n",
      "epoch: 2 step: 954, loss is 0.03208628669381142\n",
      "epoch: 2 step: 955, loss is 0.055145762860774994\n",
      "epoch: 2 step: 956, loss is 0.037113483995199203\n",
      "epoch: 2 step: 957, loss is 0.15859219431877136\n",
      "epoch: 2 step: 958, loss is 0.05515441671013832\n",
      "epoch: 2 step: 959, loss is 0.04568636789917946\n",
      "epoch: 2 step: 960, loss is 0.1794942021369934\n",
      "epoch: 2 step: 961, loss is 0.11551681160926819\n",
      "epoch: 2 step: 962, loss is 0.006266050040721893\n",
      "epoch: 2 step: 963, loss is 0.07273581624031067\n",
      "epoch: 2 step: 964, loss is 0.4394538700580597\n",
      "epoch: 2 step: 965, loss is 0.031051326543092728\n",
      "epoch: 2 step: 966, loss is 0.4213962256908417\n",
      "epoch: 2 step: 967, loss is 0.03935595601797104\n",
      "epoch: 2 step: 968, loss is 0.07745499908924103\n",
      "epoch: 2 step: 969, loss is 0.12966686487197876\n",
      "epoch: 2 step: 970, loss is 0.10603396594524384\n",
      "epoch: 2 step: 971, loss is 0.16270500421524048\n",
      "epoch: 2 step: 972, loss is 0.005586378276348114\n",
      "epoch: 2 step: 973, loss is 0.28498464822769165\n",
      "epoch: 2 step: 974, loss is 0.012167215347290039\n",
      "epoch: 2 step: 975, loss is 0.18983018398284912\n",
      "epoch: 2 step: 976, loss is 0.06487134844064713\n",
      "epoch: 2 step: 977, loss is 0.13051658868789673\n",
      "epoch: 2 step: 978, loss is 0.026285851374268532\n",
      "epoch: 2 step: 979, loss is 0.1526193767786026\n",
      "epoch: 2 step: 980, loss is 0.055117830634117126\n",
      "epoch: 2 step: 981, loss is 0.11489309370517731\n",
      "epoch: 2 step: 982, loss is 0.14984606206417084\n",
      "epoch: 2 step: 983, loss is 0.3222622275352478\n",
      "epoch: 2 step: 984, loss is 0.19234436750411987\n",
      "epoch: 2 step: 985, loss is 0.1780131757259369\n",
      "epoch: 2 step: 986, loss is 0.3169502019882202\n",
      "epoch: 2 step: 987, loss is 0.15237261354923248\n",
      "epoch: 2 step: 988, loss is 0.15621328353881836\n",
      "epoch: 2 step: 989, loss is 0.19430291652679443\n",
      "epoch: 2 step: 990, loss is 0.2786544859409332\n",
      "epoch: 2 step: 991, loss is 0.07887060195207596\n",
      "epoch: 2 step: 992, loss is 0.07986361533403397\n",
      "epoch: 2 step: 993, loss is 0.3629685044288635\n",
      "epoch: 2 step: 994, loss is 0.005474794656038284\n",
      "epoch: 2 step: 995, loss is 0.05830047279596329\n",
      "epoch: 2 step: 996, loss is 0.039628248661756516\n",
      "epoch: 2 step: 997, loss is 0.13561761379241943\n",
      "epoch: 2 step: 998, loss is 0.1577567160129547\n",
      "epoch: 2 step: 999, loss is 0.14533627033233643\n",
      "epoch: 2 step: 1000, loss is 0.016473082825541496\n",
      "epoch: 2 step: 1001, loss is 0.25469085574150085\n",
      "epoch: 2 step: 1002, loss is 0.06375082582235336\n",
      "epoch: 2 step: 1003, loss is 0.028641272336244583\n",
      "epoch: 2 step: 1004, loss is 0.08536745607852936\n",
      "epoch: 2 step: 1005, loss is 0.028156572952866554\n",
      "epoch: 2 step: 1006, loss is 0.10015329718589783\n",
      "epoch: 2 step: 1007, loss is 0.020825736224651337\n",
      "epoch: 2 step: 1008, loss is 0.09109341353178024\n",
      "epoch: 2 step: 1009, loss is 0.051078006625175476\n",
      "epoch: 2 step: 1010, loss is 0.09355442970991135\n",
      "epoch: 2 step: 1011, loss is 0.04780680313706398\n",
      "epoch: 2 step: 1012, loss is 0.18095867335796356\n",
      "epoch: 2 step: 1013, loss is 0.1678970605134964\n",
      "epoch: 2 step: 1014, loss is 0.15922607481479645\n",
      "epoch: 2 step: 1015, loss is 0.03771430253982544\n",
      "epoch: 2 step: 1016, loss is 0.31202682852745056\n",
      "epoch: 2 step: 1017, loss is 0.22641409933567047\n",
      "epoch: 2 step: 1018, loss is 0.22740283608436584\n",
      "epoch: 2 step: 1019, loss is 0.08433812856674194\n",
      "epoch: 2 step: 1020, loss is 0.2182120531797409\n",
      "epoch: 2 step: 1021, loss is 0.0557999387383461\n",
      "epoch: 2 step: 1022, loss is 0.03589935228228569\n",
      "epoch: 2 step: 1023, loss is 0.03402262553572655\n",
      "epoch: 2 step: 1024, loss is 0.01058902032673359\n",
      "epoch: 2 step: 1025, loss is 0.04299389198422432\n",
      "epoch: 2 step: 1026, loss is 0.04698552191257477\n",
      "epoch: 2 step: 1027, loss is 0.015693075954914093\n",
      "epoch: 2 step: 1028, loss is 0.009014176204800606\n",
      "epoch: 2 step: 1029, loss is 0.13262024521827698\n",
      "epoch: 2 step: 1030, loss is 0.11711572110652924\n",
      "epoch: 2 step: 1031, loss is 0.14804115891456604\n",
      "epoch: 2 step: 1032, loss is 0.14733459055423737\n",
      "epoch: 2 step: 1033, loss is 0.1220984235405922\n",
      "epoch: 2 step: 1034, loss is 0.3531040847301483\n",
      "epoch: 2 step: 1035, loss is 0.432614266872406\n",
      "epoch: 2 step: 1036, loss is 0.01971283368766308\n",
      "epoch: 2 step: 1037, loss is 0.02892562374472618\n",
      "epoch: 2 step: 1038, loss is 0.19250650703907013\n",
      "epoch: 2 step: 1039, loss is 0.1266779899597168\n",
      "epoch: 2 step: 1040, loss is 0.0806022509932518\n",
      "epoch: 2 step: 1041, loss is 0.04469265043735504\n",
      "epoch: 2 step: 1042, loss is 0.06459923088550568\n",
      "epoch: 2 step: 1043, loss is 0.10229656100273132\n",
      "epoch: 2 step: 1044, loss is 0.17772018909454346\n",
      "epoch: 2 step: 1045, loss is 0.002418455434963107\n",
      "epoch: 2 step: 1046, loss is 0.02718133106827736\n",
      "epoch: 2 step: 1047, loss is 0.2617110311985016\n",
      "epoch: 2 step: 1048, loss is 0.15181022882461548\n",
      "epoch: 2 step: 1049, loss is 0.22809860110282898\n",
      "epoch: 2 step: 1050, loss is 0.11499200761318207\n",
      "epoch: 2 step: 1051, loss is 0.16994215548038483\n",
      "epoch: 2 step: 1052, loss is 0.08819486945867538\n",
      "epoch: 2 step: 1053, loss is 0.05843620374798775\n",
      "epoch: 2 step: 1054, loss is 0.14130347967147827\n",
      "epoch: 2 step: 1055, loss is 0.09658647328615189\n",
      "epoch: 2 step: 1056, loss is 0.027494752779603004\n",
      "epoch: 2 step: 1057, loss is 0.05352858081459999\n",
      "epoch: 2 step: 1058, loss is 0.17459329962730408\n",
      "epoch: 2 step: 1059, loss is 0.20593920350074768\n",
      "epoch: 2 step: 1060, loss is 0.15362422168254852\n",
      "epoch: 2 step: 1061, loss is 0.2039174884557724\n",
      "epoch: 2 step: 1062, loss is 0.05083213374018669\n",
      "epoch: 2 step: 1063, loss is 0.10945872217416763\n",
      "epoch: 2 step: 1064, loss is 0.07596590369939804\n",
      "epoch: 2 step: 1065, loss is 0.0695754885673523\n",
      "epoch: 2 step: 1066, loss is 0.2717563807964325\n",
      "epoch: 2 step: 1067, loss is 0.07662831991910934\n",
      "epoch: 2 step: 1068, loss is 0.038600485771894455\n",
      "epoch: 2 step: 1069, loss is 0.07265554368495941\n",
      "epoch: 2 step: 1070, loss is 0.0634317398071289\n",
      "epoch: 2 step: 1071, loss is 0.24616478383541107\n",
      "epoch: 2 step: 1072, loss is 0.024296196177601814\n",
      "epoch: 2 step: 1073, loss is 0.009384909644722939\n",
      "epoch: 2 step: 1074, loss is 0.43655821681022644\n",
      "epoch: 2 step: 1075, loss is 0.08956364542245865\n",
      "epoch: 2 step: 1076, loss is 0.017763661220669746\n",
      "epoch: 2 step: 1077, loss is 0.054658420383930206\n",
      "epoch: 2 step: 1078, loss is 0.1666771024465561\n",
      "epoch: 2 step: 1079, loss is 0.057803165167570114\n",
      "epoch: 2 step: 1080, loss is 0.06965048611164093\n",
      "epoch: 2 step: 1081, loss is 0.01802213117480278\n",
      "epoch: 2 step: 1082, loss is 0.12850120663642883\n",
      "epoch: 2 step: 1083, loss is 0.18997198343276978\n",
      "epoch: 2 step: 1084, loss is 0.03810188174247742\n",
      "epoch: 2 step: 1085, loss is 0.31993013620376587\n",
      "epoch: 2 step: 1086, loss is 0.10036652535200119\n",
      "epoch: 2 step: 1087, loss is 0.047839343547821045\n",
      "epoch: 2 step: 1088, loss is 0.13378801941871643\n",
      "epoch: 2 step: 1089, loss is 0.05784517526626587\n",
      "epoch: 2 step: 1090, loss is 0.20534607768058777\n",
      "epoch: 2 step: 1091, loss is 0.3632203936576843\n",
      "epoch: 2 step: 1092, loss is 0.08751032501459122\n",
      "epoch: 2 step: 1093, loss is 0.029181472957134247\n",
      "epoch: 2 step: 1094, loss is 0.14240363240242004\n",
      "epoch: 2 step: 1095, loss is 0.17709402740001678\n",
      "epoch: 2 step: 1096, loss is 0.2026040107011795\n",
      "epoch: 2 step: 1097, loss is 0.029650315642356873\n",
      "epoch: 2 step: 1098, loss is 0.03324517980217934\n",
      "epoch: 2 step: 1099, loss is 0.08789654821157455\n",
      "epoch: 2 step: 1100, loss is 0.16207584738731384\n",
      "epoch: 2 step: 1101, loss is 0.14381460845470428\n",
      "epoch: 2 step: 1102, loss is 0.36606359481811523\n",
      "epoch: 2 step: 1103, loss is 0.026606548577547073\n",
      "epoch: 2 step: 1104, loss is 0.04714907333254814\n",
      "epoch: 2 step: 1105, loss is 0.1479005217552185\n",
      "epoch: 2 step: 1106, loss is 0.07928124815225601\n",
      "epoch: 2 step: 1107, loss is 0.14850500226020813\n",
      "epoch: 2 step: 1108, loss is 0.06611844152212143\n",
      "epoch: 2 step: 1109, loss is 0.1373629868030548\n",
      "epoch: 2 step: 1110, loss is 0.04477240517735481\n",
      "epoch: 2 step: 1111, loss is 0.029208436608314514\n",
      "epoch: 2 step: 1112, loss is 0.33018001914024353\n",
      "epoch: 2 step: 1113, loss is 0.11700300127267838\n",
      "epoch: 2 step: 1114, loss is 0.2084018439054489\n",
      "epoch: 2 step: 1115, loss is 0.10750998556613922\n",
      "epoch: 2 step: 1116, loss is 0.47364285588264465\n",
      "epoch: 2 step: 1117, loss is 0.023265734314918518\n",
      "epoch: 2 step: 1118, loss is 0.11771892756223679\n",
      "epoch: 2 step: 1119, loss is 0.02895219810307026\n",
      "epoch: 2 step: 1120, loss is 0.0856885239481926\n",
      "epoch: 2 step: 1121, loss is 0.04677619785070419\n",
      "epoch: 2 step: 1122, loss is 0.14643988013267517\n",
      "epoch: 2 step: 1123, loss is 0.016452085226774216\n",
      "epoch: 2 step: 1124, loss is 0.07055974751710892\n",
      "epoch: 2 step: 1125, loss is 0.35174718499183655\n",
      "epoch: 2 step: 1126, loss is 0.24080081284046173\n",
      "epoch: 2 step: 1127, loss is 0.031093498691916466\n",
      "epoch: 2 step: 1128, loss is 0.11470739543437958\n",
      "epoch: 2 step: 1129, loss is 0.04217448830604553\n",
      "epoch: 2 step: 1130, loss is 0.23780281841754913\n",
      "epoch: 2 step: 1131, loss is 0.12914523482322693\n",
      "epoch: 2 step: 1132, loss is 0.011456429027020931\n",
      "epoch: 2 step: 1133, loss is 0.14924925565719604\n",
      "epoch: 2 step: 1134, loss is 0.14310230314731598\n",
      "epoch: 2 step: 1135, loss is 0.018189996480941772\n",
      "epoch: 2 step: 1136, loss is 0.06822086870670319\n",
      "epoch: 2 step: 1137, loss is 0.1788579821586609\n",
      "epoch: 2 step: 1138, loss is 0.0199801754206419\n",
      "epoch: 2 step: 1139, loss is 0.051195431500673294\n",
      "epoch: 2 step: 1140, loss is 0.0592229925096035\n",
      "epoch: 2 step: 1141, loss is 0.0705023854970932\n",
      "epoch: 2 step: 1142, loss is 0.09994731843471527\n",
      "epoch: 2 step: 1143, loss is 0.17749017477035522\n",
      "epoch: 2 step: 1144, loss is 0.035011112689971924\n",
      "epoch: 2 step: 1145, loss is 0.19329671561717987\n",
      "epoch: 2 step: 1146, loss is 0.1655915081501007\n",
      "epoch: 2 step: 1147, loss is 0.022696541622281075\n",
      "epoch: 2 step: 1148, loss is 0.02314845658838749\n",
      "epoch: 2 step: 1149, loss is 0.058356355875730515\n",
      "epoch: 2 step: 1150, loss is 0.017105435952544212\n",
      "epoch: 2 step: 1151, loss is 0.0729043111205101\n",
      "epoch: 2 step: 1152, loss is 0.010050773620605469\n",
      "epoch: 2 step: 1153, loss is 0.1470096856355667\n",
      "epoch: 2 step: 1154, loss is 0.0818919986486435\n",
      "epoch: 2 step: 1155, loss is 0.05160247161984444\n",
      "epoch: 2 step: 1156, loss is 0.15361157059669495\n",
      "epoch: 2 step: 1157, loss is 0.21143531799316406\n",
      "epoch: 2 step: 1158, loss is 0.42895010113716125\n",
      "epoch: 2 step: 1159, loss is 0.04253492131829262\n",
      "epoch: 2 step: 1160, loss is 0.2893494963645935\n",
      "epoch: 2 step: 1161, loss is 0.08943873643875122\n",
      "epoch: 2 step: 1162, loss is 0.04942970350384712\n",
      "epoch: 2 step: 1163, loss is 0.1891552209854126\n",
      "epoch: 2 step: 1164, loss is 0.04342588037252426\n",
      "epoch: 2 step: 1165, loss is 0.033398859202861786\n",
      "epoch: 2 step: 1166, loss is 0.03477872535586357\n",
      "epoch: 2 step: 1167, loss is 0.17216354608535767\n",
      "epoch: 2 step: 1168, loss is 0.03825948387384415\n",
      "epoch: 2 step: 1169, loss is 0.09894256293773651\n",
      "epoch: 2 step: 1170, loss is 0.043157365173101425\n",
      "epoch: 2 step: 1171, loss is 0.03217024356126785\n",
      "epoch: 2 step: 1172, loss is 0.05091036483645439\n",
      "epoch: 2 step: 1173, loss is 0.050192929804325104\n",
      "epoch: 2 step: 1174, loss is 0.034788794815540314\n",
      "epoch: 2 step: 1175, loss is 0.11549007147550583\n",
      "epoch: 2 step: 1176, loss is 0.07030338793992996\n",
      "epoch: 2 step: 1177, loss is 0.0552651509642601\n",
      "epoch: 2 step: 1178, loss is 0.06854981184005737\n",
      "epoch: 2 step: 1179, loss is 0.09550705552101135\n",
      "epoch: 2 step: 1180, loss is 0.0220755934715271\n",
      "epoch: 2 step: 1181, loss is 0.07986176759004593\n",
      "epoch: 2 step: 1182, loss is 0.037828896194696426\n",
      "epoch: 2 step: 1183, loss is 0.075310617685318\n",
      "epoch: 2 step: 1184, loss is 0.2814750671386719\n",
      "epoch: 2 step: 1185, loss is 0.20608238875865936\n",
      "epoch: 2 step: 1186, loss is 0.11503785103559494\n",
      "epoch: 2 step: 1187, loss is 0.003240054240450263\n",
      "epoch: 2 step: 1188, loss is 0.38877180218696594\n",
      "epoch: 2 step: 1189, loss is 0.21620602905750275\n",
      "epoch: 2 step: 1190, loss is 0.042102888226509094\n",
      "epoch: 2 step: 1191, loss is 0.19811974465847015\n",
      "epoch: 2 step: 1192, loss is 0.09723513573408127\n",
      "epoch: 2 step: 1193, loss is 0.22439539432525635\n",
      "epoch: 2 step: 1194, loss is 0.1244954839348793\n",
      "epoch: 2 step: 1195, loss is 0.21507088840007782\n",
      "epoch: 2 step: 1196, loss is 0.13498517870903015\n",
      "epoch: 2 step: 1197, loss is 0.03018808364868164\n",
      "epoch: 2 step: 1198, loss is 0.13688534498214722\n",
      "epoch: 2 step: 1199, loss is 0.23877158761024475\n",
      "epoch: 2 step: 1200, loss is 0.3317238390445709\n",
      "epoch: 2 step: 1201, loss is 0.14333954453468323\n",
      "epoch: 2 step: 1202, loss is 0.07826175540685654\n",
      "epoch: 2 step: 1203, loss is 0.04927462711930275\n",
      "epoch: 2 step: 1204, loss is 0.045521579682826996\n",
      "epoch: 2 step: 1205, loss is 0.21877124905586243\n",
      "epoch: 2 step: 1206, loss is 0.04020410031080246\n",
      "epoch: 2 step: 1207, loss is 0.19810254871845245\n",
      "epoch: 2 step: 1208, loss is 0.2520217299461365\n",
      "epoch: 2 step: 1209, loss is 0.3421316146850586\n",
      "epoch: 2 step: 1210, loss is 0.39908790588378906\n",
      "epoch: 2 step: 1211, loss is 0.09277401119470596\n",
      "epoch: 2 step: 1212, loss is 0.014481741935014725\n",
      "epoch: 2 step: 1213, loss is 0.05139365792274475\n",
      "epoch: 2 step: 1214, loss is 0.02741648629307747\n",
      "epoch: 2 step: 1215, loss is 0.11628803610801697\n",
      "epoch: 2 step: 1216, loss is 0.04265720024704933\n",
      "epoch: 2 step: 1217, loss is 0.0602070651948452\n",
      "epoch: 2 step: 1218, loss is 0.2335456758737564\n",
      "epoch: 2 step: 1219, loss is 0.30820733308792114\n",
      "epoch: 2 step: 1220, loss is 0.2335965633392334\n",
      "epoch: 2 step: 1221, loss is 0.27717289328575134\n",
      "epoch: 2 step: 1222, loss is 0.01938682049512863\n",
      "epoch: 2 step: 1223, loss is 0.07169365137815475\n",
      "epoch: 2 step: 1224, loss is 0.02931812033057213\n",
      "epoch: 2 step: 1225, loss is 0.03572440519928932\n",
      "epoch: 2 step: 1226, loss is 0.18463297188282013\n",
      "epoch: 2 step: 1227, loss is 0.17629830539226532\n",
      "epoch: 2 step: 1228, loss is 0.07739273458719254\n",
      "epoch: 2 step: 1229, loss is 0.16007636487483978\n",
      "epoch: 2 step: 1230, loss is 0.1101699247956276\n",
      "epoch: 2 step: 1231, loss is 0.3186282813549042\n",
      "epoch: 2 step: 1232, loss is 0.004750490188598633\n",
      "epoch: 2 step: 1233, loss is 0.10085546225309372\n",
      "epoch: 2 step: 1234, loss is 0.05064751207828522\n",
      "epoch: 2 step: 1235, loss is 0.13915279507637024\n",
      "epoch: 2 step: 1236, loss is 0.22171851992607117\n",
      "epoch: 2 step: 1237, loss is 0.09555588662624359\n",
      "epoch: 2 step: 1238, loss is 0.06397078186273575\n",
      "epoch: 2 step: 1239, loss is 0.01383701991289854\n",
      "epoch: 2 step: 1240, loss is 0.06386008113622665\n",
      "epoch: 2 step: 1241, loss is 0.028722111135721207\n",
      "epoch: 2 step: 1242, loss is 0.12368898838758469\n",
      "epoch: 2 step: 1243, loss is 0.15724949538707733\n",
      "epoch: 2 step: 1244, loss is 0.0409662127494812\n",
      "epoch: 2 step: 1245, loss is 0.049129728227853775\n",
      "epoch: 2 step: 1246, loss is 0.005164900794625282\n",
      "epoch: 2 step: 1247, loss is 0.006909768097102642\n",
      "epoch: 2 step: 1248, loss is 0.06707442551851273\n",
      "epoch: 2 step: 1249, loss is 0.01159019023180008\n",
      "epoch: 2 step: 1250, loss is 0.04717392846941948\n",
      "epoch: 2 step: 1251, loss is 0.045467715710401535\n",
      "epoch: 2 step: 1252, loss is 0.011927463114261627\n",
      "epoch: 2 step: 1253, loss is 0.33026012778282166\n",
      "epoch: 2 step: 1254, loss is 0.029713014140725136\n",
      "epoch: 2 step: 1255, loss is 0.04524005204439163\n",
      "epoch: 2 step: 1256, loss is 0.1398598700761795\n",
      "epoch: 2 step: 1257, loss is 0.16256912052631378\n",
      "epoch: 2 step: 1258, loss is 0.23349910974502563\n",
      "epoch: 2 step: 1259, loss is 0.06734833866357803\n",
      "epoch: 2 step: 1260, loss is 0.06320943683385849\n",
      "epoch: 2 step: 1261, loss is 0.1534041315317154\n",
      "epoch: 2 step: 1262, loss is 0.40740177035331726\n",
      "epoch: 2 step: 1263, loss is 0.014225497841835022\n",
      "epoch: 2 step: 1264, loss is 0.1905960589647293\n",
      "epoch: 2 step: 1265, loss is 0.0631946474313736\n",
      "epoch: 2 step: 1266, loss is 0.0380776971578598\n",
      "epoch: 2 step: 1267, loss is 0.10547956079244614\n",
      "epoch: 2 step: 1268, loss is 0.15928882360458374\n",
      "epoch: 2 step: 1269, loss is 0.13536113500595093\n",
      "epoch: 2 step: 1270, loss is 0.07598308473825455\n",
      "epoch: 2 step: 1271, loss is 0.1291634738445282\n",
      "epoch: 2 step: 1272, loss is 0.0423404686152935\n",
      "epoch: 2 step: 1273, loss is 0.19575363397598267\n",
      "epoch: 2 step: 1274, loss is 0.0046872831881046295\n",
      "epoch: 2 step: 1275, loss is 0.10174445807933807\n",
      "epoch: 2 step: 1276, loss is 0.035768236964941025\n",
      "epoch: 2 step: 1277, loss is 0.13592591881752014\n",
      "epoch: 2 step: 1278, loss is 0.2247772365808487\n",
      "epoch: 2 step: 1279, loss is 0.1660652458667755\n",
      "epoch: 2 step: 1280, loss is 0.0983879491686821\n",
      "epoch: 2 step: 1281, loss is 0.025614898651838303\n",
      "epoch: 2 step: 1282, loss is 0.10778877884149551\n",
      "epoch: 2 step: 1283, loss is 0.100827157497406\n",
      "epoch: 2 step: 1284, loss is 0.10942179709672928\n",
      "epoch: 2 step: 1285, loss is 0.1692247986793518\n",
      "epoch: 2 step: 1286, loss is 0.09816300123929977\n",
      "epoch: 2 step: 1287, loss is 0.04371768608689308\n",
      "epoch: 2 step: 1288, loss is 0.14942462742328644\n",
      "epoch: 2 step: 1289, loss is 0.17050904035568237\n",
      "epoch: 2 step: 1290, loss is 0.021205823868513107\n",
      "epoch: 2 step: 1291, loss is 0.24546697735786438\n",
      "epoch: 2 step: 1292, loss is 0.06131751835346222\n",
      "epoch: 2 step: 1293, loss is 0.2304535061120987\n",
      "epoch: 2 step: 1294, loss is 0.08664973825216293\n",
      "epoch: 2 step: 1295, loss is 0.0768192857503891\n",
      "epoch: 2 step: 1296, loss is 0.09417250007390976\n",
      "epoch: 2 step: 1297, loss is 0.21133849024772644\n",
      "epoch: 2 step: 1298, loss is 0.2116641402244568\n",
      "epoch: 2 step: 1299, loss is 0.03223692998290062\n",
      "epoch: 2 step: 1300, loss is 0.0188769344240427\n",
      "epoch: 2 step: 1301, loss is 0.07919856160879135\n",
      "epoch: 2 step: 1302, loss is 0.3002370297908783\n",
      "epoch: 2 step: 1303, loss is 0.0048022340051829815\n",
      "epoch: 2 step: 1304, loss is 0.012801970355212688\n",
      "epoch: 2 step: 1305, loss is 0.1047215610742569\n",
      "epoch: 2 step: 1306, loss is 0.07543975859880447\n",
      "epoch: 2 step: 1307, loss is 0.13979598879814148\n",
      "epoch: 2 step: 1308, loss is 0.24081751704216003\n",
      "epoch: 2 step: 1309, loss is 0.5964351892471313\n",
      "epoch: 2 step: 1310, loss is 0.09543904662132263\n",
      "epoch: 2 step: 1311, loss is 0.24783897399902344\n",
      "epoch: 2 step: 1312, loss is 0.0294270571321249\n",
      "epoch: 2 step: 1313, loss is 0.3139568567276001\n",
      "epoch: 2 step: 1314, loss is 0.0873318538069725\n",
      "epoch: 2 step: 1315, loss is 0.02396821230649948\n",
      "epoch: 2 step: 1316, loss is 0.025655176490545273\n",
      "epoch: 2 step: 1317, loss is 0.11839541047811508\n",
      "epoch: 2 step: 1318, loss is 0.11324884742498398\n",
      "epoch: 2 step: 1319, loss is 0.026193032041192055\n",
      "epoch: 2 step: 1320, loss is 0.06235964968800545\n",
      "epoch: 2 step: 1321, loss is 0.19341248273849487\n",
      "epoch: 2 step: 1322, loss is 0.1888710856437683\n",
      "epoch: 2 step: 1323, loss is 0.21183376014232635\n",
      "epoch: 2 step: 1324, loss is 0.12392731010913849\n",
      "epoch: 2 step: 1325, loss is 0.07140683382749557\n",
      "epoch: 2 step: 1326, loss is 0.08188176155090332\n",
      "epoch: 2 step: 1327, loss is 0.09581756591796875\n",
      "epoch: 2 step: 1328, loss is 0.3818078935146332\n",
      "epoch: 2 step: 1329, loss is 0.0211010929197073\n",
      "epoch: 2 step: 1330, loss is 0.18262706696987152\n",
      "epoch: 2 step: 1331, loss is 0.08577895909547806\n",
      "epoch: 2 step: 1332, loss is 0.22351062297821045\n",
      "epoch: 2 step: 1333, loss is 0.18698027729988098\n",
      "epoch: 2 step: 1334, loss is 0.054054465144872665\n",
      "epoch: 2 step: 1335, loss is 0.10301551967859268\n",
      "epoch: 2 step: 1336, loss is 0.024330148473381996\n",
      "epoch: 2 step: 1337, loss is 0.06851445883512497\n",
      "epoch: 2 step: 1338, loss is 0.03649367019534111\n",
      "epoch: 2 step: 1339, loss is 0.014721566811203957\n",
      "epoch: 2 step: 1340, loss is 0.2130724936723709\n",
      "epoch: 2 step: 1341, loss is 0.021424489095807076\n",
      "epoch: 2 step: 1342, loss is 0.019151775166392326\n",
      "epoch: 2 step: 1343, loss is 0.18733061850070953\n",
      "epoch: 2 step: 1344, loss is 0.15744927525520325\n",
      "epoch: 2 step: 1345, loss is 0.12988463044166565\n",
      "epoch: 2 step: 1346, loss is 0.03463103622198105\n",
      "epoch: 2 step: 1347, loss is 0.08061961084604263\n",
      "epoch: 2 step: 1348, loss is 0.021752795204520226\n",
      "epoch: 2 step: 1349, loss is 0.212222158908844\n",
      "epoch: 2 step: 1350, loss is 0.2797502279281616\n",
      "epoch: 2 step: 1351, loss is 0.11725476384162903\n",
      "epoch: 2 step: 1352, loss is 0.022893786430358887\n",
      "epoch: 2 step: 1353, loss is 0.0828184261918068\n",
      "epoch: 2 step: 1354, loss is 0.016212884336709976\n",
      "epoch: 2 step: 1355, loss is 0.2714148163795471\n",
      "epoch: 2 step: 1356, loss is 0.02061484195291996\n",
      "epoch: 2 step: 1357, loss is 0.04578608274459839\n",
      "epoch: 2 step: 1358, loss is 0.06103949993848801\n",
      "epoch: 2 step: 1359, loss is 0.04435593634843826\n",
      "epoch: 2 step: 1360, loss is 0.03611759841442108\n",
      "epoch: 2 step: 1361, loss is 0.1595500260591507\n",
      "epoch: 2 step: 1362, loss is 0.02643405646085739\n",
      "epoch: 2 step: 1363, loss is 0.11576373130083084\n",
      "epoch: 2 step: 1364, loss is 0.13730251789093018\n",
      "epoch: 2 step: 1365, loss is 0.19251899421215057\n",
      "epoch: 2 step: 1366, loss is 0.05399224907159805\n",
      "epoch: 2 step: 1367, loss is 0.02696947567164898\n",
      "epoch: 2 step: 1368, loss is 0.07048024982213974\n",
      "epoch: 2 step: 1369, loss is 0.01250535249710083\n",
      "epoch: 2 step: 1370, loss is 0.007781230378895998\n",
      "epoch: 2 step: 1371, loss is 0.056230656802654266\n",
      "epoch: 2 step: 1372, loss is 0.05802032724022865\n",
      "epoch: 2 step: 1373, loss is 0.04160171374678612\n",
      "epoch: 2 step: 1374, loss is 0.24177534878253937\n",
      "epoch: 2 step: 1375, loss is 0.04238022863864899\n",
      "epoch: 2 step: 1376, loss is 0.01792878657579422\n",
      "epoch: 2 step: 1377, loss is 0.08803056925535202\n",
      "epoch: 2 step: 1378, loss is 0.18733365833759308\n",
      "epoch: 2 step: 1379, loss is 0.11591146141290665\n",
      "epoch: 2 step: 1380, loss is 0.07827819883823395\n",
      "epoch: 2 step: 1381, loss is 0.2509186863899231\n",
      "epoch: 2 step: 1382, loss is 0.325360506772995\n",
      "epoch: 2 step: 1383, loss is 0.1546361744403839\n",
      "epoch: 2 step: 1384, loss is 0.11442568898200989\n",
      "epoch: 2 step: 1385, loss is 0.01728176884353161\n",
      "epoch: 2 step: 1386, loss is 0.056674908846616745\n",
      "epoch: 2 step: 1387, loss is 0.10905658453702927\n",
      "epoch: 2 step: 1388, loss is 0.0012827948667109013\n",
      "epoch: 2 step: 1389, loss is 0.14373625814914703\n",
      "epoch: 2 step: 1390, loss is 0.032225754112005234\n",
      "epoch: 2 step: 1391, loss is 0.1187170073390007\n",
      "epoch: 2 step: 1392, loss is 0.4001059830188751\n",
      "epoch: 2 step: 1393, loss is 0.042191993445158005\n",
      "epoch: 2 step: 1394, loss is 0.35961776971817017\n",
      "epoch: 2 step: 1395, loss is 0.10686059296131134\n",
      "epoch: 2 step: 1396, loss is 0.15349926054477692\n",
      "epoch: 2 step: 1397, loss is 0.15892547369003296\n",
      "epoch: 2 step: 1398, loss is 0.003943368326872587\n",
      "epoch: 2 step: 1399, loss is 0.18436338007450104\n",
      "epoch: 2 step: 1400, loss is 0.07660320401191711\n",
      "epoch: 2 step: 1401, loss is 0.2895558178424835\n",
      "epoch: 2 step: 1402, loss is 0.04160197079181671\n",
      "epoch: 2 step: 1403, loss is 0.290768027305603\n",
      "epoch: 2 step: 1404, loss is 0.13253635168075562\n",
      "epoch: 2 step: 1405, loss is 0.031425487250089645\n",
      "epoch: 2 step: 1406, loss is 0.018612390384078026\n",
      "epoch: 2 step: 1407, loss is 0.032063525170087814\n",
      "epoch: 2 step: 1408, loss is 0.3509407043457031\n",
      "epoch: 2 step: 1409, loss is 0.022526249289512634\n",
      "epoch: 2 step: 1410, loss is 0.5758645534515381\n",
      "epoch: 2 step: 1411, loss is 0.05547187104821205\n",
      "epoch: 2 step: 1412, loss is 0.010983469896018505\n",
      "epoch: 2 step: 1413, loss is 0.009588724933564663\n",
      "epoch: 2 step: 1414, loss is 0.047008905559778214\n",
      "epoch: 2 step: 1415, loss is 0.016846273094415665\n",
      "epoch: 2 step: 1416, loss is 0.38486987352371216\n",
      "epoch: 2 step: 1417, loss is 0.034332796931266785\n",
      "epoch: 2 step: 1418, loss is 0.04585832729935646\n",
      "epoch: 2 step: 1419, loss is 0.08607875555753708\n",
      "epoch: 2 step: 1420, loss is 0.08188354223966599\n",
      "epoch: 2 step: 1421, loss is 0.30307063460350037\n",
      "epoch: 2 step: 1422, loss is 0.28886497020721436\n",
      "epoch: 2 step: 1423, loss is 0.010667105205357075\n",
      "epoch: 2 step: 1424, loss is 0.15559865534305573\n",
      "epoch: 2 step: 1425, loss is 0.03595149517059326\n",
      "epoch: 2 step: 1426, loss is 0.11947855353355408\n",
      "epoch: 2 step: 1427, loss is 0.014654317870736122\n",
      "epoch: 2 step: 1428, loss is 0.030972864478826523\n",
      "epoch: 2 step: 1429, loss is 0.1857103705406189\n",
      "epoch: 2 step: 1430, loss is 0.0674137994647026\n",
      "epoch: 2 step: 1431, loss is 0.04583641514182091\n",
      "epoch: 2 step: 1432, loss is 0.04624468833208084\n",
      "epoch: 2 step: 1433, loss is 0.07808744162321091\n",
      "epoch: 2 step: 1434, loss is 0.06324517726898193\n",
      "epoch: 2 step: 1435, loss is 0.13670510053634644\n",
      "epoch: 2 step: 1436, loss is 0.007268419489264488\n",
      "epoch: 2 step: 1437, loss is 0.1262444704771042\n",
      "epoch: 2 step: 1438, loss is 0.06462962925434113\n",
      "epoch: 2 step: 1439, loss is 0.1478051394224167\n",
      "epoch: 2 step: 1440, loss is 0.05978978052735329\n",
      "epoch: 2 step: 1441, loss is 0.17012064158916473\n",
      "epoch: 2 step: 1442, loss is 0.055545318871736526\n",
      "epoch: 2 step: 1443, loss is 0.030091356486082077\n",
      "epoch: 2 step: 1444, loss is 0.048197586089372635\n",
      "epoch: 2 step: 1445, loss is 0.30564776062965393\n",
      "epoch: 2 step: 1446, loss is 0.00783889926970005\n",
      "epoch: 2 step: 1447, loss is 0.050512224435806274\n",
      "epoch: 2 step: 1448, loss is 0.1863071620464325\n",
      "epoch: 2 step: 1449, loss is 0.2269405722618103\n",
      "epoch: 2 step: 1450, loss is 0.05977824330329895\n",
      "epoch: 2 step: 1451, loss is 0.05010101571679115\n",
      "epoch: 2 step: 1452, loss is 0.26738181710243225\n",
      "epoch: 2 step: 1453, loss is 0.027649641036987305\n",
      "epoch: 2 step: 1454, loss is 0.06845656037330627\n",
      "epoch: 2 step: 1455, loss is 0.10359469801187515\n",
      "epoch: 2 step: 1456, loss is 0.32124269008636475\n",
      "epoch: 2 step: 1457, loss is 0.10311667621135712\n",
      "epoch: 2 step: 1458, loss is 0.06126515567302704\n",
      "epoch: 2 step: 1459, loss is 0.11056719720363617\n",
      "epoch: 2 step: 1460, loss is 0.057407625019550323\n",
      "epoch: 2 step: 1461, loss is 0.05685059726238251\n",
      "epoch: 2 step: 1462, loss is 0.12997396290302277\n",
      "epoch: 2 step: 1463, loss is 0.032673660665750504\n",
      "epoch: 2 step: 1464, loss is 0.023745782673358917\n",
      "epoch: 2 step: 1465, loss is 0.19133928418159485\n",
      "epoch: 2 step: 1466, loss is 0.07921373099088669\n",
      "epoch: 2 step: 1467, loss is 0.09746004641056061\n",
      "epoch: 2 step: 1468, loss is 0.009713394567370415\n",
      "epoch: 2 step: 1469, loss is 0.01421042438596487\n",
      "epoch: 2 step: 1470, loss is 0.10797371715307236\n",
      "epoch: 2 step: 1471, loss is 0.28988373279571533\n",
      "epoch: 2 step: 1472, loss is 0.02274450473487377\n",
      "epoch: 2 step: 1473, loss is 0.06326581537723541\n",
      "epoch: 2 step: 1474, loss is 0.020625486969947815\n",
      "epoch: 2 step: 1475, loss is 0.13720636069774628\n",
      "epoch: 2 step: 1476, loss is 0.006082247477024794\n",
      "epoch: 2 step: 1477, loss is 0.04615895077586174\n",
      "epoch: 2 step: 1478, loss is 0.06266625970602036\n",
      "epoch: 2 step: 1479, loss is 0.13079911470413208\n",
      "epoch: 2 step: 1480, loss is 0.11166776716709137\n",
      "epoch: 2 step: 1481, loss is 0.04623580724000931\n",
      "epoch: 2 step: 1482, loss is 0.0756659060716629\n",
      "epoch: 2 step: 1483, loss is 0.1216166689991951\n",
      "epoch: 2 step: 1484, loss is 0.05449145287275314\n",
      "epoch: 2 step: 1485, loss is 0.3364613950252533\n",
      "epoch: 2 step: 1486, loss is 0.02708115242421627\n",
      "epoch: 2 step: 1487, loss is 0.04626525938510895\n",
      "epoch: 2 step: 1488, loss is 0.10996203124523163\n",
      "epoch: 2 step: 1489, loss is 0.01636366918683052\n",
      "epoch: 2 step: 1490, loss is 0.12579897046089172\n",
      "epoch: 2 step: 1491, loss is 0.07049088925123215\n",
      "epoch: 2 step: 1492, loss is 0.3736037611961365\n",
      "epoch: 2 step: 1493, loss is 0.05592444911599159\n",
      "epoch: 2 step: 1494, loss is 0.11457502841949463\n",
      "epoch: 2 step: 1495, loss is 0.025812910869717598\n",
      "epoch: 2 step: 1496, loss is 0.030117200687527657\n",
      "epoch: 2 step: 1497, loss is 0.029630623757839203\n",
      "epoch: 2 step: 1498, loss is 0.11303680390119553\n",
      "epoch: 2 step: 1499, loss is 0.12440764158964157\n",
      "epoch: 2 step: 1500, loss is 0.0052915397100150585\n",
      "epoch: 2 step: 1501, loss is 0.32291513681411743\n",
      "epoch: 2 step: 1502, loss is 0.07179654389619827\n",
      "epoch: 2 step: 1503, loss is 0.13000045716762543\n",
      "epoch: 2 step: 1504, loss is 0.1894892156124115\n",
      "epoch: 2 step: 1505, loss is 0.10370123386383057\n",
      "epoch: 2 step: 1506, loss is 0.008725522086024284\n",
      "epoch: 2 step: 1507, loss is 0.03943529352545738\n",
      "epoch: 2 step: 1508, loss is 0.02262257970869541\n",
      "epoch: 2 step: 1509, loss is 0.08965515345335007\n",
      "epoch: 2 step: 1510, loss is 0.307624876499176\n",
      "epoch: 2 step: 1511, loss is 0.10466709733009338\n",
      "epoch: 2 step: 1512, loss is 0.2425883710384369\n",
      "epoch: 2 step: 1513, loss is 0.0332527719438076\n",
      "epoch: 2 step: 1514, loss is 0.02837156131863594\n",
      "epoch: 2 step: 1515, loss is 0.005700876470655203\n",
      "epoch: 2 step: 1516, loss is 0.04146973416209221\n",
      "epoch: 2 step: 1517, loss is 0.04720307141542435\n",
      "epoch: 2 step: 1518, loss is 0.11553795635700226\n",
      "epoch: 2 step: 1519, loss is 0.01564016379415989\n",
      "epoch: 2 step: 1520, loss is 0.12922991812229156\n",
      "epoch: 2 step: 1521, loss is 0.1719341278076172\n",
      "epoch: 2 step: 1522, loss is 0.13089589774608612\n",
      "epoch: 2 step: 1523, loss is 0.14312289655208588\n",
      "epoch: 2 step: 1524, loss is 0.18645285069942474\n",
      "epoch: 2 step: 1525, loss is 0.062486682087183\n",
      "epoch: 2 step: 1526, loss is 0.24523665010929108\n",
      "epoch: 2 step: 1527, loss is 0.04614110663533211\n",
      "epoch: 2 step: 1528, loss is 0.002301450353115797\n",
      "epoch: 2 step: 1529, loss is 0.21577949821949005\n",
      "epoch: 2 step: 1530, loss is 0.03062169998884201\n",
      "epoch: 2 step: 1531, loss is 0.030313560739159584\n",
      "epoch: 2 step: 1532, loss is 0.01331818476319313\n",
      "epoch: 2 step: 1533, loss is 0.02688491903245449\n",
      "epoch: 2 step: 1534, loss is 0.015338767319917679\n",
      "epoch: 2 step: 1535, loss is 0.2698461711406708\n",
      "epoch: 2 step: 1536, loss is 0.3205212354660034\n",
      "epoch: 2 step: 1537, loss is 0.11878139525651932\n",
      "epoch: 2 step: 1538, loss is 0.054029740393161774\n",
      "epoch: 2 step: 1539, loss is 0.15833567082881927\n",
      "epoch: 2 step: 1540, loss is 0.00438235467299819\n",
      "epoch: 2 step: 1541, loss is 0.006279547233134508\n",
      "epoch: 2 step: 1542, loss is 0.07832388579845428\n",
      "epoch: 2 step: 1543, loss is 0.006616394501179457\n",
      "epoch: 2 step: 1544, loss is 0.018709557130932808\n",
      "epoch: 2 step: 1545, loss is 0.048301272094249725\n",
      "epoch: 2 step: 1546, loss is 0.021486101672053337\n",
      "epoch: 2 step: 1547, loss is 0.005359027534723282\n",
      "epoch: 2 step: 1548, loss is 0.0048186322674155235\n",
      "epoch: 2 step: 1549, loss is 0.09782896935939789\n",
      "epoch: 2 step: 1550, loss is 0.31161147356033325\n",
      "epoch: 2 step: 1551, loss is 0.2646370530128479\n",
      "epoch: 2 step: 1552, loss is 0.05485661327838898\n",
      "epoch: 2 step: 1553, loss is 0.012990954332053661\n",
      "epoch: 2 step: 1554, loss is 0.23687484860420227\n",
      "epoch: 2 step: 1555, loss is 0.08665575087070465\n",
      "epoch: 2 step: 1556, loss is 0.2088516354560852\n",
      "epoch: 2 step: 1557, loss is 0.017884327098727226\n",
      "epoch: 2 step: 1558, loss is 0.013771569356322289\n",
      "epoch: 2 step: 1559, loss is 0.030382603406906128\n",
      "epoch: 2 step: 1560, loss is 0.18674933910369873\n",
      "epoch: 2 step: 1561, loss is 0.07485663145780563\n",
      "epoch: 2 step: 1562, loss is 0.09399233013391495\n",
      "epoch: 2 step: 1563, loss is 0.10443970561027527\n",
      "epoch: 2 step: 1564, loss is 0.030233370140194893\n",
      "epoch: 2 step: 1565, loss is 0.18037031590938568\n",
      "epoch: 2 step: 1566, loss is 0.03325203061103821\n",
      "epoch: 2 step: 1567, loss is 0.11567457020282745\n",
      "epoch: 2 step: 1568, loss is 0.2701078951358795\n",
      "epoch: 2 step: 1569, loss is 0.03354302793741226\n",
      "epoch: 2 step: 1570, loss is 0.025293324142694473\n",
      "epoch: 2 step: 1571, loss is 0.041496392339468\n",
      "epoch: 2 step: 1572, loss is 0.22456640005111694\n",
      "epoch: 2 step: 1573, loss is 0.06868594139814377\n",
      "epoch: 2 step: 1574, loss is 0.1833372861146927\n",
      "epoch: 2 step: 1575, loss is 0.112481489777565\n",
      "epoch: 2 step: 1576, loss is 0.056868232786655426\n",
      "epoch: 2 step: 1577, loss is 0.0891343206167221\n",
      "epoch: 2 step: 1578, loss is 0.008626497350633144\n",
      "epoch: 2 step: 1579, loss is 0.02249903231859207\n",
      "epoch: 2 step: 1580, loss is 0.06571526825428009\n",
      "epoch: 2 step: 1581, loss is 0.10831425338983536\n",
      "epoch: 2 step: 1582, loss is 0.01479052472859621\n",
      "epoch: 2 step: 1583, loss is 0.07972533255815506\n",
      "epoch: 2 step: 1584, loss is 0.09031019359827042\n",
      "epoch: 2 step: 1585, loss is 0.4894958436489105\n",
      "epoch: 2 step: 1586, loss is 0.20517361164093018\n",
      "epoch: 2 step: 1587, loss is 0.024104902520775795\n",
      "epoch: 2 step: 1588, loss is 0.013399728573858738\n",
      "epoch: 2 step: 1589, loss is 0.22044873237609863\n",
      "epoch: 2 step: 1590, loss is 0.1446446180343628\n",
      "epoch: 2 step: 1591, loss is 0.1851658672094345\n",
      "epoch: 2 step: 1592, loss is 0.009665114805102348\n",
      "epoch: 2 step: 1593, loss is 0.067722387611866\n",
      "epoch: 2 step: 1594, loss is 0.04502848908305168\n",
      "epoch: 2 step: 1595, loss is 0.07138927280902863\n",
      "epoch: 2 step: 1596, loss is 0.03310173377394676\n",
      "epoch: 2 step: 1597, loss is 0.032674431800842285\n",
      "epoch: 2 step: 1598, loss is 0.25613075494766235\n",
      "epoch: 2 step: 1599, loss is 0.1437949240207672\n",
      "epoch: 2 step: 1600, loss is 0.038991693407297134\n",
      "epoch: 2 step: 1601, loss is 0.03906936198472977\n",
      "epoch: 2 step: 1602, loss is 0.2832711935043335\n",
      "epoch: 2 step: 1603, loss is 0.0246263537555933\n",
      "epoch: 2 step: 1604, loss is 0.07767844945192337\n",
      "epoch: 2 step: 1605, loss is 0.08594633638858795\n",
      "epoch: 2 step: 1606, loss is 0.02833186276257038\n",
      "epoch: 2 step: 1607, loss is 0.05213771387934685\n",
      "epoch: 2 step: 1608, loss is 0.024004725739359856\n",
      "epoch: 2 step: 1609, loss is 0.14654599130153656\n",
      "epoch: 2 step: 1610, loss is 0.20405523478984833\n",
      "epoch: 2 step: 1611, loss is 0.052184347063302994\n",
      "epoch: 2 step: 1612, loss is 0.1760421097278595\n",
      "epoch: 2 step: 1613, loss is 0.08787009119987488\n",
      "epoch: 2 step: 1614, loss is 0.0890285074710846\n",
      "epoch: 2 step: 1615, loss is 0.04389321431517601\n",
      "epoch: 2 step: 1616, loss is 0.04032875970005989\n",
      "epoch: 2 step: 1617, loss is 0.10700834542512894\n",
      "epoch: 2 step: 1618, loss is 0.5377867221832275\n",
      "epoch: 2 step: 1619, loss is 0.056960009038448334\n",
      "epoch: 2 step: 1620, loss is 0.04378539323806763\n",
      "epoch: 2 step: 1621, loss is 0.2879812717437744\n",
      "epoch: 2 step: 1622, loss is 0.07582563161849976\n",
      "epoch: 2 step: 1623, loss is 0.027302565053105354\n",
      "epoch: 2 step: 1624, loss is 0.12724901735782623\n",
      "epoch: 2 step: 1625, loss is 0.06917932629585266\n",
      "epoch: 2 step: 1626, loss is 0.018405398353934288\n",
      "epoch: 2 step: 1627, loss is 0.1310194432735443\n",
      "epoch: 2 step: 1628, loss is 0.04782779514789581\n",
      "epoch: 2 step: 1629, loss is 0.02292585000395775\n",
      "epoch: 2 step: 1630, loss is 0.02975020743906498\n",
      "epoch: 2 step: 1631, loss is 0.013284505344927311\n",
      "epoch: 2 step: 1632, loss is 0.14152278006076813\n",
      "epoch: 2 step: 1633, loss is 0.06136138737201691\n",
      "epoch: 2 step: 1634, loss is 0.13847573101520538\n",
      "epoch: 2 step: 1635, loss is 0.07026318460702896\n",
      "epoch: 2 step: 1636, loss is 0.06825853884220123\n",
      "epoch: 2 step: 1637, loss is 0.011718283407390118\n",
      "epoch: 2 step: 1638, loss is 0.05530782416462898\n",
      "epoch: 2 step: 1639, loss is 0.040370773524045944\n",
      "epoch: 2 step: 1640, loss is 0.28508260846138\n",
      "epoch: 2 step: 1641, loss is 0.04684238135814667\n",
      "epoch: 2 step: 1642, loss is 0.08500335365533829\n",
      "epoch: 2 step: 1643, loss is 0.27406826615333557\n",
      "epoch: 2 step: 1644, loss is 0.027175243943929672\n",
      "epoch: 2 step: 1645, loss is 0.005399465560913086\n",
      "epoch: 2 step: 1646, loss is 0.04710092395544052\n",
      "epoch: 2 step: 1647, loss is 0.23870468139648438\n",
      "epoch: 2 step: 1648, loss is 0.1603843867778778\n",
      "epoch: 2 step: 1649, loss is 0.10379000753164291\n",
      "epoch: 2 step: 1650, loss is 0.011384358629584312\n",
      "epoch: 2 step: 1651, loss is 0.04903629049658775\n",
      "epoch: 2 step: 1652, loss is 0.10776695609092712\n",
      "epoch: 2 step: 1653, loss is 0.24515247344970703\n",
      "epoch: 2 step: 1654, loss is 0.11806625872850418\n",
      "epoch: 2 step: 1655, loss is 0.07593253999948502\n",
      "epoch: 2 step: 1656, loss is 0.024797847494482994\n",
      "epoch: 2 step: 1657, loss is 0.05149397253990173\n",
      "epoch: 2 step: 1658, loss is 0.0244806669652462\n",
      "epoch: 2 step: 1659, loss is 0.01897314190864563\n",
      "epoch: 2 step: 1660, loss is 0.2548007369041443\n",
      "epoch: 2 step: 1661, loss is 0.004363367334008217\n",
      "epoch: 2 step: 1662, loss is 0.6476632952690125\n",
      "epoch: 2 step: 1663, loss is 0.13565300405025482\n",
      "epoch: 2 step: 1664, loss is 0.11187881231307983\n",
      "epoch: 2 step: 1665, loss is 0.5768752098083496\n",
      "epoch: 2 step: 1666, loss is 0.02709689363837242\n",
      "epoch: 2 step: 1667, loss is 0.030268609523773193\n",
      "epoch: 2 step: 1668, loss is 0.15880733728408813\n",
      "epoch: 2 step: 1669, loss is 0.15075509250164032\n",
      "epoch: 2 step: 1670, loss is 0.2754548490047455\n",
      "epoch: 2 step: 1671, loss is 0.08591466397047043\n",
      "epoch: 2 step: 1672, loss is 0.0062368144281208515\n",
      "epoch: 2 step: 1673, loss is 0.017586462199687958\n",
      "epoch: 2 step: 1674, loss is 0.07039280980825424\n",
      "epoch: 2 step: 1675, loss is 0.007801474072039127\n",
      "epoch: 2 step: 1676, loss is 0.12516257166862488\n",
      "epoch: 2 step: 1677, loss is 0.2068507820367813\n",
      "epoch: 2 step: 1678, loss is 0.004570407792925835\n",
      "epoch: 2 step: 1679, loss is 0.10257350653409958\n",
      "epoch: 2 step: 1680, loss is 0.2435344159603119\n",
      "epoch: 2 step: 1681, loss is 0.12784641981124878\n",
      "epoch: 2 step: 1682, loss is 0.05340142920613289\n",
      "epoch: 2 step: 1683, loss is 0.06310927867889404\n",
      "epoch: 2 step: 1684, loss is 0.006651718635112047\n",
      "epoch: 2 step: 1685, loss is 0.15786537528038025\n",
      "epoch: 2 step: 1686, loss is 0.003887773025780916\n",
      "epoch: 2 step: 1687, loss is 0.014644931070506573\n",
      "epoch: 2 step: 1688, loss is 0.4335930049419403\n",
      "epoch: 2 step: 1689, loss is 0.07393807172775269\n",
      "epoch: 2 step: 1690, loss is 0.04806205630302429\n",
      "epoch: 2 step: 1691, loss is 0.23247167468070984\n",
      "epoch: 2 step: 1692, loss is 0.10003942996263504\n",
      "epoch: 2 step: 1693, loss is 0.02811655029654503\n",
      "epoch: 2 step: 1694, loss is 0.15674228966236115\n",
      "epoch: 2 step: 1695, loss is 0.013467134907841682\n",
      "epoch: 2 step: 1696, loss is 0.1683497130870819\n",
      "epoch: 2 step: 1697, loss is 0.21321134269237518\n",
      "epoch: 2 step: 1698, loss is 0.057830341160297394\n",
      "epoch: 2 step: 1699, loss is 0.04171154275536537\n",
      "epoch: 2 step: 1700, loss is 0.029578922316432\n",
      "epoch: 2 step: 1701, loss is 0.26739418506622314\n",
      "epoch: 2 step: 1702, loss is 0.049227677285671234\n",
      "epoch: 2 step: 1703, loss is 0.023351239040493965\n",
      "epoch: 2 step: 1704, loss is 0.04267292469739914\n",
      "epoch: 2 step: 1705, loss is 0.3104686439037323\n",
      "epoch: 2 step: 1706, loss is 0.09990835189819336\n",
      "epoch: 2 step: 1707, loss is 0.025984421372413635\n",
      "epoch: 2 step: 1708, loss is 0.02313048020005226\n",
      "epoch: 2 step: 1709, loss is 0.005376855377107859\n",
      "epoch: 2 step: 1710, loss is 0.08867555111646652\n",
      "epoch: 2 step: 1711, loss is 0.10917194187641144\n",
      "epoch: 2 step: 1712, loss is 0.03727534040808678\n",
      "epoch: 2 step: 1713, loss is 0.06463369727134705\n",
      "epoch: 2 step: 1714, loss is 0.1929967701435089\n",
      "epoch: 2 step: 1715, loss is 0.1450684815645218\n",
      "epoch: 2 step: 1716, loss is 0.0231801588088274\n",
      "epoch: 2 step: 1717, loss is 0.24198022484779358\n",
      "epoch: 2 step: 1718, loss is 0.04515821859240532\n",
      "epoch: 2 step: 1719, loss is 0.1242452934384346\n",
      "epoch: 2 step: 1720, loss is 0.04847754165530205\n",
      "epoch: 2 step: 1721, loss is 0.05228190869092941\n",
      "epoch: 2 step: 1722, loss is 0.05901850387454033\n",
      "epoch: 2 step: 1723, loss is 0.024612970650196075\n",
      "epoch: 2 step: 1724, loss is 0.08086233586072922\n",
      "epoch: 2 step: 1725, loss is 0.07590967416763306\n",
      "epoch: 2 step: 1726, loss is 0.11458008736371994\n",
      "epoch: 2 step: 1727, loss is 0.010468161664903164\n",
      "epoch: 2 step: 1728, loss is 0.21638447046279907\n",
      "epoch: 2 step: 1729, loss is 0.034544482827186584\n",
      "epoch: 2 step: 1730, loss is 0.06461496651172638\n",
      "epoch: 2 step: 1731, loss is 0.018199143931269646\n",
      "epoch: 2 step: 1732, loss is 0.033625997602939606\n",
      "epoch: 2 step: 1733, loss is 0.010603591799736023\n",
      "epoch: 2 step: 1734, loss is 0.17039209604263306\n",
      "epoch: 2 step: 1735, loss is 0.03191426396369934\n",
      "epoch: 2 step: 1736, loss is 0.1521773636341095\n",
      "epoch: 2 step: 1737, loss is 0.08186714351177216\n",
      "epoch: 2 step: 1738, loss is 0.07956737279891968\n",
      "epoch: 2 step: 1739, loss is 0.0334247462451458\n",
      "epoch: 2 step: 1740, loss is 0.2181476354598999\n",
      "epoch: 2 step: 1741, loss is 0.10455899685621262\n",
      "epoch: 2 step: 1742, loss is 0.0412905216217041\n",
      "epoch: 2 step: 1743, loss is 0.09296364337205887\n",
      "epoch: 2 step: 1744, loss is 0.005983857437968254\n",
      "epoch: 2 step: 1745, loss is 0.023839786648750305\n",
      "epoch: 2 step: 1746, loss is 0.1259962022304535\n",
      "epoch: 2 step: 1747, loss is 0.30399277806282043\n",
      "epoch: 2 step: 1748, loss is 0.02348656952381134\n",
      "epoch: 2 step: 1749, loss is 0.059431277215480804\n",
      "epoch: 2 step: 1750, loss is 0.06688924133777618\n",
      "epoch: 2 step: 1751, loss is 0.1440984606742859\n",
      "epoch: 2 step: 1752, loss is 0.05151093006134033\n",
      "epoch: 2 step: 1753, loss is 0.12128858268260956\n",
      "epoch: 2 step: 1754, loss is 0.02674800157546997\n",
      "epoch: 2 step: 1755, loss is 0.005399126093834639\n",
      "epoch: 2 step: 1756, loss is 0.1790771484375\n",
      "epoch: 2 step: 1757, loss is 0.016844941303133965\n",
      "epoch: 2 step: 1758, loss is 0.09611408412456512\n",
      "epoch: 2 step: 1759, loss is 0.21962743997573853\n",
      "epoch: 2 step: 1760, loss is 0.0727139562368393\n",
      "epoch: 2 step: 1761, loss is 0.2364213913679123\n",
      "epoch: 2 step: 1762, loss is 0.05270638316869736\n",
      "epoch: 2 step: 1763, loss is 0.09686009585857391\n",
      "epoch: 2 step: 1764, loss is 0.15179167687892914\n",
      "epoch: 2 step: 1765, loss is 0.06971913576126099\n",
      "epoch: 2 step: 1766, loss is 0.012503435835242271\n",
      "epoch: 2 step: 1767, loss is 0.06402859091758728\n",
      "epoch: 2 step: 1768, loss is 0.09072376042604446\n",
      "epoch: 2 step: 1769, loss is 0.29854729771614075\n",
      "epoch: 2 step: 1770, loss is 0.004448402673006058\n",
      "epoch: 2 step: 1771, loss is 0.09690581262111664\n",
      "epoch: 2 step: 1772, loss is 0.014906413853168488\n",
      "epoch: 2 step: 1773, loss is 0.06359576433897018\n",
      "epoch: 2 step: 1774, loss is 0.06143356114625931\n",
      "epoch: 2 step: 1775, loss is 0.11995618790388107\n",
      "epoch: 2 step: 1776, loss is 0.11400245130062103\n",
      "epoch: 2 step: 1777, loss is 0.359932005405426\n",
      "epoch: 2 step: 1778, loss is 0.06994149833917618\n",
      "epoch: 2 step: 1779, loss is 0.06308694928884506\n",
      "epoch: 2 step: 1780, loss is 0.01925145834684372\n",
      "epoch: 2 step: 1781, loss is 0.09739900380373001\n",
      "epoch: 2 step: 1782, loss is 0.13010495901107788\n",
      "epoch: 2 step: 1783, loss is 0.014221351593732834\n",
      "epoch: 2 step: 1784, loss is 0.2558959722518921\n",
      "epoch: 2 step: 1785, loss is 0.038584090769290924\n",
      "epoch: 2 step: 1786, loss is 0.2561753988265991\n",
      "epoch: 2 step: 1787, loss is 0.12927597761154175\n",
      "epoch: 2 step: 1788, loss is 0.06234521046280861\n",
      "epoch: 2 step: 1789, loss is 0.08229519426822662\n",
      "epoch: 2 step: 1790, loss is 0.17930731177330017\n",
      "epoch: 2 step: 1791, loss is 0.03818131238222122\n",
      "epoch: 2 step: 1792, loss is 0.021639177575707436\n",
      "epoch: 2 step: 1793, loss is 0.05910639837384224\n",
      "epoch: 2 step: 1794, loss is 0.01673499494791031\n",
      "epoch: 2 step: 1795, loss is 0.02262653224170208\n",
      "epoch: 2 step: 1796, loss is 0.031407006084918976\n",
      "epoch: 2 step: 1797, loss is 0.04589860513806343\n",
      "epoch: 2 step: 1798, loss is 0.18049728870391846\n",
      "epoch: 2 step: 1799, loss is 0.11410865187644958\n",
      "epoch: 2 step: 1800, loss is 0.029380613937973976\n",
      "epoch: 2 step: 1801, loss is 0.11130548268556595\n",
      "epoch: 2 step: 1802, loss is 0.22149616479873657\n",
      "epoch: 2 step: 1803, loss is 0.2375088781118393\n",
      "epoch: 2 step: 1804, loss is 0.10220221430063248\n",
      "epoch: 2 step: 1805, loss is 0.08741067349910736\n",
      "epoch: 2 step: 1806, loss is 0.038738686591386795\n",
      "epoch: 2 step: 1807, loss is 0.3033762276172638\n",
      "epoch: 2 step: 1808, loss is 0.07690504193305969\n",
      "epoch: 2 step: 1809, loss is 0.3345860242843628\n",
      "epoch: 2 step: 1810, loss is 0.047073882073163986\n",
      "epoch: 2 step: 1811, loss is 0.14285999536514282\n",
      "epoch: 2 step: 1812, loss is 0.15548638999462128\n",
      "epoch: 2 step: 1813, loss is 0.07372622191905975\n",
      "epoch: 2 step: 1814, loss is 0.023981895297765732\n",
      "epoch: 2 step: 1815, loss is 0.11712326109409332\n",
      "epoch: 2 step: 1816, loss is 0.01589244045317173\n",
      "epoch: 2 step: 1817, loss is 0.012435831129550934\n",
      "epoch: 2 step: 1818, loss is 0.20942756533622742\n",
      "epoch: 2 step: 1819, loss is 0.32987844944000244\n",
      "epoch: 2 step: 1820, loss is 0.2916339933872223\n",
      "epoch: 2 step: 1821, loss is 0.055675335228443146\n",
      "epoch: 2 step: 1822, loss is 0.13462215662002563\n",
      "epoch: 2 step: 1823, loss is 0.06318224966526031\n",
      "epoch: 2 step: 1824, loss is 0.02838338166475296\n",
      "epoch: 2 step: 1825, loss is 0.15334320068359375\n",
      "epoch: 2 step: 1826, loss is 0.15540188550949097\n",
      "epoch: 2 step: 1827, loss is 0.016840150579810143\n",
      "epoch: 2 step: 1828, loss is 0.025824835523962975\n",
      "epoch: 2 step: 1829, loss is 0.014365069568157196\n",
      "epoch: 2 step: 1830, loss is 0.2360987365245819\n",
      "epoch: 2 step: 1831, loss is 0.019746406003832817\n",
      "epoch: 2 step: 1832, loss is 0.0520043782889843\n",
      "epoch: 2 step: 1833, loss is 0.020987845957279205\n",
      "epoch: 2 step: 1834, loss is 0.14991618692874908\n",
      "epoch: 2 step: 1835, loss is 0.13561563193798065\n",
      "epoch: 2 step: 1836, loss is 0.0068825846537947655\n",
      "epoch: 2 step: 1837, loss is 0.30864614248275757\n",
      "epoch: 2 step: 1838, loss is 0.006834045518189669\n",
      "epoch: 2 step: 1839, loss is 0.028338678181171417\n",
      "epoch: 2 step: 1840, loss is 0.055080533027648926\n",
      "epoch: 2 step: 1841, loss is 0.07501859962940216\n",
      "epoch: 2 step: 1842, loss is 0.1313881278038025\n",
      "epoch: 2 step: 1843, loss is 0.028064366430044174\n",
      "epoch: 2 step: 1844, loss is 0.1144295260310173\n",
      "epoch: 2 step: 1845, loss is 0.011477068066596985\n",
      "epoch: 2 step: 1846, loss is 0.12212304770946503\n",
      "epoch: 2 step: 1847, loss is 0.08685670047998428\n",
      "epoch: 2 step: 1848, loss is 0.02840283140540123\n",
      "epoch: 2 step: 1849, loss is 0.41382813453674316\n",
      "epoch: 2 step: 1850, loss is 0.06173447147011757\n",
      "epoch: 2 step: 1851, loss is 0.21514005959033966\n",
      "epoch: 2 step: 1852, loss is 0.17735733091831207\n",
      "epoch: 2 step: 1853, loss is 0.11771102249622345\n",
      "epoch: 2 step: 1854, loss is 0.04581092298030853\n",
      "epoch: 2 step: 1855, loss is 0.09899020195007324\n",
      "epoch: 2 step: 1856, loss is 0.027071963995695114\n",
      "epoch: 2 step: 1857, loss is 0.011494075879454613\n",
      "epoch: 2 step: 1858, loss is 0.008557821623980999\n",
      "epoch: 2 step: 1859, loss is 0.06456433236598969\n",
      "epoch: 2 step: 1860, loss is 0.3309333622455597\n",
      "epoch: 2 step: 1861, loss is 0.033107247203588486\n",
      "epoch: 2 step: 1862, loss is 0.06502772122621536\n",
      "epoch: 2 step: 1863, loss is 0.005872893147170544\n",
      "epoch: 2 step: 1864, loss is 0.10616379231214523\n",
      "epoch: 2 step: 1865, loss is 0.18222013115882874\n",
      "epoch: 2 step: 1866, loss is 0.028107887133955956\n",
      "epoch: 2 step: 1867, loss is 0.039812806993722916\n",
      "epoch: 2 step: 1868, loss is 0.006991651840507984\n",
      "epoch: 2 step: 1869, loss is 0.01578044518828392\n",
      "epoch: 2 step: 1870, loss is 0.18628092110157013\n",
      "epoch: 2 step: 1871, loss is 0.05407819524407387\n",
      "epoch: 2 step: 1872, loss is 0.07411610335111618\n",
      "epoch: 2 step: 1873, loss is 0.07440558075904846\n",
      "epoch: 2 step: 1874, loss is 0.09032291918992996\n",
      "epoch: 2 step: 1875, loss is 0.0013863383792340755\n",
      "epoch: 3 step: 1, loss is 0.17605768144130707\n",
      "epoch: 3 step: 2, loss is 0.5003059506416321\n",
      "epoch: 3 step: 3, loss is 0.15564016997814178\n",
      "epoch: 3 step: 4, loss is 0.02037844806909561\n",
      "epoch: 3 step: 5, loss is 0.019226429983973503\n",
      "epoch: 3 step: 6, loss is 0.032140810042619705\n",
      "epoch: 3 step: 7, loss is 0.007344621699303389\n",
      "epoch: 3 step: 8, loss is 0.22829870879650116\n",
      "epoch: 3 step: 9, loss is 0.05759134143590927\n",
      "epoch: 3 step: 10, loss is 0.048752497881650925\n",
      "epoch: 3 step: 11, loss is 0.030061498284339905\n",
      "epoch: 3 step: 12, loss is 0.04414590448141098\n",
      "epoch: 3 step: 13, loss is 0.11791133880615234\n",
      "epoch: 3 step: 14, loss is 0.02529565803706646\n",
      "epoch: 3 step: 15, loss is 0.0858236700296402\n",
      "epoch: 3 step: 16, loss is 0.09160034358501434\n",
      "epoch: 3 step: 17, loss is 0.013610826805233955\n",
      "epoch: 3 step: 18, loss is 0.041545458137989044\n",
      "epoch: 3 step: 19, loss is 0.13498927652835846\n",
      "epoch: 3 step: 20, loss is 0.01992569863796234\n",
      "epoch: 3 step: 21, loss is 0.007931703701615334\n",
      "epoch: 3 step: 22, loss is 0.051548805087804794\n",
      "epoch: 3 step: 23, loss is 0.11494915932416916\n",
      "epoch: 3 step: 24, loss is 0.022021932527422905\n",
      "epoch: 3 step: 25, loss is 0.035705871880054474\n",
      "epoch: 3 step: 26, loss is 0.029402097687125206\n",
      "epoch: 3 step: 27, loss is 0.0668201744556427\n",
      "epoch: 3 step: 28, loss is 0.030369199812412262\n",
      "epoch: 3 step: 29, loss is 0.1801392287015915\n",
      "epoch: 3 step: 30, loss is 0.03335517644882202\n",
      "epoch: 3 step: 31, loss is 0.14541253447532654\n",
      "epoch: 3 step: 32, loss is 0.0244440995156765\n",
      "epoch: 3 step: 33, loss is 0.06203768402338028\n",
      "epoch: 3 step: 34, loss is 0.03802977129817009\n",
      "epoch: 3 step: 35, loss is 0.04743645712733269\n",
      "epoch: 3 step: 36, loss is 0.09380507469177246\n",
      "epoch: 3 step: 37, loss is 0.056671615689992905\n",
      "epoch: 3 step: 38, loss is 0.007500206120312214\n",
      "epoch: 3 step: 39, loss is 0.026331735774874687\n",
      "epoch: 3 step: 40, loss is 0.1005169078707695\n",
      "epoch: 3 step: 41, loss is 0.09839759767055511\n",
      "epoch: 3 step: 42, loss is 0.12299327552318573\n",
      "epoch: 3 step: 43, loss is 0.045250408351421356\n",
      "epoch: 3 step: 44, loss is 0.1689085066318512\n",
      "epoch: 3 step: 45, loss is 0.03913291543722153\n",
      "epoch: 3 step: 46, loss is 0.04995962977409363\n",
      "epoch: 3 step: 47, loss is 0.0432460680603981\n",
      "epoch: 3 step: 48, loss is 0.05225145071744919\n",
      "epoch: 3 step: 49, loss is 0.01285166572779417\n",
      "epoch: 3 step: 50, loss is 0.12052351981401443\n",
      "epoch: 3 step: 51, loss is 0.059193748980760574\n",
      "epoch: 3 step: 52, loss is 0.03366491198539734\n",
      "epoch: 3 step: 53, loss is 0.1176258847117424\n",
      "epoch: 3 step: 54, loss is 0.011825820431113243\n",
      "epoch: 3 step: 55, loss is 0.06369562447071075\n",
      "epoch: 3 step: 56, loss is 0.11132219433784485\n",
      "epoch: 3 step: 57, loss is 0.05647288262844086\n",
      "epoch: 3 step: 58, loss is 0.01988299749791622\n",
      "epoch: 3 step: 59, loss is 0.06762699782848358\n",
      "epoch: 3 step: 60, loss is 0.0032915021292865276\n",
      "epoch: 3 step: 61, loss is 0.13978217542171478\n",
      "epoch: 3 step: 62, loss is 0.017587486654520035\n",
      "epoch: 3 step: 63, loss is 0.05414840951561928\n",
      "epoch: 3 step: 64, loss is 0.017435871064662933\n",
      "epoch: 3 step: 65, loss is 0.19486702978610992\n",
      "epoch: 3 step: 66, loss is 0.5303084850311279\n",
      "epoch: 3 step: 67, loss is 0.18467985093593597\n",
      "epoch: 3 step: 68, loss is 0.395508736371994\n",
      "epoch: 3 step: 69, loss is 0.0052273087203502655\n",
      "epoch: 3 step: 70, loss is 0.017974302172660828\n",
      "epoch: 3 step: 71, loss is 0.015533949248492718\n",
      "epoch: 3 step: 72, loss is 0.03890065476298332\n",
      "epoch: 3 step: 73, loss is 0.017253519967198372\n",
      "epoch: 3 step: 74, loss is 0.04970443993806839\n",
      "epoch: 3 step: 75, loss is 0.33446717262268066\n",
      "epoch: 3 step: 76, loss is 0.0242635365575552\n",
      "epoch: 3 step: 77, loss is 0.03313207998871803\n",
      "epoch: 3 step: 78, loss is 0.08583996444940567\n",
      "epoch: 3 step: 79, loss is 0.030477505177259445\n",
      "epoch: 3 step: 80, loss is 0.015923086553812027\n",
      "epoch: 3 step: 81, loss is 0.08531813323497772\n",
      "epoch: 3 step: 82, loss is 0.08518778532743454\n",
      "epoch: 3 step: 83, loss is 0.21287645399570465\n",
      "epoch: 3 step: 84, loss is 0.10274111479520798\n",
      "epoch: 3 step: 85, loss is 0.02964603900909424\n",
      "epoch: 3 step: 86, loss is 0.04203470051288605\n",
      "epoch: 3 step: 87, loss is 0.07393597811460495\n",
      "epoch: 3 step: 88, loss is 0.13017643988132477\n",
      "epoch: 3 step: 89, loss is 0.007659373339265585\n",
      "epoch: 3 step: 90, loss is 0.035276103764772415\n",
      "epoch: 3 step: 91, loss is 0.165670245885849\n",
      "epoch: 3 step: 92, loss is 0.040954045951366425\n",
      "epoch: 3 step: 93, loss is 0.07038145512342453\n",
      "epoch: 3 step: 94, loss is 0.2515510618686676\n",
      "epoch: 3 step: 95, loss is 0.04351448267698288\n",
      "epoch: 3 step: 96, loss is 0.07978840172290802\n",
      "epoch: 3 step: 97, loss is 0.06704432517290115\n",
      "epoch: 3 step: 98, loss is 0.014810090884566307\n",
      "epoch: 3 step: 99, loss is 0.06074510142207146\n",
      "epoch: 3 step: 100, loss is 0.11689877510070801\n",
      "epoch: 3 step: 101, loss is 0.013042598962783813\n",
      "epoch: 3 step: 102, loss is 0.031263332813978195\n",
      "epoch: 3 step: 103, loss is 0.1913960576057434\n",
      "epoch: 3 step: 104, loss is 0.04135475307703018\n",
      "epoch: 3 step: 105, loss is 0.032832879573106766\n",
      "epoch: 3 step: 106, loss is 0.22883909940719604\n",
      "epoch: 3 step: 107, loss is 0.11762716621160507\n",
      "epoch: 3 step: 108, loss is 0.041912101209163666\n",
      "epoch: 3 step: 109, loss is 0.06281640380620956\n",
      "epoch: 3 step: 110, loss is 0.06402610242366791\n",
      "epoch: 3 step: 111, loss is 0.011515003629028797\n",
      "epoch: 3 step: 112, loss is 0.14442400634288788\n",
      "epoch: 3 step: 113, loss is 0.016493981704115868\n",
      "epoch: 3 step: 114, loss is 0.007973539642989635\n",
      "epoch: 3 step: 115, loss is 0.050632134079933167\n",
      "epoch: 3 step: 116, loss is 0.020430369302630424\n",
      "epoch: 3 step: 117, loss is 0.00995316170156002\n",
      "epoch: 3 step: 118, loss is 0.03193129971623421\n",
      "epoch: 3 step: 119, loss is 0.015229174867272377\n",
      "epoch: 3 step: 120, loss is 0.16802115738391876\n",
      "epoch: 3 step: 121, loss is 0.12704259157180786\n",
      "epoch: 3 step: 122, loss is 0.25420743227005005\n",
      "epoch: 3 step: 123, loss is 0.14052309095859528\n",
      "epoch: 3 step: 124, loss is 0.25660037994384766\n",
      "epoch: 3 step: 125, loss is 0.0959087386727333\n",
      "epoch: 3 step: 126, loss is 0.013433738611638546\n",
      "epoch: 3 step: 127, loss is 0.11358066648244858\n",
      "epoch: 3 step: 128, loss is 0.03218358755111694\n",
      "epoch: 3 step: 129, loss is 0.014758357778191566\n",
      "epoch: 3 step: 130, loss is 0.10132701694965363\n",
      "epoch: 3 step: 131, loss is 0.06421609967947006\n",
      "epoch: 3 step: 132, loss is 0.22356905043125153\n",
      "epoch: 3 step: 133, loss is 0.02187414839863777\n",
      "epoch: 3 step: 134, loss is 0.12134057283401489\n",
      "epoch: 3 step: 135, loss is 0.05415400117635727\n",
      "epoch: 3 step: 136, loss is 0.08859339356422424\n",
      "epoch: 3 step: 137, loss is 0.10147959738969803\n",
      "epoch: 3 step: 138, loss is 0.11687440425157547\n",
      "epoch: 3 step: 139, loss is 0.009421280585229397\n",
      "epoch: 3 step: 140, loss is 0.007189163938164711\n",
      "epoch: 3 step: 141, loss is 0.013391058892011642\n",
      "epoch: 3 step: 142, loss is 0.08482798933982849\n",
      "epoch: 3 step: 143, loss is 0.16927333176136017\n",
      "epoch: 3 step: 144, loss is 0.03784836456179619\n",
      "epoch: 3 step: 145, loss is 0.045171648263931274\n",
      "epoch: 3 step: 146, loss is 0.008741047233343124\n",
      "epoch: 3 step: 147, loss is 0.17117854952812195\n",
      "epoch: 3 step: 148, loss is 0.08504372835159302\n",
      "epoch: 3 step: 149, loss is 0.004275904968380928\n",
      "epoch: 3 step: 150, loss is 0.052996352314949036\n",
      "epoch: 3 step: 151, loss is 0.009958792477846146\n",
      "epoch: 3 step: 152, loss is 0.0530853196978569\n",
      "epoch: 3 step: 153, loss is 0.020961688831448555\n",
      "epoch: 3 step: 154, loss is 0.09947574138641357\n",
      "epoch: 3 step: 155, loss is 0.17470024526119232\n",
      "epoch: 3 step: 156, loss is 0.036243826150894165\n",
      "epoch: 3 step: 157, loss is 0.005502320360392332\n",
      "epoch: 3 step: 158, loss is 0.09909507632255554\n",
      "epoch: 3 step: 159, loss is 0.04572558030486107\n",
      "epoch: 3 step: 160, loss is 0.11813925951719284\n",
      "epoch: 3 step: 161, loss is 0.013674691319465637\n",
      "epoch: 3 step: 162, loss is 0.05956962704658508\n",
      "epoch: 3 step: 163, loss is 0.0322590246796608\n",
      "epoch: 3 step: 164, loss is 0.0988803282380104\n",
      "epoch: 3 step: 165, loss is 0.16219103336334229\n",
      "epoch: 3 step: 166, loss is 0.018731646239757538\n",
      "epoch: 3 step: 167, loss is 0.00954380538314581\n",
      "epoch: 3 step: 168, loss is 0.12715132534503937\n",
      "epoch: 3 step: 169, loss is 0.011893936432898045\n",
      "epoch: 3 step: 170, loss is 0.018736522644758224\n",
      "epoch: 3 step: 171, loss is 0.04959346726536751\n",
      "epoch: 3 step: 172, loss is 0.15875272452831268\n",
      "epoch: 3 step: 173, loss is 0.019494034349918365\n",
      "epoch: 3 step: 174, loss is 0.004910577088594437\n",
      "epoch: 3 step: 175, loss is 0.2619648277759552\n",
      "epoch: 3 step: 176, loss is 0.038529861718416214\n",
      "epoch: 3 step: 177, loss is 0.014053086750209332\n",
      "epoch: 3 step: 178, loss is 0.11109082400798798\n",
      "epoch: 3 step: 179, loss is 0.0027980972081422806\n",
      "epoch: 3 step: 180, loss is 0.11840862780809402\n",
      "epoch: 3 step: 181, loss is 0.033120047301054\n",
      "epoch: 3 step: 182, loss is 0.13285386562347412\n",
      "epoch: 3 step: 183, loss is 0.07808362692594528\n",
      "epoch: 3 step: 184, loss is 0.11223675310611725\n",
      "epoch: 3 step: 185, loss is 0.011271931231021881\n",
      "epoch: 3 step: 186, loss is 0.14852820336818695\n",
      "epoch: 3 step: 187, loss is 0.2823267877101898\n",
      "epoch: 3 step: 188, loss is 0.12389369308948517\n",
      "epoch: 3 step: 189, loss is 0.0782218724489212\n",
      "epoch: 3 step: 190, loss is 0.07134156674146652\n",
      "epoch: 3 step: 191, loss is 0.0032921377569437027\n",
      "epoch: 3 step: 192, loss is 0.06436866521835327\n",
      "epoch: 3 step: 193, loss is 0.05066515505313873\n",
      "epoch: 3 step: 194, loss is 0.15034396946430206\n",
      "epoch: 3 step: 195, loss is 0.3414856195449829\n",
      "epoch: 3 step: 196, loss is 0.006542891263961792\n",
      "epoch: 3 step: 197, loss is 0.030347730964422226\n",
      "epoch: 3 step: 198, loss is 0.03625648841261864\n",
      "epoch: 3 step: 199, loss is 0.002778018591925502\n",
      "epoch: 3 step: 200, loss is 0.016228998079895973\n",
      "epoch: 3 step: 201, loss is 0.12659019231796265\n",
      "epoch: 3 step: 202, loss is 0.048903968185186386\n",
      "epoch: 3 step: 203, loss is 0.06392905861139297\n",
      "epoch: 3 step: 204, loss is 0.009094376116991043\n",
      "epoch: 3 step: 205, loss is 0.1542908251285553\n",
      "epoch: 3 step: 206, loss is 0.01886729896068573\n",
      "epoch: 3 step: 207, loss is 0.026215147227048874\n",
      "epoch: 3 step: 208, loss is 0.21308781206607819\n",
      "epoch: 3 step: 209, loss is 0.0024096081033349037\n",
      "epoch: 3 step: 210, loss is 0.07166961580514908\n",
      "epoch: 3 step: 211, loss is 0.016409272328019142\n",
      "epoch: 3 step: 212, loss is 0.018210677430033684\n",
      "epoch: 3 step: 213, loss is 0.1788012981414795\n",
      "epoch: 3 step: 214, loss is 0.1539185643196106\n",
      "epoch: 3 step: 215, loss is 0.05589783191680908\n",
      "epoch: 3 step: 216, loss is 0.02469649724662304\n",
      "epoch: 3 step: 217, loss is 0.21930727362632751\n",
      "epoch: 3 step: 218, loss is 0.04575513303279877\n",
      "epoch: 3 step: 219, loss is 0.03764073923230171\n",
      "epoch: 3 step: 220, loss is 0.027310920879244804\n",
      "epoch: 3 step: 221, loss is 0.08269426226615906\n",
      "epoch: 3 step: 222, loss is 0.08200321346521378\n",
      "epoch: 3 step: 223, loss is 0.05061235651373863\n",
      "epoch: 3 step: 224, loss is 0.06860243529081345\n",
      "epoch: 3 step: 225, loss is 0.018522800877690315\n",
      "epoch: 3 step: 226, loss is 0.2634201645851135\n",
      "epoch: 3 step: 227, loss is 0.04387602210044861\n",
      "epoch: 3 step: 228, loss is 0.02456662617623806\n",
      "epoch: 3 step: 229, loss is 0.1609373837709427\n",
      "epoch: 3 step: 230, loss is 0.23148909211158752\n",
      "epoch: 3 step: 231, loss is 0.1158452033996582\n",
      "epoch: 3 step: 232, loss is 0.004171936307102442\n",
      "epoch: 3 step: 233, loss is 0.002428593812510371\n",
      "epoch: 3 step: 234, loss is 0.00380472163669765\n",
      "epoch: 3 step: 235, loss is 0.09927058964967728\n",
      "epoch: 3 step: 236, loss is 0.020475715398788452\n",
      "epoch: 3 step: 237, loss is 0.3885185718536377\n",
      "epoch: 3 step: 238, loss is 0.02041783183813095\n",
      "epoch: 3 step: 239, loss is 0.2637386918067932\n",
      "epoch: 3 step: 240, loss is 0.1666778177022934\n",
      "epoch: 3 step: 241, loss is 0.15832838416099548\n",
      "epoch: 3 step: 242, loss is 0.1686486154794693\n",
      "epoch: 3 step: 243, loss is 0.05692402645945549\n",
      "epoch: 3 step: 244, loss is 0.13419102132320404\n",
      "epoch: 3 step: 245, loss is 0.06511538475751877\n",
      "epoch: 3 step: 246, loss is 0.01385718584060669\n",
      "epoch: 3 step: 247, loss is 0.0284291859716177\n",
      "epoch: 3 step: 248, loss is 0.10429967194795609\n",
      "epoch: 3 step: 249, loss is 0.061830341815948486\n",
      "epoch: 3 step: 250, loss is 0.030781598761677742\n",
      "epoch: 3 step: 251, loss is 0.01000471506267786\n",
      "epoch: 3 step: 252, loss is 0.26404714584350586\n",
      "epoch: 3 step: 253, loss is 0.07427744567394257\n",
      "epoch: 3 step: 254, loss is 0.05924856662750244\n",
      "epoch: 3 step: 255, loss is 0.04244784265756607\n",
      "epoch: 3 step: 256, loss is 0.01836293190717697\n",
      "epoch: 3 step: 257, loss is 0.015721317380666733\n",
      "epoch: 3 step: 258, loss is 0.1609816700220108\n",
      "epoch: 3 step: 259, loss is 0.0963713750243187\n",
      "epoch: 3 step: 260, loss is 0.011296561919152737\n",
      "epoch: 3 step: 261, loss is 0.09736752510070801\n",
      "epoch: 3 step: 262, loss is 0.08922171592712402\n",
      "epoch: 3 step: 263, loss is 0.011839845217764378\n",
      "epoch: 3 step: 264, loss is 0.11638068407773972\n",
      "epoch: 3 step: 265, loss is 0.13670890033245087\n",
      "epoch: 3 step: 266, loss is 0.04455218464136124\n",
      "epoch: 3 step: 267, loss is 0.1517891138792038\n",
      "epoch: 3 step: 268, loss is 0.06289786100387573\n",
      "epoch: 3 step: 269, loss is 0.20596787333488464\n",
      "epoch: 3 step: 270, loss is 0.0640806183218956\n",
      "epoch: 3 step: 271, loss is 0.06696105748414993\n",
      "epoch: 3 step: 272, loss is 0.13052783906459808\n",
      "epoch: 3 step: 273, loss is 0.15725034475326538\n",
      "epoch: 3 step: 274, loss is 0.12987609207630157\n",
      "epoch: 3 step: 275, loss is 0.07410339266061783\n",
      "epoch: 3 step: 276, loss is 0.28972575068473816\n",
      "epoch: 3 step: 277, loss is 0.04570255056023598\n",
      "epoch: 3 step: 278, loss is 0.012573652900755405\n",
      "epoch: 3 step: 279, loss is 0.140363410115242\n",
      "epoch: 3 step: 280, loss is 0.043476104736328125\n",
      "epoch: 3 step: 281, loss is 0.031128592789173126\n",
      "epoch: 3 step: 282, loss is 0.26271453499794006\n",
      "epoch: 3 step: 283, loss is 0.11027658730745316\n",
      "epoch: 3 step: 284, loss is 0.11014404892921448\n",
      "epoch: 3 step: 285, loss is 0.4031332731246948\n",
      "epoch: 3 step: 286, loss is 0.08266092836856842\n",
      "epoch: 3 step: 287, loss is 0.016857050359249115\n",
      "epoch: 3 step: 288, loss is 0.03763086348772049\n",
      "epoch: 3 step: 289, loss is 0.17164811491966248\n",
      "epoch: 3 step: 290, loss is 0.330505907535553\n",
      "epoch: 3 step: 291, loss is 0.043580975383520126\n",
      "epoch: 3 step: 292, loss is 0.03680550307035446\n",
      "epoch: 3 step: 293, loss is 0.08344912528991699\n",
      "epoch: 3 step: 294, loss is 0.01205255463719368\n",
      "epoch: 3 step: 295, loss is 0.02392272837460041\n",
      "epoch: 3 step: 296, loss is 0.05162028223276138\n",
      "epoch: 3 step: 297, loss is 0.06332005560398102\n",
      "epoch: 3 step: 298, loss is 0.007247903849929571\n",
      "epoch: 3 step: 299, loss is 0.10915160924196243\n",
      "epoch: 3 step: 300, loss is 0.0317123644053936\n",
      "epoch: 3 step: 301, loss is 0.06758719682693481\n",
      "epoch: 3 step: 302, loss is 0.1144646480679512\n",
      "epoch: 3 step: 303, loss is 0.1824130266904831\n",
      "epoch: 3 step: 304, loss is 0.20529291033744812\n",
      "epoch: 3 step: 305, loss is 0.02857685275375843\n",
      "epoch: 3 step: 306, loss is 0.08351735770702362\n",
      "epoch: 3 step: 307, loss is 0.09186461567878723\n",
      "epoch: 3 step: 308, loss is 0.005874597933143377\n",
      "epoch: 3 step: 309, loss is 0.12196363508701324\n",
      "epoch: 3 step: 310, loss is 0.06803403049707413\n",
      "epoch: 3 step: 311, loss is 0.006281109526753426\n",
      "epoch: 3 step: 312, loss is 0.014005785807967186\n",
      "epoch: 3 step: 313, loss is 0.005001588258892298\n",
      "epoch: 3 step: 314, loss is 0.00873060803860426\n",
      "epoch: 3 step: 315, loss is 0.08170965313911438\n",
      "epoch: 3 step: 316, loss is 0.016041507944464684\n",
      "epoch: 3 step: 317, loss is 0.030428830534219742\n",
      "epoch: 3 step: 318, loss is 0.03056604415178299\n",
      "epoch: 3 step: 319, loss is 0.136441171169281\n",
      "epoch: 3 step: 320, loss is 0.010641408152878284\n",
      "epoch: 3 step: 321, loss is 0.08037540316581726\n",
      "epoch: 3 step: 322, loss is 0.010752200148999691\n",
      "epoch: 3 step: 323, loss is 0.171471506357193\n",
      "epoch: 3 step: 324, loss is 0.10254548490047455\n",
      "epoch: 3 step: 325, loss is 0.13819187879562378\n",
      "epoch: 3 step: 326, loss is 0.06489232927560806\n",
      "epoch: 3 step: 327, loss is 0.02875293605029583\n",
      "epoch: 3 step: 328, loss is 0.08903858810663223\n",
      "epoch: 3 step: 329, loss is 0.3214481472969055\n",
      "epoch: 3 step: 330, loss is 0.12333538383245468\n",
      "epoch: 3 step: 331, loss is 0.15283027291297913\n",
      "epoch: 3 step: 332, loss is 0.2951095700263977\n",
      "epoch: 3 step: 333, loss is 0.007681212387979031\n",
      "epoch: 3 step: 334, loss is 0.04823510721325874\n",
      "epoch: 3 step: 335, loss is 0.08610303699970245\n",
      "epoch: 3 step: 336, loss is 0.0021669575944542885\n",
      "epoch: 3 step: 337, loss is 0.08725341409444809\n",
      "epoch: 3 step: 338, loss is 0.01586093381047249\n",
      "epoch: 3 step: 339, loss is 0.06350712478160858\n",
      "epoch: 3 step: 340, loss is 0.1898202747106552\n",
      "epoch: 3 step: 341, loss is 0.2948918640613556\n",
      "epoch: 3 step: 342, loss is 0.03971670940518379\n",
      "epoch: 3 step: 343, loss is 0.2303714156150818\n",
      "epoch: 3 step: 344, loss is 0.05198277160525322\n",
      "epoch: 3 step: 345, loss is 0.025185277685523033\n",
      "epoch: 3 step: 346, loss is 0.01614711433649063\n",
      "epoch: 3 step: 347, loss is 0.034168168902397156\n",
      "epoch: 3 step: 348, loss is 0.018764615058898926\n",
      "epoch: 3 step: 349, loss is 0.01812106929719448\n",
      "epoch: 3 step: 350, loss is 0.4082244038581848\n",
      "epoch: 3 step: 351, loss is 0.11851529777050018\n",
      "epoch: 3 step: 352, loss is 0.041630711406469345\n",
      "epoch: 3 step: 353, loss is 0.13080953061580658\n",
      "epoch: 3 step: 354, loss is 0.0802263468503952\n",
      "epoch: 3 step: 355, loss is 0.028824668377637863\n",
      "epoch: 3 step: 356, loss is 0.05750010162591934\n",
      "epoch: 3 step: 357, loss is 0.06806360930204391\n",
      "epoch: 3 step: 358, loss is 0.034871071577072144\n",
      "epoch: 3 step: 359, loss is 0.041317444294691086\n",
      "epoch: 3 step: 360, loss is 0.11949141323566437\n",
      "epoch: 3 step: 361, loss is 0.031528349965810776\n",
      "epoch: 3 step: 362, loss is 0.015990588814020157\n",
      "epoch: 3 step: 363, loss is 0.14766834676265717\n",
      "epoch: 3 step: 364, loss is 0.054054997861385345\n",
      "epoch: 3 step: 365, loss is 0.21112798154354095\n",
      "epoch: 3 step: 366, loss is 0.04362953454256058\n",
      "epoch: 3 step: 367, loss is 0.09398652613162994\n",
      "epoch: 3 step: 368, loss is 0.20216892659664154\n",
      "epoch: 3 step: 369, loss is 0.05589291453361511\n",
      "epoch: 3 step: 370, loss is 0.01961120404303074\n",
      "epoch: 3 step: 371, loss is 0.003658057190477848\n",
      "epoch: 3 step: 372, loss is 0.002395337214693427\n",
      "epoch: 3 step: 373, loss is 0.05430043488740921\n",
      "epoch: 3 step: 374, loss is 0.08158354461193085\n",
      "epoch: 3 step: 375, loss is 0.12350142002105713\n",
      "epoch: 3 step: 376, loss is 0.01684441789984703\n",
      "epoch: 3 step: 377, loss is 0.0836700052022934\n",
      "epoch: 3 step: 378, loss is 0.12226657569408417\n",
      "epoch: 3 step: 379, loss is 0.05888557434082031\n",
      "epoch: 3 step: 380, loss is 0.051115963608026505\n",
      "epoch: 3 step: 381, loss is 0.11201190203428268\n",
      "epoch: 3 step: 382, loss is 0.171366885304451\n",
      "epoch: 3 step: 383, loss is 0.029300609603524208\n",
      "epoch: 3 step: 384, loss is 0.021053312346339226\n",
      "epoch: 3 step: 385, loss is 0.13489051163196564\n",
      "epoch: 3 step: 386, loss is 0.06206683814525604\n",
      "epoch: 3 step: 387, loss is 0.06351833045482635\n",
      "epoch: 3 step: 388, loss is 0.012465125881135464\n",
      "epoch: 3 step: 389, loss is 0.05133931338787079\n",
      "epoch: 3 step: 390, loss is 0.022957026958465576\n",
      "epoch: 3 step: 391, loss is 0.0019476523157209158\n",
      "epoch: 3 step: 392, loss is 0.3248922824859619\n",
      "epoch: 3 step: 393, loss is 0.011274763382971287\n",
      "epoch: 3 step: 394, loss is 0.08876671642065048\n",
      "epoch: 3 step: 395, loss is 0.02234550379216671\n",
      "epoch: 3 step: 396, loss is 0.04202407971024513\n",
      "epoch: 3 step: 397, loss is 0.2279466837644577\n",
      "epoch: 3 step: 398, loss is 0.17050912976264954\n",
      "epoch: 3 step: 399, loss is 0.1736120879650116\n",
      "epoch: 3 step: 400, loss is 0.15691238641738892\n",
      "epoch: 3 step: 401, loss is 0.1151331439614296\n",
      "epoch: 3 step: 402, loss is 0.04844483360648155\n",
      "epoch: 3 step: 403, loss is 0.08092588931322098\n",
      "epoch: 3 step: 404, loss is 0.22874347865581512\n",
      "epoch: 3 step: 405, loss is 0.02427496388554573\n",
      "epoch: 3 step: 406, loss is 0.13397550582885742\n",
      "epoch: 3 step: 407, loss is 0.12684525549411774\n",
      "epoch: 3 step: 408, loss is 0.07932619005441666\n",
      "epoch: 3 step: 409, loss is 0.02110176533460617\n",
      "epoch: 3 step: 410, loss is 0.016316719353199005\n",
      "epoch: 3 step: 411, loss is 0.11176519095897675\n",
      "epoch: 3 step: 412, loss is 0.08554154634475708\n",
      "epoch: 3 step: 413, loss is 0.033104050904512405\n",
      "epoch: 3 step: 414, loss is 0.09139223396778107\n",
      "epoch: 3 step: 415, loss is 0.39215806126594543\n",
      "epoch: 3 step: 416, loss is 0.11076947301626205\n",
      "epoch: 3 step: 417, loss is 0.09138279408216476\n",
      "epoch: 3 step: 418, loss is 0.024127839133143425\n",
      "epoch: 3 step: 419, loss is 0.08931072801351547\n",
      "epoch: 3 step: 420, loss is 0.13531456887722015\n",
      "epoch: 3 step: 421, loss is 0.00817244965583086\n",
      "epoch: 3 step: 422, loss is 0.025280432775616646\n",
      "epoch: 3 step: 423, loss is 0.10615813732147217\n",
      "epoch: 3 step: 424, loss is 0.08039435744285583\n",
      "epoch: 3 step: 425, loss is 0.19080907106399536\n",
      "epoch: 3 step: 426, loss is 0.03648323938250542\n",
      "epoch: 3 step: 427, loss is 0.17951974272727966\n",
      "epoch: 3 step: 428, loss is 0.02353665418922901\n",
      "epoch: 3 step: 429, loss is 0.10511849820613861\n",
      "epoch: 3 step: 430, loss is 0.02478180266916752\n",
      "epoch: 3 step: 431, loss is 0.183725044131279\n",
      "epoch: 3 step: 432, loss is 0.018183548003435135\n",
      "epoch: 3 step: 433, loss is 0.03816252201795578\n",
      "epoch: 3 step: 434, loss is 0.08430854231119156\n",
      "epoch: 3 step: 435, loss is 0.02755868062376976\n",
      "epoch: 3 step: 436, loss is 0.040862616151571274\n",
      "epoch: 3 step: 437, loss is 0.008757798001170158\n",
      "epoch: 3 step: 438, loss is 0.03461233153939247\n",
      "epoch: 3 step: 439, loss is 0.03815970569849014\n",
      "epoch: 3 step: 440, loss is 0.1020779237151146\n",
      "epoch: 3 step: 441, loss is 0.04665368050336838\n",
      "epoch: 3 step: 442, loss is 0.2964619994163513\n",
      "epoch: 3 step: 443, loss is 0.007810217794030905\n",
      "epoch: 3 step: 444, loss is 0.013520649634301662\n",
      "epoch: 3 step: 445, loss is 0.2675882875919342\n",
      "epoch: 3 step: 446, loss is 0.033137012273073196\n",
      "epoch: 3 step: 447, loss is 0.10980183631181717\n",
      "epoch: 3 step: 448, loss is 0.05717054381966591\n",
      "epoch: 3 step: 449, loss is 0.1318877786397934\n",
      "epoch: 3 step: 450, loss is 0.014476416632533073\n",
      "epoch: 3 step: 451, loss is 0.35395383834838867\n",
      "epoch: 3 step: 452, loss is 0.10540727525949478\n",
      "epoch: 3 step: 453, loss is 0.05960247665643692\n",
      "epoch: 3 step: 454, loss is 0.0054540070705115795\n",
      "epoch: 3 step: 455, loss is 0.01745898276567459\n",
      "epoch: 3 step: 456, loss is 0.030149387195706367\n",
      "epoch: 3 step: 457, loss is 0.018992353230714798\n",
      "epoch: 3 step: 458, loss is 0.1696171909570694\n",
      "epoch: 3 step: 459, loss is 0.09857471287250519\n",
      "epoch: 3 step: 460, loss is 0.19459804892539978\n",
      "epoch: 3 step: 461, loss is 0.08206699788570404\n",
      "epoch: 3 step: 462, loss is 0.02739284187555313\n",
      "epoch: 3 step: 463, loss is 0.16932713985443115\n",
      "epoch: 3 step: 464, loss is 0.013749782927334309\n",
      "epoch: 3 step: 465, loss is 0.041721194982528687\n",
      "epoch: 3 step: 466, loss is 0.009765973314642906\n",
      "epoch: 3 step: 467, loss is 0.07200835645198822\n",
      "epoch: 3 step: 468, loss is 0.03244277089834213\n",
      "epoch: 3 step: 469, loss is 0.15026706457138062\n",
      "epoch: 3 step: 470, loss is 0.06838426738977432\n",
      "epoch: 3 step: 471, loss is 0.15145215392112732\n",
      "epoch: 3 step: 472, loss is 0.018464382737874985\n",
      "epoch: 3 step: 473, loss is 0.07200514525175095\n",
      "epoch: 3 step: 474, loss is 0.15975987911224365\n",
      "epoch: 3 step: 475, loss is 0.016994813457131386\n",
      "epoch: 3 step: 476, loss is 0.15379838645458221\n",
      "epoch: 3 step: 477, loss is 0.00786937028169632\n",
      "epoch: 3 step: 478, loss is 0.026311378926038742\n",
      "epoch: 3 step: 479, loss is 0.08029630035161972\n",
      "epoch: 3 step: 480, loss is 0.19619624316692352\n",
      "epoch: 3 step: 481, loss is 0.2356821447610855\n",
      "epoch: 3 step: 482, loss is 0.10228484869003296\n",
      "epoch: 3 step: 483, loss is 0.02760123647749424\n",
      "epoch: 3 step: 484, loss is 0.22365516424179077\n",
      "epoch: 3 step: 485, loss is 0.10263293236494064\n",
      "epoch: 3 step: 486, loss is 0.0899723619222641\n",
      "epoch: 3 step: 487, loss is 0.2544385492801666\n",
      "epoch: 3 step: 488, loss is 0.08865392208099365\n",
      "epoch: 3 step: 489, loss is 0.19333286583423615\n",
      "epoch: 3 step: 490, loss is 0.04749514162540436\n",
      "epoch: 3 step: 491, loss is 0.06198686361312866\n",
      "epoch: 3 step: 492, loss is 0.08173702657222748\n",
      "epoch: 3 step: 493, loss is 0.06429846584796906\n",
      "epoch: 3 step: 494, loss is 0.030144039541482925\n",
      "epoch: 3 step: 495, loss is 0.18506433069705963\n",
      "epoch: 3 step: 496, loss is 0.06395622342824936\n",
      "epoch: 3 step: 497, loss is 0.15764638781547546\n",
      "epoch: 3 step: 498, loss is 0.020550794899463654\n",
      "epoch: 3 step: 499, loss is 0.16875940561294556\n",
      "epoch: 3 step: 500, loss is 0.006698000710457563\n",
      "epoch: 3 step: 501, loss is 0.002848802600055933\n",
      "epoch: 3 step: 502, loss is 0.011147551238536835\n",
      "epoch: 3 step: 503, loss is 0.020017044618725777\n",
      "epoch: 3 step: 504, loss is 0.08785071223974228\n",
      "epoch: 3 step: 505, loss is 0.20668625831604004\n",
      "epoch: 3 step: 506, loss is 0.016382645815610886\n",
      "epoch: 3 step: 507, loss is 0.003366051707416773\n",
      "epoch: 3 step: 508, loss is 0.00910354033112526\n",
      "epoch: 3 step: 509, loss is 0.11415165662765503\n",
      "epoch: 3 step: 510, loss is 0.21523170173168182\n",
      "epoch: 3 step: 511, loss is 0.0923074260354042\n",
      "epoch: 3 step: 512, loss is 0.024504274129867554\n",
      "epoch: 3 step: 513, loss is 0.2911190390586853\n",
      "epoch: 3 step: 514, loss is 0.16080300509929657\n",
      "epoch: 3 step: 515, loss is 0.01809539459645748\n",
      "epoch: 3 step: 516, loss is 0.016241712495684624\n",
      "epoch: 3 step: 517, loss is 0.18579277396202087\n",
      "epoch: 3 step: 518, loss is 0.030904550105333328\n",
      "epoch: 3 step: 519, loss is 0.11015420407056808\n",
      "epoch: 3 step: 520, loss is 0.010904921218752861\n",
      "epoch: 3 step: 521, loss is 0.010541817173361778\n",
      "epoch: 3 step: 522, loss is 0.02618050016462803\n",
      "epoch: 3 step: 523, loss is 0.17954622209072113\n",
      "epoch: 3 step: 524, loss is 0.10471491515636444\n",
      "epoch: 3 step: 525, loss is 0.08605695515871048\n",
      "epoch: 3 step: 526, loss is 0.05947486311197281\n",
      "epoch: 3 step: 527, loss is 0.42361146211624146\n",
      "epoch: 3 step: 528, loss is 0.14070260524749756\n",
      "epoch: 3 step: 529, loss is 0.06027118116617203\n",
      "epoch: 3 step: 530, loss is 0.004106692038476467\n",
      "epoch: 3 step: 531, loss is 0.23744766414165497\n",
      "epoch: 3 step: 532, loss is 0.06076912209391594\n",
      "epoch: 3 step: 533, loss is 0.11894101649522781\n",
      "epoch: 3 step: 534, loss is 0.05017399415373802\n",
      "epoch: 3 step: 535, loss is 0.07368777692317963\n",
      "epoch: 3 step: 536, loss is 0.1007547527551651\n",
      "epoch: 3 step: 537, loss is 0.07568532973527908\n",
      "epoch: 3 step: 538, loss is 0.12411762773990631\n",
      "epoch: 3 step: 539, loss is 0.10858337581157684\n",
      "epoch: 3 step: 540, loss is 0.011155721731483936\n",
      "epoch: 3 step: 541, loss is 0.1948290318250656\n",
      "epoch: 3 step: 542, loss is 0.310690701007843\n",
      "epoch: 3 step: 543, loss is 0.024127081036567688\n",
      "epoch: 3 step: 544, loss is 0.13750581443309784\n",
      "epoch: 3 step: 545, loss is 0.08075209707021713\n",
      "epoch: 3 step: 546, loss is 0.0037748985923826694\n",
      "epoch: 3 step: 547, loss is 0.05260998755693436\n",
      "epoch: 3 step: 548, loss is 0.05666608735918999\n",
      "epoch: 3 step: 549, loss is 0.01525095570832491\n",
      "epoch: 3 step: 550, loss is 0.14546886086463928\n",
      "epoch: 3 step: 551, loss is 0.04820074886083603\n",
      "epoch: 3 step: 552, loss is 0.037010617554187775\n",
      "epoch: 3 step: 553, loss is 0.023527542129158974\n",
      "epoch: 3 step: 554, loss is 0.004439749289304018\n",
      "epoch: 3 step: 555, loss is 0.15006199479103088\n",
      "epoch: 3 step: 556, loss is 0.09643545746803284\n",
      "epoch: 3 step: 557, loss is 0.03213730454444885\n",
      "epoch: 3 step: 558, loss is 0.02914907969534397\n",
      "epoch: 3 step: 559, loss is 0.04815817251801491\n",
      "epoch: 3 step: 560, loss is 0.1846783459186554\n",
      "epoch: 3 step: 561, loss is 0.03475998342037201\n",
      "epoch: 3 step: 562, loss is 0.11618126183748245\n",
      "epoch: 3 step: 563, loss is 0.017333626747131348\n",
      "epoch: 3 step: 564, loss is 0.05425862595438957\n",
      "epoch: 3 step: 565, loss is 0.09247991442680359\n",
      "epoch: 3 step: 566, loss is 0.2629125714302063\n",
      "epoch: 3 step: 567, loss is 0.03588486835360527\n",
      "epoch: 3 step: 568, loss is 0.1047210842370987\n",
      "epoch: 3 step: 569, loss is 0.0718354731798172\n",
      "epoch: 3 step: 570, loss is 0.0611935593187809\n",
      "epoch: 3 step: 571, loss is 0.15844941139221191\n",
      "epoch: 3 step: 572, loss is 0.07778900861740112\n",
      "epoch: 3 step: 573, loss is 0.062170710414648056\n",
      "epoch: 3 step: 574, loss is 0.04892612248659134\n",
      "epoch: 3 step: 575, loss is 0.1397896409034729\n",
      "epoch: 3 step: 576, loss is 0.007625495549291372\n",
      "epoch: 3 step: 577, loss is 0.05065078288316727\n",
      "epoch: 3 step: 578, loss is 0.05111227184534073\n",
      "epoch: 3 step: 579, loss is 0.09023455530405045\n",
      "epoch: 3 step: 580, loss is 0.04620557278394699\n",
      "epoch: 3 step: 581, loss is 0.09724877774715424\n",
      "epoch: 3 step: 582, loss is 0.05829301476478577\n",
      "epoch: 3 step: 583, loss is 0.046108685433864594\n",
      "epoch: 3 step: 584, loss is 0.018720723688602448\n",
      "epoch: 3 step: 585, loss is 0.03153092786669731\n",
      "epoch: 3 step: 586, loss is 0.08537276089191437\n",
      "epoch: 3 step: 587, loss is 0.05147513002157211\n",
      "epoch: 3 step: 588, loss is 0.06910419464111328\n",
      "epoch: 3 step: 589, loss is 0.02816593274474144\n",
      "epoch: 3 step: 590, loss is 0.03802529349923134\n",
      "epoch: 3 step: 591, loss is 0.21480756998062134\n",
      "epoch: 3 step: 592, loss is 0.04873840510845184\n",
      "epoch: 3 step: 593, loss is 0.21983186900615692\n",
      "epoch: 3 step: 594, loss is 0.2166357934474945\n",
      "epoch: 3 step: 595, loss is 0.3018176555633545\n",
      "epoch: 3 step: 596, loss is 0.003870624816045165\n",
      "epoch: 3 step: 597, loss is 0.2055777758359909\n",
      "epoch: 3 step: 598, loss is 0.4098532795906067\n",
      "epoch: 3 step: 599, loss is 0.02354450337588787\n",
      "epoch: 3 step: 600, loss is 0.10336077958345413\n",
      "epoch: 3 step: 601, loss is 0.0070124357007443905\n",
      "epoch: 3 step: 602, loss is 0.01376269944012165\n",
      "epoch: 3 step: 603, loss is 0.023430300876498222\n",
      "epoch: 3 step: 604, loss is 0.17131571471691132\n",
      "epoch: 3 step: 605, loss is 0.015965303406119347\n",
      "epoch: 3 step: 606, loss is 0.05553305894136429\n",
      "epoch: 3 step: 607, loss is 0.12295369803905487\n",
      "epoch: 3 step: 608, loss is 0.05705520883202553\n",
      "epoch: 3 step: 609, loss is 0.06926101446151733\n",
      "epoch: 3 step: 610, loss is 0.05894742161035538\n",
      "epoch: 3 step: 611, loss is 0.03795668110251427\n",
      "epoch: 3 step: 612, loss is 0.10699974745512009\n",
      "epoch: 3 step: 613, loss is 0.08507344126701355\n",
      "epoch: 3 step: 614, loss is 0.0064294901676476\n",
      "epoch: 3 step: 615, loss is 0.06444302946329117\n",
      "epoch: 3 step: 616, loss is 0.042989589273929596\n",
      "epoch: 3 step: 617, loss is 0.1252964586019516\n",
      "epoch: 3 step: 618, loss is 0.08976227790117264\n",
      "epoch: 3 step: 619, loss is 0.03377128019928932\n",
      "epoch: 3 step: 620, loss is 0.10544004291296005\n",
      "epoch: 3 step: 621, loss is 0.07360312342643738\n",
      "epoch: 3 step: 622, loss is 0.0326879508793354\n",
      "epoch: 3 step: 623, loss is 0.01996198110282421\n",
      "epoch: 3 step: 624, loss is 0.08424775302410126\n",
      "epoch: 3 step: 625, loss is 0.024488087743520737\n",
      "epoch: 3 step: 626, loss is 0.13937337696552277\n",
      "epoch: 3 step: 627, loss is 0.07667078077793121\n",
      "epoch: 3 step: 628, loss is 0.0642714649438858\n",
      "epoch: 3 step: 629, loss is 0.41179466247558594\n",
      "epoch: 3 step: 630, loss is 0.022051233798265457\n",
      "epoch: 3 step: 631, loss is 0.037460748106241226\n",
      "epoch: 3 step: 632, loss is 0.048641230911016464\n",
      "epoch: 3 step: 633, loss is 0.3855050802230835\n",
      "epoch: 3 step: 634, loss is 0.1822872906923294\n",
      "epoch: 3 step: 635, loss is 0.01657591015100479\n",
      "epoch: 3 step: 636, loss is 0.032061606645584106\n",
      "epoch: 3 step: 637, loss is 0.09247159957885742\n",
      "epoch: 3 step: 638, loss is 0.18196843564510345\n",
      "epoch: 3 step: 639, loss is 0.04764924198389053\n",
      "epoch: 3 step: 640, loss is 0.03642928972840309\n",
      "epoch: 3 step: 641, loss is 0.015349104069173336\n",
      "epoch: 3 step: 642, loss is 0.1102292463183403\n",
      "epoch: 3 step: 643, loss is 0.19259785115718842\n",
      "epoch: 3 step: 644, loss is 0.06504766643047333\n",
      "epoch: 3 step: 645, loss is 0.014424245804548264\n",
      "epoch: 3 step: 646, loss is 0.13407278060913086\n",
      "epoch: 3 step: 647, loss is 0.04105006903409958\n",
      "epoch: 3 step: 648, loss is 0.026247965171933174\n",
      "epoch: 3 step: 649, loss is 0.0650629848241806\n",
      "epoch: 3 step: 650, loss is 0.05058722943067551\n",
      "epoch: 3 step: 651, loss is 0.03486516699194908\n",
      "epoch: 3 step: 652, loss is 0.10099897533655167\n",
      "epoch: 3 step: 653, loss is 0.10410447418689728\n",
      "epoch: 3 step: 654, loss is 0.09932944923639297\n",
      "epoch: 3 step: 655, loss is 0.10636300593614578\n",
      "epoch: 3 step: 656, loss is 0.09618696570396423\n",
      "epoch: 3 step: 657, loss is 0.0884975716471672\n",
      "epoch: 3 step: 658, loss is 0.09169045090675354\n",
      "epoch: 3 step: 659, loss is 0.13956457376480103\n",
      "epoch: 3 step: 660, loss is 0.024640901014208794\n",
      "epoch: 3 step: 661, loss is 0.4184398353099823\n",
      "epoch: 3 step: 662, loss is 0.15456026792526245\n",
      "epoch: 3 step: 663, loss is 0.167881041765213\n",
      "epoch: 3 step: 664, loss is 0.0109926862642169\n",
      "epoch: 3 step: 665, loss is 0.10991822183132172\n",
      "epoch: 3 step: 666, loss is 0.05008993297815323\n",
      "epoch: 3 step: 667, loss is 0.07072798162698746\n",
      "epoch: 3 step: 668, loss is 0.16353154182434082\n",
      "epoch: 3 step: 669, loss is 0.008376875892281532\n",
      "epoch: 3 step: 670, loss is 0.061508338898420334\n",
      "epoch: 3 step: 671, loss is 0.09020094573497772\n",
      "epoch: 3 step: 672, loss is 0.06963510811328888\n",
      "epoch: 3 step: 673, loss is 0.031292449682950974\n",
      "epoch: 3 step: 674, loss is 0.02555181458592415\n",
      "epoch: 3 step: 675, loss is 0.136979341506958\n",
      "epoch: 3 step: 676, loss is 0.10070652514696121\n",
      "epoch: 3 step: 677, loss is 0.04570605233311653\n",
      "epoch: 3 step: 678, loss is 0.014500769786536694\n",
      "epoch: 3 step: 679, loss is 0.007384519558399916\n",
      "epoch: 3 step: 680, loss is 0.02046302706003189\n",
      "epoch: 3 step: 681, loss is 0.01207863911986351\n",
      "epoch: 3 step: 682, loss is 0.04516350477933884\n",
      "epoch: 3 step: 683, loss is 0.20174294710159302\n",
      "epoch: 3 step: 684, loss is 0.22134217619895935\n",
      "epoch: 3 step: 685, loss is 0.23799267411231995\n",
      "epoch: 3 step: 686, loss is 0.1496095508337021\n",
      "epoch: 3 step: 687, loss is 0.0645102933049202\n",
      "epoch: 3 step: 688, loss is 0.0084029920399189\n",
      "epoch: 3 step: 689, loss is 0.09122423082590103\n",
      "epoch: 3 step: 690, loss is 0.1641692817211151\n",
      "epoch: 3 step: 691, loss is 0.03040020726621151\n",
      "epoch: 3 step: 692, loss is 0.16384996473789215\n",
      "epoch: 3 step: 693, loss is 0.1299079954624176\n",
      "epoch: 3 step: 694, loss is 0.09145312756299973\n",
      "epoch: 3 step: 695, loss is 0.11789624392986298\n",
      "epoch: 3 step: 696, loss is 0.14163026213645935\n",
      "epoch: 3 step: 697, loss is 0.01397954672574997\n",
      "epoch: 3 step: 698, loss is 0.02128261886537075\n",
      "epoch: 3 step: 699, loss is 0.006765367928892374\n",
      "epoch: 3 step: 700, loss is 0.04712682589888573\n",
      "epoch: 3 step: 701, loss is 0.04091252014040947\n",
      "epoch: 3 step: 702, loss is 0.00468035414814949\n",
      "epoch: 3 step: 703, loss is 0.040615472942590714\n",
      "epoch: 3 step: 704, loss is 0.07868774235248566\n",
      "epoch: 3 step: 705, loss is 0.186569482088089\n",
      "epoch: 3 step: 706, loss is 0.1830427646636963\n",
      "epoch: 3 step: 707, loss is 0.08981053531169891\n",
      "epoch: 3 step: 708, loss is 0.03249434381723404\n",
      "epoch: 3 step: 709, loss is 0.04560680687427521\n",
      "epoch: 3 step: 710, loss is 0.09118235111236572\n",
      "epoch: 3 step: 711, loss is 0.06373590975999832\n",
      "epoch: 3 step: 712, loss is 0.23816096782684326\n",
      "epoch: 3 step: 713, loss is 0.06744494289159775\n",
      "epoch: 3 step: 714, loss is 0.09918411076068878\n",
      "epoch: 3 step: 715, loss is 0.019332939758896828\n",
      "epoch: 3 step: 716, loss is 0.2216583639383316\n",
      "epoch: 3 step: 717, loss is 0.035122521221637726\n",
      "epoch: 3 step: 718, loss is 0.07476740330457687\n",
      "epoch: 3 step: 719, loss is 0.04071757569909096\n",
      "epoch: 3 step: 720, loss is 0.08165138959884644\n",
      "epoch: 3 step: 721, loss is 0.024789607152342796\n",
      "epoch: 3 step: 722, loss is 0.08106434345245361\n",
      "epoch: 3 step: 723, loss is 0.24252724647521973\n",
      "epoch: 3 step: 724, loss is 0.0748477429151535\n",
      "epoch: 3 step: 725, loss is 0.025632597506046295\n",
      "epoch: 3 step: 726, loss is 0.012521623633801937\n",
      "epoch: 3 step: 727, loss is 0.06478430330753326\n",
      "epoch: 3 step: 728, loss is 0.08176939934492111\n",
      "epoch: 3 step: 729, loss is 0.3836638927459717\n",
      "epoch: 3 step: 730, loss is 0.2217235565185547\n",
      "epoch: 3 step: 731, loss is 0.017677808180451393\n",
      "epoch: 3 step: 732, loss is 0.10076598823070526\n",
      "epoch: 3 step: 733, loss is 0.06718868017196655\n",
      "epoch: 3 step: 734, loss is 0.08068405836820602\n",
      "epoch: 3 step: 735, loss is 0.04017286002635956\n",
      "epoch: 3 step: 736, loss is 0.016550468280911446\n",
      "epoch: 3 step: 737, loss is 0.04879188537597656\n",
      "epoch: 3 step: 738, loss is 0.008360734209418297\n",
      "epoch: 3 step: 739, loss is 0.0318143330514431\n",
      "epoch: 3 step: 740, loss is 0.1652206927537918\n",
      "epoch: 3 step: 741, loss is 0.02839587815105915\n",
      "epoch: 3 step: 742, loss is 0.03503409028053284\n",
      "epoch: 3 step: 743, loss is 0.11718537658452988\n",
      "epoch: 3 step: 744, loss is 0.06479842215776443\n",
      "epoch: 3 step: 745, loss is 0.002466789912432432\n",
      "epoch: 3 step: 746, loss is 0.07601794600486755\n",
      "epoch: 3 step: 747, loss is 0.060615360736846924\n",
      "epoch: 3 step: 748, loss is 0.06257655471563339\n",
      "epoch: 3 step: 749, loss is 0.0748661458492279\n",
      "epoch: 3 step: 750, loss is 0.04853305593132973\n",
      "epoch: 3 step: 751, loss is 0.06531839817762375\n",
      "epoch: 3 step: 752, loss is 0.009004193358123302\n",
      "epoch: 3 step: 753, loss is 0.032498445361852646\n",
      "epoch: 3 step: 754, loss is 0.11403525620698929\n",
      "epoch: 3 step: 755, loss is 0.19732661545276642\n",
      "epoch: 3 step: 756, loss is 0.02702825888991356\n",
      "epoch: 3 step: 757, loss is 0.13383980095386505\n",
      "epoch: 3 step: 758, loss is 0.2744161784648895\n",
      "epoch: 3 step: 759, loss is 0.04496035724878311\n",
      "epoch: 3 step: 760, loss is 0.08941502124071121\n",
      "epoch: 3 step: 761, loss is 0.04162648320198059\n",
      "epoch: 3 step: 762, loss is 0.06278850138187408\n",
      "epoch: 3 step: 763, loss is 0.009454688988626003\n",
      "epoch: 3 step: 764, loss is 0.0041807289235293865\n",
      "epoch: 3 step: 765, loss is 0.008030731230974197\n",
      "epoch: 3 step: 766, loss is 0.013440110720694065\n",
      "epoch: 3 step: 767, loss is 0.004492259118705988\n",
      "epoch: 3 step: 768, loss is 0.1039726734161377\n",
      "epoch: 3 step: 769, loss is 0.018575366586446762\n",
      "epoch: 3 step: 770, loss is 0.004534929059445858\n",
      "epoch: 3 step: 771, loss is 0.35976582765579224\n",
      "epoch: 3 step: 772, loss is 0.05843362957239151\n",
      "epoch: 3 step: 773, loss is 0.01483603473752737\n",
      "epoch: 3 step: 774, loss is 0.016849294304847717\n",
      "epoch: 3 step: 775, loss is 0.05075439438223839\n",
      "epoch: 3 step: 776, loss is 0.27530336380004883\n",
      "epoch: 3 step: 777, loss is 0.11862950026988983\n",
      "epoch: 3 step: 778, loss is 0.008524446748197079\n",
      "epoch: 3 step: 779, loss is 0.03357858955860138\n",
      "epoch: 3 step: 780, loss is 0.021249447017908096\n",
      "epoch: 3 step: 781, loss is 0.027077337726950645\n",
      "epoch: 3 step: 782, loss is 0.0024977687280625105\n",
      "epoch: 3 step: 783, loss is 0.0810336247086525\n",
      "epoch: 3 step: 784, loss is 0.009152350015938282\n",
      "epoch: 3 step: 785, loss is 0.06948468834161758\n",
      "epoch: 3 step: 786, loss is 0.03052368015050888\n",
      "epoch: 3 step: 787, loss is 0.22104333341121674\n",
      "epoch: 3 step: 788, loss is 0.009187187999486923\n",
      "epoch: 3 step: 789, loss is 0.11758587509393692\n",
      "epoch: 3 step: 790, loss is 0.02359708771109581\n",
      "epoch: 3 step: 791, loss is 0.08615461736917496\n",
      "epoch: 3 step: 792, loss is 0.25021371245384216\n",
      "epoch: 3 step: 793, loss is 0.0418710894882679\n",
      "epoch: 3 step: 794, loss is 0.1675867736339569\n",
      "epoch: 3 step: 795, loss is 0.02361290156841278\n",
      "epoch: 3 step: 796, loss is 0.0055667944252491\n",
      "epoch: 3 step: 797, loss is 0.13381296396255493\n",
      "epoch: 3 step: 798, loss is 0.00857848022133112\n",
      "epoch: 3 step: 799, loss is 0.43043553829193115\n",
      "epoch: 3 step: 800, loss is 0.0341065488755703\n",
      "epoch: 3 step: 801, loss is 0.01802005246281624\n",
      "epoch: 3 step: 802, loss is 0.08276568353176117\n",
      "epoch: 3 step: 803, loss is 0.024659190326929092\n",
      "epoch: 3 step: 804, loss is 0.02409587800502777\n",
      "epoch: 3 step: 805, loss is 0.06435909122228622\n",
      "epoch: 3 step: 806, loss is 0.031128956004977226\n",
      "epoch: 3 step: 807, loss is 0.28392794728279114\n",
      "epoch: 3 step: 808, loss is 0.016115672886371613\n",
      "epoch: 3 step: 809, loss is 0.10797674208879471\n",
      "epoch: 3 step: 810, loss is 0.0027928645722568035\n",
      "epoch: 3 step: 811, loss is 0.22637489438056946\n",
      "epoch: 3 step: 812, loss is 0.1779223382472992\n",
      "epoch: 3 step: 813, loss is 0.030940301716327667\n",
      "epoch: 3 step: 814, loss is 0.06175721064209938\n",
      "epoch: 3 step: 815, loss is 0.02262193150818348\n",
      "epoch: 3 step: 816, loss is 0.03011677786707878\n",
      "epoch: 3 step: 817, loss is 0.10640451312065125\n",
      "epoch: 3 step: 818, loss is 0.030957994982600212\n",
      "epoch: 3 step: 819, loss is 0.12634238600730896\n",
      "epoch: 3 step: 820, loss is 0.228440523147583\n",
      "epoch: 3 step: 821, loss is 0.1485811471939087\n",
      "epoch: 3 step: 822, loss is 0.2938746511936188\n",
      "epoch: 3 step: 823, loss is 0.16762293875217438\n",
      "epoch: 3 step: 824, loss is 0.06549936532974243\n",
      "epoch: 3 step: 825, loss is 0.23964864015579224\n",
      "epoch: 3 step: 826, loss is 0.3598759174346924\n",
      "epoch: 3 step: 827, loss is 0.02634613960981369\n",
      "epoch: 3 step: 828, loss is 0.0005669917445629835\n",
      "epoch: 3 step: 829, loss is 0.08605759590864182\n",
      "epoch: 3 step: 830, loss is 0.01644212193787098\n",
      "epoch: 3 step: 831, loss is 0.017643289640545845\n",
      "epoch: 3 step: 832, loss is 0.07730036973953247\n",
      "epoch: 3 step: 833, loss is 0.004216383211314678\n",
      "epoch: 3 step: 834, loss is 0.01748129352927208\n",
      "epoch: 3 step: 835, loss is 0.0031509664840996265\n",
      "epoch: 3 step: 836, loss is 0.01069672591984272\n",
      "epoch: 3 step: 837, loss is 0.11880970746278763\n",
      "epoch: 3 step: 838, loss is 0.02415691502392292\n",
      "epoch: 3 step: 839, loss is 0.023279087617993355\n",
      "epoch: 3 step: 840, loss is 0.035565026104450226\n",
      "epoch: 3 step: 841, loss is 0.03703082725405693\n",
      "epoch: 3 step: 842, loss is 0.22867451608181\n",
      "epoch: 3 step: 843, loss is 0.016278965398669243\n",
      "epoch: 3 step: 844, loss is 0.01973174512386322\n",
      "epoch: 3 step: 845, loss is 0.12318596243858337\n",
      "epoch: 3 step: 846, loss is 0.2158864140510559\n",
      "epoch: 3 step: 847, loss is 0.029053937643766403\n",
      "epoch: 3 step: 848, loss is 0.007330372929573059\n",
      "epoch: 3 step: 849, loss is 0.22183768451213837\n",
      "epoch: 3 step: 850, loss is 0.10213154554367065\n",
      "epoch: 3 step: 851, loss is 0.09853988885879517\n",
      "epoch: 3 step: 852, loss is 0.008663816377520561\n",
      "epoch: 3 step: 853, loss is 0.17873862385749817\n",
      "epoch: 3 step: 854, loss is 0.1182645931839943\n",
      "epoch: 3 step: 855, loss is 0.11172935366630554\n",
      "epoch: 3 step: 856, loss is 0.01145573053508997\n",
      "epoch: 3 step: 857, loss is 0.05063890665769577\n",
      "epoch: 3 step: 858, loss is 0.17964790761470795\n",
      "epoch: 3 step: 859, loss is 0.20110176503658295\n",
      "epoch: 3 step: 860, loss is 0.1430765986442566\n",
      "epoch: 3 step: 861, loss is 0.2928612530231476\n",
      "epoch: 3 step: 862, loss is 0.027871644124388695\n",
      "epoch: 3 step: 863, loss is 0.030521074309945107\n",
      "epoch: 3 step: 864, loss is 0.03026316501200199\n",
      "epoch: 3 step: 865, loss is 0.22667545080184937\n",
      "epoch: 3 step: 866, loss is 0.05939290300011635\n",
      "epoch: 3 step: 867, loss is 0.008646409027278423\n",
      "epoch: 3 step: 868, loss is 0.17401058971881866\n",
      "epoch: 3 step: 869, loss is 0.0326639749109745\n",
      "epoch: 3 step: 870, loss is 0.043793898075819016\n",
      "epoch: 3 step: 871, loss is 0.044342607259750366\n",
      "epoch: 3 step: 872, loss is 0.10375663638114929\n",
      "epoch: 3 step: 873, loss is 0.08238481730222702\n",
      "epoch: 3 step: 874, loss is 0.04939917102456093\n",
      "epoch: 3 step: 875, loss is 0.057914845645427704\n",
      "epoch: 3 step: 876, loss is 0.005811968818306923\n",
      "epoch: 3 step: 877, loss is 0.011044744402170181\n",
      "epoch: 3 step: 878, loss is 0.02258915826678276\n",
      "epoch: 3 step: 879, loss is 0.008317943662405014\n",
      "epoch: 3 step: 880, loss is 0.24676960706710815\n",
      "epoch: 3 step: 881, loss is 0.021274132654070854\n",
      "epoch: 3 step: 882, loss is 0.049378108233213425\n",
      "epoch: 3 step: 883, loss is 0.21310429275035858\n",
      "epoch: 3 step: 884, loss is 0.11735504865646362\n",
      "epoch: 3 step: 885, loss is 0.09389540553092957\n",
      "epoch: 3 step: 886, loss is 0.07444634288549423\n",
      "epoch: 3 step: 887, loss is 0.01025208830833435\n",
      "epoch: 3 step: 888, loss is 0.05897423252463341\n",
      "epoch: 3 step: 889, loss is 0.1871730089187622\n",
      "epoch: 3 step: 890, loss is 0.045096173882484436\n",
      "epoch: 3 step: 891, loss is 0.09841668605804443\n",
      "epoch: 3 step: 892, loss is 0.04923851788043976\n",
      "epoch: 3 step: 893, loss is 0.13367398083209991\n",
      "epoch: 3 step: 894, loss is 0.09401874989271164\n",
      "epoch: 3 step: 895, loss is 0.028382761403918266\n",
      "epoch: 3 step: 896, loss is 0.2437182068824768\n",
      "epoch: 3 step: 897, loss is 0.10466696321964264\n",
      "epoch: 3 step: 898, loss is 0.01805240474641323\n",
      "epoch: 3 step: 899, loss is 0.4076947867870331\n",
      "epoch: 3 step: 900, loss is 0.1824263632297516\n",
      "epoch: 3 step: 901, loss is 0.07969377934932709\n",
      "epoch: 3 step: 902, loss is 0.0788947120308876\n",
      "epoch: 3 step: 903, loss is 0.013297327794134617\n",
      "epoch: 3 step: 904, loss is 0.07614719867706299\n",
      "epoch: 3 step: 905, loss is 0.1116732731461525\n",
      "epoch: 3 step: 906, loss is 0.020516404882073402\n",
      "epoch: 3 step: 907, loss is 0.11747774481773376\n",
      "epoch: 3 step: 908, loss is 0.12226228415966034\n",
      "epoch: 3 step: 909, loss is 0.006971352733671665\n",
      "epoch: 3 step: 910, loss is 0.0032813679426908493\n",
      "epoch: 3 step: 911, loss is 0.23082375526428223\n",
      "epoch: 3 step: 912, loss is 0.05882299691438675\n",
      "epoch: 3 step: 913, loss is 0.1165492832660675\n",
      "epoch: 3 step: 914, loss is 0.022831737995147705\n",
      "epoch: 3 step: 915, loss is 0.019055619835853577\n",
      "epoch: 3 step: 916, loss is 0.0983552485704422\n",
      "epoch: 3 step: 917, loss is 0.011348642408847809\n",
      "epoch: 3 step: 918, loss is 0.16793586313724518\n",
      "epoch: 3 step: 919, loss is 0.1193152591586113\n",
      "epoch: 3 step: 920, loss is 0.018549064174294472\n",
      "epoch: 3 step: 921, loss is 0.03197620436549187\n",
      "epoch: 3 step: 922, loss is 0.012707779183983803\n",
      "epoch: 3 step: 923, loss is 0.08462294191122055\n",
      "epoch: 3 step: 924, loss is 0.042141128331422806\n",
      "epoch: 3 step: 925, loss is 0.030916057527065277\n",
      "epoch: 3 step: 926, loss is 0.004768792539834976\n",
      "epoch: 3 step: 927, loss is 0.24775607883930206\n",
      "epoch: 3 step: 928, loss is 0.1257256269454956\n",
      "epoch: 3 step: 929, loss is 0.05886199325323105\n",
      "epoch: 3 step: 930, loss is 0.030671676620841026\n",
      "epoch: 3 step: 931, loss is 0.02310076355934143\n",
      "epoch: 3 step: 932, loss is 0.039522845298051834\n",
      "epoch: 3 step: 933, loss is 0.0022906719241291285\n",
      "epoch: 3 step: 934, loss is 0.3836357295513153\n",
      "epoch: 3 step: 935, loss is 0.25371116399765015\n",
      "epoch: 3 step: 936, loss is 0.005954981781542301\n",
      "epoch: 3 step: 937, loss is 0.12268783897161484\n",
      "epoch: 3 step: 938, loss is 0.049795154482126236\n",
      "epoch: 3 step: 939, loss is 0.07145868241786957\n",
      "epoch: 3 step: 940, loss is 0.04857512190937996\n",
      "epoch: 3 step: 941, loss is 0.10687265545129776\n",
      "epoch: 3 step: 942, loss is 0.04071846976876259\n",
      "epoch: 3 step: 943, loss is 0.10362975299358368\n",
      "epoch: 3 step: 944, loss is 0.010318714193999767\n",
      "epoch: 3 step: 945, loss is 0.019608592614531517\n",
      "epoch: 3 step: 946, loss is 0.005615473259240389\n",
      "epoch: 3 step: 947, loss is 0.08110351115465164\n",
      "epoch: 3 step: 948, loss is 0.03904557600617409\n",
      "epoch: 3 step: 949, loss is 0.05336148664355278\n",
      "epoch: 3 step: 950, loss is 0.010312133468687534\n",
      "epoch: 3 step: 951, loss is 0.056855544447898865\n",
      "epoch: 3 step: 952, loss is 0.057318706065416336\n",
      "epoch: 3 step: 953, loss is 0.05620141327381134\n",
      "epoch: 3 step: 954, loss is 0.20018193125724792\n",
      "epoch: 3 step: 955, loss is 0.05452227219939232\n",
      "epoch: 3 step: 956, loss is 0.036882899701595306\n",
      "epoch: 3 step: 957, loss is 0.047006070613861084\n",
      "epoch: 3 step: 958, loss is 0.03100370056927204\n",
      "epoch: 3 step: 959, loss is 0.01060104463249445\n",
      "epoch: 3 step: 960, loss is 0.047237347811460495\n",
      "epoch: 3 step: 961, loss is 0.005562067497521639\n",
      "epoch: 3 step: 962, loss is 0.06357952207326889\n",
      "epoch: 3 step: 963, loss is 0.007656055968254805\n",
      "epoch: 3 step: 964, loss is 0.07981131970882416\n",
      "epoch: 3 step: 965, loss is 0.05635185167193413\n",
      "epoch: 3 step: 966, loss is 0.048660848289728165\n",
      "epoch: 3 step: 967, loss is 0.15733711421489716\n",
      "epoch: 3 step: 968, loss is 0.017848581075668335\n",
      "epoch: 3 step: 969, loss is 0.021055782213807106\n",
      "epoch: 3 step: 970, loss is 0.14718639850616455\n",
      "epoch: 3 step: 971, loss is 0.06214907392859459\n",
      "epoch: 3 step: 972, loss is 0.0019862684421241283\n",
      "epoch: 3 step: 973, loss is 0.01591195911169052\n",
      "epoch: 3 step: 974, loss is 0.08261548727750778\n",
      "epoch: 3 step: 975, loss is 0.07792004942893982\n",
      "epoch: 3 step: 976, loss is 0.09348826855421066\n",
      "epoch: 3 step: 977, loss is 0.10987219214439392\n",
      "epoch: 3 step: 978, loss is 0.037734564393758774\n",
      "epoch: 3 step: 979, loss is 0.012065041810274124\n",
      "epoch: 3 step: 980, loss is 0.008148612454533577\n",
      "epoch: 3 step: 981, loss is 0.06332904100418091\n",
      "epoch: 3 step: 982, loss is 0.0892999991774559\n",
      "epoch: 3 step: 983, loss is 0.11398980766534805\n",
      "epoch: 3 step: 984, loss is 0.13666820526123047\n",
      "epoch: 3 step: 985, loss is 0.08983185887336731\n",
      "epoch: 3 step: 986, loss is 0.07522871345281601\n",
      "epoch: 3 step: 987, loss is 0.02361440472304821\n",
      "epoch: 3 step: 988, loss is 0.11290881782770157\n",
      "epoch: 3 step: 989, loss is 0.27571168541908264\n",
      "epoch: 3 step: 990, loss is 0.051629796624183655\n",
      "epoch: 3 step: 991, loss is 0.12872107326984406\n",
      "epoch: 3 step: 992, loss is 0.0038828959222882986\n",
      "epoch: 3 step: 993, loss is 0.006683565676212311\n",
      "epoch: 3 step: 994, loss is 0.04000760614871979\n",
      "epoch: 3 step: 995, loss is 0.027425095438957214\n",
      "epoch: 3 step: 996, loss is 0.0202166885137558\n",
      "epoch: 3 step: 997, loss is 0.10211603343486786\n",
      "epoch: 3 step: 998, loss is 0.010276156477630138\n",
      "epoch: 3 step: 999, loss is 0.04909782484173775\n",
      "epoch: 3 step: 1000, loss is 0.028203682973980904\n",
      "epoch: 3 step: 1001, loss is 0.011717311106622219\n",
      "epoch: 3 step: 1002, loss is 0.11604271829128265\n",
      "epoch: 3 step: 1003, loss is 0.0210861936211586\n",
      "epoch: 3 step: 1004, loss is 0.018319834023714066\n",
      "epoch: 3 step: 1005, loss is 0.014677243307232857\n",
      "epoch: 3 step: 1006, loss is 0.003262646496295929\n",
      "epoch: 3 step: 1007, loss is 0.12460972368717194\n",
      "epoch: 3 step: 1008, loss is 0.0961965024471283\n",
      "epoch: 3 step: 1009, loss is 0.06217165291309357\n",
      "epoch: 3 step: 1010, loss is 0.07398602366447449\n",
      "epoch: 3 step: 1011, loss is 0.08124001324176788\n",
      "epoch: 3 step: 1012, loss is 0.26086172461509705\n",
      "epoch: 3 step: 1013, loss is 0.006859862245619297\n",
      "epoch: 3 step: 1014, loss is 0.07942377030849457\n",
      "epoch: 3 step: 1015, loss is 0.005900087300688028\n",
      "epoch: 3 step: 1016, loss is 0.021044524386525154\n",
      "epoch: 3 step: 1017, loss is 0.18234819173812866\n",
      "epoch: 3 step: 1018, loss is 0.3239450752735138\n",
      "epoch: 3 step: 1019, loss is 0.03474729508161545\n",
      "epoch: 3 step: 1020, loss is 0.027709659188985825\n",
      "epoch: 3 step: 1021, loss is 0.006844543851912022\n",
      "epoch: 3 step: 1022, loss is 0.20619836449623108\n",
      "epoch: 3 step: 1023, loss is 0.08222388476133347\n",
      "epoch: 3 step: 1024, loss is 0.0769832506775856\n",
      "epoch: 3 step: 1025, loss is 0.17923201620578766\n",
      "epoch: 3 step: 1026, loss is 0.021179959177970886\n",
      "epoch: 3 step: 1027, loss is 0.12727005779743195\n",
      "epoch: 3 step: 1028, loss is 0.022430334240198135\n",
      "epoch: 3 step: 1029, loss is 0.3125590682029724\n",
      "epoch: 3 step: 1030, loss is 0.007976560853421688\n",
      "epoch: 3 step: 1031, loss is 0.03556676581501961\n",
      "epoch: 3 step: 1032, loss is 0.014853036031126976\n",
      "epoch: 3 step: 1033, loss is 0.050023168325424194\n",
      "epoch: 3 step: 1034, loss is 0.48569127917289734\n",
      "epoch: 3 step: 1035, loss is 0.026673229411244392\n",
      "epoch: 3 step: 1036, loss is 0.027922971174120903\n",
      "epoch: 3 step: 1037, loss is 0.05219180881977081\n",
      "epoch: 3 step: 1038, loss is 0.020170513540506363\n",
      "epoch: 3 step: 1039, loss is 0.03505624830722809\n",
      "epoch: 3 step: 1040, loss is 0.0015579742612317204\n",
      "epoch: 3 step: 1041, loss is 0.10840970277786255\n",
      "epoch: 3 step: 1042, loss is 0.17689824104309082\n",
      "epoch: 3 step: 1043, loss is 0.17932118475437164\n",
      "epoch: 3 step: 1044, loss is 0.1064189225435257\n",
      "epoch: 3 step: 1045, loss is 0.010450216010212898\n",
      "epoch: 3 step: 1046, loss is 0.3970593214035034\n",
      "epoch: 3 step: 1047, loss is 0.030585383996367455\n",
      "epoch: 3 step: 1048, loss is 0.009155923500657082\n",
      "epoch: 3 step: 1049, loss is 0.11459235101938248\n",
      "epoch: 3 step: 1050, loss is 0.003693092381581664\n",
      "epoch: 3 step: 1051, loss is 0.009887021034955978\n",
      "epoch: 3 step: 1052, loss is 0.291989803314209\n",
      "epoch: 3 step: 1053, loss is 0.009004302322864532\n",
      "epoch: 3 step: 1054, loss is 0.05345727503299713\n",
      "epoch: 3 step: 1055, loss is 0.1909375637769699\n",
      "epoch: 3 step: 1056, loss is 0.01153138279914856\n",
      "epoch: 3 step: 1057, loss is 0.012116442434489727\n",
      "epoch: 3 step: 1058, loss is 0.09339538216590881\n",
      "epoch: 3 step: 1059, loss is 0.11483722925186157\n",
      "epoch: 3 step: 1060, loss is 0.0074554020538926125\n",
      "epoch: 3 step: 1061, loss is 0.015908321365714073\n",
      "epoch: 3 step: 1062, loss is 0.0038714196998625994\n",
      "epoch: 3 step: 1063, loss is 0.005679459776729345\n",
      "epoch: 3 step: 1064, loss is 0.13490183651447296\n",
      "epoch: 3 step: 1065, loss is 0.006768427323549986\n",
      "epoch: 3 step: 1066, loss is 0.0750960037112236\n",
      "epoch: 3 step: 1067, loss is 0.12115394324064255\n",
      "epoch: 3 step: 1068, loss is 0.002407432533800602\n",
      "epoch: 3 step: 1069, loss is 0.05218101292848587\n",
      "epoch: 3 step: 1070, loss is 0.21358180046081543\n",
      "epoch: 3 step: 1071, loss is 0.03767886385321617\n",
      "epoch: 3 step: 1072, loss is 0.007120407186448574\n",
      "epoch: 3 step: 1073, loss is 0.09229393303394318\n",
      "epoch: 3 step: 1074, loss is 0.06136753782629967\n",
      "epoch: 3 step: 1075, loss is 0.021002046763896942\n",
      "epoch: 3 step: 1076, loss is 0.11699376255273819\n",
      "epoch: 3 step: 1077, loss is 0.010592034086585045\n",
      "epoch: 3 step: 1078, loss is 0.008799979463219643\n",
      "epoch: 3 step: 1079, loss is 0.00706436624750495\n",
      "epoch: 3 step: 1080, loss is 0.12073564529418945\n",
      "epoch: 3 step: 1081, loss is 0.1690445840358734\n",
      "epoch: 3 step: 1082, loss is 0.13375359773635864\n",
      "epoch: 3 step: 1083, loss is 0.2221994698047638\n",
      "epoch: 3 step: 1084, loss is 0.028501000255346298\n",
      "epoch: 3 step: 1085, loss is 0.002453942084684968\n",
      "epoch: 3 step: 1086, loss is 0.027973605319857597\n",
      "epoch: 3 step: 1087, loss is 0.03857070580124855\n",
      "epoch: 3 step: 1088, loss is 0.13472174108028412\n",
      "epoch: 3 step: 1089, loss is 0.1646658331155777\n",
      "epoch: 3 step: 1090, loss is 0.14299467206001282\n",
      "epoch: 3 step: 1091, loss is 0.021626021713018417\n",
      "epoch: 3 step: 1092, loss is 0.008103612810373306\n",
      "epoch: 3 step: 1093, loss is 0.24417990446090698\n",
      "epoch: 3 step: 1094, loss is 0.13359463214874268\n",
      "epoch: 3 step: 1095, loss is 0.03720512241125107\n",
      "epoch: 3 step: 1096, loss is 0.5314023494720459\n",
      "epoch: 3 step: 1097, loss is 0.0024973053950816393\n",
      "epoch: 3 step: 1098, loss is 0.13958898186683655\n",
      "epoch: 3 step: 1099, loss is 0.1654171496629715\n",
      "epoch: 3 step: 1100, loss is 0.043377816677093506\n",
      "epoch: 3 step: 1101, loss is 0.14779049158096313\n",
      "epoch: 3 step: 1102, loss is 0.2609368860721588\n",
      "epoch: 3 step: 1103, loss is 0.07558725029230118\n",
      "epoch: 3 step: 1104, loss is 0.06268662214279175\n",
      "epoch: 3 step: 1105, loss is 0.02771797962486744\n",
      "epoch: 3 step: 1106, loss is 0.023006761446595192\n",
      "epoch: 3 step: 1107, loss is 0.06864877790212631\n",
      "epoch: 3 step: 1108, loss is 0.04042953625321388\n",
      "epoch: 3 step: 1109, loss is 0.13225433230400085\n",
      "epoch: 3 step: 1110, loss is 0.01505220029503107\n",
      "epoch: 3 step: 1111, loss is 0.16430246829986572\n",
      "epoch: 3 step: 1112, loss is 0.04716533422470093\n",
      "epoch: 3 step: 1113, loss is 0.1396055668592453\n",
      "epoch: 3 step: 1114, loss is 0.013624620623886585\n",
      "epoch: 3 step: 1115, loss is 0.04917001724243164\n",
      "epoch: 3 step: 1116, loss is 0.024619990959763527\n",
      "epoch: 3 step: 1117, loss is 0.08238472044467926\n",
      "epoch: 3 step: 1118, loss is 0.17541654407978058\n",
      "epoch: 3 step: 1119, loss is 0.018470993265509605\n",
      "epoch: 3 step: 1120, loss is 0.3879857361316681\n",
      "epoch: 3 step: 1121, loss is 0.026350965723395348\n",
      "epoch: 3 step: 1122, loss is 0.016677841544151306\n",
      "epoch: 3 step: 1123, loss is 0.014757550321519375\n",
      "epoch: 3 step: 1124, loss is 0.11214675009250641\n",
      "epoch: 3 step: 1125, loss is 0.41378307342529297\n",
      "epoch: 3 step: 1126, loss is 0.08384044468402863\n",
      "epoch: 3 step: 1127, loss is 0.007964491844177246\n",
      "epoch: 3 step: 1128, loss is 0.01562950201332569\n",
      "epoch: 3 step: 1129, loss is 0.011350030079483986\n",
      "epoch: 3 step: 1130, loss is 0.16811448335647583\n",
      "epoch: 3 step: 1131, loss is 0.05595658719539642\n",
      "epoch: 3 step: 1132, loss is 0.02391045168042183\n",
      "epoch: 3 step: 1133, loss is 0.07289174199104309\n",
      "epoch: 3 step: 1134, loss is 0.2622547149658203\n",
      "epoch: 3 step: 1135, loss is 0.05171949043869972\n",
      "epoch: 3 step: 1136, loss is 0.21987733244895935\n",
      "epoch: 3 step: 1137, loss is 0.11684702336788177\n",
      "epoch: 3 step: 1138, loss is 0.07940981537103653\n",
      "epoch: 3 step: 1139, loss is 0.01522451639175415\n",
      "epoch: 3 step: 1140, loss is 0.0882003977894783\n",
      "epoch: 3 step: 1141, loss is 0.05635963752865791\n",
      "epoch: 3 step: 1142, loss is 0.0081414096057415\n",
      "epoch: 3 step: 1143, loss is 0.007055840454995632\n",
      "epoch: 3 step: 1144, loss is 0.037005484104156494\n",
      "epoch: 3 step: 1145, loss is 0.1898552030324936\n",
      "epoch: 3 step: 1146, loss is 0.2664283812046051\n",
      "epoch: 3 step: 1147, loss is 0.06718000769615173\n",
      "epoch: 3 step: 1148, loss is 0.020594561472535133\n",
      "epoch: 3 step: 1149, loss is 0.24898311495780945\n",
      "epoch: 3 step: 1150, loss is 0.033942077308893204\n",
      "epoch: 3 step: 1151, loss is 0.036815859377384186\n",
      "epoch: 3 step: 1152, loss is 0.006211823783814907\n",
      "epoch: 3 step: 1153, loss is 0.23749251663684845\n",
      "epoch: 3 step: 1154, loss is 0.07909592986106873\n",
      "epoch: 3 step: 1155, loss is 0.03191448375582695\n",
      "epoch: 3 step: 1156, loss is 0.09331919252872467\n",
      "epoch: 3 step: 1157, loss is 0.01535735186189413\n",
      "epoch: 3 step: 1158, loss is 0.018580082803964615\n",
      "epoch: 3 step: 1159, loss is 0.03192270174622536\n",
      "epoch: 3 step: 1160, loss is 0.015571938827633858\n",
      "epoch: 3 step: 1161, loss is 0.004402367863804102\n",
      "epoch: 3 step: 1162, loss is 0.006493249908089638\n",
      "epoch: 3 step: 1163, loss is 0.05049199238419533\n",
      "epoch: 3 step: 1164, loss is 0.05281312018632889\n",
      "epoch: 3 step: 1165, loss is 0.02513435296714306\n",
      "epoch: 3 step: 1166, loss is 0.024947166442871094\n",
      "epoch: 3 step: 1167, loss is 0.005461904685944319\n",
      "epoch: 3 step: 1168, loss is 0.126164972782135\n",
      "epoch: 3 step: 1169, loss is 0.013073282316327095\n",
      "epoch: 3 step: 1170, loss is 0.02514118328690529\n",
      "epoch: 3 step: 1171, loss is 0.183588445186615\n",
      "epoch: 3 step: 1172, loss is 0.01913590170443058\n",
      "epoch: 3 step: 1173, loss is 0.022667929530143738\n",
      "epoch: 3 step: 1174, loss is 0.007100843358784914\n",
      "epoch: 3 step: 1175, loss is 0.036499373614788055\n",
      "epoch: 3 step: 1176, loss is 0.05369036644697189\n",
      "epoch: 3 step: 1177, loss is 0.08070266246795654\n",
      "epoch: 3 step: 1178, loss is 0.2857356667518616\n",
      "epoch: 3 step: 1179, loss is 0.6145668029785156\n",
      "epoch: 3 step: 1180, loss is 0.009765558876097202\n",
      "epoch: 3 step: 1181, loss is 0.08381886780261993\n",
      "epoch: 3 step: 1182, loss is 0.1913556158542633\n",
      "epoch: 3 step: 1183, loss is 0.011281905695796013\n",
      "epoch: 3 step: 1184, loss is 0.017572257667779922\n",
      "epoch: 3 step: 1185, loss is 0.004533560946583748\n",
      "epoch: 3 step: 1186, loss is 0.06911589950323105\n",
      "epoch: 3 step: 1187, loss is 0.09100108593702316\n",
      "epoch: 3 step: 1188, loss is 0.016292152926325798\n",
      "epoch: 3 step: 1189, loss is 0.003189715323969722\n",
      "epoch: 3 step: 1190, loss is 0.010481934063136578\n",
      "epoch: 3 step: 1191, loss is 0.09184283763170242\n",
      "epoch: 3 step: 1192, loss is 0.016801098361611366\n",
      "epoch: 3 step: 1193, loss is 0.006962239276617765\n",
      "epoch: 3 step: 1194, loss is 0.04592633992433548\n",
      "epoch: 3 step: 1195, loss is 0.0358586311340332\n",
      "epoch: 3 step: 1196, loss is 0.03332553803920746\n",
      "epoch: 3 step: 1197, loss is 0.05730685219168663\n",
      "epoch: 3 step: 1198, loss is 0.011500662192702293\n",
      "epoch: 3 step: 1199, loss is 0.05147697776556015\n",
      "epoch: 3 step: 1200, loss is 0.1167287826538086\n",
      "epoch: 3 step: 1201, loss is 0.05690767616033554\n",
      "epoch: 3 step: 1202, loss is 0.020582230761647224\n",
      "epoch: 3 step: 1203, loss is 0.04654740169644356\n",
      "epoch: 3 step: 1204, loss is 0.04325306415557861\n",
      "epoch: 3 step: 1205, loss is 0.0432656891644001\n",
      "epoch: 3 step: 1206, loss is 0.18524853885173798\n",
      "epoch: 3 step: 1207, loss is 0.08026501536369324\n",
      "epoch: 3 step: 1208, loss is 0.06440464407205582\n",
      "epoch: 3 step: 1209, loss is 0.035152122378349304\n",
      "epoch: 3 step: 1210, loss is 0.24615263938903809\n",
      "epoch: 3 step: 1211, loss is 0.06801176816225052\n",
      "epoch: 3 step: 1212, loss is 0.24259719252586365\n",
      "epoch: 3 step: 1213, loss is 0.001363861607387662\n",
      "epoch: 3 step: 1214, loss is 0.030708923935890198\n",
      "epoch: 3 step: 1215, loss is 0.09189362078905106\n",
      "epoch: 3 step: 1216, loss is 0.11633998900651932\n",
      "epoch: 3 step: 1217, loss is 0.16481275856494904\n",
      "epoch: 3 step: 1218, loss is 0.11770366877317429\n",
      "epoch: 3 step: 1219, loss is 0.10137030482292175\n",
      "epoch: 3 step: 1220, loss is 0.028646500781178474\n",
      "epoch: 3 step: 1221, loss is 0.08891812711954117\n",
      "epoch: 3 step: 1222, loss is 0.053491294384002686\n",
      "epoch: 3 step: 1223, loss is 0.04757953807711601\n",
      "epoch: 3 step: 1224, loss is 0.015315879136323929\n",
      "epoch: 3 step: 1225, loss is 0.11134197562932968\n",
      "epoch: 3 step: 1226, loss is 0.0031105526722967625\n",
      "epoch: 3 step: 1227, loss is 0.15329518914222717\n",
      "epoch: 3 step: 1228, loss is 0.004038481507450342\n",
      "epoch: 3 step: 1229, loss is 0.09162324666976929\n",
      "epoch: 3 step: 1230, loss is 0.019668877124786377\n",
      "epoch: 3 step: 1231, loss is 0.12350776791572571\n",
      "epoch: 3 step: 1232, loss is 0.0020421352237462997\n",
      "epoch: 3 step: 1233, loss is 0.01845449209213257\n",
      "epoch: 3 step: 1234, loss is 0.01838736981153488\n",
      "epoch: 3 step: 1235, loss is 0.12953902781009674\n",
      "epoch: 3 step: 1236, loss is 0.11595950275659561\n",
      "epoch: 3 step: 1237, loss is 0.18987350165843964\n",
      "epoch: 3 step: 1238, loss is 0.04950234293937683\n",
      "epoch: 3 step: 1239, loss is 0.04572661966085434\n",
      "epoch: 3 step: 1240, loss is 0.036516543477773666\n",
      "epoch: 3 step: 1241, loss is 0.17853301763534546\n",
      "epoch: 3 step: 1242, loss is 0.07736791670322418\n",
      "epoch: 3 step: 1243, loss is 0.021678175777196884\n",
      "epoch: 3 step: 1244, loss is 0.13856710493564606\n",
      "epoch: 3 step: 1245, loss is 0.04295904561877251\n",
      "epoch: 3 step: 1246, loss is 0.03302450850605965\n",
      "epoch: 3 step: 1247, loss is 0.0512242428958416\n",
      "epoch: 3 step: 1248, loss is 0.0012617682805284858\n",
      "epoch: 3 step: 1249, loss is 0.10158012062311172\n",
      "epoch: 3 step: 1250, loss is 0.01182822696864605\n",
      "epoch: 3 step: 1251, loss is 0.3368190824985504\n",
      "epoch: 3 step: 1252, loss is 0.05956215038895607\n",
      "epoch: 3 step: 1253, loss is 0.17498987913131714\n",
      "epoch: 3 step: 1254, loss is 0.3023281395435333\n",
      "epoch: 3 step: 1255, loss is 0.055793069303035736\n",
      "epoch: 3 step: 1256, loss is 0.004136890638619661\n",
      "epoch: 3 step: 1257, loss is 0.07110174745321274\n",
      "epoch: 3 step: 1258, loss is 0.1375982165336609\n",
      "epoch: 3 step: 1259, loss is 0.02998933754861355\n",
      "epoch: 3 step: 1260, loss is 0.023158561438322067\n",
      "epoch: 3 step: 1261, loss is 0.2581121623516083\n",
      "epoch: 3 step: 1262, loss is 0.09089424461126328\n",
      "epoch: 3 step: 1263, loss is 0.166002556681633\n",
      "epoch: 3 step: 1264, loss is 0.04243268817663193\n",
      "epoch: 3 step: 1265, loss is 0.08885478973388672\n",
      "epoch: 3 step: 1266, loss is 0.2149871289730072\n",
      "epoch: 3 step: 1267, loss is 0.07865399867296219\n",
      "epoch: 3 step: 1268, loss is 0.060426127165555954\n",
      "epoch: 3 step: 1269, loss is 0.19168691337108612\n",
      "epoch: 3 step: 1270, loss is 0.041061222553253174\n",
      "epoch: 3 step: 1271, loss is 0.09744790196418762\n",
      "epoch: 3 step: 1272, loss is 0.018331915140151978\n",
      "epoch: 3 step: 1273, loss is 0.01057323720306158\n",
      "epoch: 3 step: 1274, loss is 0.07518411427736282\n",
      "epoch: 3 step: 1275, loss is 0.18544232845306396\n",
      "epoch: 3 step: 1276, loss is 0.20681969821453094\n",
      "epoch: 3 step: 1277, loss is 0.10808065533638\n",
      "epoch: 3 step: 1278, loss is 0.07534552365541458\n",
      "epoch: 3 step: 1279, loss is 0.09480567276477814\n",
      "epoch: 3 step: 1280, loss is 0.01670670136809349\n",
      "epoch: 3 step: 1281, loss is 0.4759454131126404\n",
      "epoch: 3 step: 1282, loss is 0.041276320815086365\n",
      "epoch: 3 step: 1283, loss is 0.037979692220687866\n",
      "epoch: 3 step: 1284, loss is 0.0468418225646019\n",
      "epoch: 3 step: 1285, loss is 0.05376248061656952\n",
      "epoch: 3 step: 1286, loss is 0.060352977365255356\n",
      "epoch: 3 step: 1287, loss is 0.17411254346370697\n",
      "epoch: 3 step: 1288, loss is 0.05678575858473778\n",
      "epoch: 3 step: 1289, loss is 0.05122356861829758\n",
      "epoch: 3 step: 1290, loss is 0.018312064930796623\n",
      "epoch: 3 step: 1291, loss is 0.06548172980546951\n",
      "epoch: 3 step: 1292, loss is 0.03104114718735218\n",
      "epoch: 3 step: 1293, loss is 0.012022619135677814\n",
      "epoch: 3 step: 1294, loss is 0.08898649364709854\n",
      "epoch: 3 step: 1295, loss is 0.20805150270462036\n",
      "epoch: 3 step: 1296, loss is 0.12873575091362\n",
      "epoch: 3 step: 1297, loss is 0.027910392731428146\n",
      "epoch: 3 step: 1298, loss is 0.03692317754030228\n",
      "epoch: 3 step: 1299, loss is 0.009977724403142929\n",
      "epoch: 3 step: 1300, loss is 0.007827678695321083\n",
      "epoch: 3 step: 1301, loss is 0.007498448248952627\n",
      "epoch: 3 step: 1302, loss is 0.0728098526597023\n",
      "epoch: 3 step: 1303, loss is 0.0613231435418129\n",
      "epoch: 3 step: 1304, loss is 0.21513794362545013\n",
      "epoch: 3 step: 1305, loss is 0.09666174650192261\n",
      "epoch: 3 step: 1306, loss is 0.004423825070261955\n",
      "epoch: 3 step: 1307, loss is 0.04735343158245087\n",
      "epoch: 3 step: 1308, loss is 0.22067062556743622\n",
      "epoch: 3 step: 1309, loss is 0.03061417117714882\n",
      "epoch: 3 step: 1310, loss is 0.05022723600268364\n",
      "epoch: 3 step: 1311, loss is 0.10781082510948181\n",
      "epoch: 3 step: 1312, loss is 0.0890088826417923\n",
      "epoch: 3 step: 1313, loss is 0.009883702732622623\n",
      "epoch: 3 step: 1314, loss is 0.038842953741550446\n",
      "epoch: 3 step: 1315, loss is 0.060732658952474594\n",
      "epoch: 3 step: 1316, loss is 0.08901430666446686\n",
      "epoch: 3 step: 1317, loss is 0.008175657130777836\n",
      "epoch: 3 step: 1318, loss is 0.01808186061680317\n",
      "epoch: 3 step: 1319, loss is 0.013509833253920078\n",
      "epoch: 3 step: 1320, loss is 0.0581805445253849\n",
      "epoch: 3 step: 1321, loss is 0.026263145729899406\n",
      "epoch: 3 step: 1322, loss is 0.09418299049139023\n",
      "epoch: 3 step: 1323, loss is 0.05161885917186737\n",
      "epoch: 3 step: 1324, loss is 0.08917385339736938\n",
      "epoch: 3 step: 1325, loss is 0.04194222763180733\n",
      "epoch: 3 step: 1326, loss is 0.027786122635006905\n",
      "epoch: 3 step: 1327, loss is 0.007275019772350788\n",
      "epoch: 3 step: 1328, loss is 0.07899629324674606\n",
      "epoch: 3 step: 1329, loss is 0.037588879466056824\n",
      "epoch: 3 step: 1330, loss is 0.05015146732330322\n",
      "epoch: 3 step: 1331, loss is 0.15260055661201477\n",
      "epoch: 3 step: 1332, loss is 0.027633195742964745\n",
      "epoch: 3 step: 1333, loss is 0.0016399563755840063\n",
      "epoch: 3 step: 1334, loss is 0.059576258063316345\n",
      "epoch: 3 step: 1335, loss is 0.08447202295064926\n",
      "epoch: 3 step: 1336, loss is 0.0518588125705719\n",
      "epoch: 3 step: 1337, loss is 0.06530851870775223\n",
      "epoch: 3 step: 1338, loss is 0.013363974168896675\n",
      "epoch: 3 step: 1339, loss is 0.0012455203104764223\n",
      "epoch: 3 step: 1340, loss is 0.032232508063316345\n",
      "epoch: 3 step: 1341, loss is 0.06431227177381516\n",
      "epoch: 3 step: 1342, loss is 0.01712612621486187\n",
      "epoch: 3 step: 1343, loss is 0.05052836984395981\n",
      "epoch: 3 step: 1344, loss is 0.009295867756009102\n",
      "epoch: 3 step: 1345, loss is 0.01507115364074707\n",
      "epoch: 3 step: 1346, loss is 0.030977265909314156\n",
      "epoch: 3 step: 1347, loss is 0.013011747971177101\n",
      "epoch: 3 step: 1348, loss is 0.007277640979737043\n",
      "epoch: 3 step: 1349, loss is 0.12968896329402924\n",
      "epoch: 3 step: 1350, loss is 0.1528339833021164\n",
      "epoch: 3 step: 1351, loss is 0.02180573344230652\n",
      "epoch: 3 step: 1352, loss is 0.23005791008472443\n",
      "epoch: 3 step: 1353, loss is 0.11472222208976746\n",
      "epoch: 3 step: 1354, loss is 0.062151115387678146\n",
      "epoch: 3 step: 1355, loss is 0.026969991624355316\n",
      "epoch: 3 step: 1356, loss is 0.11058385670185089\n",
      "epoch: 3 step: 1357, loss is 0.03242975473403931\n",
      "epoch: 3 step: 1358, loss is 0.11200595647096634\n",
      "epoch: 3 step: 1359, loss is 0.04566917195916176\n",
      "epoch: 3 step: 1360, loss is 0.044891923666000366\n",
      "epoch: 3 step: 1361, loss is 0.13913291692733765\n",
      "epoch: 3 step: 1362, loss is 0.06618357449769974\n",
      "epoch: 3 step: 1363, loss is 0.15349087119102478\n",
      "epoch: 3 step: 1364, loss is 0.005112253595143557\n",
      "epoch: 3 step: 1365, loss is 0.00382360746152699\n",
      "epoch: 3 step: 1366, loss is 0.035722143948078156\n",
      "epoch: 3 step: 1367, loss is 0.00668687466531992\n",
      "epoch: 3 step: 1368, loss is 0.09818317741155624\n",
      "epoch: 3 step: 1369, loss is 0.03186091408133507\n",
      "epoch: 3 step: 1370, loss is 0.011374495923519135\n",
      "epoch: 3 step: 1371, loss is 0.027248697355389595\n",
      "epoch: 3 step: 1372, loss is 0.04489828273653984\n",
      "epoch: 3 step: 1373, loss is 0.05799192562699318\n",
      "epoch: 3 step: 1374, loss is 0.22794798016548157\n",
      "epoch: 3 step: 1375, loss is 0.01278475672006607\n",
      "epoch: 3 step: 1376, loss is 0.08232045918703079\n",
      "epoch: 3 step: 1377, loss is 0.019933803007006645\n",
      "epoch: 3 step: 1378, loss is 0.24736618995666504\n",
      "epoch: 3 step: 1379, loss is 0.11685960739850998\n",
      "epoch: 3 step: 1380, loss is 0.07010921090841293\n",
      "epoch: 3 step: 1381, loss is 0.07979822903871536\n",
      "epoch: 3 step: 1382, loss is 0.024191146716475487\n",
      "epoch: 3 step: 1383, loss is 0.008005106821656227\n",
      "epoch: 3 step: 1384, loss is 0.14308609068393707\n",
      "epoch: 3 step: 1385, loss is 0.10370928049087524\n",
      "epoch: 3 step: 1386, loss is 0.019758276641368866\n",
      "epoch: 3 step: 1387, loss is 0.00853746198117733\n",
      "epoch: 3 step: 1388, loss is 0.14594720304012299\n",
      "epoch: 3 step: 1389, loss is 0.14080224931240082\n",
      "epoch: 3 step: 1390, loss is 0.007816396653652191\n",
      "epoch: 3 step: 1391, loss is 0.18239912390708923\n",
      "epoch: 3 step: 1392, loss is 0.008392278105020523\n",
      "epoch: 3 step: 1393, loss is 0.0016477707540616393\n",
      "epoch: 3 step: 1394, loss is 0.17712736129760742\n",
      "epoch: 3 step: 1395, loss is 0.1219513937830925\n",
      "epoch: 3 step: 1396, loss is 0.015576403588056564\n",
      "epoch: 3 step: 1397, loss is 0.0997101441025734\n",
      "epoch: 3 step: 1398, loss is 0.14606285095214844\n",
      "epoch: 3 step: 1399, loss is 0.1572972685098648\n",
      "epoch: 3 step: 1400, loss is 0.017313096672296524\n",
      "epoch: 3 step: 1401, loss is 0.2892223298549652\n",
      "epoch: 3 step: 1402, loss is 0.1697363406419754\n",
      "epoch: 3 step: 1403, loss is 0.020573709160089493\n",
      "epoch: 3 step: 1404, loss is 0.09246476739645004\n",
      "epoch: 3 step: 1405, loss is 0.012185436673462391\n",
      "epoch: 3 step: 1406, loss is 0.27225184440612793\n",
      "epoch: 3 step: 1407, loss is 0.038935184478759766\n",
      "epoch: 3 step: 1408, loss is 0.0921710878610611\n",
      "epoch: 3 step: 1409, loss is 0.1258551925420761\n",
      "epoch: 3 step: 1410, loss is 0.11523071676492691\n",
      "epoch: 3 step: 1411, loss is 0.08395927399396896\n",
      "epoch: 3 step: 1412, loss is 0.004573735408484936\n",
      "epoch: 3 step: 1413, loss is 0.0352071188390255\n",
      "epoch: 3 step: 1414, loss is 0.12272384017705917\n",
      "epoch: 3 step: 1415, loss is 0.09253108501434326\n",
      "epoch: 3 step: 1416, loss is 0.0040238481014966965\n",
      "epoch: 3 step: 1417, loss is 0.16818207502365112\n",
      "epoch: 3 step: 1418, loss is 0.19553619623184204\n",
      "epoch: 3 step: 1419, loss is 0.13935744762420654\n",
      "epoch: 3 step: 1420, loss is 0.02073301374912262\n",
      "epoch: 3 step: 1421, loss is 0.12167847156524658\n",
      "epoch: 3 step: 1422, loss is 0.010523076169192791\n",
      "epoch: 3 step: 1423, loss is 0.05542923882603645\n",
      "epoch: 3 step: 1424, loss is 0.2358599454164505\n",
      "epoch: 3 step: 1425, loss is 0.0379711352288723\n",
      "epoch: 3 step: 1426, loss is 0.08902078866958618\n",
      "epoch: 3 step: 1427, loss is 0.024749435484409332\n",
      "epoch: 3 step: 1428, loss is 0.008703330531716347\n",
      "epoch: 3 step: 1429, loss is 0.03215990960597992\n",
      "epoch: 3 step: 1430, loss is 0.007055410649627447\n",
      "epoch: 3 step: 1431, loss is 0.21089449524879456\n",
      "epoch: 3 step: 1432, loss is 0.06450103968381882\n",
      "epoch: 3 step: 1433, loss is 0.015687057748436928\n",
      "epoch: 3 step: 1434, loss is 0.2543557286262512\n",
      "epoch: 3 step: 1435, loss is 0.12044236063957214\n",
      "epoch: 3 step: 1436, loss is 0.09084775298833847\n",
      "epoch: 3 step: 1437, loss is 0.5099791288375854\n",
      "epoch: 3 step: 1438, loss is 0.12167204916477203\n",
      "epoch: 3 step: 1439, loss is 0.02143162302672863\n",
      "epoch: 3 step: 1440, loss is 0.2101358026266098\n",
      "epoch: 3 step: 1441, loss is 0.15530851483345032\n",
      "epoch: 3 step: 1442, loss is 0.02002766728401184\n",
      "epoch: 3 step: 1443, loss is 0.006712491158396006\n",
      "epoch: 3 step: 1444, loss is 0.014916175045073032\n",
      "epoch: 3 step: 1445, loss is 0.02187432534992695\n",
      "epoch: 3 step: 1446, loss is 0.07784922420978546\n",
      "epoch: 3 step: 1447, loss is 0.059877924621105194\n",
      "epoch: 3 step: 1448, loss is 0.0076065328903496265\n",
      "epoch: 3 step: 1449, loss is 0.07035579532384872\n",
      "epoch: 3 step: 1450, loss is 0.1064266711473465\n",
      "epoch: 3 step: 1451, loss is 0.3799056112766266\n",
      "epoch: 3 step: 1452, loss is 0.07385063916444778\n",
      "epoch: 3 step: 1453, loss is 0.6386445760726929\n",
      "epoch: 3 step: 1454, loss is 0.24153681099414825\n",
      "epoch: 3 step: 1455, loss is 0.061954788863658905\n",
      "epoch: 3 step: 1456, loss is 0.03279123082756996\n",
      "epoch: 3 step: 1457, loss is 0.01987588033080101\n",
      "epoch: 3 step: 1458, loss is 0.016182664781808853\n",
      "epoch: 3 step: 1459, loss is 0.13860724866390228\n",
      "epoch: 3 step: 1460, loss is 0.05716133862733841\n",
      "epoch: 3 step: 1461, loss is 0.20834344625473022\n",
      "epoch: 3 step: 1462, loss is 0.03642179071903229\n",
      "epoch: 3 step: 1463, loss is 0.11403775215148926\n",
      "epoch: 3 step: 1464, loss is 0.04911164194345474\n",
      "epoch: 3 step: 1465, loss is 0.4106748700141907\n",
      "epoch: 3 step: 1466, loss is 0.04602843150496483\n",
      "epoch: 3 step: 1467, loss is 0.011568996123969555\n",
      "epoch: 3 step: 1468, loss is 0.11896112561225891\n",
      "epoch: 3 step: 1469, loss is 0.05181819945573807\n",
      "epoch: 3 step: 1470, loss is 0.03313829004764557\n",
      "epoch: 3 step: 1471, loss is 0.25403547286987305\n",
      "epoch: 3 step: 1472, loss is 0.12432809919118881\n",
      "epoch: 3 step: 1473, loss is 0.016589069738984108\n",
      "epoch: 3 step: 1474, loss is 0.05837097391486168\n",
      "epoch: 3 step: 1475, loss is 0.04755381494760513\n",
      "epoch: 3 step: 1476, loss is 0.04077143222093582\n",
      "epoch: 3 step: 1477, loss is 0.08626621961593628\n",
      "epoch: 3 step: 1478, loss is 0.12967883050441742\n",
      "epoch: 3 step: 1479, loss is 0.04648802429437637\n",
      "epoch: 3 step: 1480, loss is 0.14964686334133148\n",
      "epoch: 3 step: 1481, loss is 0.010645244270563126\n",
      "epoch: 3 step: 1482, loss is 0.04241389408707619\n",
      "epoch: 3 step: 1483, loss is 0.02974461205303669\n",
      "epoch: 3 step: 1484, loss is 0.1626933068037033\n",
      "epoch: 3 step: 1485, loss is 0.11398151516914368\n",
      "epoch: 3 step: 1486, loss is 0.057668957859277725\n",
      "epoch: 3 step: 1487, loss is 0.022791417315602303\n",
      "epoch: 3 step: 1488, loss is 0.1509629487991333\n",
      "epoch: 3 step: 1489, loss is 0.012260940857231617\n",
      "epoch: 3 step: 1490, loss is 0.013194598257541656\n",
      "epoch: 3 step: 1491, loss is 0.052361711859703064\n",
      "epoch: 3 step: 1492, loss is 0.046103380620479584\n",
      "epoch: 3 step: 1493, loss is 0.2108236402273178\n",
      "epoch: 3 step: 1494, loss is 0.01857016608119011\n",
      "epoch: 3 step: 1495, loss is 0.017941435799002647\n",
      "epoch: 3 step: 1496, loss is 0.019075559452176094\n",
      "epoch: 3 step: 1497, loss is 0.039739008992910385\n",
      "epoch: 3 step: 1498, loss is 0.018120449036359787\n",
      "epoch: 3 step: 1499, loss is 0.12634268403053284\n",
      "epoch: 3 step: 1500, loss is 0.015029532834887505\n",
      "epoch: 3 step: 1501, loss is 0.03507388010621071\n",
      "epoch: 3 step: 1502, loss is 0.007853829301893711\n",
      "epoch: 3 step: 1503, loss is 0.25429269671440125\n",
      "epoch: 3 step: 1504, loss is 0.009909813292324543\n",
      "epoch: 3 step: 1505, loss is 0.1472197026014328\n",
      "epoch: 3 step: 1506, loss is 0.06671269237995148\n",
      "epoch: 3 step: 1507, loss is 0.05666591599583626\n",
      "epoch: 3 step: 1508, loss is 0.048552557826042175\n",
      "epoch: 3 step: 1509, loss is 0.29740679264068604\n",
      "epoch: 3 step: 1510, loss is 0.0297293271869421\n",
      "epoch: 3 step: 1511, loss is 0.004719792865216732\n",
      "epoch: 3 step: 1512, loss is 0.02653920277953148\n",
      "epoch: 3 step: 1513, loss is 0.1479988843202591\n",
      "epoch: 3 step: 1514, loss is 0.014117575250566006\n",
      "epoch: 3 step: 1515, loss is 0.0017331104027107358\n",
      "epoch: 3 step: 1516, loss is 0.16226527094841003\n",
      "epoch: 3 step: 1517, loss is 0.33024871349334717\n",
      "epoch: 3 step: 1518, loss is 0.007466174196451902\n",
      "epoch: 3 step: 1519, loss is 0.013506072573363781\n",
      "epoch: 3 step: 1520, loss is 0.14187394082546234\n",
      "epoch: 3 step: 1521, loss is 0.005810350179672241\n",
      "epoch: 3 step: 1522, loss is 0.04934801533818245\n",
      "epoch: 3 step: 1523, loss is 0.06204080209136009\n",
      "epoch: 3 step: 1524, loss is 0.07847501337528229\n",
      "epoch: 3 step: 1525, loss is 0.0803603008389473\n",
      "epoch: 3 step: 1526, loss is 0.026052286848425865\n",
      "epoch: 3 step: 1527, loss is 0.31563401222229004\n",
      "epoch: 3 step: 1528, loss is 0.0830344408750534\n",
      "epoch: 3 step: 1529, loss is 0.027848519384860992\n",
      "epoch: 3 step: 1530, loss is 0.09716948121786118\n",
      "epoch: 3 step: 1531, loss is 0.10753396153450012\n",
      "epoch: 3 step: 1532, loss is 0.02416503056883812\n",
      "epoch: 3 step: 1533, loss is 0.001755925826728344\n",
      "epoch: 3 step: 1534, loss is 0.019665496423840523\n",
      "epoch: 3 step: 1535, loss is 0.008759326301515102\n",
      "epoch: 3 step: 1536, loss is 0.03831124305725098\n",
      "epoch: 3 step: 1537, loss is 0.08890996128320694\n",
      "epoch: 3 step: 1538, loss is 0.06356558203697205\n",
      "epoch: 3 step: 1539, loss is 0.0018702198285609484\n",
      "epoch: 3 step: 1540, loss is 0.018867749720811844\n",
      "epoch: 3 step: 1541, loss is 0.055295251309871674\n",
      "epoch: 3 step: 1542, loss is 0.031225288286805153\n",
      "epoch: 3 step: 1543, loss is 0.027529746294021606\n",
      "epoch: 3 step: 1544, loss is 0.02823670767247677\n",
      "epoch: 3 step: 1545, loss is 0.016915185377001762\n",
      "epoch: 3 step: 1546, loss is 0.021337516605854034\n",
      "epoch: 3 step: 1547, loss is 0.07368941605091095\n",
      "epoch: 3 step: 1548, loss is 0.1973174661397934\n",
      "epoch: 3 step: 1549, loss is 0.040121205151081085\n",
      "epoch: 3 step: 1550, loss is 0.04784827679395676\n",
      "epoch: 3 step: 1551, loss is 0.16217315196990967\n",
      "epoch: 3 step: 1552, loss is 0.09839021414518356\n",
      "epoch: 3 step: 1553, loss is 0.03759467974305153\n",
      "epoch: 3 step: 1554, loss is 0.12007442116737366\n",
      "epoch: 3 step: 1555, loss is 0.001291749649681151\n",
      "epoch: 3 step: 1556, loss is 0.30476993322372437\n",
      "epoch: 3 step: 1557, loss is 0.08354602009057999\n",
      "epoch: 3 step: 1558, loss is 0.08016316592693329\n",
      "epoch: 3 step: 1559, loss is 0.1491524577140808\n",
      "epoch: 3 step: 1560, loss is 0.168722003698349\n",
      "epoch: 3 step: 1561, loss is 0.11832902580499649\n",
      "epoch: 3 step: 1562, loss is 0.2030334174633026\n",
      "epoch: 3 step: 1563, loss is 0.10108786076307297\n",
      "epoch: 3 step: 1564, loss is 0.20833630859851837\n",
      "epoch: 3 step: 1565, loss is 0.08926349133253098\n",
      "epoch: 3 step: 1566, loss is 0.08882448077201843\n",
      "epoch: 3 step: 1567, loss is 0.07924238592386246\n",
      "epoch: 3 step: 1568, loss is 0.03778429329395294\n",
      "epoch: 3 step: 1569, loss is 0.01981240324676037\n",
      "epoch: 3 step: 1570, loss is 0.0809202566742897\n",
      "epoch: 3 step: 1571, loss is 0.14667843282222748\n",
      "epoch: 3 step: 1572, loss is 0.0915510281920433\n",
      "epoch: 3 step: 1573, loss is 0.1790468841791153\n",
      "epoch: 3 step: 1574, loss is 0.008220396935939789\n",
      "epoch: 3 step: 1575, loss is 0.2636153995990753\n",
      "epoch: 3 step: 1576, loss is 0.08765824139118195\n",
      "epoch: 3 step: 1577, loss is 0.015018727630376816\n",
      "epoch: 3 step: 1578, loss is 0.04325413703918457\n",
      "epoch: 3 step: 1579, loss is 0.0797451063990593\n",
      "epoch: 3 step: 1580, loss is 0.03527619317173958\n",
      "epoch: 3 step: 1581, loss is 0.19022655487060547\n",
      "epoch: 3 step: 1582, loss is 0.14606651663780212\n",
      "epoch: 3 step: 1583, loss is 0.013859285973012447\n",
      "epoch: 3 step: 1584, loss is 0.24702300131320953\n",
      "epoch: 3 step: 1585, loss is 0.007826683111488819\n",
      "epoch: 3 step: 1586, loss is 0.03787444531917572\n",
      "epoch: 3 step: 1587, loss is 0.03149912506341934\n",
      "epoch: 3 step: 1588, loss is 0.018705759197473526\n",
      "epoch: 3 step: 1589, loss is 0.468000590801239\n",
      "epoch: 3 step: 1590, loss is 0.10329260677099228\n",
      "epoch: 3 step: 1591, loss is 0.07299842685461044\n",
      "epoch: 3 step: 1592, loss is 0.021331995725631714\n",
      "epoch: 3 step: 1593, loss is 0.12805262207984924\n",
      "epoch: 3 step: 1594, loss is 0.07968584448099136\n",
      "epoch: 3 step: 1595, loss is 0.344603568315506\n",
      "epoch: 3 step: 1596, loss is 0.09072588384151459\n",
      "epoch: 3 step: 1597, loss is 0.1747368574142456\n",
      "epoch: 3 step: 1598, loss is 0.014267431572079659\n",
      "epoch: 3 step: 1599, loss is 0.10719442367553711\n",
      "epoch: 3 step: 1600, loss is 0.0376989021897316\n",
      "epoch: 3 step: 1601, loss is 0.01339794509112835\n",
      "epoch: 3 step: 1602, loss is 0.03018343821167946\n",
      "epoch: 3 step: 1603, loss is 0.05186092481017113\n",
      "epoch: 3 step: 1604, loss is 0.019655389711260796\n",
      "epoch: 3 step: 1605, loss is 0.29693683981895447\n",
      "epoch: 3 step: 1606, loss is 0.01711621694266796\n",
      "epoch: 3 step: 1607, loss is 0.19651705026626587\n",
      "epoch: 3 step: 1608, loss is 0.2582714259624481\n",
      "epoch: 3 step: 1609, loss is 0.12834730744361877\n",
      "epoch: 3 step: 1610, loss is 0.001995168859139085\n",
      "epoch: 3 step: 1611, loss is 0.004629949573427439\n",
      "epoch: 3 step: 1612, loss is 0.05408116430044174\n",
      "epoch: 3 step: 1613, loss is 0.07826251536607742\n",
      "epoch: 3 step: 1614, loss is 0.15732239186763763\n",
      "epoch: 3 step: 1615, loss is 0.10217595100402832\n",
      "epoch: 3 step: 1616, loss is 0.015411005355417728\n",
      "epoch: 3 step: 1617, loss is 0.04484334588050842\n",
      "epoch: 3 step: 1618, loss is 0.30582794547080994\n",
      "epoch: 3 step: 1619, loss is 0.0072745149955153465\n",
      "epoch: 3 step: 1620, loss is 0.2915515601634979\n",
      "epoch: 3 step: 1621, loss is 0.03843661770224571\n",
      "epoch: 3 step: 1622, loss is 0.0661049485206604\n",
      "epoch: 3 step: 1623, loss is 0.2032795399427414\n",
      "epoch: 3 step: 1624, loss is 0.06605833023786545\n",
      "epoch: 3 step: 1625, loss is 0.009377669543027878\n",
      "epoch: 3 step: 1626, loss is 0.11897370219230652\n",
      "epoch: 3 step: 1627, loss is 0.11761047691106796\n",
      "epoch: 3 step: 1628, loss is 0.1434740573167801\n",
      "epoch: 3 step: 1629, loss is 0.014543374069035053\n",
      "epoch: 3 step: 1630, loss is 0.0837092325091362\n",
      "epoch: 3 step: 1631, loss is 0.023881804198026657\n",
      "epoch: 3 step: 1632, loss is 0.11282392591238022\n",
      "epoch: 3 step: 1633, loss is 0.18614180386066437\n",
      "epoch: 3 step: 1634, loss is 0.4068564176559448\n",
      "epoch: 3 step: 1635, loss is 0.03988805413246155\n",
      "epoch: 3 step: 1636, loss is 0.17723526060581207\n",
      "epoch: 3 step: 1637, loss is 0.03447479382157326\n",
      "epoch: 3 step: 1638, loss is 0.1476670503616333\n",
      "epoch: 3 step: 1639, loss is 0.03870348632335663\n",
      "epoch: 3 step: 1640, loss is 0.029631881043314934\n",
      "epoch: 3 step: 1641, loss is 0.06141679733991623\n",
      "epoch: 3 step: 1642, loss is 0.07801103591918945\n",
      "epoch: 3 step: 1643, loss is 0.030797123908996582\n",
      "epoch: 3 step: 1644, loss is 0.013295079581439495\n",
      "epoch: 3 step: 1645, loss is 0.04792908579111099\n",
      "epoch: 3 step: 1646, loss is 0.005899465177208185\n",
      "epoch: 3 step: 1647, loss is 0.0052293436601758\n",
      "epoch: 3 step: 1648, loss is 0.07752025127410889\n",
      "epoch: 3 step: 1649, loss is 0.028849342837929726\n",
      "epoch: 3 step: 1650, loss is 0.038683969527482986\n",
      "epoch: 3 step: 1651, loss is 0.11451982706785202\n",
      "epoch: 3 step: 1652, loss is 0.10541542619466782\n",
      "epoch: 3 step: 1653, loss is 0.055281274020671844\n",
      "epoch: 3 step: 1654, loss is 0.10752503573894501\n",
      "epoch: 3 step: 1655, loss is 0.019813288003206253\n",
      "epoch: 3 step: 1656, loss is 0.010546060279011726\n",
      "epoch: 3 step: 1657, loss is 0.01190856657922268\n",
      "epoch: 3 step: 1658, loss is 0.058683767914772034\n",
      "epoch: 3 step: 1659, loss is 0.1122170239686966\n",
      "epoch: 3 step: 1660, loss is 0.013518784195184708\n",
      "epoch: 3 step: 1661, loss is 0.026265250518918037\n",
      "epoch: 3 step: 1662, loss is 0.04454312101006508\n",
      "epoch: 3 step: 1663, loss is 0.03517313301563263\n",
      "epoch: 3 step: 1664, loss is 0.1016465425491333\n",
      "epoch: 3 step: 1665, loss is 0.0018976004794239998\n",
      "epoch: 3 step: 1666, loss is 0.1422511339187622\n",
      "epoch: 3 step: 1667, loss is 0.09595370292663574\n",
      "epoch: 3 step: 1668, loss is 0.03486180305480957\n",
      "epoch: 3 step: 1669, loss is 0.030940523371100426\n",
      "epoch: 3 step: 1670, loss is 0.060755811631679535\n",
      "epoch: 3 step: 1671, loss is 0.12459596991539001\n",
      "epoch: 3 step: 1672, loss is 0.013002367690205574\n",
      "epoch: 3 step: 1673, loss is 0.01097211241722107\n",
      "epoch: 3 step: 1674, loss is 0.05146527662873268\n",
      "epoch: 3 step: 1675, loss is 0.005427108611911535\n",
      "epoch: 3 step: 1676, loss is 0.010410794056952\n",
      "epoch: 3 step: 1677, loss is 0.012332695536315441\n",
      "epoch: 3 step: 1678, loss is 0.06267126649618149\n",
      "epoch: 3 step: 1679, loss is 0.035769689828157425\n",
      "epoch: 3 step: 1680, loss is 0.005016449373215437\n",
      "epoch: 3 step: 1681, loss is 0.017524613067507744\n",
      "epoch: 3 step: 1682, loss is 0.10442788898944855\n",
      "epoch: 3 step: 1683, loss is 0.13108354806900024\n",
      "epoch: 3 step: 1684, loss is 0.06559273600578308\n",
      "epoch: 3 step: 1685, loss is 0.16101469099521637\n",
      "epoch: 3 step: 1686, loss is 0.013435732573270798\n",
      "epoch: 3 step: 1687, loss is 0.021906550973653793\n",
      "epoch: 3 step: 1688, loss is 0.0979524552822113\n",
      "epoch: 3 step: 1689, loss is 0.03868385776877403\n",
      "epoch: 3 step: 1690, loss is 0.06629198789596558\n",
      "epoch: 3 step: 1691, loss is 0.006928880233317614\n",
      "epoch: 3 step: 1692, loss is 0.03968104347586632\n",
      "epoch: 3 step: 1693, loss is 0.010983576998114586\n",
      "epoch: 3 step: 1694, loss is 0.15295514464378357\n",
      "epoch: 3 step: 1695, loss is 0.14477181434631348\n",
      "epoch: 3 step: 1696, loss is 0.004224972799420357\n",
      "epoch: 3 step: 1697, loss is 0.04233142361044884\n",
      "epoch: 3 step: 1698, loss is 0.04629405587911606\n",
      "epoch: 3 step: 1699, loss is 0.008670851588249207\n",
      "epoch: 3 step: 1700, loss is 0.07377493381500244\n",
      "epoch: 3 step: 1701, loss is 0.06817217171192169\n",
      "epoch: 3 step: 1702, loss is 0.009102909825742245\n",
      "epoch: 3 step: 1703, loss is 0.03436125069856644\n",
      "epoch: 3 step: 1704, loss is 0.014739305712282658\n",
      "epoch: 3 step: 1705, loss is 0.007648396771401167\n",
      "epoch: 3 step: 1706, loss is 0.16939586400985718\n",
      "epoch: 3 step: 1707, loss is 0.005339541472494602\n",
      "epoch: 3 step: 1708, loss is 0.009451035410165787\n",
      "epoch: 3 step: 1709, loss is 0.09868666529655457\n",
      "epoch: 3 step: 1710, loss is 0.03678875043988228\n",
      "epoch: 3 step: 1711, loss is 0.1012255996465683\n",
      "epoch: 3 step: 1712, loss is 0.026952696964144707\n",
      "epoch: 3 step: 1713, loss is 0.062441617250442505\n",
      "epoch: 3 step: 1714, loss is 0.04977283626794815\n",
      "epoch: 3 step: 1715, loss is 0.26764464378356934\n",
      "epoch: 3 step: 1716, loss is 0.050581708550453186\n",
      "epoch: 3 step: 1717, loss is 0.36212921142578125\n",
      "epoch: 3 step: 1718, loss is 0.2681199014186859\n",
      "epoch: 3 step: 1719, loss is 0.05539170280098915\n",
      "epoch: 3 step: 1720, loss is 0.2458917796611786\n",
      "epoch: 3 step: 1721, loss is 0.03209845721721649\n",
      "epoch: 3 step: 1722, loss is 0.04794555902481079\n",
      "epoch: 3 step: 1723, loss is 0.05958116799592972\n",
      "epoch: 3 step: 1724, loss is 0.020349234342575073\n",
      "epoch: 3 step: 1725, loss is 0.013641903176903725\n",
      "epoch: 3 step: 1726, loss is 0.039417896419763565\n",
      "epoch: 3 step: 1727, loss is 0.27185699343681335\n",
      "epoch: 3 step: 1728, loss is 0.03315572440624237\n",
      "epoch: 3 step: 1729, loss is 0.012661334127187729\n",
      "epoch: 3 step: 1730, loss is 0.03148871660232544\n",
      "epoch: 3 step: 1731, loss is 0.06865055859088898\n",
      "epoch: 3 step: 1732, loss is 0.009139548987150192\n",
      "epoch: 3 step: 1733, loss is 0.20832861959934235\n",
      "epoch: 3 step: 1734, loss is 0.03495728224515915\n",
      "epoch: 3 step: 1735, loss is 0.049645617604255676\n",
      "epoch: 3 step: 1736, loss is 0.07389810681343079\n",
      "epoch: 3 step: 1737, loss is 0.07571925967931747\n",
      "epoch: 3 step: 1738, loss is 0.03687587380409241\n",
      "epoch: 3 step: 1739, loss is 0.003642331575974822\n",
      "epoch: 3 step: 1740, loss is 0.0552765317261219\n",
      "epoch: 3 step: 1741, loss is 0.007387466728687286\n",
      "epoch: 3 step: 1742, loss is 0.12301670759916306\n",
      "epoch: 3 step: 1743, loss is 0.08432817459106445\n",
      "epoch: 3 step: 1744, loss is 0.021753909066319466\n",
      "epoch: 3 step: 1745, loss is 0.036303114145994186\n",
      "epoch: 3 step: 1746, loss is 0.06417309492826462\n",
      "epoch: 3 step: 1747, loss is 0.013736292719841003\n",
      "epoch: 3 step: 1748, loss is 0.017800334841012955\n",
      "epoch: 3 step: 1749, loss is 0.24172373116016388\n",
      "epoch: 3 step: 1750, loss is 0.13494040071964264\n",
      "epoch: 3 step: 1751, loss is 0.2485954314470291\n",
      "epoch: 3 step: 1752, loss is 0.01390412263572216\n",
      "epoch: 3 step: 1753, loss is 0.016832103952765465\n",
      "epoch: 3 step: 1754, loss is 0.007838474586606026\n",
      "epoch: 3 step: 1755, loss is 0.0038854419253766537\n",
      "epoch: 3 step: 1756, loss is 0.023308563977479935\n",
      "epoch: 3 step: 1757, loss is 0.02179691195487976\n",
      "epoch: 3 step: 1758, loss is 0.028543278574943542\n",
      "epoch: 3 step: 1759, loss is 0.05599640682339668\n",
      "epoch: 3 step: 1760, loss is 0.2754203677177429\n",
      "epoch: 3 step: 1761, loss is 0.06121041998267174\n",
      "epoch: 3 step: 1762, loss is 0.03210246562957764\n",
      "epoch: 3 step: 1763, loss is 0.16465507447719574\n",
      "epoch: 3 step: 1764, loss is 0.2367170751094818\n",
      "epoch: 3 step: 1765, loss is 0.03634367883205414\n",
      "epoch: 3 step: 1766, loss is 0.05251467972993851\n",
      "epoch: 3 step: 1767, loss is 0.007730783894658089\n",
      "epoch: 3 step: 1768, loss is 0.08614535629749298\n",
      "epoch: 3 step: 1769, loss is 0.008837645873427391\n",
      "epoch: 3 step: 1770, loss is 0.025591092184185982\n",
      "epoch: 3 step: 1771, loss is 0.059381965547800064\n",
      "epoch: 3 step: 1772, loss is 0.04002431780099869\n",
      "epoch: 3 step: 1773, loss is 0.16706176102161407\n",
      "epoch: 3 step: 1774, loss is 0.025847088545560837\n",
      "epoch: 3 step: 1775, loss is 0.005457197315990925\n",
      "epoch: 3 step: 1776, loss is 0.0029491449240595102\n",
      "epoch: 3 step: 1777, loss is 0.16337086260318756\n",
      "epoch: 3 step: 1778, loss is 0.08773397654294968\n",
      "epoch: 3 step: 1779, loss is 0.18812690675258636\n",
      "epoch: 3 step: 1780, loss is 0.02984921634197235\n",
      "epoch: 3 step: 1781, loss is 0.18935196101665497\n",
      "epoch: 3 step: 1782, loss is 0.38243263959884644\n",
      "epoch: 3 step: 1783, loss is 0.021871473640203476\n",
      "epoch: 3 step: 1784, loss is 0.010134278796613216\n",
      "epoch: 3 step: 1785, loss is 0.28042298555374146\n",
      "epoch: 3 step: 1786, loss is 0.022117191925644875\n",
      "epoch: 3 step: 1787, loss is 0.18079857528209686\n",
      "epoch: 3 step: 1788, loss is 0.011311901733279228\n",
      "epoch: 3 step: 1789, loss is 0.018248887732625008\n",
      "epoch: 3 step: 1790, loss is 0.3040775656700134\n",
      "epoch: 3 step: 1791, loss is 0.012972923927009106\n",
      "epoch: 3 step: 1792, loss is 0.05317094177007675\n",
      "epoch: 3 step: 1793, loss is 0.07979250699281693\n",
      "epoch: 3 step: 1794, loss is 0.16385376453399658\n",
      "epoch: 3 step: 1795, loss is 0.061722151935100555\n",
      "epoch: 3 step: 1796, loss is 0.07406944781541824\n",
      "epoch: 3 step: 1797, loss is 0.0777118057012558\n",
      "epoch: 3 step: 1798, loss is 0.15976491570472717\n",
      "epoch: 3 step: 1799, loss is 0.17419010400772095\n",
      "epoch: 3 step: 1800, loss is 0.19264695048332214\n",
      "epoch: 3 step: 1801, loss is 0.28500935435295105\n",
      "epoch: 3 step: 1802, loss is 0.032771673053503036\n",
      "epoch: 3 step: 1803, loss is 0.0687377080321312\n",
      "epoch: 3 step: 1804, loss is 0.050383634865283966\n",
      "epoch: 3 step: 1805, loss is 0.21382170915603638\n",
      "epoch: 3 step: 1806, loss is 0.12156038731336594\n",
      "epoch: 3 step: 1807, loss is 0.004229556769132614\n",
      "epoch: 3 step: 1808, loss is 0.13042974472045898\n",
      "epoch: 3 step: 1809, loss is 0.1844419687986374\n",
      "epoch: 3 step: 1810, loss is 0.06783130019903183\n",
      "epoch: 3 step: 1811, loss is 0.09940586239099503\n",
      "epoch: 3 step: 1812, loss is 0.04115433618426323\n",
      "epoch: 3 step: 1813, loss is 0.021387116983532906\n",
      "epoch: 3 step: 1814, loss is 0.07971273362636566\n",
      "epoch: 3 step: 1815, loss is 0.06773051619529724\n",
      "epoch: 3 step: 1816, loss is 0.23824800550937653\n",
      "epoch: 3 step: 1817, loss is 0.08644865453243256\n",
      "epoch: 3 step: 1818, loss is 0.036316774785518646\n",
      "epoch: 3 step: 1819, loss is 0.024806056171655655\n",
      "epoch: 3 step: 1820, loss is 0.02442077547311783\n",
      "epoch: 3 step: 1821, loss is 0.11575984954833984\n",
      "epoch: 3 step: 1822, loss is 0.1895800679922104\n",
      "epoch: 3 step: 1823, loss is 0.049117766320705414\n",
      "epoch: 3 step: 1824, loss is 0.1686762571334839\n",
      "epoch: 3 step: 1825, loss is 0.062358465045690536\n",
      "epoch: 3 step: 1826, loss is 0.09765767306089401\n",
      "epoch: 3 step: 1827, loss is 0.07169551402330399\n",
      "epoch: 3 step: 1828, loss is 0.01002910453826189\n",
      "epoch: 3 step: 1829, loss is 0.013913714326918125\n",
      "epoch: 3 step: 1830, loss is 0.0261109359562397\n",
      "epoch: 3 step: 1831, loss is 0.0343162938952446\n",
      "epoch: 3 step: 1832, loss is 0.1326947659254074\n",
      "epoch: 3 step: 1833, loss is 0.003291937056928873\n",
      "epoch: 3 step: 1834, loss is 0.06923521310091019\n",
      "epoch: 3 step: 1835, loss is 0.03713478147983551\n",
      "epoch: 3 step: 1836, loss is 0.024641981348395348\n",
      "epoch: 3 step: 1837, loss is 0.06705302000045776\n",
      "epoch: 3 step: 1838, loss is 0.011299218982458115\n",
      "epoch: 3 step: 1839, loss is 0.011745093390345573\n",
      "epoch: 3 step: 1840, loss is 0.06658271700143814\n",
      "epoch: 3 step: 1841, loss is 0.0616544745862484\n",
      "epoch: 3 step: 1842, loss is 0.033799853175878525\n",
      "epoch: 3 step: 1843, loss is 0.2745664417743683\n",
      "epoch: 3 step: 1844, loss is 0.019322087988257408\n",
      "epoch: 3 step: 1845, loss is 0.011783916503190994\n",
      "epoch: 3 step: 1846, loss is 0.03144346550107002\n",
      "epoch: 3 step: 1847, loss is 0.08188088983297348\n",
      "epoch: 3 step: 1848, loss is 0.007462211884558201\n",
      "epoch: 3 step: 1849, loss is 0.08735544234514236\n",
      "epoch: 3 step: 1850, loss is 0.009212728589773178\n",
      "epoch: 3 step: 1851, loss is 0.03722284361720085\n",
      "epoch: 3 step: 1852, loss is 0.006756620481610298\n",
      "epoch: 3 step: 1853, loss is 0.09853819757699966\n",
      "epoch: 3 step: 1854, loss is 0.04552515596151352\n",
      "epoch: 3 step: 1855, loss is 0.0067177447490394115\n",
      "epoch: 3 step: 1856, loss is 0.12725088000297546\n",
      "epoch: 3 step: 1857, loss is 0.08572441339492798\n",
      "epoch: 3 step: 1858, loss is 0.01179618202149868\n",
      "epoch: 3 step: 1859, loss is 0.007982013747096062\n",
      "epoch: 3 step: 1860, loss is 0.04430966451764107\n",
      "epoch: 3 step: 1861, loss is 0.057145167142152786\n",
      "epoch: 3 step: 1862, loss is 0.00865044817328453\n",
      "epoch: 3 step: 1863, loss is 0.3756822645664215\n",
      "epoch: 3 step: 1864, loss is 0.06011543795466423\n",
      "epoch: 3 step: 1865, loss is 0.010362832807004452\n",
      "epoch: 3 step: 1866, loss is 0.11524336040019989\n",
      "epoch: 3 step: 1867, loss is 0.01779257506132126\n",
      "epoch: 3 step: 1868, loss is 0.4639877378940582\n",
      "epoch: 3 step: 1869, loss is 0.028106173500418663\n",
      "epoch: 3 step: 1870, loss is 0.004336982499808073\n",
      "epoch: 3 step: 1871, loss is 0.12479139119386673\n",
      "epoch: 3 step: 1872, loss is 0.28783974051475525\n",
      "epoch: 3 step: 1873, loss is 0.036625467240810394\n",
      "epoch: 3 step: 1874, loss is 0.17572541534900665\n",
      "epoch: 3 step: 1875, loss is 0.00740570155903697\n",
      "epoch: 4 step: 1, loss is 0.018873289227485657\n",
      "epoch: 4 step: 2, loss is 0.07172948122024536\n",
      "epoch: 4 step: 3, loss is 0.033634960651397705\n",
      "epoch: 4 step: 4, loss is 0.04003958776593208\n",
      "epoch: 4 step: 5, loss is 0.028960837051272392\n",
      "epoch: 4 step: 6, loss is 0.09084733575582504\n",
      "epoch: 4 step: 7, loss is 0.05454340949654579\n",
      "epoch: 4 step: 8, loss is 0.027878038585186005\n",
      "epoch: 4 step: 9, loss is 0.015619281679391861\n",
      "epoch: 4 step: 10, loss is 0.04050355777144432\n",
      "epoch: 4 step: 11, loss is 0.06434858590364456\n",
      "epoch: 4 step: 12, loss is 0.015890436246991158\n",
      "epoch: 4 step: 13, loss is 0.04038670286536217\n",
      "epoch: 4 step: 14, loss is 0.014200272969901562\n",
      "epoch: 4 step: 15, loss is 0.018952740356326103\n",
      "epoch: 4 step: 16, loss is 0.06978403776884079\n",
      "epoch: 4 step: 17, loss is 0.0747794508934021\n",
      "epoch: 4 step: 18, loss is 0.019035235047340393\n",
      "epoch: 4 step: 19, loss is 0.020681776106357574\n",
      "epoch: 4 step: 20, loss is 0.00672392500564456\n",
      "epoch: 4 step: 21, loss is 0.04856708273291588\n",
      "epoch: 4 step: 22, loss is 0.04061013087630272\n",
      "epoch: 4 step: 23, loss is 0.027504654601216316\n",
      "epoch: 4 step: 24, loss is 0.0779435932636261\n",
      "epoch: 4 step: 25, loss is 0.10389532148838043\n",
      "epoch: 4 step: 26, loss is 0.0038209385238587856\n",
      "epoch: 4 step: 27, loss is 0.07512195408344269\n",
      "epoch: 4 step: 28, loss is 0.15768779814243317\n",
      "epoch: 4 step: 29, loss is 0.13286474347114563\n",
      "epoch: 4 step: 30, loss is 0.09309904277324677\n",
      "epoch: 4 step: 31, loss is 0.08904171735048294\n",
      "epoch: 4 step: 32, loss is 0.1561872512102127\n",
      "epoch: 4 step: 33, loss is 0.019867215305566788\n",
      "epoch: 4 step: 34, loss is 0.186860591173172\n",
      "epoch: 4 step: 35, loss is 0.21658428013324738\n",
      "epoch: 4 step: 36, loss is 0.11326716095209122\n",
      "epoch: 4 step: 37, loss is 0.047047294676303864\n",
      "epoch: 4 step: 38, loss is 0.15139523148536682\n",
      "epoch: 4 step: 39, loss is 0.17379990220069885\n",
      "epoch: 4 step: 40, loss is 0.013351697474718094\n",
      "epoch: 4 step: 41, loss is 0.09744324535131454\n",
      "epoch: 4 step: 42, loss is 0.2604353129863739\n",
      "epoch: 4 step: 43, loss is 0.19312798976898193\n",
      "epoch: 4 step: 44, loss is 0.016345735639333725\n",
      "epoch: 4 step: 45, loss is 0.007477762643247843\n",
      "epoch: 4 step: 46, loss is 0.019673218950629234\n",
      "epoch: 4 step: 47, loss is 0.1324426531791687\n",
      "epoch: 4 step: 48, loss is 0.031219009310007095\n",
      "epoch: 4 step: 49, loss is 0.03197675198316574\n",
      "epoch: 4 step: 50, loss is 0.039328087121248245\n",
      "epoch: 4 step: 51, loss is 0.05287497118115425\n",
      "epoch: 4 step: 52, loss is 0.24043768644332886\n",
      "epoch: 4 step: 53, loss is 0.026879828423261642\n",
      "epoch: 4 step: 54, loss is 0.007418559864163399\n",
      "epoch: 4 step: 55, loss is 0.0075952946208417416\n",
      "epoch: 4 step: 56, loss is 0.12382641434669495\n",
      "epoch: 4 step: 57, loss is 0.0035387820098549128\n",
      "epoch: 4 step: 58, loss is 0.007849856279790401\n",
      "epoch: 4 step: 59, loss is 0.018263185396790504\n",
      "epoch: 4 step: 60, loss is 0.0025944109074771404\n",
      "epoch: 4 step: 61, loss is 0.020583663135766983\n",
      "epoch: 4 step: 62, loss is 0.0439688079059124\n",
      "epoch: 4 step: 63, loss is 0.010233663022518158\n",
      "epoch: 4 step: 64, loss is 0.014106838963925838\n",
      "epoch: 4 step: 65, loss is 0.009087868966162205\n",
      "epoch: 4 step: 66, loss is 0.03805047273635864\n",
      "epoch: 4 step: 67, loss is 0.012726454995572567\n",
      "epoch: 4 step: 68, loss is 0.05115446820855141\n",
      "epoch: 4 step: 69, loss is 0.11343347281217575\n",
      "epoch: 4 step: 70, loss is 0.10946725308895111\n",
      "epoch: 4 step: 71, loss is 0.010515711270272732\n",
      "epoch: 4 step: 72, loss is 0.003912582062184811\n",
      "epoch: 4 step: 73, loss is 0.029924267902970314\n",
      "epoch: 4 step: 74, loss is 0.06273084133863449\n",
      "epoch: 4 step: 75, loss is 0.0026685153134167194\n",
      "epoch: 4 step: 76, loss is 0.043897856026887894\n",
      "epoch: 4 step: 77, loss is 0.03745543211698532\n",
      "epoch: 4 step: 78, loss is 0.008743032813072205\n",
      "epoch: 4 step: 79, loss is 0.18994739651679993\n",
      "epoch: 4 step: 80, loss is 0.10192398726940155\n",
      "epoch: 4 step: 81, loss is 0.030998462811112404\n",
      "epoch: 4 step: 82, loss is 0.10638889670372009\n",
      "epoch: 4 step: 83, loss is 0.04624142125248909\n",
      "epoch: 4 step: 84, loss is 0.019901474937796593\n",
      "epoch: 4 step: 85, loss is 0.051654718816280365\n",
      "epoch: 4 step: 86, loss is 0.24709169566631317\n",
      "epoch: 4 step: 87, loss is 0.020522264763712883\n",
      "epoch: 4 step: 88, loss is 0.009650997817516327\n",
      "epoch: 4 step: 89, loss is 0.03353956714272499\n",
      "epoch: 4 step: 90, loss is 0.007821374572813511\n",
      "epoch: 4 step: 91, loss is 0.0012146051740273833\n",
      "epoch: 4 step: 92, loss is 0.12199583649635315\n",
      "epoch: 4 step: 93, loss is 0.10528991371393204\n",
      "epoch: 4 step: 94, loss is 0.021670419722795486\n",
      "epoch: 4 step: 95, loss is 0.14955273270606995\n",
      "epoch: 4 step: 96, loss is 0.2299180030822754\n",
      "epoch: 4 step: 97, loss is 0.024792464450001717\n",
      "epoch: 4 step: 98, loss is 0.018710818141698837\n",
      "epoch: 4 step: 99, loss is 0.0029954651836305857\n",
      "epoch: 4 step: 100, loss is 0.07868286967277527\n",
      "epoch: 4 step: 101, loss is 0.2724105715751648\n",
      "epoch: 4 step: 102, loss is 0.12335294485092163\n",
      "epoch: 4 step: 103, loss is 0.01537642814218998\n",
      "epoch: 4 step: 104, loss is 0.029150787740945816\n",
      "epoch: 4 step: 105, loss is 0.061475202441215515\n",
      "epoch: 4 step: 106, loss is 0.043842826038599014\n",
      "epoch: 4 step: 107, loss is 0.096806101500988\n",
      "epoch: 4 step: 108, loss is 0.02462531067430973\n",
      "epoch: 4 step: 109, loss is 0.020644525066018105\n",
      "epoch: 4 step: 110, loss is 0.07041139900684357\n",
      "epoch: 4 step: 111, loss is 0.27076923847198486\n",
      "epoch: 4 step: 112, loss is 0.004410621710121632\n",
      "epoch: 4 step: 113, loss is 0.2016812413930893\n",
      "epoch: 4 step: 114, loss is 0.03211285546422005\n",
      "epoch: 4 step: 115, loss is 0.09083971381187439\n",
      "epoch: 4 step: 116, loss is 0.036289192736148834\n",
      "epoch: 4 step: 117, loss is 0.058942150324583054\n",
      "epoch: 4 step: 118, loss is 0.05084552243351936\n",
      "epoch: 4 step: 119, loss is 0.0928131639957428\n",
      "epoch: 4 step: 120, loss is 0.12158497422933578\n",
      "epoch: 4 step: 121, loss is 0.010361882857978344\n",
      "epoch: 4 step: 122, loss is 0.0013861532788723707\n",
      "epoch: 4 step: 123, loss is 0.008127102628350258\n",
      "epoch: 4 step: 124, loss is 0.019054632633924484\n",
      "epoch: 4 step: 125, loss is 0.024058055132627487\n",
      "epoch: 4 step: 126, loss is 0.027408920228481293\n",
      "epoch: 4 step: 127, loss is 0.03783674165606499\n",
      "epoch: 4 step: 128, loss is 0.18732942640781403\n",
      "epoch: 4 step: 129, loss is 0.123711496591568\n",
      "epoch: 4 step: 130, loss is 0.32451578974723816\n",
      "epoch: 4 step: 131, loss is 0.0538911372423172\n",
      "epoch: 4 step: 132, loss is 0.18368472158908844\n",
      "epoch: 4 step: 133, loss is 0.19341780245304108\n",
      "epoch: 4 step: 134, loss is 0.04042984917759895\n",
      "epoch: 4 step: 135, loss is 0.021694326773285866\n",
      "epoch: 4 step: 136, loss is 0.019196510314941406\n",
      "epoch: 4 step: 137, loss is 0.04441406950354576\n",
      "epoch: 4 step: 138, loss is 0.014543609693646431\n",
      "epoch: 4 step: 139, loss is 0.06241388991475105\n",
      "epoch: 4 step: 140, loss is 0.09269663691520691\n",
      "epoch: 4 step: 141, loss is 0.029282130300998688\n",
      "epoch: 4 step: 142, loss is 0.11487498134374619\n",
      "epoch: 4 step: 143, loss is 0.020146522670984268\n",
      "epoch: 4 step: 144, loss is 0.04240560904145241\n",
      "epoch: 4 step: 145, loss is 0.014947465620934963\n",
      "epoch: 4 step: 146, loss is 0.02743946574628353\n",
      "epoch: 4 step: 147, loss is 0.027947012335062027\n",
      "epoch: 4 step: 148, loss is 0.03342904895544052\n",
      "epoch: 4 step: 149, loss is 0.008227468468248844\n",
      "epoch: 4 step: 150, loss is 0.033576976507902145\n",
      "epoch: 4 step: 151, loss is 0.035698581486940384\n",
      "epoch: 4 step: 152, loss is 0.08697734773159027\n",
      "epoch: 4 step: 153, loss is 0.026441872119903564\n",
      "epoch: 4 step: 154, loss is 0.21992938220500946\n",
      "epoch: 4 step: 155, loss is 0.08163571357727051\n",
      "epoch: 4 step: 156, loss is 0.017451124265789986\n",
      "epoch: 4 step: 157, loss is 0.04365399479866028\n",
      "epoch: 4 step: 158, loss is 0.015415539965033531\n",
      "epoch: 4 step: 159, loss is 0.04062328487634659\n",
      "epoch: 4 step: 160, loss is 0.13324913382530212\n",
      "epoch: 4 step: 161, loss is 0.00544384028762579\n",
      "epoch: 4 step: 162, loss is 0.034279800951480865\n",
      "epoch: 4 step: 163, loss is 0.004603997804224491\n",
      "epoch: 4 step: 164, loss is 0.04769666865468025\n",
      "epoch: 4 step: 165, loss is 0.008585353381931782\n",
      "epoch: 4 step: 166, loss is 0.0871187299489975\n",
      "epoch: 4 step: 167, loss is 0.03780144080519676\n",
      "epoch: 4 step: 168, loss is 0.07408034056425095\n",
      "epoch: 4 step: 169, loss is 0.015542122535407543\n",
      "epoch: 4 step: 170, loss is 0.15890055894851685\n",
      "epoch: 4 step: 171, loss is 0.1373693346977234\n",
      "epoch: 4 step: 172, loss is 0.13903141021728516\n",
      "epoch: 4 step: 173, loss is 0.12000944465398788\n",
      "epoch: 4 step: 174, loss is 0.01800968125462532\n",
      "epoch: 4 step: 175, loss is 0.061852116137742996\n",
      "epoch: 4 step: 176, loss is 0.007307068444788456\n",
      "epoch: 4 step: 177, loss is 0.028000805526971817\n",
      "epoch: 4 step: 178, loss is 0.2528527081012726\n",
      "epoch: 4 step: 179, loss is 0.05090951919555664\n",
      "epoch: 4 step: 180, loss is 0.003297890070825815\n",
      "epoch: 4 step: 181, loss is 0.0700644925236702\n",
      "epoch: 4 step: 182, loss is 0.13948571681976318\n",
      "epoch: 4 step: 183, loss is 0.20068520307540894\n",
      "epoch: 4 step: 184, loss is 0.011283987201750278\n",
      "epoch: 4 step: 185, loss is 0.008185951970517635\n",
      "epoch: 4 step: 186, loss is 0.06394527852535248\n",
      "epoch: 4 step: 187, loss is 0.021301060914993286\n",
      "epoch: 4 step: 188, loss is 0.0744151696562767\n",
      "epoch: 4 step: 189, loss is 0.009133487939834595\n",
      "epoch: 4 step: 190, loss is 0.013167130760848522\n",
      "epoch: 4 step: 191, loss is 0.030565930530428886\n",
      "epoch: 4 step: 192, loss is 0.07479598373174667\n",
      "epoch: 4 step: 193, loss is 0.05028937757015228\n",
      "epoch: 4 step: 194, loss is 0.0418713241815567\n",
      "epoch: 4 step: 195, loss is 0.06493262946605682\n",
      "epoch: 4 step: 196, loss is 0.07597071677446365\n",
      "epoch: 4 step: 197, loss is 0.00945308618247509\n",
      "epoch: 4 step: 198, loss is 0.17592497169971466\n",
      "epoch: 4 step: 199, loss is 0.030115876346826553\n",
      "epoch: 4 step: 200, loss is 0.3328332304954529\n",
      "epoch: 4 step: 201, loss is 0.12609829008579254\n",
      "epoch: 4 step: 202, loss is 0.018100591376423836\n",
      "epoch: 4 step: 203, loss is 0.03427800536155701\n",
      "epoch: 4 step: 204, loss is 0.0731361135840416\n",
      "epoch: 4 step: 205, loss is 0.06529884040355682\n",
      "epoch: 4 step: 206, loss is 0.1285420060157776\n",
      "epoch: 4 step: 207, loss is 0.038864828646183014\n",
      "epoch: 4 step: 208, loss is 0.03991139307618141\n",
      "epoch: 4 step: 209, loss is 0.0052809938788414\n",
      "epoch: 4 step: 210, loss is 0.052932679653167725\n",
      "epoch: 4 step: 211, loss is 0.21330700814723969\n",
      "epoch: 4 step: 212, loss is 0.012187850661575794\n",
      "epoch: 4 step: 213, loss is 0.005663586780428886\n",
      "epoch: 4 step: 214, loss is 0.02347426675260067\n",
      "epoch: 4 step: 215, loss is 0.009188629686832428\n",
      "epoch: 4 step: 216, loss is 0.09483875334262848\n",
      "epoch: 4 step: 217, loss is 0.05299295112490654\n",
      "epoch: 4 step: 218, loss is 0.0360972099006176\n",
      "epoch: 4 step: 219, loss is 0.1125158965587616\n",
      "epoch: 4 step: 220, loss is 0.07822879403829575\n",
      "epoch: 4 step: 221, loss is 0.02508378028869629\n",
      "epoch: 4 step: 222, loss is 0.04154243692755699\n",
      "epoch: 4 step: 223, loss is 0.14158891141414642\n",
      "epoch: 4 step: 224, loss is 0.028782472014427185\n",
      "epoch: 4 step: 225, loss is 0.059407513588666916\n",
      "epoch: 4 step: 226, loss is 0.13991838693618774\n",
      "epoch: 4 step: 227, loss is 0.15780706703662872\n",
      "epoch: 4 step: 228, loss is 0.015440070070326328\n",
      "epoch: 4 step: 229, loss is 0.03125524893403053\n",
      "epoch: 4 step: 230, loss is 0.022079845890402794\n",
      "epoch: 4 step: 231, loss is 0.029400205239653587\n",
      "epoch: 4 step: 232, loss is 0.05418306589126587\n",
      "epoch: 4 step: 233, loss is 0.10654614120721817\n",
      "epoch: 4 step: 234, loss is 0.006164505146443844\n",
      "epoch: 4 step: 235, loss is 0.042243778705596924\n",
      "epoch: 4 step: 236, loss is 0.02475588768720627\n",
      "epoch: 4 step: 237, loss is 0.09259003400802612\n",
      "epoch: 4 step: 238, loss is 0.16393481194972992\n",
      "epoch: 4 step: 239, loss is 0.008559058420360088\n",
      "epoch: 4 step: 240, loss is 0.04876827076077461\n",
      "epoch: 4 step: 241, loss is 0.04505866393446922\n",
      "epoch: 4 step: 242, loss is 0.09425926953554153\n",
      "epoch: 4 step: 243, loss is 0.016876457259058952\n",
      "epoch: 4 step: 244, loss is 0.0057523371651768684\n",
      "epoch: 4 step: 245, loss is 0.02592700533568859\n",
      "epoch: 4 step: 246, loss is 0.006083622109144926\n",
      "epoch: 4 step: 247, loss is 0.07564760744571686\n",
      "epoch: 4 step: 248, loss is 0.06653996556997299\n",
      "epoch: 4 step: 249, loss is 0.01566985622048378\n",
      "epoch: 4 step: 250, loss is 0.18095634877681732\n",
      "epoch: 4 step: 251, loss is 0.003916009329259396\n",
      "epoch: 4 step: 252, loss is 0.005032279063016176\n",
      "epoch: 4 step: 253, loss is 0.2848137617111206\n",
      "epoch: 4 step: 254, loss is 0.09568166732788086\n",
      "epoch: 4 step: 255, loss is 0.14495578408241272\n",
      "epoch: 4 step: 256, loss is 0.01001033652573824\n",
      "epoch: 4 step: 257, loss is 0.004808915313333273\n",
      "epoch: 4 step: 258, loss is 0.028098726645112038\n",
      "epoch: 4 step: 259, loss is 0.03735661134123802\n",
      "epoch: 4 step: 260, loss is 0.011386677622795105\n",
      "epoch: 4 step: 261, loss is 0.03436458483338356\n",
      "epoch: 4 step: 262, loss is 0.05774660035967827\n",
      "epoch: 4 step: 263, loss is 0.00783691555261612\n",
      "epoch: 4 step: 264, loss is 0.020250676199793816\n",
      "epoch: 4 step: 265, loss is 0.02592099830508232\n",
      "epoch: 4 step: 266, loss is 0.02090085670351982\n",
      "epoch: 4 step: 267, loss is 0.040466032922267914\n",
      "epoch: 4 step: 268, loss is 0.038627687841653824\n",
      "epoch: 4 step: 269, loss is 0.010058832354843616\n",
      "epoch: 4 step: 270, loss is 0.00788755714893341\n",
      "epoch: 4 step: 271, loss is 0.09352172911167145\n",
      "epoch: 4 step: 272, loss is 0.021823206916451454\n",
      "epoch: 4 step: 273, loss is 0.07123185694217682\n",
      "epoch: 4 step: 274, loss is 0.07345353811979294\n",
      "epoch: 4 step: 275, loss is 0.004303401336073875\n",
      "epoch: 4 step: 276, loss is 0.003834703005850315\n",
      "epoch: 4 step: 277, loss is 0.0019107165280729532\n",
      "epoch: 4 step: 278, loss is 0.04597855731844902\n",
      "epoch: 4 step: 279, loss is 0.08234871923923492\n",
      "epoch: 4 step: 280, loss is 0.04504943639039993\n",
      "epoch: 4 step: 281, loss is 0.1139478012919426\n",
      "epoch: 4 step: 282, loss is 0.008652975782752037\n",
      "epoch: 4 step: 283, loss is 0.03615257889032364\n",
      "epoch: 4 step: 284, loss is 0.029445208609104156\n",
      "epoch: 4 step: 285, loss is 0.06697386503219604\n",
      "epoch: 4 step: 286, loss is 0.019211651757359505\n",
      "epoch: 4 step: 287, loss is 0.0031011486425995827\n",
      "epoch: 4 step: 288, loss is 0.062120821326971054\n",
      "epoch: 4 step: 289, loss is 0.028751682490110397\n",
      "epoch: 4 step: 290, loss is 0.04609579220414162\n",
      "epoch: 4 step: 291, loss is 0.020816199481487274\n",
      "epoch: 4 step: 292, loss is 0.2788980305194855\n",
      "epoch: 4 step: 293, loss is 0.04473898187279701\n",
      "epoch: 4 step: 294, loss is 0.17747539281845093\n",
      "epoch: 4 step: 295, loss is 0.007901354692876339\n",
      "epoch: 4 step: 296, loss is 0.03014279343187809\n",
      "epoch: 4 step: 297, loss is 0.057473137974739075\n",
      "epoch: 4 step: 298, loss is 0.029224658384919167\n",
      "epoch: 4 step: 299, loss is 0.2048901468515396\n",
      "epoch: 4 step: 300, loss is 0.1425902247428894\n",
      "epoch: 4 step: 301, loss is 0.2289963811635971\n",
      "epoch: 4 step: 302, loss is 0.007802723441272974\n",
      "epoch: 4 step: 303, loss is 0.09987838566303253\n",
      "epoch: 4 step: 304, loss is 0.13290566205978394\n",
      "epoch: 4 step: 305, loss is 0.12432274222373962\n",
      "epoch: 4 step: 306, loss is 0.02244463562965393\n",
      "epoch: 4 step: 307, loss is 0.008846302516758442\n",
      "epoch: 4 step: 308, loss is 0.13498836755752563\n",
      "epoch: 4 step: 309, loss is 0.012416014447808266\n",
      "epoch: 4 step: 310, loss is 0.11216960847377777\n",
      "epoch: 4 step: 311, loss is 0.0638054832816124\n",
      "epoch: 4 step: 312, loss is 0.10868111252784729\n",
      "epoch: 4 step: 313, loss is 0.03483705222606659\n",
      "epoch: 4 step: 314, loss is 0.0030988322105258703\n",
      "epoch: 4 step: 315, loss is 0.04426512122154236\n",
      "epoch: 4 step: 316, loss is 0.012447054497897625\n",
      "epoch: 4 step: 317, loss is 0.13790293037891388\n",
      "epoch: 4 step: 318, loss is 0.005243451800197363\n",
      "epoch: 4 step: 319, loss is 0.014167035929858685\n",
      "epoch: 4 step: 320, loss is 0.05623620003461838\n",
      "epoch: 4 step: 321, loss is 0.041542794555425644\n",
      "epoch: 4 step: 322, loss is 0.029737109318375587\n",
      "epoch: 4 step: 323, loss is 0.004648102913051844\n",
      "epoch: 4 step: 324, loss is 0.01709608919918537\n",
      "epoch: 4 step: 325, loss is 0.030841166153550148\n",
      "epoch: 4 step: 326, loss is 0.07555118948221207\n",
      "epoch: 4 step: 327, loss is 0.027578335255384445\n",
      "epoch: 4 step: 328, loss is 0.019142691045999527\n",
      "epoch: 4 step: 329, loss is 0.11576225608587265\n",
      "epoch: 4 step: 330, loss is 0.03811872750520706\n",
      "epoch: 4 step: 331, loss is 0.062360379844903946\n",
      "epoch: 4 step: 332, loss is 0.009349674917757511\n",
      "epoch: 4 step: 333, loss is 0.04669879004359245\n",
      "epoch: 4 step: 334, loss is 0.14240559935569763\n",
      "epoch: 4 step: 335, loss is 0.010978643782436848\n",
      "epoch: 4 step: 336, loss is 0.01387729961425066\n",
      "epoch: 4 step: 337, loss is 0.06739015877246857\n",
      "epoch: 4 step: 338, loss is 0.028122730553150177\n",
      "epoch: 4 step: 339, loss is 0.06204088032245636\n",
      "epoch: 4 step: 340, loss is 0.12626519799232483\n",
      "epoch: 4 step: 341, loss is 0.025084152817726135\n",
      "epoch: 4 step: 342, loss is 0.000803908973466605\n",
      "epoch: 4 step: 343, loss is 0.03305097296833992\n",
      "epoch: 4 step: 344, loss is 0.0896015539765358\n",
      "epoch: 4 step: 345, loss is 0.007388709113001823\n",
      "epoch: 4 step: 346, loss is 0.19591005146503448\n",
      "epoch: 4 step: 347, loss is 0.016282713040709496\n",
      "epoch: 4 step: 348, loss is 0.17887483537197113\n",
      "epoch: 4 step: 349, loss is 0.00657263770699501\n",
      "epoch: 4 step: 350, loss is 0.007674444001168013\n",
      "epoch: 4 step: 351, loss is 0.004867559298872948\n",
      "epoch: 4 step: 352, loss is 0.029544547200202942\n",
      "epoch: 4 step: 353, loss is 0.20170485973358154\n",
      "epoch: 4 step: 354, loss is 0.023943327367305756\n",
      "epoch: 4 step: 355, loss is 0.04295779764652252\n",
      "epoch: 4 step: 356, loss is 0.025368278846144676\n",
      "epoch: 4 step: 357, loss is 0.07023599743843079\n",
      "epoch: 4 step: 358, loss is 0.07518869638442993\n",
      "epoch: 4 step: 359, loss is 0.10513923317193985\n",
      "epoch: 4 step: 360, loss is 0.049353741109371185\n",
      "epoch: 4 step: 361, loss is 0.011406281031668186\n",
      "epoch: 4 step: 362, loss is 0.02016841247677803\n",
      "epoch: 4 step: 363, loss is 0.03185318037867546\n",
      "epoch: 4 step: 364, loss is 0.007089819293469191\n",
      "epoch: 4 step: 365, loss is 0.00921363290399313\n",
      "epoch: 4 step: 366, loss is 0.09894628077745438\n",
      "epoch: 4 step: 367, loss is 0.029196547344326973\n",
      "epoch: 4 step: 368, loss is 0.018459640443325043\n",
      "epoch: 4 step: 369, loss is 0.01854686439037323\n",
      "epoch: 4 step: 370, loss is 0.08960233628749847\n",
      "epoch: 4 step: 371, loss is 0.07764981687068939\n",
      "epoch: 4 step: 372, loss is 0.007630412466824055\n",
      "epoch: 4 step: 373, loss is 0.006781846284866333\n",
      "epoch: 4 step: 374, loss is 0.01671690121293068\n",
      "epoch: 4 step: 375, loss is 0.020292343571782112\n",
      "epoch: 4 step: 376, loss is 0.04388011619448662\n",
      "epoch: 4 step: 377, loss is 0.057116348296403885\n",
      "epoch: 4 step: 378, loss is 0.14577840268611908\n",
      "epoch: 4 step: 379, loss is 0.07220224291086197\n",
      "epoch: 4 step: 380, loss is 0.08009503781795502\n",
      "epoch: 4 step: 381, loss is 0.03476520627737045\n",
      "epoch: 4 step: 382, loss is 0.0228728037327528\n",
      "epoch: 4 step: 383, loss is 0.09089004993438721\n",
      "epoch: 4 step: 384, loss is 0.00760362995788455\n",
      "epoch: 4 step: 385, loss is 0.03170781582593918\n",
      "epoch: 4 step: 386, loss is 0.0209302119910717\n",
      "epoch: 4 step: 387, loss is 0.011210690252482891\n",
      "epoch: 4 step: 388, loss is 0.016558555886149406\n",
      "epoch: 4 step: 389, loss is 0.026269858703017235\n",
      "epoch: 4 step: 390, loss is 0.012717620469629765\n",
      "epoch: 4 step: 391, loss is 0.02767065353691578\n",
      "epoch: 4 step: 392, loss is 0.024565445259213448\n",
      "epoch: 4 step: 393, loss is 0.12278791517019272\n",
      "epoch: 4 step: 394, loss is 0.1437956839799881\n",
      "epoch: 4 step: 395, loss is 0.006648847367614508\n",
      "epoch: 4 step: 396, loss is 0.026799045503139496\n",
      "epoch: 4 step: 397, loss is 0.030826624482870102\n",
      "epoch: 4 step: 398, loss is 0.07479999214410782\n",
      "epoch: 4 step: 399, loss is 0.05757681652903557\n",
      "epoch: 4 step: 400, loss is 0.044320303946733475\n",
      "epoch: 4 step: 401, loss is 0.07155681401491165\n",
      "epoch: 4 step: 402, loss is 0.03988217934966087\n",
      "epoch: 4 step: 403, loss is 0.48648837208747864\n",
      "epoch: 4 step: 404, loss is 0.059627432376146317\n",
      "epoch: 4 step: 405, loss is 0.42389795184135437\n",
      "epoch: 4 step: 406, loss is 0.047038376331329346\n",
      "epoch: 4 step: 407, loss is 0.1091536357998848\n",
      "epoch: 4 step: 408, loss is 0.019174426794052124\n",
      "epoch: 4 step: 409, loss is 0.027884745970368385\n",
      "epoch: 4 step: 410, loss is 0.11256363987922668\n",
      "epoch: 4 step: 411, loss is 0.00794205442070961\n",
      "epoch: 4 step: 412, loss is 0.021763060241937637\n",
      "epoch: 4 step: 413, loss is 0.025180546566843987\n",
      "epoch: 4 step: 414, loss is 0.10075552761554718\n",
      "epoch: 4 step: 415, loss is 0.16624361276626587\n",
      "epoch: 4 step: 416, loss is 0.01099927257746458\n",
      "epoch: 4 step: 417, loss is 0.2717550992965698\n",
      "epoch: 4 step: 418, loss is 0.015817776322364807\n",
      "epoch: 4 step: 419, loss is 0.27429452538490295\n",
      "epoch: 4 step: 420, loss is 0.014323166571557522\n",
      "epoch: 4 step: 421, loss is 0.005384220276027918\n",
      "epoch: 4 step: 422, loss is 0.08120186626911163\n",
      "epoch: 4 step: 423, loss is 0.08848947286605835\n",
      "epoch: 4 step: 424, loss is 0.055750902742147446\n",
      "epoch: 4 step: 425, loss is 0.06590871512889862\n",
      "epoch: 4 step: 426, loss is 0.001859234063886106\n",
      "epoch: 4 step: 427, loss is 0.055601902306079865\n",
      "epoch: 4 step: 428, loss is 0.10423153638839722\n",
      "epoch: 4 step: 429, loss is 0.21249271929264069\n",
      "epoch: 4 step: 430, loss is 0.0753651112318039\n",
      "epoch: 4 step: 431, loss is 0.09963028132915497\n",
      "epoch: 4 step: 432, loss is 0.1225508451461792\n",
      "epoch: 4 step: 433, loss is 0.11691500246524811\n",
      "epoch: 4 step: 434, loss is 0.003571608569473028\n",
      "epoch: 4 step: 435, loss is 0.0025884455535560846\n",
      "epoch: 4 step: 436, loss is 0.04082704335451126\n",
      "epoch: 4 step: 437, loss is 0.0057112472131848335\n",
      "epoch: 4 step: 438, loss is 0.2114873081445694\n",
      "epoch: 4 step: 439, loss is 0.00407095393165946\n",
      "epoch: 4 step: 440, loss is 0.03019540011882782\n",
      "epoch: 4 step: 441, loss is 0.0025679345708340406\n",
      "epoch: 4 step: 442, loss is 0.13013498485088348\n",
      "epoch: 4 step: 443, loss is 0.009284152649343014\n",
      "epoch: 4 step: 444, loss is 0.008219306357204914\n",
      "epoch: 4 step: 445, loss is 0.02126849628984928\n",
      "epoch: 4 step: 446, loss is 0.08840253949165344\n",
      "epoch: 4 step: 447, loss is 0.15557798743247986\n",
      "epoch: 4 step: 448, loss is 0.0989438146352768\n",
      "epoch: 4 step: 449, loss is 0.016201822087168694\n",
      "epoch: 4 step: 450, loss is 0.014294395223259926\n",
      "epoch: 4 step: 451, loss is 0.13930724561214447\n",
      "epoch: 4 step: 452, loss is 0.0068447040393948555\n",
      "epoch: 4 step: 453, loss is 0.0653815045952797\n",
      "epoch: 4 step: 454, loss is 0.12866246700286865\n",
      "epoch: 4 step: 455, loss is 0.11278746277093887\n",
      "epoch: 4 step: 456, loss is 0.025993771851062775\n",
      "epoch: 4 step: 457, loss is 0.004477716516703367\n",
      "epoch: 4 step: 458, loss is 0.12437623739242554\n",
      "epoch: 4 step: 459, loss is 0.004048320464789867\n",
      "epoch: 4 step: 460, loss is 0.02307474985718727\n",
      "epoch: 4 step: 461, loss is 0.03868663311004639\n",
      "epoch: 4 step: 462, loss is 0.012912008911371231\n",
      "epoch: 4 step: 463, loss is 0.00858654547482729\n",
      "epoch: 4 step: 464, loss is 0.021118875592947006\n",
      "epoch: 4 step: 465, loss is 0.056736186146736145\n",
      "epoch: 4 step: 466, loss is 0.02288922481238842\n",
      "epoch: 4 step: 467, loss is 0.029475849121809006\n",
      "epoch: 4 step: 468, loss is 0.047682467848062515\n",
      "epoch: 4 step: 469, loss is 0.017049698159098625\n",
      "epoch: 4 step: 470, loss is 0.11078265309333801\n",
      "epoch: 4 step: 471, loss is 0.0690150335431099\n",
      "epoch: 4 step: 472, loss is 0.27787747979164124\n",
      "epoch: 4 step: 473, loss is 0.013463538140058517\n",
      "epoch: 4 step: 474, loss is 0.25221332907676697\n",
      "epoch: 4 step: 475, loss is 0.006873670965433121\n",
      "epoch: 4 step: 476, loss is 0.01295362040400505\n",
      "epoch: 4 step: 477, loss is 0.0029035029001533985\n",
      "epoch: 4 step: 478, loss is 0.027424301952123642\n",
      "epoch: 4 step: 479, loss is 0.023986978456377983\n",
      "epoch: 4 step: 480, loss is 0.025385841727256775\n",
      "epoch: 4 step: 481, loss is 0.04229017347097397\n",
      "epoch: 4 step: 482, loss is 0.009084456600248814\n",
      "epoch: 4 step: 483, loss is 0.007984449155628681\n",
      "epoch: 4 step: 484, loss is 0.07414015382528305\n",
      "epoch: 4 step: 485, loss is 0.012688900344073772\n",
      "epoch: 4 step: 486, loss is 0.05577471852302551\n",
      "epoch: 4 step: 487, loss is 0.033467285335063934\n",
      "epoch: 4 step: 488, loss is 0.011335158720612526\n",
      "epoch: 4 step: 489, loss is 0.0746949315071106\n",
      "epoch: 4 step: 490, loss is 0.00919273030012846\n",
      "epoch: 4 step: 491, loss is 0.033204298466444016\n",
      "epoch: 4 step: 492, loss is 0.018619118258357048\n",
      "epoch: 4 step: 493, loss is 0.06361278891563416\n",
      "epoch: 4 step: 494, loss is 0.004274746403098106\n",
      "epoch: 4 step: 495, loss is 0.4252728819847107\n",
      "epoch: 4 step: 496, loss is 0.18075723946094513\n",
      "epoch: 4 step: 497, loss is 0.0058618877083063126\n",
      "epoch: 4 step: 498, loss is 0.02533601224422455\n",
      "epoch: 4 step: 499, loss is 0.13245095312595367\n",
      "epoch: 4 step: 500, loss is 0.062012236565351486\n",
      "epoch: 4 step: 501, loss is 0.3088279068470001\n",
      "epoch: 4 step: 502, loss is 0.03415464609861374\n",
      "epoch: 4 step: 503, loss is 0.015861770138144493\n",
      "epoch: 4 step: 504, loss is 0.012531583197414875\n",
      "epoch: 4 step: 505, loss is 0.09407185763120651\n",
      "epoch: 4 step: 506, loss is 0.03193916007876396\n",
      "epoch: 4 step: 507, loss is 0.006444981321692467\n",
      "epoch: 4 step: 508, loss is 0.018629951402544975\n",
      "epoch: 4 step: 509, loss is 0.021615667268633842\n",
      "epoch: 4 step: 510, loss is 0.0019997137133032084\n",
      "epoch: 4 step: 511, loss is 0.10326123237609863\n",
      "epoch: 4 step: 512, loss is 0.606736421585083\n",
      "epoch: 4 step: 513, loss is 0.006352358963340521\n",
      "epoch: 4 step: 514, loss is 0.037600383162498474\n",
      "epoch: 4 step: 515, loss is 0.0045643458142876625\n",
      "epoch: 4 step: 516, loss is 0.016481464728713036\n",
      "epoch: 4 step: 517, loss is 0.020388975739479065\n",
      "epoch: 4 step: 518, loss is 0.020257070660591125\n",
      "epoch: 4 step: 519, loss is 0.025809237733483315\n",
      "epoch: 4 step: 520, loss is 0.023568373173475266\n",
      "epoch: 4 step: 521, loss is 0.08741345256567001\n",
      "epoch: 4 step: 522, loss is 0.018660182133316994\n",
      "epoch: 4 step: 523, loss is 0.14990155398845673\n",
      "epoch: 4 step: 524, loss is 0.17087478935718536\n",
      "epoch: 4 step: 525, loss is 0.027310583740472794\n",
      "epoch: 4 step: 526, loss is 0.005195346660912037\n",
      "epoch: 4 step: 527, loss is 0.08818894624710083\n",
      "epoch: 4 step: 528, loss is 0.09315038472414017\n",
      "epoch: 4 step: 529, loss is 0.027720360085368156\n",
      "epoch: 4 step: 530, loss is 0.10256079584360123\n",
      "epoch: 4 step: 531, loss is 0.043109044432640076\n",
      "epoch: 4 step: 532, loss is 0.0476558692753315\n",
      "epoch: 4 step: 533, loss is 0.21323542296886444\n",
      "epoch: 4 step: 534, loss is 0.2979384660720825\n",
      "epoch: 4 step: 535, loss is 0.002888601738959551\n",
      "epoch: 4 step: 536, loss is 0.0693046897649765\n",
      "epoch: 4 step: 537, loss is 0.05953601747751236\n",
      "epoch: 4 step: 538, loss is 0.09423783421516418\n",
      "epoch: 4 step: 539, loss is 0.0959346815943718\n",
      "epoch: 4 step: 540, loss is 0.04788297042250633\n",
      "epoch: 4 step: 541, loss is 0.08499839156866074\n",
      "epoch: 4 step: 542, loss is 0.01538520772010088\n",
      "epoch: 4 step: 543, loss is 0.00423089787364006\n",
      "epoch: 4 step: 544, loss is 0.010973171330988407\n",
      "epoch: 4 step: 545, loss is 0.1404227763414383\n",
      "epoch: 4 step: 546, loss is 0.04142653942108154\n",
      "epoch: 4 step: 547, loss is 0.4587787389755249\n",
      "epoch: 4 step: 548, loss is 0.014373721554875374\n",
      "epoch: 4 step: 549, loss is 0.06654366105794907\n",
      "epoch: 4 step: 550, loss is 0.3514144718647003\n",
      "epoch: 4 step: 551, loss is 0.011852525174617767\n",
      "epoch: 4 step: 552, loss is 0.09907615184783936\n",
      "epoch: 4 step: 553, loss is 0.022400738671422005\n",
      "epoch: 4 step: 554, loss is 0.2047250121831894\n",
      "epoch: 4 step: 555, loss is 0.07843457162380219\n",
      "epoch: 4 step: 556, loss is 0.13305285573005676\n",
      "epoch: 4 step: 557, loss is 0.17664708197116852\n",
      "epoch: 4 step: 558, loss is 0.08778209239244461\n",
      "epoch: 4 step: 559, loss is 0.33132174611091614\n",
      "epoch: 4 step: 560, loss is 0.05522676184773445\n",
      "epoch: 4 step: 561, loss is 0.15171433985233307\n",
      "epoch: 4 step: 562, loss is 0.167490616440773\n",
      "epoch: 4 step: 563, loss is 0.030322611331939697\n",
      "epoch: 4 step: 564, loss is 0.01194807793945074\n",
      "epoch: 4 step: 565, loss is 0.0034841434098780155\n",
      "epoch: 4 step: 566, loss is 0.005336947273463011\n",
      "epoch: 4 step: 567, loss is 0.10727553814649582\n",
      "epoch: 4 step: 568, loss is 0.009094437584280968\n",
      "epoch: 4 step: 569, loss is 0.06819537281990051\n",
      "epoch: 4 step: 570, loss is 0.033988941460847855\n",
      "epoch: 4 step: 571, loss is 0.028630847111344337\n",
      "epoch: 4 step: 572, loss is 0.015311899594962597\n",
      "epoch: 4 step: 573, loss is 0.2648911476135254\n",
      "epoch: 4 step: 574, loss is 0.08328484743833542\n",
      "epoch: 4 step: 575, loss is 0.0005446095601655543\n",
      "epoch: 4 step: 576, loss is 0.051544442772865295\n",
      "epoch: 4 step: 577, loss is 0.031283292919397354\n",
      "epoch: 4 step: 578, loss is 0.0985119566321373\n",
      "epoch: 4 step: 579, loss is 0.0039198980666697025\n",
      "epoch: 4 step: 580, loss is 0.10324091464281082\n",
      "epoch: 4 step: 581, loss is 0.0022518387995660305\n",
      "epoch: 4 step: 582, loss is 0.15963387489318848\n",
      "epoch: 4 step: 583, loss is 0.18702949583530426\n",
      "epoch: 4 step: 584, loss is 0.04961123690009117\n",
      "epoch: 4 step: 585, loss is 0.029501399025321007\n",
      "epoch: 4 step: 586, loss is 0.05883968994021416\n",
      "epoch: 4 step: 587, loss is 0.0158876720815897\n",
      "epoch: 4 step: 588, loss is 0.03114410676062107\n",
      "epoch: 4 step: 589, loss is 0.006423744838684797\n",
      "epoch: 4 step: 590, loss is 0.13725495338439941\n",
      "epoch: 4 step: 591, loss is 0.041823871433734894\n",
      "epoch: 4 step: 592, loss is 0.06008705124258995\n",
      "epoch: 4 step: 593, loss is 0.022336333990097046\n",
      "epoch: 4 step: 594, loss is 0.27928802371025085\n",
      "epoch: 4 step: 595, loss is 0.03209385648369789\n",
      "epoch: 4 step: 596, loss is 0.017011987045407295\n",
      "epoch: 4 step: 597, loss is 0.12439724802970886\n",
      "epoch: 4 step: 598, loss is 0.04085910692811012\n",
      "epoch: 4 step: 599, loss is 0.058454860001802444\n",
      "epoch: 4 step: 600, loss is 0.005818034056574106\n",
      "epoch: 4 step: 601, loss is 0.17929668724536896\n",
      "epoch: 4 step: 602, loss is 0.008723885752260685\n",
      "epoch: 4 step: 603, loss is 0.009632712230086327\n",
      "epoch: 4 step: 604, loss is 0.0948973074555397\n",
      "epoch: 4 step: 605, loss is 0.07310797274112701\n",
      "epoch: 4 step: 606, loss is 0.12676359713077545\n",
      "epoch: 4 step: 607, loss is 0.014169582165777683\n",
      "epoch: 4 step: 608, loss is 0.05676918849349022\n",
      "epoch: 4 step: 609, loss is 0.024544117972254753\n",
      "epoch: 4 step: 610, loss is 0.03515937179327011\n",
      "epoch: 4 step: 611, loss is 0.04444802924990654\n",
      "epoch: 4 step: 612, loss is 0.0247142743319273\n",
      "epoch: 4 step: 613, loss is 0.04836967960000038\n",
      "epoch: 4 step: 614, loss is 0.04850223660469055\n",
      "epoch: 4 step: 615, loss is 0.10827186703681946\n",
      "epoch: 4 step: 616, loss is 0.0013225877191871405\n",
      "epoch: 4 step: 617, loss is 0.1236526295542717\n",
      "epoch: 4 step: 618, loss is 0.08815339207649231\n",
      "epoch: 4 step: 619, loss is 0.010152218863368034\n",
      "epoch: 4 step: 620, loss is 0.009841649793088436\n",
      "epoch: 4 step: 621, loss is 0.07380547374486923\n",
      "epoch: 4 step: 622, loss is 0.10165156424045563\n",
      "epoch: 4 step: 623, loss is 0.01857653073966503\n",
      "epoch: 4 step: 624, loss is 0.012143897823989391\n",
      "epoch: 4 step: 625, loss is 0.12111010402441025\n",
      "epoch: 4 step: 626, loss is 0.03504683077335358\n",
      "epoch: 4 step: 627, loss is 0.002407391555607319\n",
      "epoch: 4 step: 628, loss is 0.0023549937177449465\n",
      "epoch: 4 step: 629, loss is 0.1738392561674118\n",
      "epoch: 4 step: 630, loss is 0.026329174637794495\n",
      "epoch: 4 step: 631, loss is 0.06937947869300842\n",
      "epoch: 4 step: 632, loss is 0.018203843384981155\n",
      "epoch: 4 step: 633, loss is 0.08030369132757187\n",
      "epoch: 4 step: 634, loss is 0.0038358343299478292\n",
      "epoch: 4 step: 635, loss is 0.08822201192378998\n",
      "epoch: 4 step: 636, loss is 0.015249422751367092\n",
      "epoch: 4 step: 637, loss is 0.011352346278727055\n",
      "epoch: 4 step: 638, loss is 0.1854826956987381\n",
      "epoch: 4 step: 639, loss is 0.07358856499195099\n",
      "epoch: 4 step: 640, loss is 0.11094895750284195\n",
      "epoch: 4 step: 641, loss is 0.02412247844040394\n",
      "epoch: 4 step: 642, loss is 0.035219427198171616\n",
      "epoch: 4 step: 643, loss is 0.16036514937877655\n",
      "epoch: 4 step: 644, loss is 0.004024324007332325\n",
      "epoch: 4 step: 645, loss is 0.1158660501241684\n",
      "epoch: 4 step: 646, loss is 0.03880057483911514\n",
      "epoch: 4 step: 647, loss is 0.03998076915740967\n",
      "epoch: 4 step: 648, loss is 0.07697566598653793\n",
      "epoch: 4 step: 649, loss is 0.14905202388763428\n",
      "epoch: 4 step: 650, loss is 0.002394538139924407\n",
      "epoch: 4 step: 651, loss is 0.243155375123024\n",
      "epoch: 4 step: 652, loss is 0.012369181029498577\n",
      "epoch: 4 step: 653, loss is 0.09846849739551544\n",
      "epoch: 4 step: 654, loss is 0.09399136900901794\n",
      "epoch: 4 step: 655, loss is 0.020069019868969917\n",
      "epoch: 4 step: 656, loss is 0.2274150848388672\n",
      "epoch: 4 step: 657, loss is 0.11510509997606277\n",
      "epoch: 4 step: 658, loss is 0.010537383146584034\n",
      "epoch: 4 step: 659, loss is 0.02263486757874489\n",
      "epoch: 4 step: 660, loss is 0.015341930091381073\n",
      "epoch: 4 step: 661, loss is 0.02725674770772457\n",
      "epoch: 4 step: 662, loss is 0.009857593104243279\n",
      "epoch: 4 step: 663, loss is 0.05592722445726395\n",
      "epoch: 4 step: 664, loss is 0.21838897466659546\n",
      "epoch: 4 step: 665, loss is 0.019263794645667076\n",
      "epoch: 4 step: 666, loss is 0.11164715141057968\n",
      "epoch: 4 step: 667, loss is 0.043808840215206146\n",
      "epoch: 4 step: 668, loss is 0.006664142478257418\n",
      "epoch: 4 step: 669, loss is 0.04926162585616112\n",
      "epoch: 4 step: 670, loss is 0.013780794106423855\n",
      "epoch: 4 step: 671, loss is 0.04053838551044464\n",
      "epoch: 4 step: 672, loss is 0.024231819435954094\n",
      "epoch: 4 step: 673, loss is 0.032543934881687164\n",
      "epoch: 4 step: 674, loss is 0.2576107382774353\n",
      "epoch: 4 step: 675, loss is 0.05006314069032669\n",
      "epoch: 4 step: 676, loss is 0.04462448135018349\n",
      "epoch: 4 step: 677, loss is 0.07759088277816772\n",
      "epoch: 4 step: 678, loss is 0.03590858727693558\n",
      "epoch: 4 step: 679, loss is 0.011057635769248009\n",
      "epoch: 4 step: 680, loss is 0.08755475282669067\n",
      "epoch: 4 step: 681, loss is 0.010842397809028625\n",
      "epoch: 4 step: 682, loss is 0.007089422084391117\n",
      "epoch: 4 step: 683, loss is 0.024850288406014442\n",
      "epoch: 4 step: 684, loss is 0.03711778298020363\n",
      "epoch: 4 step: 685, loss is 0.0289631187915802\n",
      "epoch: 4 step: 686, loss is 0.19346164166927338\n",
      "epoch: 4 step: 687, loss is 0.0941898375749588\n",
      "epoch: 4 step: 688, loss is 0.24765315651893616\n",
      "epoch: 4 step: 689, loss is 0.14963378012180328\n",
      "epoch: 4 step: 690, loss is 0.015968723222613335\n",
      "epoch: 4 step: 691, loss is 0.005887192208319902\n",
      "epoch: 4 step: 692, loss is 0.07882735133171082\n",
      "epoch: 4 step: 693, loss is 0.043603017926216125\n",
      "epoch: 4 step: 694, loss is 0.004206621553748846\n",
      "epoch: 4 step: 695, loss is 0.12436282634735107\n",
      "epoch: 4 step: 696, loss is 0.10993009060621262\n",
      "epoch: 4 step: 697, loss is 0.2592260241508484\n",
      "epoch: 4 step: 698, loss is 0.005569658242166042\n",
      "epoch: 4 step: 699, loss is 0.12545154988765717\n",
      "epoch: 4 step: 700, loss is 0.11299408227205276\n",
      "epoch: 4 step: 701, loss is 0.1114979088306427\n",
      "epoch: 4 step: 702, loss is 0.03241293504834175\n",
      "epoch: 4 step: 703, loss is 0.039541758596897125\n",
      "epoch: 4 step: 704, loss is 0.019484171643853188\n",
      "epoch: 4 step: 705, loss is 0.07799948751926422\n",
      "epoch: 4 step: 706, loss is 0.030515018850564957\n",
      "epoch: 4 step: 707, loss is 0.21185223758220673\n",
      "epoch: 4 step: 708, loss is 0.016526656225323677\n",
      "epoch: 4 step: 709, loss is 0.13178440928459167\n",
      "epoch: 4 step: 710, loss is 0.14749747514724731\n",
      "epoch: 4 step: 711, loss is 0.004170901142060757\n",
      "epoch: 4 step: 712, loss is 0.06788089126348495\n",
      "epoch: 4 step: 713, loss is 0.05910152196884155\n",
      "epoch: 4 step: 714, loss is 0.06122596934437752\n",
      "epoch: 4 step: 715, loss is 0.046161308884620667\n",
      "epoch: 4 step: 716, loss is 0.034010063856840134\n",
      "epoch: 4 step: 717, loss is 0.003949831239879131\n",
      "epoch: 4 step: 718, loss is 0.34093695878982544\n",
      "epoch: 4 step: 719, loss is 0.05687732994556427\n",
      "epoch: 4 step: 720, loss is 0.11762328445911407\n",
      "epoch: 4 step: 721, loss is 0.1730848252773285\n",
      "epoch: 4 step: 722, loss is 0.03748707100749016\n",
      "epoch: 4 step: 723, loss is 0.06195414066314697\n",
      "epoch: 4 step: 724, loss is 0.010844437405467033\n",
      "epoch: 4 step: 725, loss is 0.010782062076032162\n",
      "epoch: 4 step: 726, loss is 0.03906434774398804\n",
      "epoch: 4 step: 727, loss is 0.19222916662693024\n",
      "epoch: 4 step: 728, loss is 0.14519429206848145\n",
      "epoch: 4 step: 729, loss is 0.04655241221189499\n",
      "epoch: 4 step: 730, loss is 0.011679381132125854\n",
      "epoch: 4 step: 731, loss is 0.11504368484020233\n",
      "epoch: 4 step: 732, loss is 0.004994428716599941\n",
      "epoch: 4 step: 733, loss is 0.18309195339679718\n",
      "epoch: 4 step: 734, loss is 0.03579314053058624\n",
      "epoch: 4 step: 735, loss is 0.035895250737667084\n",
      "epoch: 4 step: 736, loss is 0.035242900252342224\n",
      "epoch: 4 step: 737, loss is 0.03499465808272362\n",
      "epoch: 4 step: 738, loss is 0.046336639672517776\n",
      "epoch: 4 step: 739, loss is 0.3336593508720398\n",
      "epoch: 4 step: 740, loss is 0.010563698597252369\n",
      "epoch: 4 step: 741, loss is 0.012071209959685802\n",
      "epoch: 4 step: 742, loss is 0.13656720519065857\n",
      "epoch: 4 step: 743, loss is 0.2904535233974457\n",
      "epoch: 4 step: 744, loss is 0.0561816431581974\n",
      "epoch: 4 step: 745, loss is 0.04377938434481621\n",
      "epoch: 4 step: 746, loss is 0.033698540180921555\n",
      "epoch: 4 step: 747, loss is 0.00115242472384125\n",
      "epoch: 4 step: 748, loss is 0.25712400674819946\n",
      "epoch: 4 step: 749, loss is 0.05188996344804764\n",
      "epoch: 4 step: 750, loss is 0.016862886026501656\n",
      "epoch: 4 step: 751, loss is 0.24776995182037354\n",
      "epoch: 4 step: 752, loss is 0.03538388013839722\n",
      "epoch: 4 step: 753, loss is 0.0033320190850645304\n",
      "epoch: 4 step: 754, loss is 0.002800783608108759\n",
      "epoch: 4 step: 755, loss is 0.1537218689918518\n",
      "epoch: 4 step: 756, loss is 0.11039912700653076\n",
      "epoch: 4 step: 757, loss is 0.0676111951470375\n",
      "epoch: 4 step: 758, loss is 0.03688376024365425\n",
      "epoch: 4 step: 759, loss is 0.03262215852737427\n",
      "epoch: 4 step: 760, loss is 0.08461704105138779\n",
      "epoch: 4 step: 761, loss is 0.02936409041285515\n",
      "epoch: 4 step: 762, loss is 0.046994008123874664\n",
      "epoch: 4 step: 763, loss is 0.024305161088705063\n",
      "epoch: 4 step: 764, loss is 0.08618000894784927\n",
      "epoch: 4 step: 765, loss is 0.037809859961271286\n",
      "epoch: 4 step: 766, loss is 0.00493740476667881\n",
      "epoch: 4 step: 767, loss is 0.0662323534488678\n",
      "epoch: 4 step: 768, loss is 0.047842707484960556\n",
      "epoch: 4 step: 769, loss is 0.0056003304198384285\n",
      "epoch: 4 step: 770, loss is 0.041444990783929825\n",
      "epoch: 4 step: 771, loss is 0.22758136689662933\n",
      "epoch: 4 step: 772, loss is 0.1630629152059555\n",
      "epoch: 4 step: 773, loss is 0.01357056014239788\n",
      "epoch: 4 step: 774, loss is 0.03774496912956238\n",
      "epoch: 4 step: 775, loss is 0.05258193984627724\n",
      "epoch: 4 step: 776, loss is 0.013179895468056202\n",
      "epoch: 4 step: 777, loss is 0.004522198811173439\n",
      "epoch: 4 step: 778, loss is 0.032995738089084625\n",
      "epoch: 4 step: 779, loss is 0.113836869597435\n",
      "epoch: 4 step: 780, loss is 0.07128677517175674\n",
      "epoch: 4 step: 781, loss is 0.0269724540412426\n",
      "epoch: 4 step: 782, loss is 0.06961322575807571\n",
      "epoch: 4 step: 783, loss is 0.01837322860956192\n",
      "epoch: 4 step: 784, loss is 0.007173547986894846\n",
      "epoch: 4 step: 785, loss is 0.124567411839962\n",
      "epoch: 4 step: 786, loss is 0.03498651087284088\n",
      "epoch: 4 step: 787, loss is 0.06727577745914459\n",
      "epoch: 4 step: 788, loss is 0.004881968256086111\n",
      "epoch: 4 step: 789, loss is 0.17473258078098297\n",
      "epoch: 4 step: 790, loss is 0.10208099335432053\n",
      "epoch: 4 step: 791, loss is 0.09294387698173523\n",
      "epoch: 4 step: 792, loss is 0.11355593800544739\n",
      "epoch: 4 step: 793, loss is 0.006405846681445837\n",
      "epoch: 4 step: 794, loss is 0.005133657716214657\n",
      "epoch: 4 step: 795, loss is 0.013788285665214062\n",
      "epoch: 4 step: 796, loss is 0.12610141932964325\n",
      "epoch: 4 step: 797, loss is 0.1579577922821045\n",
      "epoch: 4 step: 798, loss is 0.13771797716617584\n",
      "epoch: 4 step: 799, loss is 0.16447487473487854\n",
      "epoch: 4 step: 800, loss is 0.062211405485868454\n",
      "epoch: 4 step: 801, loss is 0.02342923730611801\n",
      "epoch: 4 step: 802, loss is 0.2682119905948639\n",
      "epoch: 4 step: 803, loss is 0.043880611658096313\n",
      "epoch: 4 step: 804, loss is 0.05947354808449745\n",
      "epoch: 4 step: 805, loss is 0.01422823965549469\n",
      "epoch: 4 step: 806, loss is 0.09755823016166687\n",
      "epoch: 4 step: 807, loss is 0.049885593354701996\n",
      "epoch: 4 step: 808, loss is 0.16576693952083588\n",
      "epoch: 4 step: 809, loss is 0.049666959792375565\n",
      "epoch: 4 step: 810, loss is 0.16986508667469025\n",
      "epoch: 4 step: 811, loss is 0.011049018241465092\n",
      "epoch: 4 step: 812, loss is 0.009840293787419796\n",
      "epoch: 4 step: 813, loss is 0.006494093686342239\n",
      "epoch: 4 step: 814, loss is 0.06261323392391205\n",
      "epoch: 4 step: 815, loss is 0.006099529564380646\n",
      "epoch: 4 step: 816, loss is 0.050343431532382965\n",
      "epoch: 4 step: 817, loss is 0.04435271769762039\n",
      "epoch: 4 step: 818, loss is 0.12888216972351074\n",
      "epoch: 4 step: 819, loss is 0.0016572671011090279\n",
      "epoch: 4 step: 820, loss is 0.00984689500182867\n",
      "epoch: 4 step: 821, loss is 0.1776195764541626\n",
      "epoch: 4 step: 822, loss is 0.003094674553722143\n",
      "epoch: 4 step: 823, loss is 0.013399932533502579\n",
      "epoch: 4 step: 824, loss is 0.02010941505432129\n",
      "epoch: 4 step: 825, loss is 0.011269761249423027\n",
      "epoch: 4 step: 826, loss is 0.08293417096138\n",
      "epoch: 4 step: 827, loss is 0.01693778485059738\n",
      "epoch: 4 step: 828, loss is 0.0045624724589288235\n",
      "epoch: 4 step: 829, loss is 0.00568411685526371\n",
      "epoch: 4 step: 830, loss is 0.047701284289360046\n",
      "epoch: 4 step: 831, loss is 0.052655093371868134\n",
      "epoch: 4 step: 832, loss is 0.13627545535564423\n",
      "epoch: 4 step: 833, loss is 0.0081460727378726\n",
      "epoch: 4 step: 834, loss is 0.01340716052800417\n",
      "epoch: 4 step: 835, loss is 0.028174780309200287\n",
      "epoch: 4 step: 836, loss is 0.03638588264584541\n",
      "epoch: 4 step: 837, loss is 0.005602192599326372\n",
      "epoch: 4 step: 838, loss is 0.0663565993309021\n",
      "epoch: 4 step: 839, loss is 0.009501748718321323\n",
      "epoch: 4 step: 840, loss is 0.005734106060117483\n",
      "epoch: 4 step: 841, loss is 0.0072801560163497925\n",
      "epoch: 4 step: 842, loss is 0.05296992510557175\n",
      "epoch: 4 step: 843, loss is 0.04458467662334442\n",
      "epoch: 4 step: 844, loss is 0.03982431814074516\n",
      "epoch: 4 step: 845, loss is 0.08562946319580078\n",
      "epoch: 4 step: 846, loss is 0.03157009184360504\n",
      "epoch: 4 step: 847, loss is 0.09443947672843933\n",
      "epoch: 4 step: 848, loss is 0.010018537752330303\n",
      "epoch: 4 step: 849, loss is 0.003632570384070277\n",
      "epoch: 4 step: 850, loss is 0.018506880849599838\n",
      "epoch: 4 step: 851, loss is 0.010890435427427292\n",
      "epoch: 4 step: 852, loss is 0.20324549078941345\n",
      "epoch: 4 step: 853, loss is 0.015846390277147293\n",
      "epoch: 4 step: 854, loss is 0.3103311359882355\n",
      "epoch: 4 step: 855, loss is 0.1089547872543335\n",
      "epoch: 4 step: 856, loss is 0.10491974651813507\n",
      "epoch: 4 step: 857, loss is 0.16090041399002075\n",
      "epoch: 4 step: 858, loss is 0.03977363556623459\n",
      "epoch: 4 step: 859, loss is 0.009305741637945175\n",
      "epoch: 4 step: 860, loss is 0.010561858303844929\n",
      "epoch: 4 step: 861, loss is 0.10615596175193787\n",
      "epoch: 4 step: 862, loss is 0.0215886440128088\n",
      "epoch: 4 step: 863, loss is 0.005819321610033512\n",
      "epoch: 4 step: 864, loss is 0.10792537033557892\n",
      "epoch: 4 step: 865, loss is 0.048548873513936996\n",
      "epoch: 4 step: 866, loss is 0.019051961600780487\n",
      "epoch: 4 step: 867, loss is 0.1588742882013321\n",
      "epoch: 4 step: 868, loss is 0.024941906332969666\n",
      "epoch: 4 step: 869, loss is 0.01398224476724863\n",
      "epoch: 4 step: 870, loss is 0.0049127922393381596\n",
      "epoch: 4 step: 871, loss is 0.07558786869049072\n",
      "epoch: 4 step: 872, loss is 0.04425398260354996\n",
      "epoch: 4 step: 873, loss is 0.006446095183491707\n",
      "epoch: 4 step: 874, loss is 0.0185866579413414\n",
      "epoch: 4 step: 875, loss is 0.2262343317270279\n",
      "epoch: 4 step: 876, loss is 0.0396079495549202\n",
      "epoch: 4 step: 877, loss is 0.010354247875511646\n",
      "epoch: 4 step: 878, loss is 0.04216694459319115\n",
      "epoch: 4 step: 879, loss is 0.06172316148877144\n",
      "epoch: 4 step: 880, loss is 0.04867475479841232\n",
      "epoch: 4 step: 881, loss is 0.0491146519780159\n",
      "epoch: 4 step: 882, loss is 0.13663513958454132\n",
      "epoch: 4 step: 883, loss is 0.035890959203243256\n",
      "epoch: 4 step: 884, loss is 0.038834720849990845\n",
      "epoch: 4 step: 885, loss is 0.31631115078926086\n",
      "epoch: 4 step: 886, loss is 0.33737942576408386\n",
      "epoch: 4 step: 887, loss is 0.06097814813256264\n",
      "epoch: 4 step: 888, loss is 0.029714196920394897\n",
      "epoch: 4 step: 889, loss is 0.03282264620065689\n",
      "epoch: 4 step: 890, loss is 0.11784926056861877\n",
      "epoch: 4 step: 891, loss is 0.07495224475860596\n",
      "epoch: 4 step: 892, loss is 0.28902435302734375\n",
      "epoch: 4 step: 893, loss is 0.05913681909441948\n",
      "epoch: 4 step: 894, loss is 0.0522809699177742\n",
      "epoch: 4 step: 895, loss is 0.23507159948349\n",
      "epoch: 4 step: 896, loss is 0.008135218173265457\n",
      "epoch: 4 step: 897, loss is 0.11141431331634521\n",
      "epoch: 4 step: 898, loss is 0.09801035374403\n",
      "epoch: 4 step: 899, loss is 0.09309477359056473\n",
      "epoch: 4 step: 900, loss is 0.00446196598932147\n",
      "epoch: 4 step: 901, loss is 0.011143033392727375\n",
      "epoch: 4 step: 902, loss is 0.11961174756288528\n",
      "epoch: 4 step: 903, loss is 0.004048287868499756\n",
      "epoch: 4 step: 904, loss is 0.02712484635412693\n",
      "epoch: 4 step: 905, loss is 0.004681848455220461\n",
      "epoch: 4 step: 906, loss is 0.13961496949195862\n",
      "epoch: 4 step: 907, loss is 0.0467655248939991\n",
      "epoch: 4 step: 908, loss is 0.13707727193832397\n",
      "epoch: 4 step: 909, loss is 0.02826814353466034\n",
      "epoch: 4 step: 910, loss is 0.06954445689916611\n",
      "epoch: 4 step: 911, loss is 0.0958406925201416\n",
      "epoch: 4 step: 912, loss is 0.09909895062446594\n",
      "epoch: 4 step: 913, loss is 0.006545388139784336\n",
      "epoch: 4 step: 914, loss is 0.10491102933883667\n",
      "epoch: 4 step: 915, loss is 0.01973101682960987\n",
      "epoch: 4 step: 916, loss is 0.05851944908499718\n",
      "epoch: 4 step: 917, loss is 0.02105931006371975\n",
      "epoch: 4 step: 918, loss is 0.03117510676383972\n",
      "epoch: 4 step: 919, loss is 0.06289644539356232\n",
      "epoch: 4 step: 920, loss is 0.030727198347449303\n",
      "epoch: 4 step: 921, loss is 0.09258537739515305\n",
      "epoch: 4 step: 922, loss is 0.023677943274378777\n",
      "epoch: 4 step: 923, loss is 0.05527593195438385\n",
      "epoch: 4 step: 924, loss is 0.1547425091266632\n",
      "epoch: 4 step: 925, loss is 0.20526966452598572\n",
      "epoch: 4 step: 926, loss is 0.15936847031116486\n",
      "epoch: 4 step: 927, loss is 0.1541675627231598\n",
      "epoch: 4 step: 928, loss is 0.00464780256152153\n",
      "epoch: 4 step: 929, loss is 0.008899441920220852\n",
      "epoch: 4 step: 930, loss is 0.05626421049237251\n",
      "epoch: 4 step: 931, loss is 0.021092871204018593\n",
      "epoch: 4 step: 932, loss is 0.056422945111989975\n",
      "epoch: 4 step: 933, loss is 0.048738524317741394\n",
      "epoch: 4 step: 934, loss is 0.17396342754364014\n",
      "epoch: 4 step: 935, loss is 0.15525083243846893\n",
      "epoch: 4 step: 936, loss is 0.031008128076791763\n",
      "epoch: 4 step: 937, loss is 0.005135050043463707\n",
      "epoch: 4 step: 938, loss is 0.1514088213443756\n",
      "epoch: 4 step: 939, loss is 0.06342111527919769\n",
      "epoch: 4 step: 940, loss is 0.04176286235451698\n",
      "epoch: 4 step: 941, loss is 0.10285942256450653\n",
      "epoch: 4 step: 942, loss is 0.002096294891089201\n",
      "epoch: 4 step: 943, loss is 0.004099795129150152\n",
      "epoch: 4 step: 944, loss is 0.06692300736904144\n",
      "epoch: 4 step: 945, loss is 0.09745997935533524\n",
      "epoch: 4 step: 946, loss is 0.07328054308891296\n",
      "epoch: 4 step: 947, loss is 0.04633069038391113\n",
      "epoch: 4 step: 948, loss is 0.13667038083076477\n",
      "epoch: 4 step: 949, loss is 0.21475455164909363\n",
      "epoch: 4 step: 950, loss is 0.10200167447328568\n",
      "epoch: 4 step: 951, loss is 0.0036530308425426483\n",
      "epoch: 4 step: 952, loss is 0.11239901185035706\n",
      "epoch: 4 step: 953, loss is 0.07455013692378998\n",
      "epoch: 4 step: 954, loss is 0.1820686161518097\n",
      "epoch: 4 step: 955, loss is 0.00792046170681715\n",
      "epoch: 4 step: 956, loss is 0.005369781516492367\n",
      "epoch: 4 step: 957, loss is 0.001865052036009729\n",
      "epoch: 4 step: 958, loss is 0.010020690970122814\n",
      "epoch: 4 step: 959, loss is 0.030574651435017586\n",
      "epoch: 4 step: 960, loss is 0.014447271823883057\n",
      "epoch: 4 step: 961, loss is 0.08515006303787231\n",
      "epoch: 4 step: 962, loss is 0.17700234055519104\n",
      "epoch: 4 step: 963, loss is 0.005815789569169283\n",
      "epoch: 4 step: 964, loss is 0.017339056357741356\n",
      "epoch: 4 step: 965, loss is 0.1839451789855957\n",
      "epoch: 4 step: 966, loss is 0.01875126361846924\n",
      "epoch: 4 step: 967, loss is 0.2788693308830261\n",
      "epoch: 4 step: 968, loss is 0.0010093586752191186\n",
      "epoch: 4 step: 969, loss is 0.08305858075618744\n",
      "epoch: 4 step: 970, loss is 0.05197782814502716\n",
      "epoch: 4 step: 971, loss is 0.011741787195205688\n",
      "epoch: 4 step: 972, loss is 0.0474105179309845\n",
      "epoch: 4 step: 973, loss is 0.028087273240089417\n",
      "epoch: 4 step: 974, loss is 0.0537450946867466\n",
      "epoch: 4 step: 975, loss is 0.1515478491783142\n",
      "epoch: 4 step: 976, loss is 0.08166655153036118\n",
      "epoch: 4 step: 977, loss is 0.05453312024474144\n",
      "epoch: 4 step: 978, loss is 0.20816001296043396\n",
      "epoch: 4 step: 979, loss is 0.03326278179883957\n",
      "epoch: 4 step: 980, loss is 0.1083439514040947\n",
      "epoch: 4 step: 981, loss is 0.006991223897784948\n",
      "epoch: 4 step: 982, loss is 0.052407316863536835\n",
      "epoch: 4 step: 983, loss is 0.004964842926710844\n",
      "epoch: 4 step: 984, loss is 0.015126741491258144\n",
      "epoch: 4 step: 985, loss is 0.01801467314362526\n",
      "epoch: 4 step: 986, loss is 0.07589855790138245\n",
      "epoch: 4 step: 987, loss is 0.1339874416589737\n",
      "epoch: 4 step: 988, loss is 0.11587784439325333\n",
      "epoch: 4 step: 989, loss is 0.17497114837169647\n",
      "epoch: 4 step: 990, loss is 0.010426961816847324\n",
      "epoch: 4 step: 991, loss is 0.1678876131772995\n",
      "epoch: 4 step: 992, loss is 0.0663047656416893\n",
      "epoch: 4 step: 993, loss is 0.008900457061827183\n",
      "epoch: 4 step: 994, loss is 0.09436491131782532\n",
      "epoch: 4 step: 995, loss is 0.014829965308308601\n",
      "epoch: 4 step: 996, loss is 0.006129179149866104\n",
      "epoch: 4 step: 997, loss is 0.03118051588535309\n",
      "epoch: 4 step: 998, loss is 0.007146637886762619\n",
      "epoch: 4 step: 999, loss is 0.017063595354557037\n",
      "epoch: 4 step: 1000, loss is 0.05030611902475357\n",
      "epoch: 4 step: 1001, loss is 0.09083282947540283\n",
      "epoch: 4 step: 1002, loss is 0.015199661254882812\n",
      "epoch: 4 step: 1003, loss is 0.22930963337421417\n",
      "epoch: 4 step: 1004, loss is 0.010302945040166378\n",
      "epoch: 4 step: 1005, loss is 0.035736650228500366\n",
      "epoch: 4 step: 1006, loss is 0.01014782302081585\n",
      "epoch: 4 step: 1007, loss is 0.16784816980361938\n",
      "epoch: 4 step: 1008, loss is 0.0013490138808265328\n",
      "epoch: 4 step: 1009, loss is 0.05243377387523651\n",
      "epoch: 4 step: 1010, loss is 0.053043756633996964\n",
      "epoch: 4 step: 1011, loss is 0.08307503163814545\n",
      "epoch: 4 step: 1012, loss is 0.012558115646243095\n",
      "epoch: 4 step: 1013, loss is 0.008858155459165573\n",
      "epoch: 4 step: 1014, loss is 0.013698323629796505\n",
      "epoch: 4 step: 1015, loss is 0.00892406702041626\n",
      "epoch: 4 step: 1016, loss is 0.0013734748354181647\n",
      "epoch: 4 step: 1017, loss is 0.0911366194486618\n",
      "epoch: 4 step: 1018, loss is 0.0031866624485701323\n",
      "epoch: 4 step: 1019, loss is 0.15835414826869965\n",
      "epoch: 4 step: 1020, loss is 0.0047792671248316765\n",
      "epoch: 4 step: 1021, loss is 0.10592640191316605\n",
      "epoch: 4 step: 1022, loss is 0.04925084114074707\n",
      "epoch: 4 step: 1023, loss is 0.08452852815389633\n",
      "epoch: 4 step: 1024, loss is 0.030478425323963165\n",
      "epoch: 4 step: 1025, loss is 0.033376116305589676\n",
      "epoch: 4 step: 1026, loss is 0.04675329476594925\n",
      "epoch: 4 step: 1027, loss is 0.11881640553474426\n",
      "epoch: 4 step: 1028, loss is 0.07727012038230896\n",
      "epoch: 4 step: 1029, loss is 0.03576061129570007\n",
      "epoch: 4 step: 1030, loss is 0.013896161690354347\n",
      "epoch: 4 step: 1031, loss is 0.11783645302057266\n",
      "epoch: 4 step: 1032, loss is 0.05256330594420433\n",
      "epoch: 4 step: 1033, loss is 0.05389263853430748\n",
      "epoch: 4 step: 1034, loss is 0.04644457995891571\n",
      "epoch: 4 step: 1035, loss is 0.02293573133647442\n",
      "epoch: 4 step: 1036, loss is 0.010484615340828896\n",
      "epoch: 4 step: 1037, loss is 0.00954018160700798\n",
      "epoch: 4 step: 1038, loss is 0.013908233493566513\n",
      "epoch: 4 step: 1039, loss is 0.03359103947877884\n",
      "epoch: 4 step: 1040, loss is 0.13905666768550873\n",
      "epoch: 4 step: 1041, loss is 0.036541711539030075\n",
      "epoch: 4 step: 1042, loss is 0.0743572786450386\n",
      "epoch: 4 step: 1043, loss is 0.11921216547489166\n",
      "epoch: 4 step: 1044, loss is 0.001580856740474701\n",
      "epoch: 4 step: 1045, loss is 0.03899195045232773\n",
      "epoch: 4 step: 1046, loss is 0.3093772828578949\n",
      "epoch: 4 step: 1047, loss is 0.019038880243897438\n",
      "epoch: 4 step: 1048, loss is 0.054269514977931976\n",
      "epoch: 4 step: 1049, loss is 0.0291491337120533\n",
      "epoch: 4 step: 1050, loss is 0.093193918466568\n",
      "epoch: 4 step: 1051, loss is 0.10738199949264526\n",
      "epoch: 4 step: 1052, loss is 0.06551670283079147\n",
      "epoch: 4 step: 1053, loss is 0.052917830646038055\n",
      "epoch: 4 step: 1054, loss is 0.049730874598026276\n",
      "epoch: 4 step: 1055, loss is 0.22759443521499634\n",
      "epoch: 4 step: 1056, loss is 0.028273290023207664\n",
      "epoch: 4 step: 1057, loss is 0.0044824290089309216\n",
      "epoch: 4 step: 1058, loss is 0.004100609105080366\n",
      "epoch: 4 step: 1059, loss is 0.06669309735298157\n",
      "epoch: 4 step: 1060, loss is 0.050006330013275146\n",
      "epoch: 4 step: 1061, loss is 0.01087790448218584\n",
      "epoch: 4 step: 1062, loss is 0.005476783495396376\n",
      "epoch: 4 step: 1063, loss is 0.012649890966713428\n",
      "epoch: 4 step: 1064, loss is 0.14203417301177979\n",
      "epoch: 4 step: 1065, loss is 0.3067249059677124\n",
      "epoch: 4 step: 1066, loss is 0.016001753509044647\n",
      "epoch: 4 step: 1067, loss is 0.14903540909290314\n",
      "epoch: 4 step: 1068, loss is 0.17960010468959808\n",
      "epoch: 4 step: 1069, loss is 0.07711206376552582\n",
      "epoch: 4 step: 1070, loss is 0.11799359321594238\n",
      "epoch: 4 step: 1071, loss is 0.0912914127111435\n",
      "epoch: 4 step: 1072, loss is 0.1541466861963272\n",
      "epoch: 4 step: 1073, loss is 0.016328323632478714\n",
      "epoch: 4 step: 1074, loss is 0.0709318295121193\n",
      "epoch: 4 step: 1075, loss is 0.38764604926109314\n",
      "epoch: 4 step: 1076, loss is 0.028933869674801826\n",
      "epoch: 4 step: 1077, loss is 0.007429982535541058\n",
      "epoch: 4 step: 1078, loss is 0.013364817947149277\n",
      "epoch: 4 step: 1079, loss is 0.000962056394200772\n",
      "epoch: 4 step: 1080, loss is 0.1386650949716568\n",
      "epoch: 4 step: 1081, loss is 0.09080269187688828\n",
      "epoch: 4 step: 1082, loss is 0.015716709196567535\n",
      "epoch: 4 step: 1083, loss is 0.07556433230638504\n",
      "epoch: 4 step: 1084, loss is 0.019478868693113327\n",
      "epoch: 4 step: 1085, loss is 0.11314751207828522\n",
      "epoch: 4 step: 1086, loss is 0.228448748588562\n",
      "epoch: 4 step: 1087, loss is 0.014912975952029228\n",
      "epoch: 4 step: 1088, loss is 0.03701126575469971\n",
      "epoch: 4 step: 1089, loss is 0.08197686076164246\n",
      "epoch: 4 step: 1090, loss is 0.04540756344795227\n",
      "epoch: 4 step: 1091, loss is 0.2784893214702606\n",
      "epoch: 4 step: 1092, loss is 0.00670753326267004\n",
      "epoch: 4 step: 1093, loss is 0.18021263182163239\n",
      "epoch: 4 step: 1094, loss is 0.0637676790356636\n",
      "epoch: 4 step: 1095, loss is 0.09606683999300003\n",
      "epoch: 4 step: 1096, loss is 0.2536192536354065\n",
      "epoch: 4 step: 1097, loss is 0.022369131445884705\n",
      "epoch: 4 step: 1098, loss is 0.018491441383957863\n",
      "epoch: 4 step: 1099, loss is 0.005997317377477884\n",
      "epoch: 4 step: 1100, loss is 0.033124394714832306\n",
      "epoch: 4 step: 1101, loss is 0.08399250358343124\n",
      "epoch: 4 step: 1102, loss is 0.022212885320186615\n",
      "epoch: 4 step: 1103, loss is 0.03079909458756447\n",
      "epoch: 4 step: 1104, loss is 0.028291866183280945\n",
      "epoch: 4 step: 1105, loss is 0.0995609238743782\n",
      "epoch: 4 step: 1106, loss is 0.007029829081147909\n",
      "epoch: 4 step: 1107, loss is 0.015739699825644493\n",
      "epoch: 4 step: 1108, loss is 0.00472433352842927\n",
      "epoch: 4 step: 1109, loss is 0.0060730427503585815\n",
      "epoch: 4 step: 1110, loss is 0.06874580681324005\n",
      "epoch: 4 step: 1111, loss is 0.11781599372625351\n",
      "epoch: 4 step: 1112, loss is 0.22491216659545898\n",
      "epoch: 4 step: 1113, loss is 0.02090361900627613\n",
      "epoch: 4 step: 1114, loss is 0.01883276365697384\n",
      "epoch: 4 step: 1115, loss is 0.03679388761520386\n",
      "epoch: 4 step: 1116, loss is 0.016134539619088173\n",
      "epoch: 4 step: 1117, loss is 0.10083645582199097\n",
      "epoch: 4 step: 1118, loss is 0.04334159940481186\n",
      "epoch: 4 step: 1119, loss is 0.14447642862796783\n",
      "epoch: 4 step: 1120, loss is 0.05511670559644699\n",
      "epoch: 4 step: 1121, loss is 0.323912113904953\n",
      "epoch: 4 step: 1122, loss is 0.03317045047879219\n",
      "epoch: 4 step: 1123, loss is 0.05010821670293808\n",
      "epoch: 4 step: 1124, loss is 0.002453388413414359\n",
      "epoch: 4 step: 1125, loss is 0.08949179202318192\n",
      "epoch: 4 step: 1126, loss is 0.1086609736084938\n",
      "epoch: 4 step: 1127, loss is 0.11109556257724762\n",
      "epoch: 4 step: 1128, loss is 0.014481284655630589\n",
      "epoch: 4 step: 1129, loss is 0.01493938360363245\n",
      "epoch: 4 step: 1130, loss is 0.037871796637773514\n",
      "epoch: 4 step: 1131, loss is 0.0014170752838253975\n",
      "epoch: 4 step: 1132, loss is 0.004350098315626383\n",
      "epoch: 4 step: 1133, loss is 0.002888563321903348\n",
      "epoch: 4 step: 1134, loss is 0.18956232070922852\n",
      "epoch: 4 step: 1135, loss is 0.039875343441963196\n",
      "epoch: 4 step: 1136, loss is 0.03201257437467575\n",
      "epoch: 4 step: 1137, loss is 0.008319594897329807\n",
      "epoch: 4 step: 1138, loss is 0.09200160950422287\n",
      "epoch: 4 step: 1139, loss is 0.010094728320837021\n",
      "epoch: 4 step: 1140, loss is 0.10452963411808014\n",
      "epoch: 4 step: 1141, loss is 0.08368513733148575\n",
      "epoch: 4 step: 1142, loss is 0.12048792093992233\n",
      "epoch: 4 step: 1143, loss is 0.005072891712188721\n",
      "epoch: 4 step: 1144, loss is 0.02282137982547283\n",
      "epoch: 4 step: 1145, loss is 0.11259898543357849\n",
      "epoch: 4 step: 1146, loss is 0.1300952136516571\n",
      "epoch: 4 step: 1147, loss is 0.0030757447239011526\n",
      "epoch: 4 step: 1148, loss is 0.012412204407155514\n",
      "epoch: 4 step: 1149, loss is 0.006252427119761705\n",
      "epoch: 4 step: 1150, loss is 0.06287892907857895\n",
      "epoch: 4 step: 1151, loss is 0.145051971077919\n",
      "epoch: 4 step: 1152, loss is 0.020364858210086823\n",
      "epoch: 4 step: 1153, loss is 0.371238648891449\n",
      "epoch: 4 step: 1154, loss is 0.012683306820690632\n",
      "epoch: 4 step: 1155, loss is 0.02727777138352394\n",
      "epoch: 4 step: 1156, loss is 0.06991443783044815\n",
      "epoch: 4 step: 1157, loss is 0.012953028082847595\n",
      "epoch: 4 step: 1158, loss is 0.016398627310991287\n",
      "epoch: 4 step: 1159, loss is 0.002710952190682292\n",
      "epoch: 4 step: 1160, loss is 0.004108797293156385\n",
      "epoch: 4 step: 1161, loss is 0.007438573054969311\n",
      "epoch: 4 step: 1162, loss is 0.05003420263528824\n",
      "epoch: 4 step: 1163, loss is 0.08024991303682327\n",
      "epoch: 4 step: 1164, loss is 0.026659028604626656\n",
      "epoch: 4 step: 1165, loss is 0.0021252064034342766\n",
      "epoch: 4 step: 1166, loss is 0.09449482709169388\n",
      "epoch: 4 step: 1167, loss is 0.22547121345996857\n",
      "epoch: 4 step: 1168, loss is 0.05610833689570427\n",
      "epoch: 4 step: 1169, loss is 0.08037856966257095\n",
      "epoch: 4 step: 1170, loss is 0.1282934993505478\n",
      "epoch: 4 step: 1171, loss is 0.07622801512479782\n",
      "epoch: 4 step: 1172, loss is 0.023114066570997238\n",
      "epoch: 4 step: 1173, loss is 0.04299337789416313\n",
      "epoch: 4 step: 1174, loss is 0.06825030595064163\n",
      "epoch: 4 step: 1175, loss is 0.07605654001235962\n",
      "epoch: 4 step: 1176, loss is 0.007237528450787067\n",
      "epoch: 4 step: 1177, loss is 0.18752403557300568\n",
      "epoch: 4 step: 1178, loss is 0.1104351282119751\n",
      "epoch: 4 step: 1179, loss is 0.22262683510780334\n",
      "epoch: 4 step: 1180, loss is 0.014790371060371399\n",
      "epoch: 4 step: 1181, loss is 0.20521144568920135\n",
      "epoch: 4 step: 1182, loss is 0.08768811821937561\n",
      "epoch: 4 step: 1183, loss is 0.1731278896331787\n",
      "epoch: 4 step: 1184, loss is 0.0036721588112413883\n",
      "epoch: 4 step: 1185, loss is 0.040373969823122025\n",
      "epoch: 4 step: 1186, loss is 0.10621500760316849\n",
      "epoch: 4 step: 1187, loss is 0.04625845327973366\n",
      "epoch: 4 step: 1188, loss is 0.036117423325777054\n",
      "epoch: 4 step: 1189, loss is 0.09898065030574799\n",
      "epoch: 4 step: 1190, loss is 0.061373271048069\n",
      "epoch: 4 step: 1191, loss is 0.17734579741954803\n",
      "epoch: 4 step: 1192, loss is 0.01577477715909481\n",
      "epoch: 4 step: 1193, loss is 0.018660038709640503\n",
      "epoch: 4 step: 1194, loss is 0.00518602691590786\n",
      "epoch: 4 step: 1195, loss is 0.021268853917717934\n",
      "epoch: 4 step: 1196, loss is 0.10568346083164215\n",
      "epoch: 4 step: 1197, loss is 0.0007192126358859241\n",
      "epoch: 4 step: 1198, loss is 0.00931769609451294\n",
      "epoch: 4 step: 1199, loss is 0.13946934044361115\n",
      "epoch: 4 step: 1200, loss is 0.07455228269100189\n",
      "epoch: 4 step: 1201, loss is 0.0036985643673688173\n",
      "epoch: 4 step: 1202, loss is 0.0003983069327659905\n",
      "epoch: 4 step: 1203, loss is 0.006342249922454357\n",
      "epoch: 4 step: 1204, loss is 0.12296871840953827\n",
      "epoch: 4 step: 1205, loss is 0.028714997693896294\n",
      "epoch: 4 step: 1206, loss is 0.0645768865942955\n",
      "epoch: 4 step: 1207, loss is 0.03487000986933708\n",
      "epoch: 4 step: 1208, loss is 0.020406555384397507\n",
      "epoch: 4 step: 1209, loss is 0.0935211107134819\n",
      "epoch: 4 step: 1210, loss is 0.08176779001951218\n",
      "epoch: 4 step: 1211, loss is 0.00899038091301918\n",
      "epoch: 4 step: 1212, loss is 0.02629142813384533\n",
      "epoch: 4 step: 1213, loss is 0.08330106735229492\n",
      "epoch: 4 step: 1214, loss is 0.07101558893918991\n",
      "epoch: 4 step: 1215, loss is 0.007787717971950769\n",
      "epoch: 4 step: 1216, loss is 0.10131195932626724\n",
      "epoch: 4 step: 1217, loss is 0.031741682440042496\n",
      "epoch: 4 step: 1218, loss is 0.00475312490016222\n",
      "epoch: 4 step: 1219, loss is 0.0030392499174922705\n",
      "epoch: 4 step: 1220, loss is 0.003320680931210518\n",
      "epoch: 4 step: 1221, loss is 0.006025967188179493\n",
      "epoch: 4 step: 1222, loss is 0.023349778726696968\n",
      "epoch: 4 step: 1223, loss is 0.002841811627149582\n",
      "epoch: 4 step: 1224, loss is 0.14969919621944427\n",
      "epoch: 4 step: 1225, loss is 0.05834595486521721\n",
      "epoch: 4 step: 1226, loss is 0.08577525615692139\n",
      "epoch: 4 step: 1227, loss is 0.13840225338935852\n",
      "epoch: 4 step: 1228, loss is 0.0711914524435997\n",
      "epoch: 4 step: 1229, loss is 0.01056516170501709\n",
      "epoch: 4 step: 1230, loss is 0.011804548092186451\n",
      "epoch: 4 step: 1231, loss is 0.002150576328858733\n",
      "epoch: 4 step: 1232, loss is 0.052958983927965164\n",
      "epoch: 4 step: 1233, loss is 0.056379321962594986\n",
      "epoch: 4 step: 1234, loss is 0.002725503873080015\n",
      "epoch: 4 step: 1235, loss is 0.13004665076732635\n",
      "epoch: 4 step: 1236, loss is 0.0014796084724366665\n",
      "epoch: 4 step: 1237, loss is 0.05028831958770752\n",
      "epoch: 4 step: 1238, loss is 0.016299013048410416\n",
      "epoch: 4 step: 1239, loss is 0.03159850835800171\n",
      "epoch: 4 step: 1240, loss is 0.05067068710923195\n",
      "epoch: 4 step: 1241, loss is 0.030756371095776558\n",
      "epoch: 4 step: 1242, loss is 0.026348618790507317\n",
      "epoch: 4 step: 1243, loss is 0.20776547491550446\n",
      "epoch: 4 step: 1244, loss is 0.0014587061014026403\n",
      "epoch: 4 step: 1245, loss is 0.0006122503546066582\n",
      "epoch: 4 step: 1246, loss is 0.024174632504582405\n",
      "epoch: 4 step: 1247, loss is 0.031880877912044525\n",
      "epoch: 4 step: 1248, loss is 0.006776375696063042\n",
      "epoch: 4 step: 1249, loss is 0.07220766693353653\n",
      "epoch: 4 step: 1250, loss is 0.12309122830629349\n",
      "epoch: 4 step: 1251, loss is 0.05865537002682686\n",
      "epoch: 4 step: 1252, loss is 0.03486311063170433\n",
      "epoch: 4 step: 1253, loss is 0.005418159533292055\n",
      "epoch: 4 step: 1254, loss is 0.10079467296600342\n",
      "epoch: 4 step: 1255, loss is 0.0023499485105276108\n",
      "epoch: 4 step: 1256, loss is 0.06241164356470108\n",
      "epoch: 4 step: 1257, loss is 0.11784333735704422\n",
      "epoch: 4 step: 1258, loss is 0.054381679743528366\n",
      "epoch: 4 step: 1259, loss is 0.0397922620177269\n",
      "epoch: 4 step: 1260, loss is 0.058094240725040436\n",
      "epoch: 4 step: 1261, loss is 0.02081947773694992\n",
      "epoch: 4 step: 1262, loss is 0.024413535371422768\n",
      "epoch: 4 step: 1263, loss is 0.12646637856960297\n",
      "epoch: 4 step: 1264, loss is 0.05021284893155098\n",
      "epoch: 4 step: 1265, loss is 0.014223594218492508\n",
      "epoch: 4 step: 1266, loss is 0.1127053052186966\n",
      "epoch: 4 step: 1267, loss is 0.025757063180208206\n",
      "epoch: 4 step: 1268, loss is 0.01274014450609684\n",
      "epoch: 4 step: 1269, loss is 0.061733320355415344\n",
      "epoch: 4 step: 1270, loss is 0.017314519733190536\n",
      "epoch: 4 step: 1271, loss is 0.215583935379982\n",
      "epoch: 4 step: 1272, loss is 0.05359053611755371\n",
      "epoch: 4 step: 1273, loss is 0.05330228433012962\n",
      "epoch: 4 step: 1274, loss is 0.018971305340528488\n",
      "epoch: 4 step: 1275, loss is 0.08338762074708939\n",
      "epoch: 4 step: 1276, loss is 0.043244849890470505\n",
      "epoch: 4 step: 1277, loss is 0.0545773059129715\n",
      "epoch: 4 step: 1278, loss is 0.20806825160980225\n",
      "epoch: 4 step: 1279, loss is 0.4021633565425873\n",
      "epoch: 4 step: 1280, loss is 0.08056893944740295\n",
      "epoch: 4 step: 1281, loss is 0.05552180856466293\n",
      "epoch: 4 step: 1282, loss is 0.006362557411193848\n",
      "epoch: 4 step: 1283, loss is 0.014311181381344795\n",
      "epoch: 4 step: 1284, loss is 0.21376492083072662\n",
      "epoch: 4 step: 1285, loss is 0.016536980867385864\n",
      "epoch: 4 step: 1286, loss is 0.005851388443261385\n",
      "epoch: 4 step: 1287, loss is 0.12838701903820038\n",
      "epoch: 4 step: 1288, loss is 0.10906907916069031\n",
      "epoch: 4 step: 1289, loss is 0.001996635925024748\n",
      "epoch: 4 step: 1290, loss is 0.030385617166757584\n",
      "epoch: 4 step: 1291, loss is 0.07755225896835327\n",
      "epoch: 4 step: 1292, loss is 0.1622893065214157\n",
      "epoch: 4 step: 1293, loss is 0.09447453916072845\n",
      "epoch: 4 step: 1294, loss is 0.031070707365870476\n",
      "epoch: 4 step: 1295, loss is 0.08475249260663986\n",
      "epoch: 4 step: 1296, loss is 0.001482411869801581\n",
      "epoch: 4 step: 1297, loss is 0.03210119530558586\n",
      "epoch: 4 step: 1298, loss is 0.017908450216054916\n",
      "epoch: 4 step: 1299, loss is 0.02685227058827877\n",
      "epoch: 4 step: 1300, loss is 0.006756035145372152\n",
      "epoch: 4 step: 1301, loss is 0.022799743339419365\n",
      "epoch: 4 step: 1302, loss is 0.019645223394036293\n",
      "epoch: 4 step: 1303, loss is 0.02078825607895851\n",
      "epoch: 4 step: 1304, loss is 0.2988412082195282\n",
      "epoch: 4 step: 1305, loss is 0.13588054478168488\n",
      "epoch: 4 step: 1306, loss is 0.03785098344087601\n",
      "epoch: 4 step: 1307, loss is 0.018836230039596558\n",
      "epoch: 4 step: 1308, loss is 0.0871196910738945\n",
      "epoch: 4 step: 1309, loss is 0.07204429805278778\n",
      "epoch: 4 step: 1310, loss is 0.016851477324962616\n",
      "epoch: 4 step: 1311, loss is 0.023499511182308197\n",
      "epoch: 4 step: 1312, loss is 0.09078740328550339\n",
      "epoch: 4 step: 1313, loss is 0.00856211967766285\n",
      "epoch: 4 step: 1314, loss is 0.04908180236816406\n",
      "epoch: 4 step: 1315, loss is 0.0027311008889228106\n",
      "epoch: 4 step: 1316, loss is 0.009131568484008312\n",
      "epoch: 4 step: 1317, loss is 0.020539486780762672\n",
      "epoch: 4 step: 1318, loss is 0.08308586478233337\n",
      "epoch: 4 step: 1319, loss is 0.022349288687109947\n",
      "epoch: 4 step: 1320, loss is 0.05142093822360039\n",
      "epoch: 4 step: 1321, loss is 0.022586019709706306\n",
      "epoch: 4 step: 1322, loss is 0.008296134881675243\n",
      "epoch: 4 step: 1323, loss is 0.043468061834573746\n",
      "epoch: 4 step: 1324, loss is 0.07732732594013214\n",
      "epoch: 4 step: 1325, loss is 0.02943788841366768\n",
      "epoch: 4 step: 1326, loss is 0.004014205187559128\n",
      "epoch: 4 step: 1327, loss is 0.061722759157419205\n",
      "epoch: 4 step: 1328, loss is 0.06715303659439087\n",
      "epoch: 4 step: 1329, loss is 0.03174604848027229\n",
      "epoch: 4 step: 1330, loss is 0.017627209424972534\n",
      "epoch: 4 step: 1331, loss is 0.14056970179080963\n",
      "epoch: 4 step: 1332, loss is 0.004459253512322903\n",
      "epoch: 4 step: 1333, loss is 0.02108764834702015\n",
      "epoch: 4 step: 1334, loss is 0.05120747536420822\n",
      "epoch: 4 step: 1335, loss is 0.004132557660341263\n",
      "epoch: 4 step: 1336, loss is 0.014300165697932243\n",
      "epoch: 4 step: 1337, loss is 0.1291186511516571\n",
      "epoch: 4 step: 1338, loss is 0.07434695214033127\n",
      "epoch: 4 step: 1339, loss is 0.0040410952642560005\n",
      "epoch: 4 step: 1340, loss is 0.09455277025699615\n",
      "epoch: 4 step: 1341, loss is 0.028197305276989937\n",
      "epoch: 4 step: 1342, loss is 0.01722736656665802\n",
      "epoch: 4 step: 1343, loss is 0.0013093281304463744\n",
      "epoch: 4 step: 1344, loss is 0.023532267659902573\n",
      "epoch: 4 step: 1345, loss is 0.0773928165435791\n",
      "epoch: 4 step: 1346, loss is 0.006966781802475452\n",
      "epoch: 4 step: 1347, loss is 0.02600526250898838\n",
      "epoch: 4 step: 1348, loss is 0.005141196306794882\n",
      "epoch: 4 step: 1349, loss is 0.06339258700609207\n",
      "epoch: 4 step: 1350, loss is 0.11331456154584885\n",
      "epoch: 4 step: 1351, loss is 0.04350854828953743\n",
      "epoch: 4 step: 1352, loss is 0.04908866062760353\n",
      "epoch: 4 step: 1353, loss is 0.06787703931331635\n",
      "epoch: 4 step: 1354, loss is 0.004025797359645367\n",
      "epoch: 4 step: 1355, loss is 0.22603341937065125\n",
      "epoch: 4 step: 1356, loss is 0.05165361240506172\n",
      "epoch: 4 step: 1357, loss is 0.014611093327403069\n",
      "epoch: 4 step: 1358, loss is 0.02514997310936451\n",
      "epoch: 4 step: 1359, loss is 0.06636150181293488\n",
      "epoch: 4 step: 1360, loss is 0.10425203293561935\n",
      "epoch: 4 step: 1361, loss is 0.2765902578830719\n",
      "epoch: 4 step: 1362, loss is 0.17526358366012573\n",
      "epoch: 4 step: 1363, loss is 0.33196884393692017\n",
      "epoch: 4 step: 1364, loss is 0.0013933787122368813\n",
      "epoch: 4 step: 1365, loss is 0.037599217146635056\n",
      "epoch: 4 step: 1366, loss is 0.03788282722234726\n",
      "epoch: 4 step: 1367, loss is 0.11007892340421677\n",
      "epoch: 4 step: 1368, loss is 0.003347984282299876\n",
      "epoch: 4 step: 1369, loss is 0.020084064453840256\n",
      "epoch: 4 step: 1370, loss is 0.07015227526426315\n",
      "epoch: 4 step: 1371, loss is 0.25156551599502563\n",
      "epoch: 4 step: 1372, loss is 0.08907780051231384\n",
      "epoch: 4 step: 1373, loss is 0.14589278399944305\n",
      "epoch: 4 step: 1374, loss is 0.011022652499377728\n",
      "epoch: 4 step: 1375, loss is 0.16294021904468536\n",
      "epoch: 4 step: 1376, loss is 0.09479229897260666\n",
      "epoch: 4 step: 1377, loss is 0.05322161316871643\n",
      "epoch: 4 step: 1378, loss is 0.2479555755853653\n",
      "epoch: 4 step: 1379, loss is 0.0711977481842041\n",
      "epoch: 4 step: 1380, loss is 0.014847122132778168\n",
      "epoch: 4 step: 1381, loss is 0.03288482502102852\n",
      "epoch: 4 step: 1382, loss is 0.013796265237033367\n",
      "epoch: 4 step: 1383, loss is 0.43304795026779175\n",
      "epoch: 4 step: 1384, loss is 0.002152674598619342\n",
      "epoch: 4 step: 1385, loss is 0.060132041573524475\n",
      "epoch: 4 step: 1386, loss is 0.20054543018341064\n",
      "epoch: 4 step: 1387, loss is 0.025209657847881317\n",
      "epoch: 4 step: 1388, loss is 0.003790812101215124\n",
      "epoch: 4 step: 1389, loss is 0.0861395075917244\n",
      "epoch: 4 step: 1390, loss is 0.03737727180123329\n",
      "epoch: 4 step: 1391, loss is 0.11350087821483612\n",
      "epoch: 4 step: 1392, loss is 0.0038528209552168846\n",
      "epoch: 4 step: 1393, loss is 0.054363761097192764\n",
      "epoch: 4 step: 1394, loss is 0.012168535962700844\n",
      "epoch: 4 step: 1395, loss is 0.010999061167240143\n",
      "epoch: 4 step: 1396, loss is 0.15168529748916626\n",
      "epoch: 4 step: 1397, loss is 0.07874646782875061\n",
      "epoch: 4 step: 1398, loss is 0.03188241273164749\n",
      "epoch: 4 step: 1399, loss is 0.02271846868097782\n",
      "epoch: 4 step: 1400, loss is 0.10353537648916245\n",
      "epoch: 4 step: 1401, loss is 0.05759787559509277\n",
      "epoch: 4 step: 1402, loss is 0.4131736159324646\n",
      "epoch: 4 step: 1403, loss is 0.04987936094403267\n",
      "epoch: 4 step: 1404, loss is 0.06612229347229004\n",
      "epoch: 4 step: 1405, loss is 0.043747395277023315\n",
      "epoch: 4 step: 1406, loss is 0.017536185681819916\n",
      "epoch: 4 step: 1407, loss is 0.10493174940347672\n",
      "epoch: 4 step: 1408, loss is 0.05849236622452736\n",
      "epoch: 4 step: 1409, loss is 0.14865310490131378\n",
      "epoch: 4 step: 1410, loss is 0.24353119730949402\n",
      "epoch: 4 step: 1411, loss is 0.07140804827213287\n",
      "epoch: 4 step: 1412, loss is 0.005593238864094019\n",
      "epoch: 4 step: 1413, loss is 0.12250135093927383\n",
      "epoch: 4 step: 1414, loss is 0.10493719577789307\n",
      "epoch: 4 step: 1415, loss is 0.1471184492111206\n",
      "epoch: 4 step: 1416, loss is 0.13113394379615784\n",
      "epoch: 4 step: 1417, loss is 0.03381572663784027\n",
      "epoch: 4 step: 1418, loss is 0.015024539083242416\n",
      "epoch: 4 step: 1419, loss is 0.08620412647724152\n",
      "epoch: 4 step: 1420, loss is 0.014968079514801502\n",
      "epoch: 4 step: 1421, loss is 0.007153214421123266\n",
      "epoch: 4 step: 1422, loss is 0.07737306505441666\n",
      "epoch: 4 step: 1423, loss is 0.11749516427516937\n",
      "epoch: 4 step: 1424, loss is 0.0852428749203682\n",
      "epoch: 4 step: 1425, loss is 0.04206693544983864\n",
      "epoch: 4 step: 1426, loss is 0.0904075875878334\n",
      "epoch: 4 step: 1427, loss is 0.003006270155310631\n",
      "epoch: 4 step: 1428, loss is 0.05622470751404762\n",
      "epoch: 4 step: 1429, loss is 0.02133459970355034\n",
      "epoch: 4 step: 1430, loss is 0.11251626908779144\n",
      "epoch: 4 step: 1431, loss is 0.06476666033267975\n",
      "epoch: 4 step: 1432, loss is 0.05304509028792381\n",
      "epoch: 4 step: 1433, loss is 0.3016529679298401\n",
      "epoch: 4 step: 1434, loss is 0.020934855565428734\n",
      "epoch: 4 step: 1435, loss is 0.045456357300281525\n",
      "epoch: 4 step: 1436, loss is 0.13992486894130707\n",
      "epoch: 4 step: 1437, loss is 0.040045496076345444\n",
      "epoch: 4 step: 1438, loss is 0.24025896191596985\n",
      "epoch: 4 step: 1439, loss is 0.009123957715928555\n",
      "epoch: 4 step: 1440, loss is 0.3032718002796173\n",
      "epoch: 4 step: 1441, loss is 0.2136463075876236\n",
      "epoch: 4 step: 1442, loss is 0.0007231053896248341\n",
      "epoch: 4 step: 1443, loss is 0.1167987585067749\n",
      "epoch: 4 step: 1444, loss is 0.07629358023405075\n",
      "epoch: 4 step: 1445, loss is 0.23248210549354553\n",
      "epoch: 4 step: 1446, loss is 0.15671411156654358\n",
      "epoch: 4 step: 1447, loss is 0.10993500798940659\n",
      "epoch: 4 step: 1448, loss is 0.18581365048885345\n",
      "epoch: 4 step: 1449, loss is 0.019305070862174034\n",
      "epoch: 4 step: 1450, loss is 0.11276480555534363\n",
      "epoch: 4 step: 1451, loss is 0.06657649576663971\n",
      "epoch: 4 step: 1452, loss is 0.028788719326257706\n",
      "epoch: 4 step: 1453, loss is 0.02507905475795269\n",
      "epoch: 4 step: 1454, loss is 0.030158335343003273\n",
      "epoch: 4 step: 1455, loss is 0.10121974349021912\n",
      "epoch: 4 step: 1456, loss is 0.13163433969020844\n",
      "epoch: 4 step: 1457, loss is 0.042642742395401\n",
      "epoch: 4 step: 1458, loss is 0.040860798209905624\n",
      "epoch: 4 step: 1459, loss is 0.044711679220199585\n",
      "epoch: 4 step: 1460, loss is 0.022266685962677002\n",
      "epoch: 4 step: 1461, loss is 0.343460351228714\n",
      "epoch: 4 step: 1462, loss is 0.08787497133016586\n",
      "epoch: 4 step: 1463, loss is 0.03715219721198082\n",
      "epoch: 4 step: 1464, loss is 0.01967678591609001\n",
      "epoch: 4 step: 1465, loss is 0.02027881145477295\n",
      "epoch: 4 step: 1466, loss is 0.022188998758792877\n",
      "epoch: 4 step: 1467, loss is 0.16385619342327118\n",
      "epoch: 4 step: 1468, loss is 0.012123627588152885\n",
      "epoch: 4 step: 1469, loss is 0.04875299334526062\n",
      "epoch: 4 step: 1470, loss is 0.01654154807329178\n",
      "epoch: 4 step: 1471, loss is 0.04222230985760689\n",
      "epoch: 4 step: 1472, loss is 0.02343999408185482\n",
      "epoch: 4 step: 1473, loss is 0.0754571482539177\n",
      "epoch: 4 step: 1474, loss is 0.04219009727239609\n",
      "epoch: 4 step: 1475, loss is 0.16070447862148285\n",
      "epoch: 4 step: 1476, loss is 0.039579425007104874\n",
      "epoch: 4 step: 1477, loss is 0.08197921514511108\n",
      "epoch: 4 step: 1478, loss is 0.0550801083445549\n",
      "epoch: 4 step: 1479, loss is 0.05868976563215256\n",
      "epoch: 4 step: 1480, loss is 0.048816755414009094\n",
      "epoch: 4 step: 1481, loss is 0.02676265873014927\n",
      "epoch: 4 step: 1482, loss is 0.07219318300485611\n",
      "epoch: 4 step: 1483, loss is 0.047956839203834534\n",
      "epoch: 4 step: 1484, loss is 0.023860754445195198\n",
      "epoch: 4 step: 1485, loss is 0.05192958191037178\n",
      "epoch: 4 step: 1486, loss is 0.006598434876650572\n",
      "epoch: 4 step: 1487, loss is 0.1552462875843048\n",
      "epoch: 4 step: 1488, loss is 0.021511448547244072\n",
      "epoch: 4 step: 1489, loss is 0.04714778810739517\n",
      "epoch: 4 step: 1490, loss is 0.00955776497721672\n",
      "epoch: 4 step: 1491, loss is 0.125032439827919\n",
      "epoch: 4 step: 1492, loss is 0.10567986220121384\n",
      "epoch: 4 step: 1493, loss is 0.028860315680503845\n",
      "epoch: 4 step: 1494, loss is 0.001441096537746489\n",
      "epoch: 4 step: 1495, loss is 0.10241658240556717\n",
      "epoch: 4 step: 1496, loss is 0.005004714708775282\n",
      "epoch: 4 step: 1497, loss is 0.1748826503753662\n",
      "epoch: 4 step: 1498, loss is 0.05967215821146965\n",
      "epoch: 4 step: 1499, loss is 0.17183664441108704\n",
      "epoch: 4 step: 1500, loss is 0.013253741897642612\n",
      "epoch: 4 step: 1501, loss is 0.003219293663278222\n",
      "epoch: 4 step: 1502, loss is 0.1576874703168869\n",
      "epoch: 4 step: 1503, loss is 0.042251355946063995\n",
      "epoch: 4 step: 1504, loss is 0.030895475298166275\n",
      "epoch: 4 step: 1505, loss is 0.027175962924957275\n",
      "epoch: 4 step: 1506, loss is 0.03817210718989372\n",
      "epoch: 4 step: 1507, loss is 0.14786821603775024\n",
      "epoch: 4 step: 1508, loss is 0.10248935967683792\n",
      "epoch: 4 step: 1509, loss is 0.22634519636631012\n",
      "epoch: 4 step: 1510, loss is 0.017901141196489334\n",
      "epoch: 4 step: 1511, loss is 0.04853171855211258\n",
      "epoch: 4 step: 1512, loss is 0.0027045004535466433\n",
      "epoch: 4 step: 1513, loss is 0.005166776943951845\n",
      "epoch: 4 step: 1514, loss is 0.002009572694078088\n",
      "epoch: 4 step: 1515, loss is 0.02763628400862217\n",
      "epoch: 4 step: 1516, loss is 0.0043167369440197945\n",
      "epoch: 4 step: 1517, loss is 0.014750583097338676\n",
      "epoch: 4 step: 1518, loss is 0.008978191763162613\n",
      "epoch: 4 step: 1519, loss is 0.3177151381969452\n",
      "epoch: 4 step: 1520, loss is 0.012805094011127949\n",
      "epoch: 4 step: 1521, loss is 0.13380123674869537\n",
      "epoch: 4 step: 1522, loss is 0.03952508419752121\n",
      "epoch: 4 step: 1523, loss is 0.009434482082724571\n",
      "epoch: 4 step: 1524, loss is 0.015012328512966633\n",
      "epoch: 4 step: 1525, loss is 0.11384548246860504\n",
      "epoch: 4 step: 1526, loss is 0.014160110615193844\n",
      "epoch: 4 step: 1527, loss is 0.013623565435409546\n",
      "epoch: 4 step: 1528, loss is 0.102186419069767\n",
      "epoch: 4 step: 1529, loss is 0.015425393357872963\n",
      "epoch: 4 step: 1530, loss is 0.07733455300331116\n",
      "epoch: 4 step: 1531, loss is 0.10361865162849426\n",
      "epoch: 4 step: 1532, loss is 0.17358523607254028\n",
      "epoch: 4 step: 1533, loss is 0.050467174500226974\n",
      "epoch: 4 step: 1534, loss is 0.017646662890911102\n",
      "epoch: 4 step: 1535, loss is 0.0594584085047245\n",
      "epoch: 4 step: 1536, loss is 0.027314351871609688\n",
      "epoch: 4 step: 1537, loss is 0.12058014422655106\n",
      "epoch: 4 step: 1538, loss is 0.08243371546268463\n",
      "epoch: 4 step: 1539, loss is 0.0897078737616539\n",
      "epoch: 4 step: 1540, loss is 0.00966659840196371\n",
      "epoch: 4 step: 1541, loss is 0.009163947775959969\n",
      "epoch: 4 step: 1542, loss is 0.05051349848508835\n",
      "epoch: 4 step: 1543, loss is 0.03506752476096153\n",
      "epoch: 4 step: 1544, loss is 0.11113020032644272\n",
      "epoch: 4 step: 1545, loss is 0.0029933557379990816\n",
      "epoch: 4 step: 1546, loss is 0.08415355533361435\n",
      "epoch: 4 step: 1547, loss is 0.0023141067940741777\n",
      "epoch: 4 step: 1548, loss is 0.11448080837726593\n",
      "epoch: 4 step: 1549, loss is 0.13924455642700195\n",
      "epoch: 4 step: 1550, loss is 0.07259755581617355\n",
      "epoch: 4 step: 1551, loss is 0.0879502221941948\n",
      "epoch: 4 step: 1552, loss is 0.008141432888805866\n",
      "epoch: 4 step: 1553, loss is 0.01823277585208416\n",
      "epoch: 4 step: 1554, loss is 0.010905219241976738\n",
      "epoch: 4 step: 1555, loss is 0.05712166056036949\n",
      "epoch: 4 step: 1556, loss is 0.015553072094917297\n",
      "epoch: 4 step: 1557, loss is 0.13687486946582794\n",
      "epoch: 4 step: 1558, loss is 0.0032723667100071907\n",
      "epoch: 4 step: 1559, loss is 0.011339038610458374\n",
      "epoch: 4 step: 1560, loss is 0.07410886138677597\n",
      "epoch: 4 step: 1561, loss is 0.018369529396295547\n",
      "epoch: 4 step: 1562, loss is 0.053674984723329544\n",
      "epoch: 4 step: 1563, loss is 0.05121835693717003\n",
      "epoch: 4 step: 1564, loss is 0.03210494667291641\n",
      "epoch: 4 step: 1565, loss is 0.016167744994163513\n",
      "epoch: 4 step: 1566, loss is 0.032490141689777374\n",
      "epoch: 4 step: 1567, loss is 0.09250085055828094\n",
      "epoch: 4 step: 1568, loss is 0.009845282882452011\n",
      "epoch: 4 step: 1569, loss is 0.0024003221187740564\n",
      "epoch: 4 step: 1570, loss is 0.050594184547662735\n",
      "epoch: 4 step: 1571, loss is 0.01870540902018547\n",
      "epoch: 4 step: 1572, loss is 0.010396560654044151\n",
      "epoch: 4 step: 1573, loss is 0.08288363367319107\n",
      "epoch: 4 step: 1574, loss is 0.07790298014879227\n",
      "epoch: 4 step: 1575, loss is 0.06072868034243584\n",
      "epoch: 4 step: 1576, loss is 0.10624267905950546\n",
      "epoch: 4 step: 1577, loss is 0.017334138974547386\n",
      "epoch: 4 step: 1578, loss is 0.029727905988693237\n",
      "epoch: 4 step: 1579, loss is 0.04816139116883278\n",
      "epoch: 4 step: 1580, loss is 0.09285598248243332\n",
      "epoch: 4 step: 1581, loss is 0.1477411538362503\n",
      "epoch: 4 step: 1582, loss is 0.10219860076904297\n",
      "epoch: 4 step: 1583, loss is 0.02440008521080017\n",
      "epoch: 4 step: 1584, loss is 0.041570521891117096\n",
      "epoch: 4 step: 1585, loss is 0.15069304406642914\n",
      "epoch: 4 step: 1586, loss is 0.016043879091739655\n",
      "epoch: 4 step: 1587, loss is 0.0672750249505043\n",
      "epoch: 4 step: 1588, loss is 0.0006205159006640315\n",
      "epoch: 4 step: 1589, loss is 0.0148462587967515\n",
      "epoch: 4 step: 1590, loss is 0.19905708730220795\n",
      "epoch: 4 step: 1591, loss is 0.026288650929927826\n",
      "epoch: 4 step: 1592, loss is 0.0030533268582075834\n",
      "epoch: 4 step: 1593, loss is 0.0648866519331932\n",
      "epoch: 4 step: 1594, loss is 0.045659489929676056\n",
      "epoch: 4 step: 1595, loss is 0.018186816945672035\n",
      "epoch: 4 step: 1596, loss is 0.0822153389453888\n",
      "epoch: 4 step: 1597, loss is 0.001186935231089592\n",
      "epoch: 4 step: 1598, loss is 0.06700507551431656\n",
      "epoch: 4 step: 1599, loss is 0.01939874142408371\n",
      "epoch: 4 step: 1600, loss is 0.20612341165542603\n",
      "epoch: 4 step: 1601, loss is 0.022094344720244408\n",
      "epoch: 4 step: 1602, loss is 0.18323679268360138\n",
      "epoch: 4 step: 1603, loss is 0.022076591849327087\n",
      "epoch: 4 step: 1604, loss is 0.1893390715122223\n",
      "epoch: 4 step: 1605, loss is 0.002844269387423992\n",
      "epoch: 4 step: 1606, loss is 0.1086571142077446\n",
      "epoch: 4 step: 1607, loss is 0.028682047501206398\n",
      "epoch: 4 step: 1608, loss is 0.09718985855579376\n",
      "epoch: 4 step: 1609, loss is 0.08929762244224548\n",
      "epoch: 4 step: 1610, loss is 0.02767443284392357\n",
      "epoch: 4 step: 1611, loss is 0.05485484376549721\n",
      "epoch: 4 step: 1612, loss is 0.028609737753868103\n",
      "epoch: 4 step: 1613, loss is 0.003617448965087533\n",
      "epoch: 4 step: 1614, loss is 0.12560193240642548\n",
      "epoch: 4 step: 1615, loss is 0.015148671343922615\n",
      "epoch: 4 step: 1616, loss is 0.10435686260461807\n",
      "epoch: 4 step: 1617, loss is 0.17741449177265167\n",
      "epoch: 4 step: 1618, loss is 0.012325070798397064\n",
      "epoch: 4 step: 1619, loss is 0.05850233510136604\n",
      "epoch: 4 step: 1620, loss is 0.12844081223011017\n",
      "epoch: 4 step: 1621, loss is 0.015588110312819481\n",
      "epoch: 4 step: 1622, loss is 0.0548773817718029\n",
      "epoch: 4 step: 1623, loss is 0.0702759400010109\n",
      "epoch: 4 step: 1624, loss is 0.0012828136095777154\n",
      "epoch: 4 step: 1625, loss is 0.06400686502456665\n",
      "epoch: 4 step: 1626, loss is 0.05927092954516411\n",
      "epoch: 4 step: 1627, loss is 0.043983131647109985\n",
      "epoch: 4 step: 1628, loss is 0.0381753146648407\n",
      "epoch: 4 step: 1629, loss is 0.35652145743370056\n",
      "epoch: 4 step: 1630, loss is 0.20190337300300598\n",
      "epoch: 4 step: 1631, loss is 0.07281939685344696\n",
      "epoch: 4 step: 1632, loss is 0.012730376794934273\n",
      "epoch: 4 step: 1633, loss is 0.126325786113739\n",
      "epoch: 4 step: 1634, loss is 0.16721051931381226\n",
      "epoch: 4 step: 1635, loss is 0.1873895674943924\n",
      "epoch: 4 step: 1636, loss is 0.007163006346672773\n",
      "epoch: 4 step: 1637, loss is 0.025310074910521507\n",
      "epoch: 4 step: 1638, loss is 0.003366616787388921\n",
      "epoch: 4 step: 1639, loss is 0.010183771140873432\n",
      "epoch: 4 step: 1640, loss is 0.0023431843146681786\n",
      "epoch: 4 step: 1641, loss is 0.05211209878325462\n",
      "epoch: 4 step: 1642, loss is 0.05012952536344528\n",
      "epoch: 4 step: 1643, loss is 0.00738736754283309\n",
      "epoch: 4 step: 1644, loss is 0.15551447868347168\n",
      "epoch: 4 step: 1645, loss is 0.009610914625227451\n",
      "epoch: 4 step: 1646, loss is 0.020894141867756844\n",
      "epoch: 4 step: 1647, loss is 0.011412500403821468\n",
      "epoch: 4 step: 1648, loss is 0.3269600570201874\n",
      "epoch: 4 step: 1649, loss is 0.00786563940346241\n",
      "epoch: 4 step: 1650, loss is 0.10853054374456406\n",
      "epoch: 4 step: 1651, loss is 0.09880410879850388\n",
      "epoch: 4 step: 1652, loss is 0.007844910956919193\n",
      "epoch: 4 step: 1653, loss is 0.01451217569410801\n",
      "epoch: 4 step: 1654, loss is 0.028933312743902206\n",
      "epoch: 4 step: 1655, loss is 0.0035442467778921127\n",
      "epoch: 4 step: 1656, loss is 0.25280240178108215\n",
      "epoch: 4 step: 1657, loss is 0.018395643681287766\n",
      "epoch: 4 step: 1658, loss is 0.08459301292896271\n",
      "epoch: 4 step: 1659, loss is 0.05138334259390831\n",
      "epoch: 4 step: 1660, loss is 0.022804507985711098\n",
      "epoch: 4 step: 1661, loss is 0.07259687781333923\n",
      "epoch: 4 step: 1662, loss is 0.0017942935228347778\n",
      "epoch: 4 step: 1663, loss is 0.20754067599773407\n",
      "epoch: 4 step: 1664, loss is 0.09198103845119476\n",
      "epoch: 4 step: 1665, loss is 0.010104760527610779\n",
      "epoch: 4 step: 1666, loss is 0.05771734192967415\n",
      "epoch: 4 step: 1667, loss is 0.015523510053753853\n",
      "epoch: 4 step: 1668, loss is 0.03730294108390808\n",
      "epoch: 4 step: 1669, loss is 0.0338079035282135\n",
      "epoch: 4 step: 1670, loss is 0.015124764293432236\n",
      "epoch: 4 step: 1671, loss is 0.09098204970359802\n",
      "epoch: 4 step: 1672, loss is 0.12561491131782532\n",
      "epoch: 4 step: 1673, loss is 0.021080048754811287\n",
      "epoch: 4 step: 1674, loss is 0.1005355641245842\n",
      "epoch: 4 step: 1675, loss is 0.03549725562334061\n",
      "epoch: 4 step: 1676, loss is 0.03660394996404648\n",
      "epoch: 4 step: 1677, loss is 0.01573646441102028\n",
      "epoch: 4 step: 1678, loss is 0.042566001415252686\n",
      "epoch: 4 step: 1679, loss is 0.027404384687542915\n",
      "epoch: 4 step: 1680, loss is 0.09886036813259125\n",
      "epoch: 4 step: 1681, loss is 0.07296016067266464\n",
      "epoch: 4 step: 1682, loss is 0.0648498386144638\n",
      "epoch: 4 step: 1683, loss is 0.06517176330089569\n",
      "epoch: 4 step: 1684, loss is 0.36687660217285156\n",
      "epoch: 4 step: 1685, loss is 0.01746637560427189\n",
      "epoch: 4 step: 1686, loss is 0.03780841827392578\n",
      "epoch: 4 step: 1687, loss is 0.07495883852243423\n",
      "epoch: 4 step: 1688, loss is 0.005543174222111702\n",
      "epoch: 4 step: 1689, loss is 0.0383969321846962\n",
      "epoch: 4 step: 1690, loss is 0.12683440744876862\n",
      "epoch: 4 step: 1691, loss is 0.0026717553846538067\n",
      "epoch: 4 step: 1692, loss is 0.005651318468153477\n",
      "epoch: 4 step: 1693, loss is 0.18814672529697418\n",
      "epoch: 4 step: 1694, loss is 0.0022123989183455706\n",
      "epoch: 4 step: 1695, loss is 0.0009007751359604299\n",
      "epoch: 4 step: 1696, loss is 0.06933693587779999\n",
      "epoch: 4 step: 1697, loss is 0.1618024706840515\n",
      "epoch: 4 step: 1698, loss is 0.08880212903022766\n",
      "epoch: 4 step: 1699, loss is 0.02238837629556656\n",
      "epoch: 4 step: 1700, loss is 0.12706202268600464\n",
      "epoch: 4 step: 1701, loss is 0.005400484893471003\n",
      "epoch: 4 step: 1702, loss is 0.02369745820760727\n",
      "epoch: 4 step: 1703, loss is 0.09914177656173706\n",
      "epoch: 4 step: 1704, loss is 0.013151492923498154\n",
      "epoch: 4 step: 1705, loss is 0.0075250305235385895\n",
      "epoch: 4 step: 1706, loss is 0.05755569040775299\n",
      "epoch: 4 step: 1707, loss is 0.0013308716006577015\n",
      "epoch: 4 step: 1708, loss is 0.006153787020593882\n",
      "epoch: 4 step: 1709, loss is 0.009898277930915356\n",
      "epoch: 4 step: 1710, loss is 0.037098441272974014\n",
      "epoch: 4 step: 1711, loss is 0.0007306566112674773\n",
      "epoch: 4 step: 1712, loss is 0.0062742517329752445\n",
      "epoch: 4 step: 1713, loss is 0.005250509362667799\n",
      "epoch: 4 step: 1714, loss is 0.09764186292886734\n",
      "epoch: 4 step: 1715, loss is 0.046112772077322006\n",
      "epoch: 4 step: 1716, loss is 0.0365062840282917\n",
      "epoch: 4 step: 1717, loss is 0.03158501908183098\n",
      "epoch: 4 step: 1718, loss is 0.09486933052539825\n",
      "epoch: 4 step: 1719, loss is 0.00928600411862135\n",
      "epoch: 4 step: 1720, loss is 0.052869200706481934\n",
      "epoch: 4 step: 1721, loss is 0.008587838150560856\n",
      "epoch: 4 step: 1722, loss is 0.011495270766317844\n",
      "epoch: 4 step: 1723, loss is 0.0796596109867096\n",
      "epoch: 4 step: 1724, loss is 0.08550418168306351\n",
      "epoch: 4 step: 1725, loss is 0.2154826670885086\n",
      "epoch: 4 step: 1726, loss is 0.0022313296794891357\n",
      "epoch: 4 step: 1727, loss is 0.19700555503368378\n",
      "epoch: 4 step: 1728, loss is 0.02610344998538494\n",
      "epoch: 4 step: 1729, loss is 0.010822602547705173\n",
      "epoch: 4 step: 1730, loss is 0.2972586154937744\n",
      "epoch: 4 step: 1731, loss is 0.0005170963704586029\n",
      "epoch: 4 step: 1732, loss is 0.0075980303809046745\n",
      "epoch: 4 step: 1733, loss is 0.15811097621917725\n",
      "epoch: 4 step: 1734, loss is 0.09219187498092651\n",
      "epoch: 4 step: 1735, loss is 0.026715101674199104\n",
      "epoch: 4 step: 1736, loss is 0.13439175486564636\n",
      "epoch: 4 step: 1737, loss is 0.06401577591896057\n",
      "epoch: 4 step: 1738, loss is 0.029191480949521065\n",
      "epoch: 4 step: 1739, loss is 0.01596037484705448\n",
      "epoch: 4 step: 1740, loss is 0.004479780327528715\n",
      "epoch: 4 step: 1741, loss is 0.004234366584569216\n",
      "epoch: 4 step: 1742, loss is 0.019429761916399002\n",
      "epoch: 4 step: 1743, loss is 0.0063832844607532024\n",
      "epoch: 4 step: 1744, loss is 0.002287027658894658\n",
      "epoch: 4 step: 1745, loss is 0.010048730298876762\n",
      "epoch: 4 step: 1746, loss is 0.16536225378513336\n",
      "epoch: 4 step: 1747, loss is 0.006421160418540239\n",
      "epoch: 4 step: 1748, loss is 0.0603121854364872\n",
      "epoch: 4 step: 1749, loss is 0.047367267310619354\n",
      "epoch: 4 step: 1750, loss is 0.06577426195144653\n",
      "epoch: 4 step: 1751, loss is 0.04118296876549721\n",
      "epoch: 4 step: 1752, loss is 0.00972079299390316\n",
      "epoch: 4 step: 1753, loss is 0.005492727272212505\n",
      "epoch: 4 step: 1754, loss is 0.005559025797992945\n",
      "epoch: 4 step: 1755, loss is 0.22455628216266632\n",
      "epoch: 4 step: 1756, loss is 0.010120631195604801\n",
      "epoch: 4 step: 1757, loss is 0.006308745592832565\n",
      "epoch: 4 step: 1758, loss is 0.015394038520753384\n",
      "epoch: 4 step: 1759, loss is 0.1791464388370514\n",
      "epoch: 4 step: 1760, loss is 0.059525903314352036\n",
      "epoch: 4 step: 1761, loss is 0.006667690817266703\n",
      "epoch: 4 step: 1762, loss is 0.0013206343865022063\n",
      "epoch: 4 step: 1763, loss is 0.2891477644443512\n",
      "epoch: 4 step: 1764, loss is 0.0021876320242881775\n",
      "epoch: 4 step: 1765, loss is 0.19324181973934174\n",
      "epoch: 4 step: 1766, loss is 0.005456582643091679\n",
      "epoch: 4 step: 1767, loss is 0.010894221253693104\n",
      "epoch: 4 step: 1768, loss is 0.0003017843118868768\n",
      "epoch: 4 step: 1769, loss is 0.013060523197054863\n",
      "epoch: 4 step: 1770, loss is 0.08832065016031265\n",
      "epoch: 4 step: 1771, loss is 0.013829110190272331\n",
      "epoch: 4 step: 1772, loss is 0.1292499452829361\n",
      "epoch: 4 step: 1773, loss is 0.1875758320093155\n",
      "epoch: 4 step: 1774, loss is 0.005016664508730173\n",
      "epoch: 4 step: 1775, loss is 0.029921075329184532\n",
      "epoch: 4 step: 1776, loss is 0.01317551825195551\n",
      "epoch: 4 step: 1777, loss is 0.06126929447054863\n",
      "epoch: 4 step: 1778, loss is 0.022068055346608162\n",
      "epoch: 4 step: 1779, loss is 0.0016809169901534915\n",
      "epoch: 4 step: 1780, loss is 0.04953473061323166\n",
      "epoch: 4 step: 1781, loss is 0.4164851903915405\n",
      "epoch: 4 step: 1782, loss is 0.0023224942851811647\n",
      "epoch: 4 step: 1783, loss is 0.009985456243157387\n",
      "epoch: 4 step: 1784, loss is 0.1370445191860199\n",
      "epoch: 4 step: 1785, loss is 0.0013331703376024961\n",
      "epoch: 4 step: 1786, loss is 0.03201337158679962\n",
      "epoch: 4 step: 1787, loss is 0.06785275042057037\n",
      "epoch: 4 step: 1788, loss is 0.01772003434598446\n",
      "epoch: 4 step: 1789, loss is 0.16633856296539307\n",
      "epoch: 4 step: 1790, loss is 0.006822953931987286\n",
      "epoch: 4 step: 1791, loss is 0.1403292864561081\n",
      "epoch: 4 step: 1792, loss is 0.10162006318569183\n",
      "epoch: 4 step: 1793, loss is 0.16543187201023102\n",
      "epoch: 4 step: 1794, loss is 0.05194895714521408\n",
      "epoch: 4 step: 1795, loss is 0.0036638928577303886\n",
      "epoch: 4 step: 1796, loss is 0.23416408896446228\n",
      "epoch: 4 step: 1797, loss is 0.07853194326162338\n",
      "epoch: 4 step: 1798, loss is 0.027884358540177345\n",
      "epoch: 4 step: 1799, loss is 0.008802284486591816\n",
      "epoch: 4 step: 1800, loss is 0.06182435154914856\n",
      "epoch: 4 step: 1801, loss is 0.04729585722088814\n",
      "epoch: 4 step: 1802, loss is 0.09122183918952942\n",
      "epoch: 4 step: 1803, loss is 0.058860182762145996\n",
      "epoch: 4 step: 1804, loss is 0.16743727028369904\n",
      "epoch: 4 step: 1805, loss is 0.08069390803575516\n",
      "epoch: 4 step: 1806, loss is 0.16807538270950317\n",
      "epoch: 4 step: 1807, loss is 0.003158648032695055\n",
      "epoch: 4 step: 1808, loss is 0.27930697798728943\n",
      "epoch: 4 step: 1809, loss is 0.07682662457227707\n",
      "epoch: 4 step: 1810, loss is 0.1388344168663025\n",
      "epoch: 4 step: 1811, loss is 0.14429756999015808\n",
      "epoch: 4 step: 1812, loss is 0.25430819392204285\n",
      "epoch: 4 step: 1813, loss is 0.1061633974313736\n",
      "epoch: 4 step: 1814, loss is 0.018998999148607254\n",
      "epoch: 4 step: 1815, loss is 0.0019521231297403574\n",
      "epoch: 4 step: 1816, loss is 0.12171245366334915\n",
      "epoch: 4 step: 1817, loss is 0.08238496631383896\n",
      "epoch: 4 step: 1818, loss is 0.016477560624480247\n",
      "epoch: 4 step: 1819, loss is 0.012526370584964752\n",
      "epoch: 4 step: 1820, loss is 0.011852318421006203\n",
      "epoch: 4 step: 1821, loss is 0.23837681114673615\n",
      "epoch: 4 step: 1822, loss is 0.0914679765701294\n",
      "epoch: 4 step: 1823, loss is 0.1265571415424347\n",
      "epoch: 4 step: 1824, loss is 0.05407283455133438\n",
      "epoch: 4 step: 1825, loss is 0.2916504144668579\n",
      "epoch: 4 step: 1826, loss is 0.006607706192880869\n",
      "epoch: 4 step: 1827, loss is 0.015539444983005524\n",
      "epoch: 4 step: 1828, loss is 0.02221629209816456\n",
      "epoch: 4 step: 1829, loss is 0.022563017904758453\n",
      "epoch: 4 step: 1830, loss is 0.06134064123034477\n",
      "epoch: 4 step: 1831, loss is 0.12766796350479126\n",
      "epoch: 4 step: 1832, loss is 0.048547398298978806\n",
      "epoch: 4 step: 1833, loss is 0.003459163708612323\n",
      "epoch: 4 step: 1834, loss is 0.005475865211337805\n",
      "epoch: 4 step: 1835, loss is 0.020452162250876427\n",
      "epoch: 4 step: 1836, loss is 0.06500566005706787\n",
      "epoch: 4 step: 1837, loss is 0.018124697729945183\n",
      "epoch: 4 step: 1838, loss is 0.010247019119560719\n",
      "epoch: 4 step: 1839, loss is 0.03245106711983681\n",
      "epoch: 4 step: 1840, loss is 0.0441838763654232\n",
      "epoch: 4 step: 1841, loss is 0.3378352224826813\n",
      "epoch: 4 step: 1842, loss is 0.1538061797618866\n",
      "epoch: 4 step: 1843, loss is 0.03705982118844986\n",
      "epoch: 4 step: 1844, loss is 0.04470903426408768\n",
      "epoch: 4 step: 1845, loss is 0.04181411489844322\n",
      "epoch: 4 step: 1846, loss is 0.24555765092372894\n",
      "epoch: 4 step: 1847, loss is 0.08541643619537354\n",
      "epoch: 4 step: 1848, loss is 0.059242673218250275\n",
      "epoch: 4 step: 1849, loss is 0.02190130576491356\n",
      "epoch: 4 step: 1850, loss is 0.00515450444072485\n",
      "epoch: 4 step: 1851, loss is 0.0413682721555233\n",
      "epoch: 4 step: 1852, loss is 0.07118374854326248\n",
      "epoch: 4 step: 1853, loss is 0.02178012952208519\n",
      "epoch: 4 step: 1854, loss is 0.2788095772266388\n",
      "epoch: 4 step: 1855, loss is 0.0033389113377779722\n",
      "epoch: 4 step: 1856, loss is 0.4004966914653778\n",
      "epoch: 4 step: 1857, loss is 0.0020822484511882067\n",
      "epoch: 4 step: 1858, loss is 0.008103571832180023\n",
      "epoch: 4 step: 1859, loss is 0.02836494706571102\n",
      "epoch: 4 step: 1860, loss is 0.10077296197414398\n",
      "epoch: 4 step: 1861, loss is 0.05362260341644287\n",
      "epoch: 4 step: 1862, loss is 0.015057364478707314\n",
      "epoch: 4 step: 1863, loss is 0.01643385924398899\n",
      "epoch: 4 step: 1864, loss is 0.1503620147705078\n",
      "epoch: 4 step: 1865, loss is 0.246436208486557\n",
      "epoch: 4 step: 1866, loss is 0.0038583651185035706\n",
      "epoch: 4 step: 1867, loss is 0.07007703185081482\n",
      "epoch: 4 step: 1868, loss is 0.007530599366873503\n",
      "epoch: 4 step: 1869, loss is 0.0034047707449644804\n",
      "epoch: 4 step: 1870, loss is 0.0339617021381855\n",
      "epoch: 4 step: 1871, loss is 0.002633582102134824\n",
      "epoch: 4 step: 1872, loss is 0.002178541850298643\n",
      "epoch: 4 step: 1873, loss is 0.010611510835587978\n",
      "epoch: 4 step: 1874, loss is 0.18218129873275757\n",
      "epoch: 4 step: 1875, loss is 0.009027538821101189\n",
      "epoch: 5 step: 1, loss is 0.014063763432204723\n",
      "epoch: 5 step: 2, loss is 0.06115058436989784\n",
      "epoch: 5 step: 3, loss is 0.11891093850135803\n",
      "epoch: 5 step: 4, loss is 0.027975425124168396\n",
      "epoch: 5 step: 5, loss is 0.001511800684966147\n",
      "epoch: 5 step: 6, loss is 0.07746744155883789\n",
      "epoch: 5 step: 7, loss is 0.035869378596544266\n",
      "epoch: 5 step: 8, loss is 0.10890289396047592\n",
      "epoch: 5 step: 9, loss is 0.15787629783153534\n",
      "epoch: 5 step: 10, loss is 0.006071234121918678\n",
      "epoch: 5 step: 11, loss is 0.09318408370018005\n",
      "epoch: 5 step: 12, loss is 0.026833588257431984\n",
      "epoch: 5 step: 13, loss is 0.12780587375164032\n",
      "epoch: 5 step: 14, loss is 0.048561446368694305\n",
      "epoch: 5 step: 15, loss is 0.0036395012866705656\n",
      "epoch: 5 step: 16, loss is 0.0880001112818718\n",
      "epoch: 5 step: 17, loss is 0.01573874056339264\n",
      "epoch: 5 step: 18, loss is 0.11224164068698883\n",
      "epoch: 5 step: 19, loss is 0.056381165981292725\n",
      "epoch: 5 step: 20, loss is 0.015739282593131065\n",
      "epoch: 5 step: 21, loss is 0.019862882792949677\n",
      "epoch: 5 step: 22, loss is 0.003942457493394613\n",
      "epoch: 5 step: 23, loss is 0.0017701203469187021\n",
      "epoch: 5 step: 24, loss is 0.026076344773173332\n",
      "epoch: 5 step: 25, loss is 0.07936882227659225\n",
      "epoch: 5 step: 26, loss is 0.0036546653136610985\n",
      "epoch: 5 step: 27, loss is 0.23422634601593018\n",
      "epoch: 5 step: 28, loss is 0.01898168958723545\n",
      "epoch: 5 step: 29, loss is 0.0032773667480796576\n",
      "epoch: 5 step: 30, loss is 0.037844423204660416\n",
      "epoch: 5 step: 31, loss is 0.009353057481348515\n",
      "epoch: 5 step: 32, loss is 0.04935417324304581\n",
      "epoch: 5 step: 33, loss is 0.09013289213180542\n",
      "epoch: 5 step: 34, loss is 0.06740080565214157\n",
      "epoch: 5 step: 35, loss is 0.14968757331371307\n",
      "epoch: 5 step: 36, loss is 0.0406988300383091\n",
      "epoch: 5 step: 37, loss is 0.014976251870393753\n",
      "epoch: 5 step: 38, loss is 0.03859862685203552\n",
      "epoch: 5 step: 39, loss is 0.061056263744831085\n",
      "epoch: 5 step: 40, loss is 0.04615943878889084\n",
      "epoch: 5 step: 41, loss is 0.08225300163030624\n",
      "epoch: 5 step: 42, loss is 0.09046211838722229\n",
      "epoch: 5 step: 43, loss is 0.08579418808221817\n",
      "epoch: 5 step: 44, loss is 0.008343665860593319\n",
      "epoch: 5 step: 45, loss is 0.0043958076275885105\n",
      "epoch: 5 step: 46, loss is 0.056666936725378036\n",
      "epoch: 5 step: 47, loss is 0.058754801750183105\n",
      "epoch: 5 step: 48, loss is 0.0011315926676616073\n",
      "epoch: 5 step: 49, loss is 0.022703252732753754\n",
      "epoch: 5 step: 50, loss is 0.01869170553982258\n",
      "epoch: 5 step: 51, loss is 0.01868201047182083\n",
      "epoch: 5 step: 52, loss is 0.015383889898657799\n",
      "epoch: 5 step: 53, loss is 0.02216586098074913\n",
      "epoch: 5 step: 54, loss is 0.038586027920246124\n",
      "epoch: 5 step: 55, loss is 0.020321602001786232\n",
      "epoch: 5 step: 56, loss is 0.01317345816642046\n",
      "epoch: 5 step: 57, loss is 0.007418905384838581\n",
      "epoch: 5 step: 58, loss is 0.0054526254534721375\n",
      "epoch: 5 step: 59, loss is 0.14656241238117218\n",
      "epoch: 5 step: 60, loss is 0.0172586590051651\n",
      "epoch: 5 step: 61, loss is 0.032139647752046585\n",
      "epoch: 5 step: 62, loss is 0.0245879665017128\n",
      "epoch: 5 step: 63, loss is 0.08268846571445465\n",
      "epoch: 5 step: 64, loss is 0.19785374402999878\n",
      "epoch: 5 step: 65, loss is 0.00827430933713913\n",
      "epoch: 5 step: 66, loss is 0.0031326590105891228\n",
      "epoch: 5 step: 67, loss is 0.11697989702224731\n",
      "epoch: 5 step: 68, loss is 0.004714546259492636\n",
      "epoch: 5 step: 69, loss is 0.043183401226997375\n",
      "epoch: 5 step: 70, loss is 0.03084629774093628\n",
      "epoch: 5 step: 71, loss is 0.002196816960349679\n",
      "epoch: 5 step: 72, loss is 0.0027483245357871056\n",
      "epoch: 5 step: 73, loss is 0.06634866446256638\n",
      "epoch: 5 step: 74, loss is 0.030610617250204086\n",
      "epoch: 5 step: 75, loss is 0.01268097385764122\n",
      "epoch: 5 step: 76, loss is 0.021200817078351974\n",
      "epoch: 5 step: 77, loss is 0.1391899436712265\n",
      "epoch: 5 step: 78, loss is 0.0007048419793136418\n",
      "epoch: 5 step: 79, loss is 0.019064974039793015\n",
      "epoch: 5 step: 80, loss is 0.008887133561074734\n",
      "epoch: 5 step: 81, loss is 0.026027144864201546\n",
      "epoch: 5 step: 82, loss is 0.17147821187973022\n",
      "epoch: 5 step: 83, loss is 0.020955119282007217\n",
      "epoch: 5 step: 84, loss is 0.06958179175853729\n",
      "epoch: 5 step: 85, loss is 0.0175729189068079\n",
      "epoch: 5 step: 86, loss is 0.005552174523472786\n",
      "epoch: 5 step: 87, loss is 0.005276257637888193\n",
      "epoch: 5 step: 88, loss is 0.02173583395779133\n",
      "epoch: 5 step: 89, loss is 0.0035937773063778877\n",
      "epoch: 5 step: 90, loss is 0.06423887610435486\n",
      "epoch: 5 step: 91, loss is 0.1283682882785797\n",
      "epoch: 5 step: 92, loss is 0.04421532526612282\n",
      "epoch: 5 step: 93, loss is 0.051820140331983566\n",
      "epoch: 5 step: 94, loss is 0.007790064439177513\n",
      "epoch: 5 step: 95, loss is 0.017399312928318977\n",
      "epoch: 5 step: 96, loss is 0.16219955682754517\n",
      "epoch: 5 step: 97, loss is 0.0012307269498705864\n",
      "epoch: 5 step: 98, loss is 0.01943342760205269\n",
      "epoch: 5 step: 99, loss is 0.004262571223080158\n",
      "epoch: 5 step: 100, loss is 0.144134521484375\n",
      "epoch: 5 step: 101, loss is 0.09366481751203537\n",
      "epoch: 5 step: 102, loss is 0.03467513248324394\n",
      "epoch: 5 step: 103, loss is 0.005645077209919691\n",
      "epoch: 5 step: 104, loss is 0.009393034502863884\n",
      "epoch: 5 step: 105, loss is 0.00517305638641119\n",
      "epoch: 5 step: 106, loss is 0.006162559613585472\n",
      "epoch: 5 step: 107, loss is 0.013997971080243587\n",
      "epoch: 5 step: 108, loss is 0.0019742599688470364\n",
      "epoch: 5 step: 109, loss is 0.101511649787426\n",
      "epoch: 5 step: 110, loss is 0.028941944241523743\n",
      "epoch: 5 step: 111, loss is 0.004590215627104044\n",
      "epoch: 5 step: 112, loss is 0.007301055360585451\n",
      "epoch: 5 step: 113, loss is 0.004095201380550861\n",
      "epoch: 5 step: 114, loss is 0.06405937671661377\n",
      "epoch: 5 step: 115, loss is 0.1382986158132553\n",
      "epoch: 5 step: 116, loss is 0.20266008377075195\n",
      "epoch: 5 step: 117, loss is 0.018951402977108955\n",
      "epoch: 5 step: 118, loss is 0.006360839121043682\n",
      "epoch: 5 step: 119, loss is 0.002092960523441434\n",
      "epoch: 5 step: 120, loss is 0.09557399153709412\n",
      "epoch: 5 step: 121, loss is 0.0055109113454818726\n",
      "epoch: 5 step: 122, loss is 0.05325917899608612\n",
      "epoch: 5 step: 123, loss is 0.0014364138478413224\n",
      "epoch: 5 step: 124, loss is 0.031999241560697556\n",
      "epoch: 5 step: 125, loss is 0.005805754102766514\n",
      "epoch: 5 step: 126, loss is 0.04552218317985535\n",
      "epoch: 5 step: 127, loss is 0.034598395228385925\n",
      "epoch: 5 step: 128, loss is 0.1016419306397438\n",
      "epoch: 5 step: 129, loss is 0.011599870398640633\n",
      "epoch: 5 step: 130, loss is 0.01020075660198927\n",
      "epoch: 5 step: 131, loss is 0.131849005818367\n",
      "epoch: 5 step: 132, loss is 0.04916984587907791\n",
      "epoch: 5 step: 133, loss is 0.10785482823848724\n",
      "epoch: 5 step: 134, loss is 0.021534403786063194\n",
      "epoch: 5 step: 135, loss is 0.014906670898199081\n",
      "epoch: 5 step: 136, loss is 0.025275377556681633\n",
      "epoch: 5 step: 137, loss is 0.014034515246748924\n",
      "epoch: 5 step: 138, loss is 0.01051421370357275\n",
      "epoch: 5 step: 139, loss is 0.0044108796864748\n",
      "epoch: 5 step: 140, loss is 0.011975064873695374\n",
      "epoch: 5 step: 141, loss is 0.025945477187633514\n",
      "epoch: 5 step: 142, loss is 0.003301645163446665\n",
      "epoch: 5 step: 143, loss is 0.024047911167144775\n",
      "epoch: 5 step: 144, loss is 0.009787269867956638\n",
      "epoch: 5 step: 145, loss is 0.00801955908536911\n",
      "epoch: 5 step: 146, loss is 0.25348618626594543\n",
      "epoch: 5 step: 147, loss is 0.0007416190346702933\n",
      "epoch: 5 step: 148, loss is 0.00966851320117712\n",
      "epoch: 5 step: 149, loss is 0.008794145658612251\n",
      "epoch: 5 step: 150, loss is 0.028050845488905907\n",
      "epoch: 5 step: 151, loss is 0.06759773939847946\n",
      "epoch: 5 step: 152, loss is 0.006951342802494764\n",
      "epoch: 5 step: 153, loss is 0.12530158460140228\n",
      "epoch: 5 step: 154, loss is 0.07946592569351196\n",
      "epoch: 5 step: 155, loss is 0.035728681832551956\n",
      "epoch: 5 step: 156, loss is 0.05913056805729866\n",
      "epoch: 5 step: 157, loss is 0.00498448871076107\n",
      "epoch: 5 step: 158, loss is 0.029374808073043823\n",
      "epoch: 5 step: 159, loss is 0.0025394598487764597\n",
      "epoch: 5 step: 160, loss is 0.039429981261491776\n",
      "epoch: 5 step: 161, loss is 0.04635302349925041\n",
      "epoch: 5 step: 162, loss is 0.0026501985266804695\n",
      "epoch: 5 step: 163, loss is 0.02979922853410244\n",
      "epoch: 5 step: 164, loss is 0.3224978744983673\n",
      "epoch: 5 step: 165, loss is 0.2171289175748825\n",
      "epoch: 5 step: 166, loss is 0.013028809800744057\n",
      "epoch: 5 step: 167, loss is 0.006662517320364714\n",
      "epoch: 5 step: 168, loss is 0.07755642384290695\n",
      "epoch: 5 step: 169, loss is 0.0477154366672039\n",
      "epoch: 5 step: 170, loss is 0.2249719202518463\n",
      "epoch: 5 step: 171, loss is 0.07787945866584778\n",
      "epoch: 5 step: 172, loss is 0.029873017221689224\n",
      "epoch: 5 step: 173, loss is 0.04884518310427666\n",
      "epoch: 5 step: 174, loss is 0.006655029021203518\n",
      "epoch: 5 step: 175, loss is 0.0012377375969663262\n",
      "epoch: 5 step: 176, loss is 0.013687667436897755\n",
      "epoch: 5 step: 177, loss is 0.029485607519745827\n",
      "epoch: 5 step: 178, loss is 0.010590780526399612\n",
      "epoch: 5 step: 179, loss is 0.017318202182650566\n",
      "epoch: 5 step: 180, loss is 0.15791703760623932\n",
      "epoch: 5 step: 181, loss is 0.09557171165943146\n",
      "epoch: 5 step: 182, loss is 0.011869870126247406\n",
      "epoch: 5 step: 183, loss is 0.017411939799785614\n",
      "epoch: 5 step: 184, loss is 0.04986472800374031\n",
      "epoch: 5 step: 185, loss is 0.002248756354674697\n",
      "epoch: 5 step: 186, loss is 0.0346999429166317\n",
      "epoch: 5 step: 187, loss is 0.07858739793300629\n",
      "epoch: 5 step: 188, loss is 0.20113587379455566\n",
      "epoch: 5 step: 189, loss is 0.21877402067184448\n",
      "epoch: 5 step: 190, loss is 0.08561340719461441\n",
      "epoch: 5 step: 191, loss is 0.08289439976215363\n",
      "epoch: 5 step: 192, loss is 0.02835327759385109\n",
      "epoch: 5 step: 193, loss is 0.022603489458560944\n",
      "epoch: 5 step: 194, loss is 0.009119187481701374\n",
      "epoch: 5 step: 195, loss is 0.0387890562415123\n",
      "epoch: 5 step: 196, loss is 0.048685476183891296\n",
      "epoch: 5 step: 197, loss is 0.011136379092931747\n",
      "epoch: 5 step: 198, loss is 0.03225640580058098\n",
      "epoch: 5 step: 199, loss is 0.11144983768463135\n",
      "epoch: 5 step: 200, loss is 0.023995941504836082\n",
      "epoch: 5 step: 201, loss is 0.0005867871223017573\n",
      "epoch: 5 step: 202, loss is 0.011832893826067448\n",
      "epoch: 5 step: 203, loss is 0.031116001307964325\n",
      "epoch: 5 step: 204, loss is 0.004230095539242029\n",
      "epoch: 5 step: 205, loss is 0.04538483917713165\n",
      "epoch: 5 step: 206, loss is 0.03580719977617264\n",
      "epoch: 5 step: 207, loss is 0.03221534565091133\n",
      "epoch: 5 step: 208, loss is 0.03858853504061699\n",
      "epoch: 5 step: 209, loss is 0.10842989385128021\n",
      "epoch: 5 step: 210, loss is 0.04931145906448364\n",
      "epoch: 5 step: 211, loss is 0.0018736158963292837\n",
      "epoch: 5 step: 212, loss is 0.09835551679134369\n",
      "epoch: 5 step: 213, loss is 0.019059156998991966\n",
      "epoch: 5 step: 214, loss is 0.09542522579431534\n",
      "epoch: 5 step: 215, loss is 0.19496122002601624\n",
      "epoch: 5 step: 216, loss is 0.10947313904762268\n",
      "epoch: 5 step: 217, loss is 0.027426930144429207\n",
      "epoch: 5 step: 218, loss is 0.047528225928545\n",
      "epoch: 5 step: 219, loss is 0.010041899047791958\n",
      "epoch: 5 step: 220, loss is 0.05073857307434082\n",
      "epoch: 5 step: 221, loss is 0.005965132731944323\n",
      "epoch: 5 step: 222, loss is 0.019017308950424194\n",
      "epoch: 5 step: 223, loss is 0.008784642443060875\n",
      "epoch: 5 step: 224, loss is 0.005837400443851948\n",
      "epoch: 5 step: 225, loss is 0.07406669110059738\n",
      "epoch: 5 step: 226, loss is 0.017308689653873444\n",
      "epoch: 5 step: 227, loss is 0.16286101937294006\n",
      "epoch: 5 step: 228, loss is 0.01690208911895752\n",
      "epoch: 5 step: 229, loss is 0.08558117598295212\n",
      "epoch: 5 step: 230, loss is 0.1951301097869873\n",
      "epoch: 5 step: 231, loss is 0.0019811673555523157\n",
      "epoch: 5 step: 232, loss is 0.13053911924362183\n",
      "epoch: 5 step: 233, loss is 0.07872723788022995\n",
      "epoch: 5 step: 234, loss is 0.029326172545552254\n",
      "epoch: 5 step: 235, loss is 0.014559919014573097\n",
      "epoch: 5 step: 236, loss is 0.02672673761844635\n",
      "epoch: 5 step: 237, loss is 0.024230025708675385\n",
      "epoch: 5 step: 238, loss is 0.03126956522464752\n",
      "epoch: 5 step: 239, loss is 0.20276989042758942\n",
      "epoch: 5 step: 240, loss is 0.13614554703235626\n",
      "epoch: 5 step: 241, loss is 0.051775749772787094\n",
      "epoch: 5 step: 242, loss is 0.0031959780026227236\n",
      "epoch: 5 step: 243, loss is 0.010562317445874214\n",
      "epoch: 5 step: 244, loss is 0.03536422923207283\n",
      "epoch: 5 step: 245, loss is 0.19769622385501862\n",
      "epoch: 5 step: 246, loss is 0.08445577323436737\n",
      "epoch: 5 step: 247, loss is 0.009031854569911957\n",
      "epoch: 5 step: 248, loss is 0.011826938018202782\n",
      "epoch: 5 step: 249, loss is 0.10732952505350113\n",
      "epoch: 5 step: 250, loss is 0.0868501141667366\n",
      "epoch: 5 step: 251, loss is 0.2495741844177246\n",
      "epoch: 5 step: 252, loss is 0.000783118128310889\n",
      "epoch: 5 step: 253, loss is 0.013621138408780098\n",
      "epoch: 5 step: 254, loss is 0.003339708549901843\n",
      "epoch: 5 step: 255, loss is 0.06758909672498703\n",
      "epoch: 5 step: 256, loss is 0.016574939712882042\n",
      "epoch: 5 step: 257, loss is 0.0417274534702301\n",
      "epoch: 5 step: 258, loss is 0.004005123861134052\n",
      "epoch: 5 step: 259, loss is 0.08986756205558777\n",
      "epoch: 5 step: 260, loss is 0.04769891873002052\n",
      "epoch: 5 step: 261, loss is 0.003953620325773954\n",
      "epoch: 5 step: 262, loss is 0.0062550934962928295\n",
      "epoch: 5 step: 263, loss is 0.038815490901470184\n",
      "epoch: 5 step: 264, loss is 0.0010522062657400966\n",
      "epoch: 5 step: 265, loss is 0.017519306391477585\n",
      "epoch: 5 step: 266, loss is 0.04914262145757675\n",
      "epoch: 5 step: 267, loss is 0.06540480256080627\n",
      "epoch: 5 step: 268, loss is 0.0023501012474298477\n",
      "epoch: 5 step: 269, loss is 0.0432160459458828\n",
      "epoch: 5 step: 270, loss is 0.020919783040881157\n",
      "epoch: 5 step: 271, loss is 0.004465305712074041\n",
      "epoch: 5 step: 272, loss is 0.0040779635310173035\n",
      "epoch: 5 step: 273, loss is 0.002010186668485403\n",
      "epoch: 5 step: 274, loss is 0.014400874264538288\n",
      "epoch: 5 step: 275, loss is 0.03036886639893055\n",
      "epoch: 5 step: 276, loss is 0.0016678975662216544\n",
      "epoch: 5 step: 277, loss is 0.013921499252319336\n",
      "epoch: 5 step: 278, loss is 0.004332987125962973\n",
      "epoch: 5 step: 279, loss is 0.0015291165327653289\n",
      "epoch: 5 step: 280, loss is 0.07610495388507843\n",
      "epoch: 5 step: 281, loss is 0.03619943931698799\n",
      "epoch: 5 step: 282, loss is 0.0005617752904072404\n",
      "epoch: 5 step: 283, loss is 0.06880075484514236\n",
      "epoch: 5 step: 284, loss is 0.049788910895586014\n",
      "epoch: 5 step: 285, loss is 0.08240897208452225\n",
      "epoch: 5 step: 286, loss is 0.16941285133361816\n",
      "epoch: 5 step: 287, loss is 0.036069244146347046\n",
      "epoch: 5 step: 288, loss is 0.016548264771699905\n",
      "epoch: 5 step: 289, loss is 0.03282110393047333\n",
      "epoch: 5 step: 290, loss is 0.03746194764971733\n",
      "epoch: 5 step: 291, loss is 0.015821760520339012\n",
      "epoch: 5 step: 292, loss is 0.05580192431807518\n",
      "epoch: 5 step: 293, loss is 0.001011104672215879\n",
      "epoch: 5 step: 294, loss is 0.024994120001792908\n",
      "epoch: 5 step: 295, loss is 0.022476356476545334\n",
      "epoch: 5 step: 296, loss is 0.0017484650015830994\n",
      "epoch: 5 step: 297, loss is 0.2806157171726227\n",
      "epoch: 5 step: 298, loss is 0.013185564428567886\n",
      "epoch: 5 step: 299, loss is 0.014980727806687355\n",
      "epoch: 5 step: 300, loss is 0.0484895296394825\n",
      "epoch: 5 step: 301, loss is 0.03544241562485695\n",
      "epoch: 5 step: 302, loss is 0.1486525982618332\n",
      "epoch: 5 step: 303, loss is 0.019428815692663193\n",
      "epoch: 5 step: 304, loss is 0.07081151753664017\n",
      "epoch: 5 step: 305, loss is 0.0369267500936985\n",
      "epoch: 5 step: 306, loss is 0.0007016624440439045\n",
      "epoch: 5 step: 307, loss is 0.13989773392677307\n",
      "epoch: 5 step: 308, loss is 0.044341061264276505\n",
      "epoch: 5 step: 309, loss is 0.017021747305989265\n",
      "epoch: 5 step: 310, loss is 0.006379725877195597\n",
      "epoch: 5 step: 311, loss is 0.03295173868536949\n",
      "epoch: 5 step: 312, loss is 0.09634001553058624\n",
      "epoch: 5 step: 313, loss is 0.010057603940367699\n",
      "epoch: 5 step: 314, loss is 0.11416301131248474\n",
      "epoch: 5 step: 315, loss is 0.14352893829345703\n",
      "epoch: 5 step: 316, loss is 0.04331966117024422\n",
      "epoch: 5 step: 317, loss is 0.0023450693115592003\n",
      "epoch: 5 step: 318, loss is 0.01812123693525791\n",
      "epoch: 5 step: 319, loss is 0.005536630284041166\n",
      "epoch: 5 step: 320, loss is 0.06110348179936409\n",
      "epoch: 5 step: 321, loss is 0.009880578145384789\n",
      "epoch: 5 step: 322, loss is 0.023156937211751938\n",
      "epoch: 5 step: 323, loss is 0.003466993570327759\n",
      "epoch: 5 step: 324, loss is 0.101099893450737\n",
      "epoch: 5 step: 325, loss is 0.04292578995227814\n",
      "epoch: 5 step: 326, loss is 0.016087012365460396\n",
      "epoch: 5 step: 327, loss is 0.041798658668994904\n",
      "epoch: 5 step: 328, loss is 0.04898546636104584\n",
      "epoch: 5 step: 329, loss is 0.026885371655225754\n",
      "epoch: 5 step: 330, loss is 0.026043707504868507\n",
      "epoch: 5 step: 331, loss is 0.05737193301320076\n",
      "epoch: 5 step: 332, loss is 0.033034831285476685\n",
      "epoch: 5 step: 333, loss is 0.008679569698870182\n",
      "epoch: 5 step: 334, loss is 0.015889136120676994\n",
      "epoch: 5 step: 335, loss is 0.03038509003818035\n",
      "epoch: 5 step: 336, loss is 0.0016751002985984087\n",
      "epoch: 5 step: 337, loss is 0.03949471190571785\n",
      "epoch: 5 step: 338, loss is 0.02370724454522133\n",
      "epoch: 5 step: 339, loss is 0.002979255747050047\n",
      "epoch: 5 step: 340, loss is 0.08382021635770798\n",
      "epoch: 5 step: 341, loss is 0.011064646765589714\n",
      "epoch: 5 step: 342, loss is 0.004513975698500872\n",
      "epoch: 5 step: 343, loss is 0.10372409969568253\n",
      "epoch: 5 step: 344, loss is 0.03591154143214226\n",
      "epoch: 5 step: 345, loss is 0.006622659508138895\n",
      "epoch: 5 step: 346, loss is 0.20608556270599365\n",
      "epoch: 5 step: 347, loss is 0.061136599630117416\n",
      "epoch: 5 step: 348, loss is 0.012007870711386204\n",
      "epoch: 5 step: 349, loss is 0.07045761495828629\n",
      "epoch: 5 step: 350, loss is 0.009450893849134445\n",
      "epoch: 5 step: 351, loss is 0.04789651557803154\n",
      "epoch: 5 step: 352, loss is 0.008693692274391651\n",
      "epoch: 5 step: 353, loss is 0.015168452635407448\n",
      "epoch: 5 step: 354, loss is 0.07179571688175201\n",
      "epoch: 5 step: 355, loss is 0.04978281259536743\n",
      "epoch: 5 step: 356, loss is 0.0333428755402565\n",
      "epoch: 5 step: 357, loss is 0.01422708947211504\n",
      "epoch: 5 step: 358, loss is 0.13472536206245422\n",
      "epoch: 5 step: 359, loss is 0.19409742951393127\n",
      "epoch: 5 step: 360, loss is 0.0067777023650705814\n",
      "epoch: 5 step: 361, loss is 0.06304421275854111\n",
      "epoch: 5 step: 362, loss is 0.08410532772541046\n",
      "epoch: 5 step: 363, loss is 0.07449822872877121\n",
      "epoch: 5 step: 364, loss is 0.0024038637056946754\n",
      "epoch: 5 step: 365, loss is 0.021720144897699356\n",
      "epoch: 5 step: 366, loss is 0.046026185154914856\n",
      "epoch: 5 step: 367, loss is 0.018512142822146416\n",
      "epoch: 5 step: 368, loss is 0.004097431898117065\n",
      "epoch: 5 step: 369, loss is 0.013106103986501694\n",
      "epoch: 5 step: 370, loss is 0.031276486814022064\n",
      "epoch: 5 step: 371, loss is 0.011846262961626053\n",
      "epoch: 5 step: 372, loss is 0.004645716864615679\n",
      "epoch: 5 step: 373, loss is 0.007941781543195248\n",
      "epoch: 5 step: 374, loss is 0.035350363701581955\n",
      "epoch: 5 step: 375, loss is 0.005054717417806387\n",
      "epoch: 5 step: 376, loss is 0.004787700716406107\n",
      "epoch: 5 step: 377, loss is 0.019858628511428833\n",
      "epoch: 5 step: 378, loss is 0.008170037530362606\n",
      "epoch: 5 step: 379, loss is 0.002361901104450226\n",
      "epoch: 5 step: 380, loss is 0.02081625908613205\n",
      "epoch: 5 step: 381, loss is 0.008713928051292896\n",
      "epoch: 5 step: 382, loss is 0.03860782831907272\n",
      "epoch: 5 step: 383, loss is 0.06791237741708755\n",
      "epoch: 5 step: 384, loss is 0.04883047938346863\n",
      "epoch: 5 step: 385, loss is 0.003014917951077223\n",
      "epoch: 5 step: 386, loss is 0.04082724452018738\n",
      "epoch: 5 step: 387, loss is 0.05412248522043228\n",
      "epoch: 5 step: 388, loss is 0.01021642703562975\n",
      "epoch: 5 step: 389, loss is 0.019466474652290344\n",
      "epoch: 5 step: 390, loss is 0.004923070315271616\n",
      "epoch: 5 step: 391, loss is 0.017289435490965843\n",
      "epoch: 5 step: 392, loss is 0.005339129362255335\n",
      "epoch: 5 step: 393, loss is 0.0023636245168745518\n",
      "epoch: 5 step: 394, loss is 0.011678836308419704\n",
      "epoch: 5 step: 395, loss is 0.029584482312202454\n",
      "epoch: 5 step: 396, loss is 0.04444321244955063\n",
      "epoch: 5 step: 397, loss is 0.10025079548358917\n",
      "epoch: 5 step: 398, loss is 0.05649961903691292\n",
      "epoch: 5 step: 399, loss is 0.06071677803993225\n",
      "epoch: 5 step: 400, loss is 0.15455211699008942\n",
      "epoch: 5 step: 401, loss is 0.10799700766801834\n",
      "epoch: 5 step: 402, loss is 0.04411817342042923\n",
      "epoch: 5 step: 403, loss is 0.0917624905705452\n",
      "epoch: 5 step: 404, loss is 0.06586585938930511\n",
      "epoch: 5 step: 405, loss is 0.07361079007387161\n",
      "epoch: 5 step: 406, loss is 0.03935517743229866\n",
      "epoch: 5 step: 407, loss is 0.024040445685386658\n",
      "epoch: 5 step: 408, loss is 0.040825534611940384\n",
      "epoch: 5 step: 409, loss is 0.03882542997598648\n",
      "epoch: 5 step: 410, loss is 0.08198434114456177\n",
      "epoch: 5 step: 411, loss is 0.004054383374750614\n",
      "epoch: 5 step: 412, loss is 0.006256210617721081\n",
      "epoch: 5 step: 413, loss is 0.22646427154541016\n",
      "epoch: 5 step: 414, loss is 0.17083799839019775\n",
      "epoch: 5 step: 415, loss is 0.1867559403181076\n",
      "epoch: 5 step: 416, loss is 0.014471345581114292\n",
      "epoch: 5 step: 417, loss is 0.012572702951729298\n",
      "epoch: 5 step: 418, loss is 0.04262560233473778\n",
      "epoch: 5 step: 419, loss is 0.42415332794189453\n",
      "epoch: 5 step: 420, loss is 0.0074109872803092\n",
      "epoch: 5 step: 421, loss is 0.1043047308921814\n",
      "epoch: 5 step: 422, loss is 0.1360870897769928\n",
      "epoch: 5 step: 423, loss is 0.033611346036195755\n",
      "epoch: 5 step: 424, loss is 0.03867993503808975\n",
      "epoch: 5 step: 425, loss is 0.021399155259132385\n",
      "epoch: 5 step: 426, loss is 0.049385957419872284\n",
      "epoch: 5 step: 427, loss is 0.021321436390280724\n",
      "epoch: 5 step: 428, loss is 0.060757748782634735\n",
      "epoch: 5 step: 429, loss is 0.1994144767522812\n",
      "epoch: 5 step: 430, loss is 0.05115294083952904\n",
      "epoch: 5 step: 431, loss is 0.021326038986444473\n",
      "epoch: 5 step: 432, loss is 0.09191644191741943\n",
      "epoch: 5 step: 433, loss is 0.009392524138092995\n",
      "epoch: 5 step: 434, loss is 0.004415930714458227\n",
      "epoch: 5 step: 435, loss is 0.03264443576335907\n",
      "epoch: 5 step: 436, loss is 0.019273042678833008\n",
      "epoch: 5 step: 437, loss is 0.009481053799390793\n",
      "epoch: 5 step: 438, loss is 0.004917610436677933\n",
      "epoch: 5 step: 439, loss is 0.00681680953130126\n",
      "epoch: 5 step: 440, loss is 0.004457129165530205\n",
      "epoch: 5 step: 441, loss is 0.024423304945230484\n",
      "epoch: 5 step: 442, loss is 0.010301855392754078\n",
      "epoch: 5 step: 443, loss is 0.00822583120316267\n",
      "epoch: 5 step: 444, loss is 0.03936178982257843\n",
      "epoch: 5 step: 445, loss is 0.0658705085515976\n",
      "epoch: 5 step: 446, loss is 0.012054391205310822\n",
      "epoch: 5 step: 447, loss is 0.0024719969369471073\n",
      "epoch: 5 step: 448, loss is 0.3508289158344269\n",
      "epoch: 5 step: 449, loss is 0.04910986125469208\n",
      "epoch: 5 step: 450, loss is 0.09887004643678665\n",
      "epoch: 5 step: 451, loss is 0.0016914322040975094\n",
      "epoch: 5 step: 452, loss is 0.10955354571342468\n",
      "epoch: 5 step: 453, loss is 0.015481931157410145\n",
      "epoch: 5 step: 454, loss is 0.1035289540886879\n",
      "epoch: 5 step: 455, loss is 0.17551234364509583\n",
      "epoch: 5 step: 456, loss is 0.0018009322229772806\n",
      "epoch: 5 step: 457, loss is 0.0048203193582594395\n",
      "epoch: 5 step: 458, loss is 0.15384797751903534\n",
      "epoch: 5 step: 459, loss is 0.11094067990779877\n",
      "epoch: 5 step: 460, loss is 0.0553564615547657\n",
      "epoch: 5 step: 461, loss is 0.015445365570485592\n",
      "epoch: 5 step: 462, loss is 0.1308176964521408\n",
      "epoch: 5 step: 463, loss is 0.08708981424570084\n",
      "epoch: 5 step: 464, loss is 0.08309119939804077\n",
      "epoch: 5 step: 465, loss is 0.011876932345330715\n",
      "epoch: 5 step: 466, loss is 0.09073282033205032\n",
      "epoch: 5 step: 467, loss is 0.20654679834842682\n",
      "epoch: 5 step: 468, loss is 0.009941807016730309\n",
      "epoch: 5 step: 469, loss is 0.05236607789993286\n",
      "epoch: 5 step: 470, loss is 0.156524658203125\n",
      "epoch: 5 step: 471, loss is 0.19922205805778503\n",
      "epoch: 5 step: 472, loss is 0.017130965366959572\n",
      "epoch: 5 step: 473, loss is 0.07334492355585098\n",
      "epoch: 5 step: 474, loss is 0.1805662214756012\n",
      "epoch: 5 step: 475, loss is 0.11170889437198639\n",
      "epoch: 5 step: 476, loss is 0.0025768561754375696\n",
      "epoch: 5 step: 477, loss is 0.008183392696082592\n",
      "epoch: 5 step: 478, loss is 0.16873611509799957\n",
      "epoch: 5 step: 479, loss is 0.09302908927202225\n",
      "epoch: 5 step: 480, loss is 0.017268458381295204\n",
      "epoch: 5 step: 481, loss is 0.2858419120311737\n",
      "epoch: 5 step: 482, loss is 0.014509228989481926\n",
      "epoch: 5 step: 483, loss is 0.07018470019102097\n",
      "epoch: 5 step: 484, loss is 0.038368381559848785\n",
      "epoch: 5 step: 485, loss is 0.07409588992595673\n",
      "epoch: 5 step: 486, loss is 0.0017985630547627807\n",
      "epoch: 5 step: 487, loss is 0.07999210804700851\n",
      "epoch: 5 step: 488, loss is 0.025215256959199905\n",
      "epoch: 5 step: 489, loss is 0.14665192365646362\n",
      "epoch: 5 step: 490, loss is 0.020551113411784172\n",
      "epoch: 5 step: 491, loss is 0.054242271929979324\n",
      "epoch: 5 step: 492, loss is 0.01603756658732891\n",
      "epoch: 5 step: 493, loss is 0.09086892753839493\n",
      "epoch: 5 step: 494, loss is 0.04803214967250824\n",
      "epoch: 5 step: 495, loss is 0.04041565954685211\n",
      "epoch: 5 step: 496, loss is 0.26471027731895447\n",
      "epoch: 5 step: 497, loss is 0.027100348845124245\n",
      "epoch: 5 step: 498, loss is 0.13008101284503937\n",
      "epoch: 5 step: 499, loss is 0.12301371991634369\n",
      "epoch: 5 step: 500, loss is 0.02583630196750164\n",
      "epoch: 5 step: 501, loss is 0.15353497862815857\n",
      "epoch: 5 step: 502, loss is 0.013388117775321007\n",
      "epoch: 5 step: 503, loss is 0.014550477266311646\n",
      "epoch: 5 step: 504, loss is 0.06986574083566666\n",
      "epoch: 5 step: 505, loss is 0.037276044487953186\n",
      "epoch: 5 step: 506, loss is 0.06872731447219849\n",
      "epoch: 5 step: 507, loss is 0.03591510280966759\n",
      "epoch: 5 step: 508, loss is 0.005669891368597746\n",
      "epoch: 5 step: 509, loss is 0.037930216640233994\n",
      "epoch: 5 step: 510, loss is 0.016318429261446\n",
      "epoch: 5 step: 511, loss is 0.07350694388151169\n",
      "epoch: 5 step: 512, loss is 0.00706471549347043\n",
      "epoch: 5 step: 513, loss is 0.058772701770067215\n",
      "epoch: 5 step: 514, loss is 0.0024587195366621017\n",
      "epoch: 5 step: 515, loss is 0.008318441919982433\n",
      "epoch: 5 step: 516, loss is 0.008052904158830643\n",
      "epoch: 5 step: 517, loss is 0.02542571723461151\n",
      "epoch: 5 step: 518, loss is 0.15390421450138092\n",
      "epoch: 5 step: 519, loss is 0.020949939265847206\n",
      "epoch: 5 step: 520, loss is 0.02588316798210144\n",
      "epoch: 5 step: 521, loss is 0.14174242317676544\n",
      "epoch: 5 step: 522, loss is 0.02217806875705719\n",
      "epoch: 5 step: 523, loss is 0.009108194150030613\n",
      "epoch: 5 step: 524, loss is 0.04281124472618103\n",
      "epoch: 5 step: 525, loss is 0.1225486546754837\n",
      "epoch: 5 step: 526, loss is 0.001664998708292842\n",
      "epoch: 5 step: 527, loss is 0.03084598295390606\n",
      "epoch: 5 step: 528, loss is 0.005214414559304714\n",
      "epoch: 5 step: 529, loss is 0.008769115433096886\n",
      "epoch: 5 step: 530, loss is 0.22477181255817413\n",
      "epoch: 5 step: 531, loss is 0.00684831477701664\n",
      "epoch: 5 step: 532, loss is 0.04786801338195801\n",
      "epoch: 5 step: 533, loss is 0.05765138939023018\n",
      "epoch: 5 step: 534, loss is 0.027146819978952408\n",
      "epoch: 5 step: 535, loss is 0.057726114988327026\n",
      "epoch: 5 step: 536, loss is 0.025141550227999687\n",
      "epoch: 5 step: 537, loss is 0.0007260228740051389\n",
      "epoch: 5 step: 538, loss is 0.12831363081932068\n",
      "epoch: 5 step: 539, loss is 0.2471044957637787\n",
      "epoch: 5 step: 540, loss is 0.15445730090141296\n",
      "epoch: 5 step: 541, loss is 0.01845225691795349\n",
      "epoch: 5 step: 542, loss is 0.03257746994495392\n",
      "epoch: 5 step: 543, loss is 0.08270672708749771\n",
      "epoch: 5 step: 544, loss is 0.008114397525787354\n",
      "epoch: 5 step: 545, loss is 0.008475254289805889\n",
      "epoch: 5 step: 546, loss is 0.141541987657547\n",
      "epoch: 5 step: 547, loss is 0.2418927401304245\n",
      "epoch: 5 step: 548, loss is 0.011317860335111618\n",
      "epoch: 5 step: 549, loss is 0.14712032675743103\n",
      "epoch: 5 step: 550, loss is 0.0240703746676445\n",
      "epoch: 5 step: 551, loss is 0.01684449426829815\n",
      "epoch: 5 step: 552, loss is 0.022497106343507767\n",
      "epoch: 5 step: 553, loss is 0.014592167921364307\n",
      "epoch: 5 step: 554, loss is 0.04257640242576599\n",
      "epoch: 5 step: 555, loss is 0.017042959108948708\n",
      "epoch: 5 step: 556, loss is 0.03598221391439438\n",
      "epoch: 5 step: 557, loss is 0.00562239671126008\n",
      "epoch: 5 step: 558, loss is 0.019340869039297104\n",
      "epoch: 5 step: 559, loss is 0.0024886883329600096\n",
      "epoch: 5 step: 560, loss is 0.0024912485387176275\n",
      "epoch: 5 step: 561, loss is 0.006233401596546173\n",
      "epoch: 5 step: 562, loss is 0.02737678587436676\n",
      "epoch: 5 step: 563, loss is 0.0547056719660759\n",
      "epoch: 5 step: 564, loss is 0.015265294350683689\n",
      "epoch: 5 step: 565, loss is 0.03655657172203064\n",
      "epoch: 5 step: 566, loss is 0.028627365827560425\n",
      "epoch: 5 step: 567, loss is 0.14861805737018585\n",
      "epoch: 5 step: 568, loss is 0.0014944544527679682\n",
      "epoch: 5 step: 569, loss is 0.005637560971081257\n",
      "epoch: 5 step: 570, loss is 0.011969117447733879\n",
      "epoch: 5 step: 571, loss is 0.010669440031051636\n",
      "epoch: 5 step: 572, loss is 0.0020409910939633846\n",
      "epoch: 5 step: 573, loss is 0.00023741391487419605\n",
      "epoch: 5 step: 574, loss is 0.03815083205699921\n",
      "epoch: 5 step: 575, loss is 0.10630079358816147\n",
      "epoch: 5 step: 576, loss is 0.028465842828154564\n",
      "epoch: 5 step: 577, loss is 0.08005142956972122\n",
      "epoch: 5 step: 578, loss is 0.03409533575177193\n",
      "epoch: 5 step: 579, loss is 0.0699966698884964\n",
      "epoch: 5 step: 580, loss is 0.020716072991490364\n",
      "epoch: 5 step: 581, loss is 0.015315352939069271\n",
      "epoch: 5 step: 582, loss is 0.07817402482032776\n",
      "epoch: 5 step: 583, loss is 0.02528955228626728\n",
      "epoch: 5 step: 584, loss is 0.002093792427331209\n",
      "epoch: 5 step: 585, loss is 0.0457867868244648\n",
      "epoch: 5 step: 586, loss is 0.04262062907218933\n",
      "epoch: 5 step: 587, loss is 0.22240398824214935\n",
      "epoch: 5 step: 588, loss is 0.025863369926810265\n",
      "epoch: 5 step: 589, loss is 0.08174648135900497\n",
      "epoch: 5 step: 590, loss is 0.008948946371674538\n",
      "epoch: 5 step: 591, loss is 0.05771447345614433\n",
      "epoch: 5 step: 592, loss is 0.005978074856102467\n",
      "epoch: 5 step: 593, loss is 0.10272820293903351\n",
      "epoch: 5 step: 594, loss is 0.10751974582672119\n",
      "epoch: 5 step: 595, loss is 0.05294843018054962\n",
      "epoch: 5 step: 596, loss is 0.029105301946401596\n",
      "epoch: 5 step: 597, loss is 0.010196558199822903\n",
      "epoch: 5 step: 598, loss is 0.07223496586084366\n",
      "epoch: 5 step: 599, loss is 0.11124329268932343\n",
      "epoch: 5 step: 600, loss is 0.017276525497436523\n",
      "epoch: 5 step: 601, loss is 0.001778170233592391\n",
      "epoch: 5 step: 602, loss is 0.243239626288414\n",
      "epoch: 5 step: 603, loss is 0.006820693612098694\n",
      "epoch: 5 step: 604, loss is 0.20043730735778809\n",
      "epoch: 5 step: 605, loss is 0.028356587514281273\n",
      "epoch: 5 step: 606, loss is 0.014573912136256695\n",
      "epoch: 5 step: 607, loss is 0.027774186804890633\n",
      "epoch: 5 step: 608, loss is 0.03657189756631851\n",
      "epoch: 5 step: 609, loss is 0.005128434393554926\n",
      "epoch: 5 step: 610, loss is 0.018201611936092377\n",
      "epoch: 5 step: 611, loss is 0.062737837433815\n",
      "epoch: 5 step: 612, loss is 0.016131319105625153\n",
      "epoch: 5 step: 613, loss is 0.006988867651671171\n",
      "epoch: 5 step: 614, loss is 0.040072593837976456\n",
      "epoch: 5 step: 615, loss is 0.03493106737732887\n",
      "epoch: 5 step: 616, loss is 0.025960318744182587\n",
      "epoch: 5 step: 617, loss is 0.009831434115767479\n",
      "epoch: 5 step: 618, loss is 0.08226115256547928\n",
      "epoch: 5 step: 619, loss is 0.0028377408161759377\n",
      "epoch: 5 step: 620, loss is 0.0010836371220648289\n",
      "epoch: 5 step: 621, loss is 0.011560034938156605\n",
      "epoch: 5 step: 622, loss is 0.02303295210003853\n",
      "epoch: 5 step: 623, loss is 0.026047473773360252\n",
      "epoch: 5 step: 624, loss is 0.020627886056900024\n",
      "epoch: 5 step: 625, loss is 0.0009700648952275515\n",
      "epoch: 5 step: 626, loss is 0.009364267811179161\n",
      "epoch: 5 step: 627, loss is 0.20155611634254456\n",
      "epoch: 5 step: 628, loss is 0.005764314904808998\n",
      "epoch: 5 step: 629, loss is 0.000717724789865315\n",
      "epoch: 5 step: 630, loss is 0.061381638050079346\n",
      "epoch: 5 step: 631, loss is 0.0795307382941246\n",
      "epoch: 5 step: 632, loss is 0.05919778719544411\n",
      "epoch: 5 step: 633, loss is 0.0017190328799188137\n",
      "epoch: 5 step: 634, loss is 0.011590142734348774\n",
      "epoch: 5 step: 635, loss is 0.001455611316487193\n",
      "epoch: 5 step: 636, loss is 0.13267552852630615\n",
      "epoch: 5 step: 637, loss is 0.00800745002925396\n",
      "epoch: 5 step: 638, loss is 0.06901688128709793\n",
      "epoch: 5 step: 639, loss is 0.05329367518424988\n",
      "epoch: 5 step: 640, loss is 0.016299881041049957\n",
      "epoch: 5 step: 641, loss is 0.0032301251776516438\n",
      "epoch: 5 step: 642, loss is 0.001091731945052743\n",
      "epoch: 5 step: 643, loss is 0.00041675177635625005\n",
      "epoch: 5 step: 644, loss is 0.08831578493118286\n",
      "epoch: 5 step: 645, loss is 0.10236825793981552\n",
      "epoch: 5 step: 646, loss is 0.013200739398598671\n",
      "epoch: 5 step: 647, loss is 0.0063769579865038395\n",
      "epoch: 5 step: 648, loss is 0.007261643186211586\n",
      "epoch: 5 step: 649, loss is 0.027753282338380814\n",
      "epoch: 5 step: 650, loss is 0.002816498279571533\n",
      "epoch: 5 step: 651, loss is 0.0014322822680696845\n",
      "epoch: 5 step: 652, loss is 0.0089413458481431\n",
      "epoch: 5 step: 653, loss is 0.002092285081744194\n",
      "epoch: 5 step: 654, loss is 0.022908955812454224\n",
      "epoch: 5 step: 655, loss is 0.03390520066022873\n",
      "epoch: 5 step: 656, loss is 0.20321916043758392\n",
      "epoch: 5 step: 657, loss is 0.007467283401638269\n",
      "epoch: 5 step: 658, loss is 0.0028984379023313522\n",
      "epoch: 5 step: 659, loss is 0.007084712851792574\n",
      "epoch: 5 step: 660, loss is 0.0008163913153111935\n",
      "epoch: 5 step: 661, loss is 0.013579605147242546\n",
      "epoch: 5 step: 662, loss is 0.0011348446132615209\n",
      "epoch: 5 step: 663, loss is 0.014978133141994476\n",
      "epoch: 5 step: 664, loss is 0.17027126252651215\n",
      "epoch: 5 step: 665, loss is 0.003554191440343857\n",
      "epoch: 5 step: 666, loss is 0.008586529642343521\n",
      "epoch: 5 step: 667, loss is 0.050807494670152664\n",
      "epoch: 5 step: 668, loss is 0.0021277694031596184\n",
      "epoch: 5 step: 669, loss is 0.036686744540929794\n",
      "epoch: 5 step: 670, loss is 0.0019663486164063215\n",
      "epoch: 5 step: 671, loss is 0.0036038807593286037\n",
      "epoch: 5 step: 672, loss is 0.006046933587640524\n",
      "epoch: 5 step: 673, loss is 0.028103100135922432\n",
      "epoch: 5 step: 674, loss is 0.0013024720828980207\n",
      "epoch: 5 step: 675, loss is 0.034902822226285934\n",
      "epoch: 5 step: 676, loss is 0.0024663773365318775\n",
      "epoch: 5 step: 677, loss is 0.0002225835487479344\n",
      "epoch: 5 step: 678, loss is 0.008569475263357162\n",
      "epoch: 5 step: 679, loss is 0.0756974145770073\n",
      "epoch: 5 step: 680, loss is 0.015816569328308105\n",
      "epoch: 5 step: 681, loss is 0.05933019891381264\n",
      "epoch: 5 step: 682, loss is 0.014126825146377087\n",
      "epoch: 5 step: 683, loss is 0.04635359346866608\n",
      "epoch: 5 step: 684, loss is 0.016921764239668846\n",
      "epoch: 5 step: 685, loss is 0.005020687822252512\n",
      "epoch: 5 step: 686, loss is 0.0029137572273612022\n",
      "epoch: 5 step: 687, loss is 0.14307813346385956\n",
      "epoch: 5 step: 688, loss is 0.003279671771451831\n",
      "epoch: 5 step: 689, loss is 0.06457505375146866\n",
      "epoch: 5 step: 690, loss is 0.1773872822523117\n",
      "epoch: 5 step: 691, loss is 0.06004742160439491\n",
      "epoch: 5 step: 692, loss is 0.002534833736717701\n",
      "epoch: 5 step: 693, loss is 0.021116027608513832\n",
      "epoch: 5 step: 694, loss is 0.02751314640045166\n",
      "epoch: 5 step: 695, loss is 0.014450130052864552\n",
      "epoch: 5 step: 696, loss is 0.08402857184410095\n",
      "epoch: 5 step: 697, loss is 0.008885534480214119\n",
      "epoch: 5 step: 698, loss is 0.12848006188869476\n",
      "epoch: 5 step: 699, loss is 0.11612717807292938\n",
      "epoch: 5 step: 700, loss is 0.08568418025970459\n",
      "epoch: 5 step: 701, loss is 0.006897265091538429\n",
      "epoch: 5 step: 702, loss is 0.14107072353363037\n",
      "epoch: 5 step: 703, loss is 0.013593481853604317\n",
      "epoch: 5 step: 704, loss is 0.029918015003204346\n",
      "epoch: 5 step: 705, loss is 0.026868470013141632\n",
      "epoch: 5 step: 706, loss is 0.010377734899520874\n",
      "epoch: 5 step: 707, loss is 0.005299833137542009\n",
      "epoch: 5 step: 708, loss is 0.05478418990969658\n",
      "epoch: 5 step: 709, loss is 0.011728446930646896\n",
      "epoch: 5 step: 710, loss is 0.013496206142008305\n",
      "epoch: 5 step: 711, loss is 0.13140353560447693\n",
      "epoch: 5 step: 712, loss is 0.1972985416650772\n",
      "epoch: 5 step: 713, loss is 0.0039032152853906155\n",
      "epoch: 5 step: 714, loss is 0.024260176345705986\n",
      "epoch: 5 step: 715, loss is 0.008729378692805767\n",
      "epoch: 5 step: 716, loss is 0.02457045204937458\n",
      "epoch: 5 step: 717, loss is 0.006909073330461979\n",
      "epoch: 5 step: 718, loss is 0.027642743661999702\n",
      "epoch: 5 step: 719, loss is 0.002190025057643652\n",
      "epoch: 5 step: 720, loss is 0.009899712167680264\n",
      "epoch: 5 step: 721, loss is 0.06405976414680481\n",
      "epoch: 5 step: 722, loss is 0.017210843041539192\n",
      "epoch: 5 step: 723, loss is 0.0026893061585724354\n",
      "epoch: 5 step: 724, loss is 0.007500329986214638\n",
      "epoch: 5 step: 725, loss is 0.0029071986209601164\n",
      "epoch: 5 step: 726, loss is 0.09738727658987045\n",
      "epoch: 5 step: 727, loss is 0.01659163273870945\n",
      "epoch: 5 step: 728, loss is 0.014459570869803429\n",
      "epoch: 5 step: 729, loss is 0.01915477029979229\n",
      "epoch: 5 step: 730, loss is 0.0030318659264594316\n",
      "epoch: 5 step: 731, loss is 0.0011729598045349121\n",
      "epoch: 5 step: 732, loss is 0.0007279671262949705\n",
      "epoch: 5 step: 733, loss is 0.0038195461966097355\n",
      "epoch: 5 step: 734, loss is 0.001012326218187809\n",
      "epoch: 5 step: 735, loss is 0.008323806338012218\n",
      "epoch: 5 step: 736, loss is 0.0014252832625061274\n",
      "epoch: 5 step: 737, loss is 0.3683473467826843\n",
      "epoch: 5 step: 738, loss is 0.003357178531587124\n",
      "epoch: 5 step: 739, loss is 0.05534324049949646\n",
      "epoch: 5 step: 740, loss is 0.017915522679686546\n",
      "epoch: 5 step: 741, loss is 0.017050087451934814\n",
      "epoch: 5 step: 742, loss is 0.06499016284942627\n",
      "epoch: 5 step: 743, loss is 0.0957190990447998\n",
      "epoch: 5 step: 744, loss is 0.07019456475973129\n",
      "epoch: 5 step: 745, loss is 0.3125201463699341\n",
      "epoch: 5 step: 746, loss is 0.041307754814624786\n",
      "epoch: 5 step: 747, loss is 0.19309930503368378\n",
      "epoch: 5 step: 748, loss is 0.16113634407520294\n",
      "epoch: 5 step: 749, loss is 0.24376413226127625\n",
      "epoch: 5 step: 750, loss is 0.3289221525192261\n",
      "epoch: 5 step: 751, loss is 0.0023380357306450605\n",
      "epoch: 5 step: 752, loss is 0.005752707831561565\n",
      "epoch: 5 step: 753, loss is 0.013709536753594875\n",
      "epoch: 5 step: 754, loss is 0.004528252873569727\n",
      "epoch: 5 step: 755, loss is 0.017531607300043106\n",
      "epoch: 5 step: 756, loss is 0.09720586985349655\n",
      "epoch: 5 step: 757, loss is 0.011835722252726555\n",
      "epoch: 5 step: 758, loss is 0.03310445696115494\n",
      "epoch: 5 step: 759, loss is 0.10393578559160233\n",
      "epoch: 5 step: 760, loss is 0.025277191773056984\n",
      "epoch: 5 step: 761, loss is 0.0099177910014987\n",
      "epoch: 5 step: 762, loss is 0.1028599813580513\n",
      "epoch: 5 step: 763, loss is 0.020561067387461662\n",
      "epoch: 5 step: 764, loss is 0.003083391347900033\n",
      "epoch: 5 step: 765, loss is 0.0537029467523098\n",
      "epoch: 5 step: 766, loss is 0.00221449532546103\n",
      "epoch: 5 step: 767, loss is 0.06096857413649559\n",
      "epoch: 5 step: 768, loss is 0.0744510293006897\n",
      "epoch: 5 step: 769, loss is 0.21047964692115784\n",
      "epoch: 5 step: 770, loss is 0.033088408410549164\n",
      "epoch: 5 step: 771, loss is 0.012666070833802223\n",
      "epoch: 5 step: 772, loss is 0.01951526664197445\n",
      "epoch: 5 step: 773, loss is 0.0014045335119590163\n",
      "epoch: 5 step: 774, loss is 0.003090798621997237\n",
      "epoch: 5 step: 775, loss is 0.05136720836162567\n",
      "epoch: 5 step: 776, loss is 0.02398344874382019\n",
      "epoch: 5 step: 777, loss is 0.15934590995311737\n",
      "epoch: 5 step: 778, loss is 0.014491788111627102\n",
      "epoch: 5 step: 779, loss is 0.004327850416302681\n",
      "epoch: 5 step: 780, loss is 0.002943729981780052\n",
      "epoch: 5 step: 781, loss is 0.012309550307691097\n",
      "epoch: 5 step: 782, loss is 0.14014597237110138\n",
      "epoch: 5 step: 783, loss is 0.12031644582748413\n",
      "epoch: 5 step: 784, loss is 0.10264299064874649\n",
      "epoch: 5 step: 785, loss is 0.07370591163635254\n",
      "epoch: 5 step: 786, loss is 0.012395394034683704\n",
      "epoch: 5 step: 787, loss is 0.035417310893535614\n",
      "epoch: 5 step: 788, loss is 0.004337006714195013\n",
      "epoch: 5 step: 789, loss is 0.010847595520317554\n",
      "epoch: 5 step: 790, loss is 0.022968720644712448\n",
      "epoch: 5 step: 791, loss is 0.16076882183551788\n",
      "epoch: 5 step: 792, loss is 0.0005774641176685691\n",
      "epoch: 5 step: 793, loss is 0.0018823272548615932\n",
      "epoch: 5 step: 794, loss is 0.0961640477180481\n",
      "epoch: 5 step: 795, loss is 0.0950172170996666\n",
      "epoch: 5 step: 796, loss is 0.0765918716788292\n",
      "epoch: 5 step: 797, loss is 0.026929402723908424\n",
      "epoch: 5 step: 798, loss is 0.0028654432389885187\n",
      "epoch: 5 step: 799, loss is 0.00822548195719719\n",
      "epoch: 5 step: 800, loss is 0.020591018721461296\n",
      "epoch: 5 step: 801, loss is 0.002276065293699503\n",
      "epoch: 5 step: 802, loss is 0.005768826697021723\n",
      "epoch: 5 step: 803, loss is 0.10391567647457123\n",
      "epoch: 5 step: 804, loss is 0.005123272072523832\n",
      "epoch: 5 step: 805, loss is 0.15266595780849457\n",
      "epoch: 5 step: 806, loss is 0.006527577061206102\n",
      "epoch: 5 step: 807, loss is 0.01228219736367464\n",
      "epoch: 5 step: 808, loss is 0.012749177403748035\n",
      "epoch: 5 step: 809, loss is 0.036716561764478683\n",
      "epoch: 5 step: 810, loss is 0.024587854743003845\n",
      "epoch: 5 step: 811, loss is 0.010540506802499294\n",
      "epoch: 5 step: 812, loss is 0.05070239678025246\n",
      "epoch: 5 step: 813, loss is 0.08504180610179901\n",
      "epoch: 5 step: 814, loss is 0.10644897818565369\n",
      "epoch: 5 step: 815, loss is 0.03382350876927376\n",
      "epoch: 5 step: 816, loss is 0.007787910755723715\n",
      "epoch: 5 step: 817, loss is 0.09684409201145172\n",
      "epoch: 5 step: 818, loss is 0.1288377046585083\n",
      "epoch: 5 step: 819, loss is 0.032437920570373535\n",
      "epoch: 5 step: 820, loss is 0.00246335007250309\n",
      "epoch: 5 step: 821, loss is 0.03143330663442612\n",
      "epoch: 5 step: 822, loss is 0.0034679078962653875\n",
      "epoch: 5 step: 823, loss is 0.0710994303226471\n",
      "epoch: 5 step: 824, loss is 0.07367100566625595\n",
      "epoch: 5 step: 825, loss is 0.017016448080539703\n",
      "epoch: 5 step: 826, loss is 0.008455158211290836\n",
      "epoch: 5 step: 827, loss is 0.013828771188855171\n",
      "epoch: 5 step: 828, loss is 0.0013453115243464708\n",
      "epoch: 5 step: 829, loss is 0.02981739118695259\n",
      "epoch: 5 step: 830, loss is 0.0035649845376610756\n",
      "epoch: 5 step: 831, loss is 0.008775533176958561\n",
      "epoch: 5 step: 832, loss is 0.018667081370949745\n",
      "epoch: 5 step: 833, loss is 0.08410979807376862\n",
      "epoch: 5 step: 834, loss is 0.06978914141654968\n",
      "epoch: 5 step: 835, loss is 0.02105681039392948\n",
      "epoch: 5 step: 836, loss is 0.006276316940784454\n",
      "epoch: 5 step: 837, loss is 0.3055144250392914\n",
      "epoch: 5 step: 838, loss is 0.036122702062129974\n",
      "epoch: 5 step: 839, loss is 0.08159931749105453\n",
      "epoch: 5 step: 840, loss is 0.14602285623550415\n",
      "epoch: 5 step: 841, loss is 0.1606094241142273\n",
      "epoch: 5 step: 842, loss is 0.13780170679092407\n",
      "epoch: 5 step: 843, loss is 0.23334816098213196\n",
      "epoch: 5 step: 844, loss is 0.013333545997738838\n",
      "epoch: 5 step: 845, loss is 0.014742892235517502\n",
      "epoch: 5 step: 846, loss is 0.04647206887602806\n",
      "epoch: 5 step: 847, loss is 0.011466009542346\n",
      "epoch: 5 step: 848, loss is 0.012204153463244438\n",
      "epoch: 5 step: 849, loss is 0.026782691478729248\n",
      "epoch: 5 step: 850, loss is 0.024976780638098717\n",
      "epoch: 5 step: 851, loss is 0.007206132169812918\n",
      "epoch: 5 step: 852, loss is 0.02248334139585495\n",
      "epoch: 5 step: 853, loss is 0.03651520982384682\n",
      "epoch: 5 step: 854, loss is 0.04087480902671814\n",
      "epoch: 5 step: 855, loss is 0.057482797652482986\n",
      "epoch: 5 step: 856, loss is 0.010219944640994072\n",
      "epoch: 5 step: 857, loss is 0.03340579569339752\n",
      "epoch: 5 step: 858, loss is 0.025246229022741318\n",
      "epoch: 5 step: 859, loss is 0.2519272267818451\n",
      "epoch: 5 step: 860, loss is 0.1261918842792511\n",
      "epoch: 5 step: 861, loss is 0.030503127723932266\n",
      "epoch: 5 step: 862, loss is 0.02421014942228794\n",
      "epoch: 5 step: 863, loss is 0.03364209830760956\n",
      "epoch: 5 step: 864, loss is 0.10992485284805298\n",
      "epoch: 5 step: 865, loss is 0.0024649668484926224\n",
      "epoch: 5 step: 866, loss is 0.013426180928945541\n",
      "epoch: 5 step: 867, loss is 0.08261997997760773\n",
      "epoch: 5 step: 868, loss is 0.039304569363594055\n",
      "epoch: 5 step: 869, loss is 0.2597683072090149\n",
      "epoch: 5 step: 870, loss is 0.03219761326909065\n",
      "epoch: 5 step: 871, loss is 0.0548337884247303\n",
      "epoch: 5 step: 872, loss is 0.01715121790766716\n",
      "epoch: 5 step: 873, loss is 0.0229042861610651\n",
      "epoch: 5 step: 874, loss is 0.004795036744326353\n",
      "epoch: 5 step: 875, loss is 0.02390654757618904\n",
      "epoch: 5 step: 876, loss is 0.04758349806070328\n",
      "epoch: 5 step: 877, loss is 0.013695823028683662\n",
      "epoch: 5 step: 878, loss is 0.06719473004341125\n",
      "epoch: 5 step: 879, loss is 0.007112432736903429\n",
      "epoch: 5 step: 880, loss is 0.2576048970222473\n",
      "epoch: 5 step: 881, loss is 0.09363307058811188\n",
      "epoch: 5 step: 882, loss is 0.005584836006164551\n",
      "epoch: 5 step: 883, loss is 0.048209693282842636\n",
      "epoch: 5 step: 884, loss is 0.00289357197470963\n",
      "epoch: 5 step: 885, loss is 0.029081283137202263\n",
      "epoch: 5 step: 886, loss is 0.006334901787340641\n",
      "epoch: 5 step: 887, loss is 0.033633213490247726\n",
      "epoch: 5 step: 888, loss is 0.08133493363857269\n",
      "epoch: 5 step: 889, loss is 0.10729246586561203\n",
      "epoch: 5 step: 890, loss is 0.017868783324956894\n",
      "epoch: 5 step: 891, loss is 0.0125141441822052\n",
      "epoch: 5 step: 892, loss is 0.026397986337542534\n",
      "epoch: 5 step: 893, loss is 0.06509152799844742\n",
      "epoch: 5 step: 894, loss is 0.07632561773061752\n",
      "epoch: 5 step: 895, loss is 0.17184951901435852\n",
      "epoch: 5 step: 896, loss is 0.046315357089042664\n",
      "epoch: 5 step: 897, loss is 0.10971922427415848\n",
      "epoch: 5 step: 898, loss is 0.0030568530783057213\n",
      "epoch: 5 step: 899, loss is 0.05562574788928032\n",
      "epoch: 5 step: 900, loss is 0.009575753472745419\n",
      "epoch: 5 step: 901, loss is 0.024005645886063576\n",
      "epoch: 5 step: 902, loss is 0.012164724059402943\n",
      "epoch: 5 step: 903, loss is 0.0008449216838926077\n",
      "epoch: 5 step: 904, loss is 0.014002031646668911\n",
      "epoch: 5 step: 905, loss is 0.008028654381632805\n",
      "epoch: 5 step: 906, loss is 0.08424661308526993\n",
      "epoch: 5 step: 907, loss is 0.03379695862531662\n",
      "epoch: 5 step: 908, loss is 0.023367056623101234\n",
      "epoch: 5 step: 909, loss is 0.0014834074536338449\n",
      "epoch: 5 step: 910, loss is 0.0961744636297226\n",
      "epoch: 5 step: 911, loss is 0.02005656063556671\n",
      "epoch: 5 step: 912, loss is 0.14653469622135162\n",
      "epoch: 5 step: 913, loss is 0.14497967064380646\n",
      "epoch: 5 step: 914, loss is 0.01012409571558237\n",
      "epoch: 5 step: 915, loss is 0.20445486903190613\n",
      "epoch: 5 step: 916, loss is 0.002492938656359911\n",
      "epoch: 5 step: 917, loss is 0.01769072189927101\n",
      "epoch: 5 step: 918, loss is 0.022079549729824066\n",
      "epoch: 5 step: 919, loss is 0.05431545898318291\n",
      "epoch: 5 step: 920, loss is 0.1905265897512436\n",
      "epoch: 5 step: 921, loss is 0.27280929684638977\n",
      "epoch: 5 step: 922, loss is 0.0879753828048706\n",
      "epoch: 5 step: 923, loss is 0.09400572627782822\n",
      "epoch: 5 step: 924, loss is 0.03896934911608696\n",
      "epoch: 5 step: 925, loss is 0.0048176501877605915\n",
      "epoch: 5 step: 926, loss is 0.027930935844779015\n",
      "epoch: 5 step: 927, loss is 0.08408307284116745\n",
      "epoch: 5 step: 928, loss is 0.013763145543634892\n",
      "epoch: 5 step: 929, loss is 0.020670028403401375\n",
      "epoch: 5 step: 930, loss is 0.00733056478202343\n",
      "epoch: 5 step: 931, loss is 0.05718189477920532\n",
      "epoch: 5 step: 932, loss is 0.028568359091877937\n",
      "epoch: 5 step: 933, loss is 0.04011330381035805\n",
      "epoch: 5 step: 934, loss is 0.020397966727614403\n",
      "epoch: 5 step: 935, loss is 0.0244953241199255\n",
      "epoch: 5 step: 936, loss is 0.07123030722141266\n",
      "epoch: 5 step: 937, loss is 0.014116767793893814\n",
      "epoch: 5 step: 938, loss is 0.033731888979673386\n",
      "epoch: 5 step: 939, loss is 0.016996579244732857\n",
      "epoch: 5 step: 940, loss is 0.003952888771891594\n",
      "epoch: 5 step: 941, loss is 0.035300906747579575\n",
      "epoch: 5 step: 942, loss is 0.07701148837804794\n",
      "epoch: 5 step: 943, loss is 0.0045332335866987705\n",
      "epoch: 5 step: 944, loss is 0.003840879537165165\n",
      "epoch: 5 step: 945, loss is 0.003585736732929945\n",
      "epoch: 5 step: 946, loss is 0.006382739637047052\n",
      "epoch: 5 step: 947, loss is 0.027514826506376266\n",
      "epoch: 5 step: 948, loss is 0.0075370739214122295\n",
      "epoch: 5 step: 949, loss is 0.15575802326202393\n",
      "epoch: 5 step: 950, loss is 0.0347461961209774\n",
      "epoch: 5 step: 951, loss is 0.02957727573812008\n",
      "epoch: 5 step: 952, loss is 0.09279558062553406\n",
      "epoch: 5 step: 953, loss is 0.011059146374464035\n",
      "epoch: 5 step: 954, loss is 0.170002281665802\n",
      "epoch: 5 step: 955, loss is 0.019305720925331116\n",
      "epoch: 5 step: 956, loss is 0.02383449301123619\n",
      "epoch: 5 step: 957, loss is 0.0062195779755711555\n",
      "epoch: 5 step: 958, loss is 0.011468982324004173\n",
      "epoch: 5 step: 959, loss is 0.28386345505714417\n",
      "epoch: 5 step: 960, loss is 0.0019632182084023952\n",
      "epoch: 5 step: 961, loss is 0.0014213311951607466\n",
      "epoch: 5 step: 962, loss is 0.00047719007125124335\n",
      "epoch: 5 step: 963, loss is 0.04816159978508949\n",
      "epoch: 5 step: 964, loss is 0.019500618800520897\n",
      "epoch: 5 step: 965, loss is 0.020969189703464508\n",
      "epoch: 5 step: 966, loss is 0.08023906499147415\n",
      "epoch: 5 step: 967, loss is 0.2271210104227066\n",
      "epoch: 5 step: 968, loss is 0.14301057159900665\n",
      "epoch: 5 step: 969, loss is 0.01364829484373331\n",
      "epoch: 5 step: 970, loss is 0.05584242567420006\n",
      "epoch: 5 step: 971, loss is 0.007714347448199987\n",
      "epoch: 5 step: 972, loss is 0.16399091482162476\n",
      "epoch: 5 step: 973, loss is 0.0964788943529129\n",
      "epoch: 5 step: 974, loss is 0.11894389986991882\n",
      "epoch: 5 step: 975, loss is 0.0028538156766444445\n",
      "epoch: 5 step: 976, loss is 0.11212193965911865\n",
      "epoch: 5 step: 977, loss is 0.0233710128813982\n",
      "epoch: 5 step: 978, loss is 0.08109312504529953\n",
      "epoch: 5 step: 979, loss is 0.03665337711572647\n",
      "epoch: 5 step: 980, loss is 0.06935746222734451\n",
      "epoch: 5 step: 981, loss is 0.06325346231460571\n",
      "epoch: 5 step: 982, loss is 0.06506277620792389\n",
      "epoch: 5 step: 983, loss is 0.02909555286169052\n",
      "epoch: 5 step: 984, loss is 0.1195807009935379\n",
      "epoch: 5 step: 985, loss is 0.012239677831530571\n",
      "epoch: 5 step: 986, loss is 0.003237740835174918\n",
      "epoch: 5 step: 987, loss is 0.13652893900871277\n",
      "epoch: 5 step: 988, loss is 0.03903898596763611\n",
      "epoch: 5 step: 989, loss is 0.00980384275317192\n",
      "epoch: 5 step: 990, loss is 0.19290241599082947\n",
      "epoch: 5 step: 991, loss is 0.005806666798889637\n",
      "epoch: 5 step: 992, loss is 0.023366114124655724\n",
      "epoch: 5 step: 993, loss is 0.11352130025625229\n",
      "epoch: 5 step: 994, loss is 0.04044923558831215\n",
      "epoch: 5 step: 995, loss is 0.26476675271987915\n",
      "epoch: 5 step: 996, loss is 0.05252937227487564\n",
      "epoch: 5 step: 997, loss is 0.1370585709810257\n",
      "epoch: 5 step: 998, loss is 0.06928049772977829\n",
      "epoch: 5 step: 999, loss is 0.02058110572397709\n",
      "epoch: 5 step: 1000, loss is 0.005749709904193878\n",
      "epoch: 5 step: 1001, loss is 0.006614673417061567\n",
      "epoch: 5 step: 1002, loss is 0.034665461629629135\n",
      "epoch: 5 step: 1003, loss is 0.22900226712226868\n",
      "epoch: 5 step: 1004, loss is 0.004058187361806631\n",
      "epoch: 5 step: 1005, loss is 0.06235403195023537\n",
      "epoch: 5 step: 1006, loss is 0.004056145437061787\n",
      "epoch: 5 step: 1007, loss is 0.22239607572555542\n",
      "epoch: 5 step: 1008, loss is 0.0009914975380524993\n",
      "epoch: 5 step: 1009, loss is 0.009519482962787151\n",
      "epoch: 5 step: 1010, loss is 0.030172189697623253\n",
      "epoch: 5 step: 1011, loss is 0.15974639356136322\n",
      "epoch: 5 step: 1012, loss is 0.04228543862700462\n",
      "epoch: 5 step: 1013, loss is 0.1931295245885849\n",
      "epoch: 5 step: 1014, loss is 0.0037745069712400436\n",
      "epoch: 5 step: 1015, loss is 0.43666496872901917\n",
      "epoch: 5 step: 1016, loss is 0.04026390612125397\n",
      "epoch: 5 step: 1017, loss is 0.011032968759536743\n",
      "epoch: 5 step: 1018, loss is 0.014514295384287834\n",
      "epoch: 5 step: 1019, loss is 0.034483104944229126\n",
      "epoch: 5 step: 1020, loss is 0.001591857522726059\n",
      "epoch: 5 step: 1021, loss is 0.014261873438954353\n",
      "epoch: 5 step: 1022, loss is 0.008778601884841919\n",
      "epoch: 5 step: 1023, loss is 0.006683095823973417\n",
      "epoch: 5 step: 1024, loss is 0.06574299186468124\n",
      "epoch: 5 step: 1025, loss is 0.030403153970837593\n",
      "epoch: 5 step: 1026, loss is 0.04146784916520119\n",
      "epoch: 5 step: 1027, loss is 0.13572119176387787\n",
      "epoch: 5 step: 1028, loss is 0.009368207305669785\n",
      "epoch: 5 step: 1029, loss is 0.0011012658942490816\n",
      "epoch: 5 step: 1030, loss is 0.10724689811468124\n",
      "epoch: 5 step: 1031, loss is 0.06967588514089584\n",
      "epoch: 5 step: 1032, loss is 0.015895647928118706\n",
      "epoch: 5 step: 1033, loss is 0.032452404499053955\n",
      "epoch: 5 step: 1034, loss is 0.11834575235843658\n",
      "epoch: 5 step: 1035, loss is 0.0355578288435936\n",
      "epoch: 5 step: 1036, loss is 0.004144355654716492\n",
      "epoch: 5 step: 1037, loss is 0.009799604304134846\n",
      "epoch: 5 step: 1038, loss is 0.02571379765868187\n",
      "epoch: 5 step: 1039, loss is 0.09347500652074814\n",
      "epoch: 5 step: 1040, loss is 0.26915082335472107\n",
      "epoch: 5 step: 1041, loss is 0.12860380113124847\n",
      "epoch: 5 step: 1042, loss is 0.1530950963497162\n",
      "epoch: 5 step: 1043, loss is 0.0245039165019989\n",
      "epoch: 5 step: 1044, loss is 0.028491172939538956\n",
      "epoch: 5 step: 1045, loss is 0.04366808757185936\n",
      "epoch: 5 step: 1046, loss is 0.02779363840818405\n",
      "epoch: 5 step: 1047, loss is 0.028828231617808342\n",
      "epoch: 5 step: 1048, loss is 0.33091628551483154\n",
      "epoch: 5 step: 1049, loss is 0.0016375263221561909\n",
      "epoch: 5 step: 1050, loss is 0.013435057364404202\n",
      "epoch: 5 step: 1051, loss is 0.05430500954389572\n",
      "epoch: 5 step: 1052, loss is 0.02400360442698002\n",
      "epoch: 5 step: 1053, loss is 0.01469765231013298\n",
      "epoch: 5 step: 1054, loss is 0.006993856746703386\n",
      "epoch: 5 step: 1055, loss is 0.007565826177597046\n",
      "epoch: 5 step: 1056, loss is 0.02930169738829136\n",
      "epoch: 5 step: 1057, loss is 0.10844985395669937\n",
      "epoch: 5 step: 1058, loss is 0.00044502332457341254\n",
      "epoch: 5 step: 1059, loss is 0.03836595639586449\n",
      "epoch: 5 step: 1060, loss is 0.00426302058622241\n",
      "epoch: 5 step: 1061, loss is 0.021060200408101082\n",
      "epoch: 5 step: 1062, loss is 0.041859954595565796\n",
      "epoch: 5 step: 1063, loss is 0.03147219866514206\n",
      "epoch: 5 step: 1064, loss is 0.04747067391872406\n",
      "epoch: 5 step: 1065, loss is 0.10749553889036179\n",
      "epoch: 5 step: 1066, loss is 0.031164484098553658\n",
      "epoch: 5 step: 1067, loss is 0.16130003333091736\n",
      "epoch: 5 step: 1068, loss is 0.04522848501801491\n",
      "epoch: 5 step: 1069, loss is 0.09454334527254105\n",
      "epoch: 5 step: 1070, loss is 0.016512414440512657\n",
      "epoch: 5 step: 1071, loss is 0.3584947884082794\n",
      "epoch: 5 step: 1072, loss is 0.010129312053322792\n",
      "epoch: 5 step: 1073, loss is 0.010528476908802986\n",
      "epoch: 5 step: 1074, loss is 0.10588046163320541\n",
      "epoch: 5 step: 1075, loss is 0.015020630322396755\n",
      "epoch: 5 step: 1076, loss is 0.0009553618729114532\n",
      "epoch: 5 step: 1077, loss is 0.01852487400174141\n",
      "epoch: 5 step: 1078, loss is 0.009778437204658985\n",
      "epoch: 5 step: 1079, loss is 0.14340652525424957\n",
      "epoch: 5 step: 1080, loss is 0.019903449341654778\n",
      "epoch: 5 step: 1081, loss is 0.016576535999774933\n",
      "epoch: 5 step: 1082, loss is 0.006781405303627253\n",
      "epoch: 5 step: 1083, loss is 0.1352054923772812\n",
      "epoch: 5 step: 1084, loss is 0.13009576499462128\n",
      "epoch: 5 step: 1085, loss is 0.0017696483992040157\n",
      "epoch: 5 step: 1086, loss is 0.07231930643320084\n",
      "epoch: 5 step: 1087, loss is 0.0029862723313272\n",
      "epoch: 5 step: 1088, loss is 0.004523253068327904\n",
      "epoch: 5 step: 1089, loss is 0.005545015912503004\n",
      "epoch: 5 step: 1090, loss is 0.027419162914156914\n",
      "epoch: 5 step: 1091, loss is 0.04931645095348358\n",
      "epoch: 5 step: 1092, loss is 0.04850757122039795\n",
      "epoch: 5 step: 1093, loss is 0.012639643624424934\n",
      "epoch: 5 step: 1094, loss is 0.009617791511118412\n",
      "epoch: 5 step: 1095, loss is 0.006585945840924978\n",
      "epoch: 5 step: 1096, loss is 0.01869177632033825\n",
      "epoch: 5 step: 1097, loss is 0.005375188309699297\n",
      "epoch: 5 step: 1098, loss is 0.3313610255718231\n",
      "epoch: 5 step: 1099, loss is 0.03252224624156952\n",
      "epoch: 5 step: 1100, loss is 0.004884330090135336\n",
      "epoch: 5 step: 1101, loss is 0.20874257385730743\n",
      "epoch: 5 step: 1102, loss is 0.0029509789310395718\n",
      "epoch: 5 step: 1103, loss is 0.15135903656482697\n",
      "epoch: 5 step: 1104, loss is 0.1493007242679596\n",
      "epoch: 5 step: 1105, loss is 0.2379898875951767\n",
      "epoch: 5 step: 1106, loss is 0.008869330398738384\n",
      "epoch: 5 step: 1107, loss is 0.018105996772646904\n",
      "epoch: 5 step: 1108, loss is 0.02709130011498928\n",
      "epoch: 5 step: 1109, loss is 0.059690654277801514\n",
      "epoch: 5 step: 1110, loss is 0.018861664459109306\n",
      "epoch: 5 step: 1111, loss is 0.005148258060216904\n",
      "epoch: 5 step: 1112, loss is 0.1753115952014923\n",
      "epoch: 5 step: 1113, loss is 0.15951943397521973\n",
      "epoch: 5 step: 1114, loss is 0.004416723269969225\n",
      "epoch: 5 step: 1115, loss is 0.02929040417075157\n",
      "epoch: 5 step: 1116, loss is 0.03068995662033558\n",
      "epoch: 5 step: 1117, loss is 0.008074377663433552\n",
      "epoch: 5 step: 1118, loss is 0.0064787655137479305\n",
      "epoch: 5 step: 1119, loss is 0.06717495620250702\n",
      "epoch: 5 step: 1120, loss is 0.017538124695420265\n",
      "epoch: 5 step: 1121, loss is 0.0924701914191246\n",
      "epoch: 5 step: 1122, loss is 0.08920669555664062\n",
      "epoch: 5 step: 1123, loss is 0.18793867528438568\n",
      "epoch: 5 step: 1124, loss is 0.020295098423957825\n",
      "epoch: 5 step: 1125, loss is 0.01164549496024847\n",
      "epoch: 5 step: 1126, loss is 0.0902044028043747\n",
      "epoch: 5 step: 1127, loss is 0.029167601838707924\n",
      "epoch: 5 step: 1128, loss is 0.16440777480602264\n",
      "epoch: 5 step: 1129, loss is 0.03250366821885109\n",
      "epoch: 5 step: 1130, loss is 0.16587479412555695\n",
      "epoch: 5 step: 1131, loss is 0.0025975615717470646\n",
      "epoch: 5 step: 1132, loss is 0.04175436124205589\n",
      "epoch: 5 step: 1133, loss is 0.03256871923804283\n",
      "epoch: 5 step: 1134, loss is 0.056502725929021835\n",
      "epoch: 5 step: 1135, loss is 0.003938049077987671\n",
      "epoch: 5 step: 1136, loss is 0.007251586299389601\n",
      "epoch: 5 step: 1137, loss is 0.003678781446069479\n",
      "epoch: 5 step: 1138, loss is 0.03740633279085159\n",
      "epoch: 5 step: 1139, loss is 0.007123492192476988\n",
      "epoch: 5 step: 1140, loss is 0.16311758756637573\n",
      "epoch: 5 step: 1141, loss is 0.005485316272825003\n",
      "epoch: 5 step: 1142, loss is 0.007648714352399111\n",
      "epoch: 5 step: 1143, loss is 0.07237911224365234\n",
      "epoch: 5 step: 1144, loss is 0.11748510599136353\n",
      "epoch: 5 step: 1145, loss is 0.029800720512866974\n",
      "epoch: 5 step: 1146, loss is 0.24497318267822266\n",
      "epoch: 5 step: 1147, loss is 0.08209815621376038\n",
      "epoch: 5 step: 1148, loss is 0.044409871101379395\n",
      "epoch: 5 step: 1149, loss is 0.03423035889863968\n",
      "epoch: 5 step: 1150, loss is 0.12133600562810898\n",
      "epoch: 5 step: 1151, loss is 0.0022584283724427223\n",
      "epoch: 5 step: 1152, loss is 0.010751535184681416\n",
      "epoch: 5 step: 1153, loss is 0.09844563156366348\n",
      "epoch: 5 step: 1154, loss is 0.009043904021382332\n",
      "epoch: 5 step: 1155, loss is 0.04162631556391716\n",
      "epoch: 5 step: 1156, loss is 0.008528785780072212\n",
      "epoch: 5 step: 1157, loss is 0.012865106575191021\n",
      "epoch: 5 step: 1158, loss is 0.04220227524638176\n",
      "epoch: 5 step: 1159, loss is 0.004416730720549822\n",
      "epoch: 5 step: 1160, loss is 0.017177162691950798\n",
      "epoch: 5 step: 1161, loss is 0.12335118651390076\n",
      "epoch: 5 step: 1162, loss is 0.03618234395980835\n",
      "epoch: 5 step: 1163, loss is 0.07993883639574051\n",
      "epoch: 5 step: 1164, loss is 0.015184289775788784\n",
      "epoch: 5 step: 1165, loss is 0.008828496560454369\n",
      "epoch: 5 step: 1166, loss is 0.016697635874152184\n",
      "epoch: 5 step: 1167, loss is 0.02486865036189556\n",
      "epoch: 5 step: 1168, loss is 0.07527653872966766\n",
      "epoch: 5 step: 1169, loss is 0.016947798430919647\n",
      "epoch: 5 step: 1170, loss is 0.10120870918035507\n",
      "epoch: 5 step: 1171, loss is 0.07441631704568863\n",
      "epoch: 5 step: 1172, loss is 0.00584857352077961\n",
      "epoch: 5 step: 1173, loss is 0.293815016746521\n",
      "epoch: 5 step: 1174, loss is 0.09977442771196365\n",
      "epoch: 5 step: 1175, loss is 0.12293243408203125\n",
      "epoch: 5 step: 1176, loss is 0.1639789491891861\n",
      "epoch: 5 step: 1177, loss is 0.003532943781465292\n",
      "epoch: 5 step: 1178, loss is 0.14019735157489777\n",
      "epoch: 5 step: 1179, loss is 0.04630118981003761\n",
      "epoch: 5 step: 1180, loss is 0.04300983250141144\n",
      "epoch: 5 step: 1181, loss is 0.16227325797080994\n",
      "epoch: 5 step: 1182, loss is 0.01097289752215147\n",
      "epoch: 5 step: 1183, loss is 0.055664390325546265\n",
      "epoch: 5 step: 1184, loss is 0.01950381137430668\n",
      "epoch: 5 step: 1185, loss is 0.18467740714550018\n",
      "epoch: 5 step: 1186, loss is 0.005807612556964159\n",
      "epoch: 5 step: 1187, loss is 0.05713704600930214\n",
      "epoch: 5 step: 1188, loss is 0.1657084822654724\n",
      "epoch: 5 step: 1189, loss is 0.0644109919667244\n",
      "epoch: 5 step: 1190, loss is 0.1210721880197525\n",
      "epoch: 5 step: 1191, loss is 0.01988116465508938\n",
      "epoch: 5 step: 1192, loss is 0.007294000126421452\n",
      "epoch: 5 step: 1193, loss is 0.04990248754620552\n",
      "epoch: 5 step: 1194, loss is 0.08073483407497406\n",
      "epoch: 5 step: 1195, loss is 0.10585121810436249\n",
      "epoch: 5 step: 1196, loss is 0.060847409069538116\n",
      "epoch: 5 step: 1197, loss is 0.04815278947353363\n",
      "epoch: 5 step: 1198, loss is 0.1180042028427124\n",
      "epoch: 5 step: 1199, loss is 0.01192884985357523\n",
      "epoch: 5 step: 1200, loss is 0.009679666720330715\n",
      "epoch: 5 step: 1201, loss is 0.11584198474884033\n",
      "epoch: 5 step: 1202, loss is 0.23568744957447052\n",
      "epoch: 5 step: 1203, loss is 0.011265484616160393\n",
      "epoch: 5 step: 1204, loss is 0.003941275645047426\n",
      "epoch: 5 step: 1205, loss is 0.0663813129067421\n",
      "epoch: 5 step: 1206, loss is 0.003389338031411171\n",
      "epoch: 5 step: 1207, loss is 0.024060750380158424\n",
      "epoch: 5 step: 1208, loss is 0.009233396500349045\n",
      "epoch: 5 step: 1209, loss is 0.007420196197926998\n",
      "epoch: 5 step: 1210, loss is 0.003962218761444092\n",
      "epoch: 5 step: 1211, loss is 0.29010453820228577\n",
      "epoch: 5 step: 1212, loss is 0.005532969255000353\n",
      "epoch: 5 step: 1213, loss is 0.022015444934368134\n",
      "epoch: 5 step: 1214, loss is 0.08445034921169281\n",
      "epoch: 5 step: 1215, loss is 0.12595750391483307\n",
      "epoch: 5 step: 1216, loss is 0.015009177848696709\n",
      "epoch: 5 step: 1217, loss is 0.11618369817733765\n",
      "epoch: 5 step: 1218, loss is 0.016755610704421997\n",
      "epoch: 5 step: 1219, loss is 0.043884776532649994\n",
      "epoch: 5 step: 1220, loss is 0.017493268474936485\n",
      "epoch: 5 step: 1221, loss is 0.04082448408007622\n",
      "epoch: 5 step: 1222, loss is 0.08754994720220566\n",
      "epoch: 5 step: 1223, loss is 0.031188728287816048\n",
      "epoch: 5 step: 1224, loss is 0.02840549312531948\n",
      "epoch: 5 step: 1225, loss is 0.0018419025000184774\n",
      "epoch: 5 step: 1226, loss is 0.09140907973051071\n",
      "epoch: 5 step: 1227, loss is 0.002296511782333255\n",
      "epoch: 5 step: 1228, loss is 0.17680764198303223\n",
      "epoch: 5 step: 1229, loss is 0.05457332730293274\n",
      "epoch: 5 step: 1230, loss is 0.02011909708380699\n",
      "epoch: 5 step: 1231, loss is 0.024449938908219337\n",
      "epoch: 5 step: 1232, loss is 0.03373292461037636\n",
      "epoch: 5 step: 1233, loss is 0.012434764765202999\n",
      "epoch: 5 step: 1234, loss is 0.010834340006113052\n",
      "epoch: 5 step: 1235, loss is 0.028264256194233894\n",
      "epoch: 5 step: 1236, loss is 0.08923681825399399\n",
      "epoch: 5 step: 1237, loss is 0.005350774619728327\n",
      "epoch: 5 step: 1238, loss is 0.008568230085074902\n",
      "epoch: 5 step: 1239, loss is 0.05097100883722305\n",
      "epoch: 5 step: 1240, loss is 0.0800788402557373\n",
      "epoch: 5 step: 1241, loss is 0.008743871934711933\n",
      "epoch: 5 step: 1242, loss is 0.06818212568759918\n",
      "epoch: 5 step: 1243, loss is 0.005069013684988022\n",
      "epoch: 5 step: 1244, loss is 0.030598914250731468\n",
      "epoch: 5 step: 1245, loss is 0.010740693658590317\n",
      "epoch: 5 step: 1246, loss is 0.06349796056747437\n",
      "epoch: 5 step: 1247, loss is 0.10445661097764969\n",
      "epoch: 5 step: 1248, loss is 0.008321995846927166\n",
      "epoch: 5 step: 1249, loss is 0.021720638498663902\n",
      "epoch: 5 step: 1250, loss is 0.1391366869211197\n",
      "epoch: 5 step: 1251, loss is 0.004564865026623011\n",
      "epoch: 5 step: 1252, loss is 0.0055081807076931\n",
      "epoch: 5 step: 1253, loss is 0.004231088794767857\n",
      "epoch: 5 step: 1254, loss is 0.15001992881298065\n",
      "epoch: 5 step: 1255, loss is 0.030453551560640335\n",
      "epoch: 5 step: 1256, loss is 0.05896376445889473\n",
      "epoch: 5 step: 1257, loss is 0.0041556572541594505\n",
      "epoch: 5 step: 1258, loss is 0.02009703777730465\n",
      "epoch: 5 step: 1259, loss is 0.36013269424438477\n",
      "epoch: 5 step: 1260, loss is 0.041690051555633545\n",
      "epoch: 5 step: 1261, loss is 0.00900570210069418\n",
      "epoch: 5 step: 1262, loss is 0.0012594766449183226\n",
      "epoch: 5 step: 1263, loss is 0.20506229996681213\n",
      "epoch: 5 step: 1264, loss is 0.012309243902564049\n",
      "epoch: 5 step: 1265, loss is 0.03502046316862106\n",
      "epoch: 5 step: 1266, loss is 0.008749900385737419\n",
      "epoch: 5 step: 1267, loss is 0.026882536709308624\n",
      "epoch: 5 step: 1268, loss is 0.24085493385791779\n",
      "epoch: 5 step: 1269, loss is 0.1613922119140625\n",
      "epoch: 5 step: 1270, loss is 0.14923995733261108\n",
      "epoch: 5 step: 1271, loss is 0.003039685310795903\n",
      "epoch: 5 step: 1272, loss is 0.005193630233407021\n",
      "epoch: 5 step: 1273, loss is 0.3094347417354584\n",
      "epoch: 5 step: 1274, loss is 0.05715414881706238\n",
      "epoch: 5 step: 1275, loss is 0.011908411979675293\n",
      "epoch: 5 step: 1276, loss is 0.00525917811319232\n",
      "epoch: 5 step: 1277, loss is 0.003862205892801285\n",
      "epoch: 5 step: 1278, loss is 0.008904604241251945\n",
      "epoch: 5 step: 1279, loss is 0.04252956062555313\n",
      "epoch: 5 step: 1280, loss is 0.004406173713505268\n",
      "epoch: 5 step: 1281, loss is 0.1124458909034729\n",
      "epoch: 5 step: 1282, loss is 0.001377616892568767\n",
      "epoch: 5 step: 1283, loss is 0.006476903799921274\n",
      "epoch: 5 step: 1284, loss is 0.039365004748106\n",
      "epoch: 5 step: 1285, loss is 0.018519261851906776\n",
      "epoch: 5 step: 1286, loss is 0.051158610731363297\n",
      "epoch: 5 step: 1287, loss is 0.10018326342105865\n",
      "epoch: 5 step: 1288, loss is 0.012452440336346626\n",
      "epoch: 5 step: 1289, loss is 0.25238072872161865\n",
      "epoch: 5 step: 1290, loss is 0.11244253069162369\n",
      "epoch: 5 step: 1291, loss is 0.01999572664499283\n",
      "epoch: 5 step: 1292, loss is 0.0018329471349716187\n",
      "epoch: 5 step: 1293, loss is 0.15227621793746948\n",
      "epoch: 5 step: 1294, loss is 0.005283283069729805\n",
      "epoch: 5 step: 1295, loss is 0.27215060591697693\n",
      "epoch: 5 step: 1296, loss is 0.1474238485097885\n",
      "epoch: 5 step: 1297, loss is 0.20998746156692505\n",
      "epoch: 5 step: 1298, loss is 0.001107838237658143\n",
      "epoch: 5 step: 1299, loss is 0.008311270736157894\n",
      "epoch: 5 step: 1300, loss is 0.08872237801551819\n",
      "epoch: 5 step: 1301, loss is 0.031245334073901176\n",
      "epoch: 5 step: 1302, loss is 0.010934057645499706\n",
      "epoch: 5 step: 1303, loss is 0.022364793345332146\n",
      "epoch: 5 step: 1304, loss is 0.04646393656730652\n",
      "epoch: 5 step: 1305, loss is 0.06612366437911987\n",
      "epoch: 5 step: 1306, loss is 0.015508129261434078\n",
      "epoch: 5 step: 1307, loss is 0.11966390907764435\n",
      "epoch: 5 step: 1308, loss is 0.10679994523525238\n",
      "epoch: 5 step: 1309, loss is 0.3037066161632538\n",
      "epoch: 5 step: 1310, loss is 0.08787323534488678\n",
      "epoch: 5 step: 1311, loss is 0.11533795297145844\n",
      "epoch: 5 step: 1312, loss is 0.07648707926273346\n",
      "epoch: 5 step: 1313, loss is 0.03843295946717262\n",
      "epoch: 5 step: 1314, loss is 0.01058860681951046\n",
      "epoch: 5 step: 1315, loss is 0.019674383103847504\n",
      "epoch: 5 step: 1316, loss is 0.1670624017715454\n",
      "epoch: 5 step: 1317, loss is 0.04734334349632263\n",
      "epoch: 5 step: 1318, loss is 0.23115335404872894\n",
      "epoch: 5 step: 1319, loss is 0.04294876754283905\n",
      "epoch: 5 step: 1320, loss is 0.05344001203775406\n",
      "epoch: 5 step: 1321, loss is 0.12838704884052277\n",
      "epoch: 5 step: 1322, loss is 0.1053810715675354\n",
      "epoch: 5 step: 1323, loss is 0.07006128132343292\n",
      "epoch: 5 step: 1324, loss is 0.023929428309202194\n",
      "epoch: 5 step: 1325, loss is 0.013296667486429214\n",
      "epoch: 5 step: 1326, loss is 0.11059246957302094\n",
      "epoch: 5 step: 1327, loss is 0.050821006298065186\n",
      "epoch: 5 step: 1328, loss is 0.046066369861364365\n",
      "epoch: 5 step: 1329, loss is 0.13660717010498047\n",
      "epoch: 5 step: 1330, loss is 0.10960254818201065\n",
      "epoch: 5 step: 1331, loss is 0.11444159597158432\n",
      "epoch: 5 step: 1332, loss is 0.2580007016658783\n",
      "epoch: 5 step: 1333, loss is 0.009649300016462803\n",
      "epoch: 5 step: 1334, loss is 0.25495445728302\n",
      "epoch: 5 step: 1335, loss is 0.05556496977806091\n",
      "epoch: 5 step: 1336, loss is 0.0022976952604949474\n",
      "epoch: 5 step: 1337, loss is 0.004090994596481323\n",
      "epoch: 5 step: 1338, loss is 0.05823765695095062\n",
      "epoch: 5 step: 1339, loss is 0.005008537322282791\n",
      "epoch: 5 step: 1340, loss is 0.012328301556408405\n",
      "epoch: 5 step: 1341, loss is 0.0014541347045451403\n",
      "epoch: 5 step: 1342, loss is 0.255554735660553\n",
      "epoch: 5 step: 1343, loss is 0.03879307582974434\n",
      "epoch: 5 step: 1344, loss is 0.057701658457517624\n",
      "epoch: 5 step: 1345, loss is 0.025042057037353516\n",
      "epoch: 5 step: 1346, loss is 0.0037521307822316885\n",
      "epoch: 5 step: 1347, loss is 0.08030662685632706\n",
      "epoch: 5 step: 1348, loss is 0.01621204800903797\n",
      "epoch: 5 step: 1349, loss is 0.03389179706573486\n",
      "epoch: 5 step: 1350, loss is 0.003976850770413876\n",
      "epoch: 5 step: 1351, loss is 0.20056968927383423\n",
      "epoch: 5 step: 1352, loss is 0.018298257142305374\n",
      "epoch: 5 step: 1353, loss is 0.04454723373055458\n",
      "epoch: 5 step: 1354, loss is 0.032923273742198944\n",
      "epoch: 5 step: 1355, loss is 0.12548643350601196\n",
      "epoch: 5 step: 1356, loss is 0.031070459634065628\n",
      "epoch: 5 step: 1357, loss is 0.20320409536361694\n",
      "epoch: 5 step: 1358, loss is 0.007570285350084305\n",
      "epoch: 5 step: 1359, loss is 0.00617616530507803\n",
      "epoch: 5 step: 1360, loss is 0.0023714618291705847\n",
      "epoch: 5 step: 1361, loss is 0.01348754484206438\n",
      "epoch: 5 step: 1362, loss is 0.04595222324132919\n",
      "epoch: 5 step: 1363, loss is 0.005182523746043444\n",
      "epoch: 5 step: 1364, loss is 0.14813333749771118\n",
      "epoch: 5 step: 1365, loss is 0.037527382373809814\n",
      "epoch: 5 step: 1366, loss is 0.1658470779657364\n",
      "epoch: 5 step: 1367, loss is 0.08612782508134842\n",
      "epoch: 5 step: 1368, loss is 0.01355019398033619\n",
      "epoch: 5 step: 1369, loss is 0.028557196259498596\n",
      "epoch: 5 step: 1370, loss is 0.09336505085229874\n",
      "epoch: 5 step: 1371, loss is 0.003686893032863736\n",
      "epoch: 5 step: 1372, loss is 0.014583814889192581\n",
      "epoch: 5 step: 1373, loss is 0.02399212308228016\n",
      "epoch: 5 step: 1374, loss is 0.059134844690561295\n",
      "epoch: 5 step: 1375, loss is 0.19332347810268402\n",
      "epoch: 5 step: 1376, loss is 0.19109448790550232\n",
      "epoch: 5 step: 1377, loss is 0.008931555785238743\n",
      "epoch: 5 step: 1378, loss is 0.05458148941397667\n",
      "epoch: 5 step: 1379, loss is 0.12437885999679565\n",
      "epoch: 5 step: 1380, loss is 0.029025832191109657\n",
      "epoch: 5 step: 1381, loss is 0.02495509758591652\n",
      "epoch: 5 step: 1382, loss is 0.04803849384188652\n",
      "epoch: 5 step: 1383, loss is 0.1461142897605896\n",
      "epoch: 5 step: 1384, loss is 0.008181738667190075\n",
      "epoch: 5 step: 1385, loss is 0.023688821122050285\n",
      "epoch: 5 step: 1386, loss is 0.0072929756715893745\n",
      "epoch: 5 step: 1387, loss is 0.006256181746721268\n",
      "epoch: 5 step: 1388, loss is 0.011930114589631557\n",
      "epoch: 5 step: 1389, loss is 0.020611517131328583\n",
      "epoch: 5 step: 1390, loss is 0.03507280349731445\n",
      "epoch: 5 step: 1391, loss is 0.02233920805156231\n",
      "epoch: 5 step: 1392, loss is 0.15065230429172516\n",
      "epoch: 5 step: 1393, loss is 0.020426996052265167\n",
      "epoch: 5 step: 1394, loss is 0.0042310538701713085\n",
      "epoch: 5 step: 1395, loss is 0.011137913912534714\n",
      "epoch: 5 step: 1396, loss is 0.03883248195052147\n",
      "epoch: 5 step: 1397, loss is 0.0427536778151989\n",
      "epoch: 5 step: 1398, loss is 0.01398484781384468\n",
      "epoch: 5 step: 1399, loss is 0.08449090272188187\n",
      "epoch: 5 step: 1400, loss is 0.09639453142881393\n",
      "epoch: 5 step: 1401, loss is 0.009077250026166439\n",
      "epoch: 5 step: 1402, loss is 0.02998216636478901\n",
      "epoch: 5 step: 1403, loss is 0.03833372890949249\n",
      "epoch: 5 step: 1404, loss is 0.008903519250452518\n",
      "epoch: 5 step: 1405, loss is 0.010206079110503197\n",
      "epoch: 5 step: 1406, loss is 0.18670949339866638\n",
      "epoch: 5 step: 1407, loss is 0.03252972662448883\n",
      "epoch: 5 step: 1408, loss is 0.009045754559338093\n",
      "epoch: 5 step: 1409, loss is 0.041697196662425995\n",
      "epoch: 5 step: 1410, loss is 0.006424871273338795\n",
      "epoch: 5 step: 1411, loss is 0.12790611386299133\n",
      "epoch: 5 step: 1412, loss is 0.00716963317245245\n",
      "epoch: 5 step: 1413, loss is 0.037813346832990646\n",
      "epoch: 5 step: 1414, loss is 0.17915651202201843\n",
      "epoch: 5 step: 1415, loss is 0.07115516811609268\n",
      "epoch: 5 step: 1416, loss is 0.002982070203870535\n",
      "epoch: 5 step: 1417, loss is 0.1889277547597885\n",
      "epoch: 5 step: 1418, loss is 0.07873857021331787\n",
      "epoch: 5 step: 1419, loss is 0.0010042593348771334\n",
      "epoch: 5 step: 1420, loss is 0.004984230734407902\n",
      "epoch: 5 step: 1421, loss is 0.09477792680263519\n",
      "epoch: 5 step: 1422, loss is 0.012595279142260551\n",
      "epoch: 5 step: 1423, loss is 0.10106499493122101\n",
      "epoch: 5 step: 1424, loss is 0.004432152956724167\n",
      "epoch: 5 step: 1425, loss is 0.021942617371678352\n",
      "epoch: 5 step: 1426, loss is 0.09322057664394379\n",
      "epoch: 5 step: 1427, loss is 0.006931254640221596\n",
      "epoch: 5 step: 1428, loss is 0.40671029686927795\n",
      "epoch: 5 step: 1429, loss is 0.13801419734954834\n",
      "epoch: 5 step: 1430, loss is 0.01142240222543478\n",
      "epoch: 5 step: 1431, loss is 0.15925827622413635\n",
      "epoch: 5 step: 1432, loss is 0.022930650040507317\n",
      "epoch: 5 step: 1433, loss is 0.07216637581586838\n",
      "epoch: 5 step: 1434, loss is 0.0034035572316497564\n",
      "epoch: 5 step: 1435, loss is 0.029689250513911247\n",
      "epoch: 5 step: 1436, loss is 0.022858591750264168\n",
      "epoch: 5 step: 1437, loss is 0.0777902603149414\n",
      "epoch: 5 step: 1438, loss is 0.12896187603473663\n",
      "epoch: 5 step: 1439, loss is 0.004848837852478027\n",
      "epoch: 5 step: 1440, loss is 0.015532389283180237\n",
      "epoch: 5 step: 1441, loss is 0.03527781739830971\n",
      "epoch: 5 step: 1442, loss is 0.10117608308792114\n",
      "epoch: 5 step: 1443, loss is 0.008098009042441845\n",
      "epoch: 5 step: 1444, loss is 0.0088698361068964\n",
      "epoch: 5 step: 1445, loss is 0.0019357618875801563\n",
      "epoch: 5 step: 1446, loss is 0.02445269003510475\n",
      "epoch: 5 step: 1447, loss is 0.009106666781008244\n",
      "epoch: 5 step: 1448, loss is 0.13603118062019348\n",
      "epoch: 5 step: 1449, loss is 0.10173392295837402\n",
      "epoch: 5 step: 1450, loss is 0.02090989053249359\n",
      "epoch: 5 step: 1451, loss is 0.050219181925058365\n",
      "epoch: 5 step: 1452, loss is 0.002257091458886862\n",
      "epoch: 5 step: 1453, loss is 0.23518763482570648\n",
      "epoch: 5 step: 1454, loss is 0.011062728241086006\n",
      "epoch: 5 step: 1455, loss is 0.08320298045873642\n",
      "epoch: 5 step: 1456, loss is 0.009768389165401459\n",
      "epoch: 5 step: 1457, loss is 0.10412244498729706\n",
      "epoch: 5 step: 1458, loss is 0.02290467545390129\n",
      "epoch: 5 step: 1459, loss is 0.004731112625449896\n",
      "epoch: 5 step: 1460, loss is 0.01131130289286375\n",
      "epoch: 5 step: 1461, loss is 0.012456504628062248\n",
      "epoch: 5 step: 1462, loss is 0.002621160354465246\n",
      "epoch: 5 step: 1463, loss is 0.04276145249605179\n",
      "epoch: 5 step: 1464, loss is 0.011565563268959522\n",
      "epoch: 5 step: 1465, loss is 0.20244865119457245\n",
      "epoch: 5 step: 1466, loss is 0.01434585452079773\n",
      "epoch: 5 step: 1467, loss is 0.0008883746340870857\n",
      "epoch: 5 step: 1468, loss is 0.052137892693281174\n",
      "epoch: 5 step: 1469, loss is 0.03695276752114296\n",
      "epoch: 5 step: 1470, loss is 0.17457352578639984\n",
      "epoch: 5 step: 1471, loss is 0.16186587512493134\n",
      "epoch: 5 step: 1472, loss is 0.04535446688532829\n",
      "epoch: 5 step: 1473, loss is 0.04197842255234718\n",
      "epoch: 5 step: 1474, loss is 0.10040580481290817\n",
      "epoch: 5 step: 1475, loss is 0.0017833492020145059\n",
      "epoch: 5 step: 1476, loss is 0.0015870154602453113\n",
      "epoch: 5 step: 1477, loss is 0.29228150844573975\n",
      "epoch: 5 step: 1478, loss is 0.04780876636505127\n",
      "epoch: 5 step: 1479, loss is 0.1737770289182663\n",
      "epoch: 5 step: 1480, loss is 0.011657750234007835\n",
      "epoch: 5 step: 1481, loss is 0.024533841758966446\n",
      "epoch: 5 step: 1482, loss is 0.008632831275463104\n",
      "epoch: 5 step: 1483, loss is 0.13554741442203522\n",
      "epoch: 5 step: 1484, loss is 0.003085921984165907\n",
      "epoch: 5 step: 1485, loss is 0.007239007391035557\n",
      "epoch: 5 step: 1486, loss is 0.051584504544734955\n",
      "epoch: 5 step: 1487, loss is 0.6519782543182373\n",
      "epoch: 5 step: 1488, loss is 0.08724237233400345\n",
      "epoch: 5 step: 1489, loss is 0.015708129853010178\n",
      "epoch: 5 step: 1490, loss is 0.08263402432203293\n",
      "epoch: 5 step: 1491, loss is 0.0931834802031517\n",
      "epoch: 5 step: 1492, loss is 0.07884027808904648\n",
      "epoch: 5 step: 1493, loss is 0.04422830045223236\n",
      "epoch: 5 step: 1494, loss is 0.019489169120788574\n",
      "epoch: 5 step: 1495, loss is 0.12873977422714233\n",
      "epoch: 5 step: 1496, loss is 0.053143735975027084\n",
      "epoch: 5 step: 1497, loss is 0.055570125579833984\n",
      "epoch: 5 step: 1498, loss is 0.09544643759727478\n",
      "epoch: 5 step: 1499, loss is 0.12949460744857788\n",
      "epoch: 5 step: 1500, loss is 0.015010638162493706\n",
      "epoch: 5 step: 1501, loss is 0.029395945370197296\n",
      "epoch: 5 step: 1502, loss is 0.025406023487448692\n",
      "epoch: 5 step: 1503, loss is 0.016535768285393715\n",
      "epoch: 5 step: 1504, loss is 0.20970040559768677\n",
      "epoch: 5 step: 1505, loss is 0.16556468605995178\n",
      "epoch: 5 step: 1506, loss is 0.01345138717442751\n",
      "epoch: 5 step: 1507, loss is 0.009008493274450302\n",
      "epoch: 5 step: 1508, loss is 0.0040638926438987255\n",
      "epoch: 5 step: 1509, loss is 0.0054986062459647655\n",
      "epoch: 5 step: 1510, loss is 0.0030562952160835266\n",
      "epoch: 5 step: 1511, loss is 0.0327632836997509\n",
      "epoch: 5 step: 1512, loss is 0.023114413022994995\n",
      "epoch: 5 step: 1513, loss is 0.02380097471177578\n",
      "epoch: 5 step: 1514, loss is 0.058174509555101395\n",
      "epoch: 5 step: 1515, loss is 0.06507854908704758\n",
      "epoch: 5 step: 1516, loss is 0.2164430022239685\n",
      "epoch: 5 step: 1517, loss is 0.008682572282850742\n",
      "epoch: 5 step: 1518, loss is 0.005017681512981653\n",
      "epoch: 5 step: 1519, loss is 0.015322013758122921\n",
      "epoch: 5 step: 1520, loss is 0.007593884132802486\n",
      "epoch: 5 step: 1521, loss is 0.014201574958860874\n",
      "epoch: 5 step: 1522, loss is 0.022000860422849655\n",
      "epoch: 5 step: 1523, loss is 0.040060847997665405\n",
      "epoch: 5 step: 1524, loss is 0.06503090262413025\n",
      "epoch: 5 step: 1525, loss is 0.002195530803874135\n",
      "epoch: 5 step: 1526, loss is 0.012362321838736534\n",
      "epoch: 5 step: 1527, loss is 0.16723234951496124\n",
      "epoch: 5 step: 1528, loss is 0.02881079912185669\n",
      "epoch: 5 step: 1529, loss is 0.01898004114627838\n",
      "epoch: 5 step: 1530, loss is 0.0075415861792862415\n",
      "epoch: 5 step: 1531, loss is 0.016469230875372887\n",
      "epoch: 5 step: 1532, loss is 0.16178180277347565\n",
      "epoch: 5 step: 1533, loss is 0.12823741137981415\n",
      "epoch: 5 step: 1534, loss is 0.008706689812242985\n",
      "epoch: 5 step: 1535, loss is 0.0021641904022544622\n",
      "epoch: 5 step: 1536, loss is 0.15487930178642273\n",
      "epoch: 5 step: 1537, loss is 0.0355914942920208\n",
      "epoch: 5 step: 1538, loss is 0.04926317557692528\n",
      "epoch: 5 step: 1539, loss is 0.0020454437471926212\n",
      "epoch: 5 step: 1540, loss is 0.018614593893289566\n",
      "epoch: 5 step: 1541, loss is 0.004540103953331709\n",
      "epoch: 5 step: 1542, loss is 0.005107146222144365\n",
      "epoch: 5 step: 1543, loss is 0.010045413859188557\n",
      "epoch: 5 step: 1544, loss is 0.16891208291053772\n",
      "epoch: 5 step: 1545, loss is 0.020369241014122963\n",
      "epoch: 5 step: 1546, loss is 0.044770363718271255\n",
      "epoch: 5 step: 1547, loss is 0.023170972242951393\n",
      "epoch: 5 step: 1548, loss is 0.10757876187562943\n",
      "epoch: 5 step: 1549, loss is 0.002555443439632654\n",
      "epoch: 5 step: 1550, loss is 0.10118759423494339\n",
      "epoch: 5 step: 1551, loss is 0.005539348348975182\n",
      "epoch: 5 step: 1552, loss is 0.16061672568321228\n",
      "epoch: 5 step: 1553, loss is 0.00672608008608222\n",
      "epoch: 5 step: 1554, loss is 0.14398452639579773\n",
      "epoch: 5 step: 1555, loss is 0.02010936476290226\n",
      "epoch: 5 step: 1556, loss is 0.3174663484096527\n",
      "epoch: 5 step: 1557, loss is 0.17882801592350006\n",
      "epoch: 5 step: 1558, loss is 0.013772773556411266\n",
      "epoch: 5 step: 1559, loss is 0.005172920413315296\n",
      "epoch: 5 step: 1560, loss is 0.050261497497558594\n",
      "epoch: 5 step: 1561, loss is 0.022980082780122757\n",
      "epoch: 5 step: 1562, loss is 0.10019285976886749\n",
      "epoch: 5 step: 1563, loss is 0.2567884922027588\n",
      "epoch: 5 step: 1564, loss is 0.15899740159511566\n",
      "epoch: 5 step: 1565, loss is 0.2960646152496338\n",
      "epoch: 5 step: 1566, loss is 0.003104613395407796\n",
      "epoch: 5 step: 1567, loss is 0.0013868141686543822\n",
      "epoch: 5 step: 1568, loss is 0.13807781040668488\n",
      "epoch: 5 step: 1569, loss is 0.025185512378811836\n",
      "epoch: 5 step: 1570, loss is 0.026985570788383484\n",
      "epoch: 5 step: 1571, loss is 0.007050130981951952\n",
      "epoch: 5 step: 1572, loss is 0.03354529291391373\n",
      "epoch: 5 step: 1573, loss is 0.2471291720867157\n",
      "epoch: 5 step: 1574, loss is 0.11496781557798386\n",
      "epoch: 5 step: 1575, loss is 0.21245916187763214\n",
      "epoch: 5 step: 1576, loss is 0.014216550625860691\n",
      "epoch: 5 step: 1577, loss is 0.03227933868765831\n",
      "epoch: 5 step: 1578, loss is 0.1567687839269638\n",
      "epoch: 5 step: 1579, loss is 0.012148045934736729\n",
      "epoch: 5 step: 1580, loss is 0.025316083803772926\n",
      "epoch: 5 step: 1581, loss is 0.0024252182338386774\n",
      "epoch: 5 step: 1582, loss is 0.020447324961423874\n",
      "epoch: 5 step: 1583, loss is 0.04481125622987747\n",
      "epoch: 5 step: 1584, loss is 0.04450753703713417\n",
      "epoch: 5 step: 1585, loss is 0.10564734041690826\n",
      "epoch: 5 step: 1586, loss is 0.045620907098054886\n",
      "epoch: 5 step: 1587, loss is 0.17775538563728333\n",
      "epoch: 5 step: 1588, loss is 0.3206208646297455\n",
      "epoch: 5 step: 1589, loss is 0.18239714205265045\n",
      "epoch: 5 step: 1590, loss is 0.10563601553440094\n",
      "epoch: 5 step: 1591, loss is 0.10147355496883392\n",
      "epoch: 5 step: 1592, loss is 0.06424427777528763\n",
      "epoch: 5 step: 1593, loss is 0.007286750711500645\n",
      "epoch: 5 step: 1594, loss is 0.03240622207522392\n",
      "epoch: 5 step: 1595, loss is 0.004347171634435654\n",
      "epoch: 5 step: 1596, loss is 0.0058318208903074265\n",
      "epoch: 5 step: 1597, loss is 0.00857793353497982\n",
      "epoch: 5 step: 1598, loss is 0.019714582711458206\n",
      "epoch: 5 step: 1599, loss is 0.01527180802077055\n",
      "epoch: 5 step: 1600, loss is 0.004757072776556015\n",
      "epoch: 5 step: 1601, loss is 0.14185674488544464\n",
      "epoch: 5 step: 1602, loss is 0.0477350652217865\n",
      "epoch: 5 step: 1603, loss is 0.0011451591271907091\n",
      "epoch: 5 step: 1604, loss is 0.03273099288344383\n",
      "epoch: 5 step: 1605, loss is 0.01327928714454174\n",
      "epoch: 5 step: 1606, loss is 0.01958109810948372\n",
      "epoch: 5 step: 1607, loss is 0.03026486001908779\n",
      "epoch: 5 step: 1608, loss is 0.011120256967842579\n",
      "epoch: 5 step: 1609, loss is 0.008547983132302761\n",
      "epoch: 5 step: 1610, loss is 0.0881863608956337\n",
      "epoch: 5 step: 1611, loss is 0.13529996573925018\n",
      "epoch: 5 step: 1612, loss is 0.10979872941970825\n",
      "epoch: 5 step: 1613, loss is 0.026619277894496918\n",
      "epoch: 5 step: 1614, loss is 0.031047480180859566\n",
      "epoch: 5 step: 1615, loss is 0.023377088829874992\n",
      "epoch: 5 step: 1616, loss is 0.03198128566145897\n",
      "epoch: 5 step: 1617, loss is 0.09064178168773651\n",
      "epoch: 5 step: 1618, loss is 0.0010860456386581063\n",
      "epoch: 5 step: 1619, loss is 0.10472994297742844\n",
      "epoch: 5 step: 1620, loss is 0.009406356140971184\n",
      "epoch: 5 step: 1621, loss is 0.008445067331194878\n",
      "epoch: 5 step: 1622, loss is 0.0006637926562689245\n",
      "epoch: 5 step: 1623, loss is 0.017996681854128838\n",
      "epoch: 5 step: 1624, loss is 0.05382748693227768\n",
      "epoch: 5 step: 1625, loss is 0.04185349494218826\n",
      "epoch: 5 step: 1626, loss is 0.01754317246377468\n",
      "epoch: 5 step: 1627, loss is 0.06404819339513779\n",
      "epoch: 5 step: 1628, loss is 0.017490774393081665\n",
      "epoch: 5 step: 1629, loss is 0.11318141222000122\n",
      "epoch: 5 step: 1630, loss is 0.02113049477338791\n",
      "epoch: 5 step: 1631, loss is 0.0037649378646165133\n",
      "epoch: 5 step: 1632, loss is 0.04819174110889435\n",
      "epoch: 5 step: 1633, loss is 0.05423862114548683\n",
      "epoch: 5 step: 1634, loss is 0.0005605373880825937\n",
      "epoch: 5 step: 1635, loss is 0.07151034474372864\n",
      "epoch: 5 step: 1636, loss is 0.09741248190402985\n",
      "epoch: 5 step: 1637, loss is 0.025114839896559715\n",
      "epoch: 5 step: 1638, loss is 0.03714413940906525\n",
      "epoch: 5 step: 1639, loss is 0.0032464792020618916\n",
      "epoch: 5 step: 1640, loss is 0.010410984046757221\n",
      "epoch: 5 step: 1641, loss is 0.016980325803160667\n",
      "epoch: 5 step: 1642, loss is 0.09222663193941116\n",
      "epoch: 5 step: 1643, loss is 0.03597070649266243\n",
      "epoch: 5 step: 1644, loss is 0.013436480425298214\n",
      "epoch: 5 step: 1645, loss is 0.028466729447245598\n",
      "epoch: 5 step: 1646, loss is 0.0014780694618821144\n",
      "epoch: 5 step: 1647, loss is 0.0023420301731675863\n",
      "epoch: 5 step: 1648, loss is 0.06888002902269363\n",
      "epoch: 5 step: 1649, loss is 0.0008899695822037756\n",
      "epoch: 5 step: 1650, loss is 0.002139168558642268\n",
      "epoch: 5 step: 1651, loss is 0.08317974209785461\n",
      "epoch: 5 step: 1652, loss is 0.07743862271308899\n",
      "epoch: 5 step: 1653, loss is 0.020828578621149063\n",
      "epoch: 5 step: 1654, loss is 0.3597715198993683\n",
      "epoch: 5 step: 1655, loss is 0.2596496641635895\n",
      "epoch: 5 step: 1656, loss is 0.007185538299381733\n",
      "epoch: 5 step: 1657, loss is 0.11930792778730392\n",
      "epoch: 5 step: 1658, loss is 0.06293534487485886\n",
      "epoch: 5 step: 1659, loss is 0.0030800197273492813\n",
      "epoch: 5 step: 1660, loss is 0.08070268481969833\n",
      "epoch: 5 step: 1661, loss is 0.3440552353858948\n",
      "epoch: 5 step: 1662, loss is 0.005493072792887688\n",
      "epoch: 5 step: 1663, loss is 0.12874449789524078\n",
      "epoch: 5 step: 1664, loss is 0.018409930169582367\n",
      "epoch: 5 step: 1665, loss is 0.035234298557043076\n",
      "epoch: 5 step: 1666, loss is 0.0044330088421702385\n",
      "epoch: 5 step: 1667, loss is 0.0561247393488884\n",
      "epoch: 5 step: 1668, loss is 0.03643951565027237\n",
      "epoch: 5 step: 1669, loss is 0.026567980647087097\n",
      "epoch: 5 step: 1670, loss is 0.09089676290750504\n",
      "epoch: 5 step: 1671, loss is 0.031749095767736435\n",
      "epoch: 5 step: 1672, loss is 0.017285238951444626\n",
      "epoch: 5 step: 1673, loss is 0.0035914580803364515\n",
      "epoch: 5 step: 1674, loss is 0.016591239720582962\n",
      "epoch: 5 step: 1675, loss is 0.04207829013466835\n",
      "epoch: 5 step: 1676, loss is 0.06872391700744629\n",
      "epoch: 5 step: 1677, loss is 0.2500131130218506\n",
      "epoch: 5 step: 1678, loss is 0.0023701339960098267\n",
      "epoch: 5 step: 1679, loss is 0.158626526594162\n",
      "epoch: 5 step: 1680, loss is 0.006926886737346649\n",
      "epoch: 5 step: 1681, loss is 0.006486197933554649\n",
      "epoch: 5 step: 1682, loss is 0.025863561779260635\n",
      "epoch: 5 step: 1683, loss is 0.05392623320221901\n",
      "epoch: 5 step: 1684, loss is 0.10949300974607468\n",
      "epoch: 5 step: 1685, loss is 0.038057632744312286\n",
      "epoch: 5 step: 1686, loss is 0.014138614758849144\n",
      "epoch: 5 step: 1687, loss is 0.00435221754014492\n",
      "epoch: 5 step: 1688, loss is 0.45146018266677856\n",
      "epoch: 5 step: 1689, loss is 0.14095443487167358\n",
      "epoch: 5 step: 1690, loss is 0.0749279037117958\n",
      "epoch: 5 step: 1691, loss is 0.09389112144708633\n",
      "epoch: 5 step: 1692, loss is 0.16930054128170013\n",
      "epoch: 5 step: 1693, loss is 0.04131677746772766\n",
      "epoch: 5 step: 1694, loss is 0.0497952401638031\n",
      "epoch: 5 step: 1695, loss is 0.02034146338701248\n",
      "epoch: 5 step: 1696, loss is 0.008239798247814178\n",
      "epoch: 5 step: 1697, loss is 0.007577728480100632\n",
      "epoch: 5 step: 1698, loss is 0.035110678523778915\n",
      "epoch: 5 step: 1699, loss is 0.018683189526200294\n",
      "epoch: 5 step: 1700, loss is 0.013558153994381428\n",
      "epoch: 5 step: 1701, loss is 0.006328158546239138\n",
      "epoch: 5 step: 1702, loss is 0.010717599652707577\n",
      "epoch: 5 step: 1703, loss is 0.13088659942150116\n",
      "epoch: 5 step: 1704, loss is 0.06102177873253822\n",
      "epoch: 5 step: 1705, loss is 0.2211463898420334\n",
      "epoch: 5 step: 1706, loss is 0.027385476976633072\n",
      "epoch: 5 step: 1707, loss is 0.015044623985886574\n",
      "epoch: 5 step: 1708, loss is 0.03747251257300377\n",
      "epoch: 5 step: 1709, loss is 0.03147607669234276\n",
      "epoch: 5 step: 1710, loss is 0.18303094804286957\n",
      "epoch: 5 step: 1711, loss is 0.09945903718471527\n",
      "epoch: 5 step: 1712, loss is 0.10373696684837341\n",
      "epoch: 5 step: 1713, loss is 0.013128673657774925\n",
      "epoch: 5 step: 1714, loss is 0.10314235091209412\n",
      "epoch: 5 step: 1715, loss is 0.08200807869434357\n",
      "epoch: 5 step: 1716, loss is 0.019078684970736504\n",
      "epoch: 5 step: 1717, loss is 0.06107964739203453\n",
      "epoch: 5 step: 1718, loss is 0.008092375472187996\n",
      "epoch: 5 step: 1719, loss is 0.024800337851047516\n",
      "epoch: 5 step: 1720, loss is 0.0026791412383317947\n",
      "epoch: 5 step: 1721, loss is 0.20267927646636963\n",
      "epoch: 5 step: 1722, loss is 0.005123373586684465\n",
      "epoch: 5 step: 1723, loss is 0.1405709832906723\n",
      "epoch: 5 step: 1724, loss is 0.005074790678918362\n",
      "epoch: 5 step: 1725, loss is 0.006399679463356733\n",
      "epoch: 5 step: 1726, loss is 0.027170706540346146\n",
      "epoch: 5 step: 1727, loss is 0.014870183542370796\n",
      "epoch: 5 step: 1728, loss is 0.006389262154698372\n",
      "epoch: 5 step: 1729, loss is 0.005223144311457872\n",
      "epoch: 5 step: 1730, loss is 0.0771823450922966\n",
      "epoch: 5 step: 1731, loss is 0.006705326493829489\n",
      "epoch: 5 step: 1732, loss is 0.043915726244449615\n",
      "epoch: 5 step: 1733, loss is 0.00929163582623005\n",
      "epoch: 5 step: 1734, loss is 0.007483550813049078\n",
      "epoch: 5 step: 1735, loss is 0.12767183780670166\n",
      "epoch: 5 step: 1736, loss is 0.07535335421562195\n",
      "epoch: 5 step: 1737, loss is 0.08260621130466461\n",
      "epoch: 5 step: 1738, loss is 0.0012750983005389571\n",
      "epoch: 5 step: 1739, loss is 0.003429529257118702\n",
      "epoch: 5 step: 1740, loss is 0.006951119285076857\n",
      "epoch: 5 step: 1741, loss is 0.1335461139678955\n",
      "epoch: 5 step: 1742, loss is 0.0016112633747979999\n",
      "epoch: 5 step: 1743, loss is 0.007538318168371916\n",
      "epoch: 5 step: 1744, loss is 0.0139277009293437\n",
      "epoch: 5 step: 1745, loss is 0.025238024070858955\n",
      "epoch: 5 step: 1746, loss is 0.007885938510298729\n",
      "epoch: 5 step: 1747, loss is 0.23714664578437805\n",
      "epoch: 5 step: 1748, loss is 0.0032244937028735876\n",
      "epoch: 5 step: 1749, loss is 0.01902269944548607\n",
      "epoch: 5 step: 1750, loss is 0.0035529539454728365\n",
      "epoch: 5 step: 1751, loss is 0.0408545546233654\n",
      "epoch: 5 step: 1752, loss is 0.007115825545042753\n",
      "epoch: 5 step: 1753, loss is 0.0489378459751606\n",
      "epoch: 5 step: 1754, loss is 0.009595083072781563\n",
      "epoch: 5 step: 1755, loss is 0.032008424401283264\n",
      "epoch: 5 step: 1756, loss is 0.19397397339344025\n",
      "epoch: 5 step: 1757, loss is 0.00612693652510643\n",
      "epoch: 5 step: 1758, loss is 0.02553519234061241\n",
      "epoch: 5 step: 1759, loss is 0.05558602511882782\n",
      "epoch: 5 step: 1760, loss is 0.24075564742088318\n",
      "epoch: 5 step: 1761, loss is 0.03518633544445038\n",
      "epoch: 5 step: 1762, loss is 0.015238592401146889\n",
      "epoch: 5 step: 1763, loss is 0.03815283253788948\n",
      "epoch: 5 step: 1764, loss is 0.056661367416381836\n",
      "epoch: 5 step: 1765, loss is 0.010047734715044498\n",
      "epoch: 5 step: 1766, loss is 0.05036132410168648\n",
      "epoch: 5 step: 1767, loss is 0.14280998706817627\n",
      "epoch: 5 step: 1768, loss is 0.1116018295288086\n",
      "epoch: 5 step: 1769, loss is 0.03718787431716919\n",
      "epoch: 5 step: 1770, loss is 0.028714122250676155\n",
      "epoch: 5 step: 1771, loss is 0.013754504732787609\n",
      "epoch: 5 step: 1772, loss is 0.0525815486907959\n",
      "epoch: 5 step: 1773, loss is 0.041290707886219025\n",
      "epoch: 5 step: 1774, loss is 0.0886182188987732\n",
      "epoch: 5 step: 1775, loss is 0.036180414259433746\n",
      "epoch: 5 step: 1776, loss is 0.011064122430980206\n",
      "epoch: 5 step: 1777, loss is 0.07460951805114746\n",
      "epoch: 5 step: 1778, loss is 0.004026264883577824\n",
      "epoch: 5 step: 1779, loss is 0.13388247787952423\n",
      "epoch: 5 step: 1780, loss is 0.02478170581161976\n",
      "epoch: 5 step: 1781, loss is 0.09027381241321564\n",
      "epoch: 5 step: 1782, loss is 0.011193917132914066\n",
      "epoch: 5 step: 1783, loss is 0.13905340433120728\n",
      "epoch: 5 step: 1784, loss is 0.007763247936964035\n",
      "epoch: 5 step: 1785, loss is 0.02455519326031208\n",
      "epoch: 5 step: 1786, loss is 0.08305606245994568\n",
      "epoch: 5 step: 1787, loss is 0.04805554077029228\n",
      "epoch: 5 step: 1788, loss is 0.009574703872203827\n",
      "epoch: 5 step: 1789, loss is 0.07228413224220276\n",
      "epoch: 5 step: 1790, loss is 0.10306093096733093\n",
      "epoch: 5 step: 1791, loss is 0.003298727795481682\n",
      "epoch: 5 step: 1792, loss is 0.01697627082467079\n",
      "epoch: 5 step: 1793, loss is 0.026476331055164337\n",
      "epoch: 5 step: 1794, loss is 0.0050760856829583645\n",
      "epoch: 5 step: 1795, loss is 0.006797602865844965\n",
      "epoch: 5 step: 1796, loss is 0.010901068337261677\n",
      "epoch: 5 step: 1797, loss is 0.06632272899150848\n",
      "epoch: 5 step: 1798, loss is 0.05275236815214157\n",
      "epoch: 5 step: 1799, loss is 0.015461382456123829\n",
      "epoch: 5 step: 1800, loss is 0.016783177852630615\n",
      "epoch: 5 step: 1801, loss is 0.04927155375480652\n",
      "epoch: 5 step: 1802, loss is 0.24210040271282196\n",
      "epoch: 5 step: 1803, loss is 0.34562036395072937\n",
      "epoch: 5 step: 1804, loss is 0.38210633397102356\n",
      "epoch: 5 step: 1805, loss is 0.11594969034194946\n",
      "epoch: 5 step: 1806, loss is 0.0469948910176754\n",
      "epoch: 5 step: 1807, loss is 0.05027955770492554\n",
      "epoch: 5 step: 1808, loss is 0.034191492944955826\n",
      "epoch: 5 step: 1809, loss is 0.0017909995513036847\n",
      "epoch: 5 step: 1810, loss is 0.060750916600227356\n",
      "epoch: 5 step: 1811, loss is 0.008799453265964985\n",
      "epoch: 5 step: 1812, loss is 0.07361230254173279\n",
      "epoch: 5 step: 1813, loss is 0.02127009816467762\n",
      "epoch: 5 step: 1814, loss is 0.0031030818354338408\n",
      "epoch: 5 step: 1815, loss is 0.009350672364234924\n",
      "epoch: 5 step: 1816, loss is 0.11781561374664307\n",
      "epoch: 5 step: 1817, loss is 0.10822226107120514\n",
      "epoch: 5 step: 1818, loss is 0.17169389128684998\n",
      "epoch: 5 step: 1819, loss is 0.011921793222427368\n",
      "epoch: 5 step: 1820, loss is 0.08963221311569214\n",
      "epoch: 5 step: 1821, loss is 0.07919994741678238\n",
      "epoch: 5 step: 1822, loss is 0.03296588733792305\n",
      "epoch: 5 step: 1823, loss is 0.12097059935331345\n",
      "epoch: 5 step: 1824, loss is 0.0228860005736351\n",
      "epoch: 5 step: 1825, loss is 0.20958198606967926\n",
      "epoch: 5 step: 1826, loss is 0.035804156213998795\n",
      "epoch: 5 step: 1827, loss is 0.2610405385494232\n",
      "epoch: 5 step: 1828, loss is 0.19816561043262482\n",
      "epoch: 5 step: 1829, loss is 0.33474811911582947\n",
      "epoch: 5 step: 1830, loss is 0.047158364206552505\n",
      "epoch: 5 step: 1831, loss is 0.004233215469866991\n",
      "epoch: 5 step: 1832, loss is 0.05336393788456917\n",
      "epoch: 5 step: 1833, loss is 0.0017174138920381665\n",
      "epoch: 5 step: 1834, loss is 0.007700411602854729\n",
      "epoch: 5 step: 1835, loss is 0.04775422066450119\n",
      "epoch: 5 step: 1836, loss is 0.027822254225611687\n",
      "epoch: 5 step: 1837, loss is 0.18055987358093262\n",
      "epoch: 5 step: 1838, loss is 0.1258447915315628\n",
      "epoch: 5 step: 1839, loss is 0.010727079585194588\n",
      "epoch: 5 step: 1840, loss is 0.034158915281295776\n",
      "epoch: 5 step: 1841, loss is 0.09638673067092896\n",
      "epoch: 5 step: 1842, loss is 0.07337211817502975\n",
      "epoch: 5 step: 1843, loss is 0.023699715733528137\n",
      "epoch: 5 step: 1844, loss is 0.11621834337711334\n",
      "epoch: 5 step: 1845, loss is 0.017111066728830338\n",
      "epoch: 5 step: 1846, loss is 0.1587556153535843\n",
      "epoch: 5 step: 1847, loss is 0.13968278467655182\n",
      "epoch: 5 step: 1848, loss is 0.0016345077892765403\n",
      "epoch: 5 step: 1849, loss is 0.09773541986942291\n",
      "epoch: 5 step: 1850, loss is 0.009156592190265656\n",
      "epoch: 5 step: 1851, loss is 0.018066124990582466\n",
      "epoch: 5 step: 1852, loss is 0.0359417088329792\n",
      "epoch: 5 step: 1853, loss is 0.0365140475332737\n",
      "epoch: 5 step: 1854, loss is 0.06418760120868683\n",
      "epoch: 5 step: 1855, loss is 0.042510293424129486\n",
      "epoch: 5 step: 1856, loss is 0.008845954202115536\n",
      "epoch: 5 step: 1857, loss is 0.0055946712382137775\n",
      "epoch: 5 step: 1858, loss is 0.010359708219766617\n",
      "epoch: 5 step: 1859, loss is 0.0034894845448434353\n",
      "epoch: 5 step: 1860, loss is 0.009148960001766682\n",
      "epoch: 5 step: 1861, loss is 0.11192435771226883\n",
      "epoch: 5 step: 1862, loss is 0.13282105326652527\n",
      "epoch: 5 step: 1863, loss is 0.0025299224071204662\n",
      "epoch: 5 step: 1864, loss is 0.0019321173895150423\n",
      "epoch: 5 step: 1865, loss is 0.019255205988883972\n",
      "epoch: 5 step: 1866, loss is 0.020561223849654198\n",
      "epoch: 5 step: 1867, loss is 0.10026383399963379\n",
      "epoch: 5 step: 1868, loss is 0.014185268431901932\n",
      "epoch: 5 step: 1869, loss is 0.09523935616016388\n",
      "epoch: 5 step: 1870, loss is 0.11725424975156784\n",
      "epoch: 5 step: 1871, loss is 0.04260922595858574\n",
      "epoch: 5 step: 1872, loss is 0.0008628914365544915\n",
      "epoch: 5 step: 1873, loss is 0.022102024406194687\n",
      "epoch: 5 step: 1874, loss is 0.018947159871459007\n",
      "epoch: 5 step: 1875, loss is 0.16542036831378937\n",
      "epoch: 6 step: 1, loss is 0.006228613667190075\n",
      "epoch: 6 step: 2, loss is 0.012855308130383492\n",
      "epoch: 6 step: 3, loss is 0.035553742200136185\n",
      "epoch: 6 step: 4, loss is 0.022321077063679695\n",
      "epoch: 6 step: 5, loss is 0.2853098511695862\n",
      "epoch: 6 step: 6, loss is 0.012149159796535969\n",
      "epoch: 6 step: 7, loss is 0.06962224841117859\n",
      "epoch: 6 step: 8, loss is 0.004694249480962753\n",
      "epoch: 6 step: 9, loss is 0.0025929196272045374\n",
      "epoch: 6 step: 10, loss is 0.014591019600629807\n",
      "epoch: 6 step: 11, loss is 0.053394824266433716\n",
      "epoch: 6 step: 12, loss is 0.00402090884745121\n",
      "epoch: 6 step: 13, loss is 0.10869024693965912\n",
      "epoch: 6 step: 14, loss is 0.0790850892663002\n",
      "epoch: 6 step: 15, loss is 0.0810888260602951\n",
      "epoch: 6 step: 16, loss is 0.01473594456911087\n",
      "epoch: 6 step: 17, loss is 0.0017322306521236897\n",
      "epoch: 6 step: 18, loss is 0.11099336296319962\n",
      "epoch: 6 step: 19, loss is 0.042371056973934174\n",
      "epoch: 6 step: 20, loss is 0.010473284870386124\n",
      "epoch: 6 step: 21, loss is 0.27301180362701416\n",
      "epoch: 6 step: 22, loss is 0.028868751600384712\n",
      "epoch: 6 step: 23, loss is 0.020664691925048828\n",
      "epoch: 6 step: 24, loss is 0.0010842789197340608\n",
      "epoch: 6 step: 25, loss is 0.010280931368470192\n",
      "epoch: 6 step: 26, loss is 0.08995641022920609\n",
      "epoch: 6 step: 27, loss is 0.007511597592383623\n",
      "epoch: 6 step: 28, loss is 0.007183885667473078\n",
      "epoch: 6 step: 29, loss is 0.04371615871787071\n",
      "epoch: 6 step: 30, loss is 0.016128050163388252\n",
      "epoch: 6 step: 31, loss is 0.04948745667934418\n",
      "epoch: 6 step: 32, loss is 0.01629405841231346\n",
      "epoch: 6 step: 33, loss is 0.003403482958674431\n",
      "epoch: 6 step: 34, loss is 0.004764363635331392\n",
      "epoch: 6 step: 35, loss is 0.013384713791310787\n",
      "epoch: 6 step: 36, loss is 0.06753434240818024\n",
      "epoch: 6 step: 37, loss is 0.11360650509595871\n",
      "epoch: 6 step: 38, loss is 0.05656979978084564\n",
      "epoch: 6 step: 39, loss is 0.027555499225854874\n",
      "epoch: 6 step: 40, loss is 0.12472115457057953\n",
      "epoch: 6 step: 41, loss is 0.00896725244820118\n",
      "epoch: 6 step: 42, loss is 0.007992488332092762\n",
      "epoch: 6 step: 43, loss is 0.07902010530233383\n",
      "epoch: 6 step: 44, loss is 0.15833128988742828\n",
      "epoch: 6 step: 45, loss is 0.015867676585912704\n",
      "epoch: 6 step: 46, loss is 0.0435853935778141\n",
      "epoch: 6 step: 47, loss is 0.2994788885116577\n",
      "epoch: 6 step: 48, loss is 0.007804906461387873\n",
      "epoch: 6 step: 49, loss is 0.0011108128819614649\n",
      "epoch: 6 step: 50, loss is 0.010861868038773537\n",
      "epoch: 6 step: 51, loss is 0.005757713224738836\n",
      "epoch: 6 step: 52, loss is 0.022006560117006302\n",
      "epoch: 6 step: 53, loss is 0.005460943561047316\n",
      "epoch: 6 step: 54, loss is 0.006872985046356916\n",
      "epoch: 6 step: 55, loss is 0.010126065462827682\n",
      "epoch: 6 step: 56, loss is 0.5548392534255981\n",
      "epoch: 6 step: 57, loss is 0.05691012740135193\n",
      "epoch: 6 step: 58, loss is 0.010813659057021141\n",
      "epoch: 6 step: 59, loss is 0.008091139607131481\n",
      "epoch: 6 step: 60, loss is 0.05954281985759735\n",
      "epoch: 6 step: 61, loss is 0.07218323647975922\n",
      "epoch: 6 step: 62, loss is 0.012281843461096287\n",
      "epoch: 6 step: 63, loss is 0.0846024751663208\n",
      "epoch: 6 step: 64, loss is 0.015435006469488144\n",
      "epoch: 6 step: 65, loss is 0.02751181647181511\n",
      "epoch: 6 step: 66, loss is 0.033706218004226685\n",
      "epoch: 6 step: 67, loss is 0.012727971188724041\n",
      "epoch: 6 step: 68, loss is 0.05399790778756142\n",
      "epoch: 6 step: 69, loss is 0.06476130336523056\n",
      "epoch: 6 step: 70, loss is 0.00573488837108016\n",
      "epoch: 6 step: 71, loss is 0.11551165580749512\n",
      "epoch: 6 step: 72, loss is 0.1310432106256485\n",
      "epoch: 6 step: 73, loss is 0.02293434925377369\n",
      "epoch: 6 step: 74, loss is 0.055037811398506165\n",
      "epoch: 6 step: 75, loss is 0.04437732324004173\n",
      "epoch: 6 step: 76, loss is 0.18121902644634247\n",
      "epoch: 6 step: 77, loss is 0.01127567794173956\n",
      "epoch: 6 step: 78, loss is 0.007479061372578144\n",
      "epoch: 6 step: 79, loss is 0.054132457822561264\n",
      "epoch: 6 step: 80, loss is 0.0907721072435379\n",
      "epoch: 6 step: 81, loss is 0.04308432340621948\n",
      "epoch: 6 step: 82, loss is 0.008747808635234833\n",
      "epoch: 6 step: 83, loss is 0.03631293773651123\n",
      "epoch: 6 step: 84, loss is 0.027816010639071465\n",
      "epoch: 6 step: 85, loss is 0.032828543335199356\n",
      "epoch: 6 step: 86, loss is 0.01602441631257534\n",
      "epoch: 6 step: 87, loss is 0.0565621592104435\n",
      "epoch: 6 step: 88, loss is 0.04309763386845589\n",
      "epoch: 6 step: 89, loss is 0.0013829309027642012\n",
      "epoch: 6 step: 90, loss is 0.0022375730331987143\n",
      "epoch: 6 step: 91, loss is 0.045718804001808167\n",
      "epoch: 6 step: 92, loss is 0.08724202960729599\n",
      "epoch: 6 step: 93, loss is 0.007849812507629395\n",
      "epoch: 6 step: 94, loss is 0.023980725556612015\n",
      "epoch: 6 step: 95, loss is 0.04840895161032677\n",
      "epoch: 6 step: 96, loss is 0.025196334347128868\n",
      "epoch: 6 step: 97, loss is 0.015168484300374985\n",
      "epoch: 6 step: 98, loss is 0.009030946530401707\n",
      "epoch: 6 step: 99, loss is 0.00975574366748333\n",
      "epoch: 6 step: 100, loss is 0.037852942943573\n",
      "epoch: 6 step: 101, loss is 0.01336485892534256\n",
      "epoch: 6 step: 102, loss is 0.00891802180558443\n",
      "epoch: 6 step: 103, loss is 0.0012355544604361057\n",
      "epoch: 6 step: 104, loss is 0.04474832862615585\n",
      "epoch: 6 step: 105, loss is 0.14785942435264587\n",
      "epoch: 6 step: 106, loss is 0.002835645340383053\n",
      "epoch: 6 step: 107, loss is 0.0021412153728306293\n",
      "epoch: 6 step: 108, loss is 0.013931852765381336\n",
      "epoch: 6 step: 109, loss is 0.008178231306374073\n",
      "epoch: 6 step: 110, loss is 0.008256143890321255\n",
      "epoch: 6 step: 111, loss is 0.056037697941064835\n",
      "epoch: 6 step: 112, loss is 0.013363189063966274\n",
      "epoch: 6 step: 113, loss is 0.013290975242853165\n",
      "epoch: 6 step: 114, loss is 0.08185749500989914\n",
      "epoch: 6 step: 115, loss is 0.013966456055641174\n",
      "epoch: 6 step: 116, loss is 0.08408799022436142\n",
      "epoch: 6 step: 117, loss is 0.16174139082431793\n",
      "epoch: 6 step: 118, loss is 0.018477147445082664\n",
      "epoch: 6 step: 119, loss is 0.007200170774012804\n",
      "epoch: 6 step: 120, loss is 0.0014359268825501204\n",
      "epoch: 6 step: 121, loss is 0.23159940540790558\n",
      "epoch: 6 step: 122, loss is 0.012417960911989212\n",
      "epoch: 6 step: 123, loss is 0.05142716318368912\n",
      "epoch: 6 step: 124, loss is 0.02234332263469696\n",
      "epoch: 6 step: 125, loss is 0.004704819992184639\n",
      "epoch: 6 step: 126, loss is 0.04994811490178108\n",
      "epoch: 6 step: 127, loss is 0.04812087118625641\n",
      "epoch: 6 step: 128, loss is 0.03781254589557648\n",
      "epoch: 6 step: 129, loss is 0.005643281154334545\n",
      "epoch: 6 step: 130, loss is 0.010783884674310684\n",
      "epoch: 6 step: 131, loss is 0.10502008348703384\n",
      "epoch: 6 step: 132, loss is 0.01197787281125784\n",
      "epoch: 6 step: 133, loss is 0.020975487306714058\n",
      "epoch: 6 step: 134, loss is 0.029691992327570915\n",
      "epoch: 6 step: 135, loss is 0.027025394141674042\n",
      "epoch: 6 step: 136, loss is 0.03232445567846298\n",
      "epoch: 6 step: 137, loss is 0.005578830372542143\n",
      "epoch: 6 step: 138, loss is 0.14627619087696075\n",
      "epoch: 6 step: 139, loss is 0.009337540715932846\n",
      "epoch: 6 step: 140, loss is 0.11732394993305206\n",
      "epoch: 6 step: 141, loss is 0.03932992368936539\n",
      "epoch: 6 step: 142, loss is 0.005315159447491169\n",
      "epoch: 6 step: 143, loss is 0.03137669712305069\n",
      "epoch: 6 step: 144, loss is 0.08440084755420685\n",
      "epoch: 6 step: 145, loss is 0.01572543941438198\n",
      "epoch: 6 step: 146, loss is 0.04544512927532196\n",
      "epoch: 6 step: 147, loss is 0.050953324884176254\n",
      "epoch: 6 step: 148, loss is 0.01205933466553688\n",
      "epoch: 6 step: 149, loss is 0.0027968850918114185\n",
      "epoch: 6 step: 150, loss is 0.03616422787308693\n",
      "epoch: 6 step: 151, loss is 0.0159786157310009\n",
      "epoch: 6 step: 152, loss is 0.00977380108088255\n",
      "epoch: 6 step: 153, loss is 0.008284114301204681\n",
      "epoch: 6 step: 154, loss is 0.08698896318674088\n",
      "epoch: 6 step: 155, loss is 0.01551809348165989\n",
      "epoch: 6 step: 156, loss is 0.023068975657224655\n",
      "epoch: 6 step: 157, loss is 0.016869038343429565\n",
      "epoch: 6 step: 158, loss is 0.004394879564642906\n",
      "epoch: 6 step: 159, loss is 0.04288558289408684\n",
      "epoch: 6 step: 160, loss is 0.006234371103346348\n",
      "epoch: 6 step: 161, loss is 0.0027407403104007244\n",
      "epoch: 6 step: 162, loss is 0.061391763389110565\n",
      "epoch: 6 step: 163, loss is 0.055645108222961426\n",
      "epoch: 6 step: 164, loss is 0.03893587365746498\n",
      "epoch: 6 step: 165, loss is 0.1730770468711853\n",
      "epoch: 6 step: 166, loss is 0.3151598870754242\n",
      "epoch: 6 step: 167, loss is 0.03514666110277176\n",
      "epoch: 6 step: 168, loss is 0.013156038708984852\n",
      "epoch: 6 step: 169, loss is 0.05322274938225746\n",
      "epoch: 6 step: 170, loss is 0.006516095716506243\n",
      "epoch: 6 step: 171, loss is 0.010932311415672302\n",
      "epoch: 6 step: 172, loss is 0.032879047095775604\n",
      "epoch: 6 step: 173, loss is 0.005539262667298317\n",
      "epoch: 6 step: 174, loss is 0.0056743258610367775\n",
      "epoch: 6 step: 175, loss is 0.010750679299235344\n",
      "epoch: 6 step: 176, loss is 0.01917170360684395\n",
      "epoch: 6 step: 177, loss is 0.0523710660636425\n",
      "epoch: 6 step: 178, loss is 0.010689444839954376\n",
      "epoch: 6 step: 179, loss is 0.02553393878042698\n",
      "epoch: 6 step: 180, loss is 0.12575680017471313\n",
      "epoch: 6 step: 181, loss is 0.07125955820083618\n",
      "epoch: 6 step: 182, loss is 0.03257356956601143\n",
      "epoch: 6 step: 183, loss is 0.0062951259315013885\n",
      "epoch: 6 step: 184, loss is 0.04470799118280411\n",
      "epoch: 6 step: 185, loss is 0.007895837537944317\n",
      "epoch: 6 step: 186, loss is 0.08088649809360504\n",
      "epoch: 6 step: 187, loss is 0.12497042864561081\n",
      "epoch: 6 step: 188, loss is 0.02535758912563324\n",
      "epoch: 6 step: 189, loss is 0.005258793011307716\n",
      "epoch: 6 step: 190, loss is 0.039086777716875076\n",
      "epoch: 6 step: 191, loss is 0.0019501971546560526\n",
      "epoch: 6 step: 192, loss is 0.1738152652978897\n",
      "epoch: 6 step: 193, loss is 0.09748589992523193\n",
      "epoch: 6 step: 194, loss is 0.05524982511997223\n",
      "epoch: 6 step: 195, loss is 0.016882551833987236\n",
      "epoch: 6 step: 196, loss is 0.0831424817442894\n",
      "epoch: 6 step: 197, loss is 0.058580487966537476\n",
      "epoch: 6 step: 198, loss is 0.003263544524088502\n",
      "epoch: 6 step: 199, loss is 0.053651053458452225\n",
      "epoch: 6 step: 200, loss is 0.03339560702443123\n",
      "epoch: 6 step: 201, loss is 0.011220747604966164\n",
      "epoch: 6 step: 202, loss is 0.009067595936357975\n",
      "epoch: 6 step: 203, loss is 0.0059752208180725574\n",
      "epoch: 6 step: 204, loss is 0.0034798537380993366\n",
      "epoch: 6 step: 205, loss is 0.045488014817237854\n",
      "epoch: 6 step: 206, loss is 0.029195934534072876\n",
      "epoch: 6 step: 207, loss is 0.03784121945500374\n",
      "epoch: 6 step: 208, loss is 0.012930491007864475\n",
      "epoch: 6 step: 209, loss is 0.011257203295826912\n",
      "epoch: 6 step: 210, loss is 0.013961512595415115\n",
      "epoch: 6 step: 211, loss is 0.02051399089396\n",
      "epoch: 6 step: 212, loss is 0.0010269642807543278\n",
      "epoch: 6 step: 213, loss is 0.00414842227473855\n",
      "epoch: 6 step: 214, loss is 0.042797546833753586\n",
      "epoch: 6 step: 215, loss is 0.0227064061909914\n",
      "epoch: 6 step: 216, loss is 0.018581882119178772\n",
      "epoch: 6 step: 217, loss is 0.0004447859537322074\n",
      "epoch: 6 step: 218, loss is 0.0015997190494090319\n",
      "epoch: 6 step: 219, loss is 0.0011827003909274936\n",
      "epoch: 6 step: 220, loss is 0.001935867010615766\n",
      "epoch: 6 step: 221, loss is 0.009313900955021381\n",
      "epoch: 6 step: 222, loss is 0.10852430015802383\n",
      "epoch: 6 step: 223, loss is 0.002045152010396123\n",
      "epoch: 6 step: 224, loss is 0.329653799533844\n",
      "epoch: 6 step: 225, loss is 0.0014416194753721356\n",
      "epoch: 6 step: 226, loss is 0.001663594739511609\n",
      "epoch: 6 step: 227, loss is 0.045133523643016815\n",
      "epoch: 6 step: 228, loss is 0.004897597245872021\n",
      "epoch: 6 step: 229, loss is 0.1234130859375\n",
      "epoch: 6 step: 230, loss is 0.001213561394251883\n",
      "epoch: 6 step: 231, loss is 0.003050201339647174\n",
      "epoch: 6 step: 232, loss is 0.005070801358669996\n",
      "epoch: 6 step: 233, loss is 0.16146987676620483\n",
      "epoch: 6 step: 234, loss is 0.022092971950769424\n",
      "epoch: 6 step: 235, loss is 0.13956309854984283\n",
      "epoch: 6 step: 236, loss is 0.03955112025141716\n",
      "epoch: 6 step: 237, loss is 0.0015564830973744392\n",
      "epoch: 6 step: 238, loss is 0.16395890712738037\n",
      "epoch: 6 step: 239, loss is 0.04228300601243973\n",
      "epoch: 6 step: 240, loss is 0.0002800867659971118\n",
      "epoch: 6 step: 241, loss is 0.004554225597530603\n",
      "epoch: 6 step: 242, loss is 0.12315347790718079\n",
      "epoch: 6 step: 243, loss is 0.11030147224664688\n",
      "epoch: 6 step: 244, loss is 0.05882485955953598\n",
      "epoch: 6 step: 245, loss is 0.0027273930609226227\n",
      "epoch: 6 step: 246, loss is 0.008291956968605518\n",
      "epoch: 6 step: 247, loss is 0.0023987654130905867\n",
      "epoch: 6 step: 248, loss is 0.033698830753564835\n",
      "epoch: 6 step: 249, loss is 0.023684054613113403\n",
      "epoch: 6 step: 250, loss is 0.0030434606596827507\n",
      "epoch: 6 step: 251, loss is 0.04068617895245552\n",
      "epoch: 6 step: 252, loss is 0.041174501180648804\n",
      "epoch: 6 step: 253, loss is 0.0005847490392625332\n",
      "epoch: 6 step: 254, loss is 0.010996825993061066\n",
      "epoch: 6 step: 255, loss is 0.007771489676088095\n",
      "epoch: 6 step: 256, loss is 0.0019640091340988874\n",
      "epoch: 6 step: 257, loss is 0.026771321892738342\n",
      "epoch: 6 step: 258, loss is 0.15192513167858124\n",
      "epoch: 6 step: 259, loss is 0.127126082777977\n",
      "epoch: 6 step: 260, loss is 0.05432526022195816\n",
      "epoch: 6 step: 261, loss is 0.035480011254549026\n",
      "epoch: 6 step: 262, loss is 0.0018386311130598187\n",
      "epoch: 6 step: 263, loss is 0.10720224678516388\n",
      "epoch: 6 step: 264, loss is 0.0003208476409781724\n",
      "epoch: 6 step: 265, loss is 0.06262601166963577\n",
      "epoch: 6 step: 266, loss is 0.03549042344093323\n",
      "epoch: 6 step: 267, loss is 0.0009044570615515113\n",
      "epoch: 6 step: 268, loss is 0.0013832149561494589\n",
      "epoch: 6 step: 269, loss is 0.008992410264909267\n",
      "epoch: 6 step: 270, loss is 0.012625126168131828\n",
      "epoch: 6 step: 271, loss is 0.032698217779397964\n",
      "epoch: 6 step: 272, loss is 0.11833720654249191\n",
      "epoch: 6 step: 273, loss is 0.3183780610561371\n",
      "epoch: 6 step: 274, loss is 0.01209831703454256\n",
      "epoch: 6 step: 275, loss is 0.05007968842983246\n",
      "epoch: 6 step: 276, loss is 0.014443052932620049\n",
      "epoch: 6 step: 277, loss is 0.006130542140454054\n",
      "epoch: 6 step: 278, loss is 0.010240570642054081\n",
      "epoch: 6 step: 279, loss is 0.003944748546928167\n",
      "epoch: 6 step: 280, loss is 0.011002371087670326\n",
      "epoch: 6 step: 281, loss is 0.0006068643415346742\n",
      "epoch: 6 step: 282, loss is 0.0023372131399810314\n",
      "epoch: 6 step: 283, loss is 0.007677885703742504\n",
      "epoch: 6 step: 284, loss is 0.044688254594802856\n",
      "epoch: 6 step: 285, loss is 0.010656614787876606\n",
      "epoch: 6 step: 286, loss is 0.12118765711784363\n",
      "epoch: 6 step: 287, loss is 0.21010297536849976\n",
      "epoch: 6 step: 288, loss is 0.051249489188194275\n",
      "epoch: 6 step: 289, loss is 0.007078902330249548\n",
      "epoch: 6 step: 290, loss is 0.007358680013567209\n",
      "epoch: 6 step: 291, loss is 0.10229559987783432\n",
      "epoch: 6 step: 292, loss is 0.0003376555978320539\n",
      "epoch: 6 step: 293, loss is 0.0008529091719537973\n",
      "epoch: 6 step: 294, loss is 0.01660003699362278\n",
      "epoch: 6 step: 295, loss is 0.0007366074132733047\n",
      "epoch: 6 step: 296, loss is 0.011975478380918503\n",
      "epoch: 6 step: 297, loss is 0.006496647372841835\n",
      "epoch: 6 step: 298, loss is 0.06537334620952606\n",
      "epoch: 6 step: 299, loss is 0.11662393808364868\n",
      "epoch: 6 step: 300, loss is 0.000449151499196887\n",
      "epoch: 6 step: 301, loss is 0.028268514201045036\n",
      "epoch: 6 step: 302, loss is 0.003270612098276615\n",
      "epoch: 6 step: 303, loss is 0.07883508503437042\n",
      "epoch: 6 step: 304, loss is 0.000463018543086946\n",
      "epoch: 6 step: 305, loss is 0.04490981251001358\n",
      "epoch: 6 step: 306, loss is 0.270904004573822\n",
      "epoch: 6 step: 307, loss is 0.014495923183858395\n",
      "epoch: 6 step: 308, loss is 0.004189458210021257\n",
      "epoch: 6 step: 309, loss is 0.0398990772664547\n",
      "epoch: 6 step: 310, loss is 0.045571811497211456\n",
      "epoch: 6 step: 311, loss is 0.013946001417934895\n",
      "epoch: 6 step: 312, loss is 0.011126469820737839\n",
      "epoch: 6 step: 313, loss is 0.0699056088924408\n",
      "epoch: 6 step: 314, loss is 0.0018949792720377445\n",
      "epoch: 6 step: 315, loss is 0.05585978925228119\n",
      "epoch: 6 step: 316, loss is 0.22658103704452515\n",
      "epoch: 6 step: 317, loss is 0.004323848057538271\n",
      "epoch: 6 step: 318, loss is 0.007752479985356331\n",
      "epoch: 6 step: 319, loss is 0.009273783303797245\n",
      "epoch: 6 step: 320, loss is 0.0004037391336169094\n",
      "epoch: 6 step: 321, loss is 0.0022691688500344753\n",
      "epoch: 6 step: 322, loss is 0.006384973414242268\n",
      "epoch: 6 step: 323, loss is 0.011434254236519337\n",
      "epoch: 6 step: 324, loss is 0.013461957685649395\n",
      "epoch: 6 step: 325, loss is 0.08729387074708939\n",
      "epoch: 6 step: 326, loss is 0.00025557397748343647\n",
      "epoch: 6 step: 327, loss is 0.0412675179541111\n",
      "epoch: 6 step: 328, loss is 0.0364985354244709\n",
      "epoch: 6 step: 329, loss is 0.06852417439222336\n",
      "epoch: 6 step: 330, loss is 0.030781973153352737\n",
      "epoch: 6 step: 331, loss is 0.02719954587519169\n",
      "epoch: 6 step: 332, loss is 0.07987424731254578\n",
      "epoch: 6 step: 333, loss is 0.181399866938591\n",
      "epoch: 6 step: 334, loss is 0.2977585196495056\n",
      "epoch: 6 step: 335, loss is 0.29968875646591187\n",
      "epoch: 6 step: 336, loss is 0.03230803459882736\n",
      "epoch: 6 step: 337, loss is 0.0005687027587555349\n",
      "epoch: 6 step: 338, loss is 0.005930866580456495\n",
      "epoch: 6 step: 339, loss is 0.07381739467382431\n",
      "epoch: 6 step: 340, loss is 0.026682386174798012\n",
      "epoch: 6 step: 341, loss is 0.07042548805475235\n",
      "epoch: 6 step: 342, loss is 0.25991350412368774\n",
      "epoch: 6 step: 343, loss is 0.05174736678600311\n",
      "epoch: 6 step: 344, loss is 0.013912433758378029\n",
      "epoch: 6 step: 345, loss is 0.01468308549374342\n",
      "epoch: 6 step: 346, loss is 0.12663686275482178\n",
      "epoch: 6 step: 347, loss is 0.3330760896205902\n",
      "epoch: 6 step: 348, loss is 0.006740691605955362\n",
      "epoch: 6 step: 349, loss is 0.29459118843078613\n",
      "epoch: 6 step: 350, loss is 0.003132048062980175\n",
      "epoch: 6 step: 351, loss is 0.0036852352786809206\n",
      "epoch: 6 step: 352, loss is 0.08830992132425308\n",
      "epoch: 6 step: 353, loss is 0.017630578950047493\n",
      "epoch: 6 step: 354, loss is 0.003748584073036909\n",
      "epoch: 6 step: 355, loss is 0.03962908685207367\n",
      "epoch: 6 step: 356, loss is 0.14242732524871826\n",
      "epoch: 6 step: 357, loss is 0.09378784894943237\n",
      "epoch: 6 step: 358, loss is 0.0002491887134965509\n",
      "epoch: 6 step: 359, loss is 0.002525575924664736\n",
      "epoch: 6 step: 360, loss is 0.15036605298519135\n",
      "epoch: 6 step: 361, loss is 0.0887589231133461\n",
      "epoch: 6 step: 362, loss is 0.25361841917037964\n",
      "epoch: 6 step: 363, loss is 0.08549035340547562\n",
      "epoch: 6 step: 364, loss is 0.11458336561918259\n",
      "epoch: 6 step: 365, loss is 0.14216598868370056\n",
      "epoch: 6 step: 366, loss is 0.10219826549291611\n",
      "epoch: 6 step: 367, loss is 0.09807103127241135\n",
      "epoch: 6 step: 368, loss is 0.01210237480700016\n",
      "epoch: 6 step: 369, loss is 0.06629452109336853\n",
      "epoch: 6 step: 370, loss is 0.07898605614900589\n",
      "epoch: 6 step: 371, loss is 0.009342036210000515\n",
      "epoch: 6 step: 372, loss is 0.019918862730264664\n",
      "epoch: 6 step: 373, loss is 0.010483409278094769\n",
      "epoch: 6 step: 374, loss is 0.3222927451133728\n",
      "epoch: 6 step: 375, loss is 0.07719837129116058\n",
      "epoch: 6 step: 376, loss is 0.009157491847872734\n",
      "epoch: 6 step: 377, loss is 0.012233428657054901\n",
      "epoch: 6 step: 378, loss is 0.05833737924695015\n",
      "epoch: 6 step: 379, loss is 0.0312701016664505\n",
      "epoch: 6 step: 380, loss is 0.012053146958351135\n",
      "epoch: 6 step: 381, loss is 0.021307865157723427\n",
      "epoch: 6 step: 382, loss is 0.1343911737203598\n",
      "epoch: 6 step: 383, loss is 0.00023858285567257553\n",
      "epoch: 6 step: 384, loss is 0.031256482005119324\n",
      "epoch: 6 step: 385, loss is 0.007107941433787346\n",
      "epoch: 6 step: 386, loss is 0.0012382284039631486\n",
      "epoch: 6 step: 387, loss is 0.0054197669960558414\n",
      "epoch: 6 step: 388, loss is 0.02640966884791851\n",
      "epoch: 6 step: 389, loss is 0.0738178938627243\n",
      "epoch: 6 step: 390, loss is 0.004378443583846092\n",
      "epoch: 6 step: 391, loss is 0.008265410549938679\n",
      "epoch: 6 step: 392, loss is 0.0014602465089410543\n",
      "epoch: 6 step: 393, loss is 0.005771507043391466\n",
      "epoch: 6 step: 394, loss is 0.011449003592133522\n",
      "epoch: 6 step: 395, loss is 0.0006053357501514256\n",
      "epoch: 6 step: 396, loss is 0.02768268622457981\n",
      "epoch: 6 step: 397, loss is 0.12358909845352173\n",
      "epoch: 6 step: 398, loss is 0.010380401276051998\n",
      "epoch: 6 step: 399, loss is 0.0521465428173542\n",
      "epoch: 6 step: 400, loss is 0.15299151837825775\n",
      "epoch: 6 step: 401, loss is 0.0013791444944217801\n",
      "epoch: 6 step: 402, loss is 0.16630740463733673\n",
      "epoch: 6 step: 403, loss is 0.012293878011405468\n",
      "epoch: 6 step: 404, loss is 0.030064815655350685\n",
      "epoch: 6 step: 405, loss is 0.24204984307289124\n",
      "epoch: 6 step: 406, loss is 0.04008778929710388\n",
      "epoch: 6 step: 407, loss is 0.18399228155612946\n",
      "epoch: 6 step: 408, loss is 0.023452728986740112\n",
      "epoch: 6 step: 409, loss is 0.01101991068571806\n",
      "epoch: 6 step: 410, loss is 0.06529119610786438\n",
      "epoch: 6 step: 411, loss is 0.04380488768219948\n",
      "epoch: 6 step: 412, loss is 0.15350210666656494\n",
      "epoch: 6 step: 413, loss is 0.04133596271276474\n",
      "epoch: 6 step: 414, loss is 0.049260180443525314\n",
      "epoch: 6 step: 415, loss is 0.005695766769349575\n",
      "epoch: 6 step: 416, loss is 0.028744077309966087\n",
      "epoch: 6 step: 417, loss is 0.12225501239299774\n",
      "epoch: 6 step: 418, loss is 0.07467189431190491\n",
      "epoch: 6 step: 419, loss is 0.17270208895206451\n",
      "epoch: 6 step: 420, loss is 0.4658372104167938\n",
      "epoch: 6 step: 421, loss is 0.007875395938754082\n",
      "epoch: 6 step: 422, loss is 0.033138688653707504\n",
      "epoch: 6 step: 423, loss is 0.0214507058262825\n",
      "epoch: 6 step: 424, loss is 0.002622604137286544\n",
      "epoch: 6 step: 425, loss is 0.02538924664258957\n",
      "epoch: 6 step: 426, loss is 0.031165441498160362\n",
      "epoch: 6 step: 427, loss is 0.09070423245429993\n",
      "epoch: 6 step: 428, loss is 0.0051580751314759254\n",
      "epoch: 6 step: 429, loss is 0.044147081673145294\n",
      "epoch: 6 step: 430, loss is 0.009138344787061214\n",
      "epoch: 6 step: 431, loss is 0.026064777746796608\n",
      "epoch: 6 step: 432, loss is 0.011002576909959316\n",
      "epoch: 6 step: 433, loss is 0.09777890145778656\n",
      "epoch: 6 step: 434, loss is 0.03420956805348396\n",
      "epoch: 6 step: 435, loss is 0.020649073645472527\n",
      "epoch: 6 step: 436, loss is 0.34367233514785767\n",
      "epoch: 6 step: 437, loss is 0.058421432971954346\n",
      "epoch: 6 step: 438, loss is 0.013433308340609074\n",
      "epoch: 6 step: 439, loss is 0.004857959691435099\n",
      "epoch: 6 step: 440, loss is 0.027088290080428123\n",
      "epoch: 6 step: 441, loss is 0.04751571640372276\n",
      "epoch: 6 step: 442, loss is 0.009308112785220146\n",
      "epoch: 6 step: 443, loss is 0.001057266490533948\n",
      "epoch: 6 step: 444, loss is 0.019712399691343307\n",
      "epoch: 6 step: 445, loss is 0.0932704359292984\n",
      "epoch: 6 step: 446, loss is 0.005002398043870926\n",
      "epoch: 6 step: 447, loss is 0.00868985801935196\n",
      "epoch: 6 step: 448, loss is 0.01028545293956995\n",
      "epoch: 6 step: 449, loss is 0.00704473489895463\n",
      "epoch: 6 step: 450, loss is 0.0004955021431669593\n",
      "epoch: 6 step: 451, loss is 0.01126235444098711\n",
      "epoch: 6 step: 452, loss is 0.002927883993834257\n",
      "epoch: 6 step: 453, loss is 0.025097588077187538\n",
      "epoch: 6 step: 454, loss is 0.008489124476909637\n",
      "epoch: 6 step: 455, loss is 0.05448956415057182\n",
      "epoch: 6 step: 456, loss is 0.008487281389534473\n",
      "epoch: 6 step: 457, loss is 0.05791270732879639\n",
      "epoch: 6 step: 458, loss is 0.014943945221602917\n",
      "epoch: 6 step: 459, loss is 0.016404680907726288\n",
      "epoch: 6 step: 460, loss is 0.09400057047605515\n",
      "epoch: 6 step: 461, loss is 0.05389416217803955\n",
      "epoch: 6 step: 462, loss is 0.0025298460386693478\n",
      "epoch: 6 step: 463, loss is 0.0025320383720099926\n",
      "epoch: 6 step: 464, loss is 0.028951168060302734\n",
      "epoch: 6 step: 465, loss is 0.06222538277506828\n",
      "epoch: 6 step: 466, loss is 0.04029340669512749\n",
      "epoch: 6 step: 467, loss is 0.014095131307840347\n",
      "epoch: 6 step: 468, loss is 0.13539445400238037\n",
      "epoch: 6 step: 469, loss is 0.0020343163050711155\n",
      "epoch: 6 step: 470, loss is 0.03161465376615524\n",
      "epoch: 6 step: 471, loss is 0.004448829218745232\n",
      "epoch: 6 step: 472, loss is 0.024087706580758095\n",
      "epoch: 6 step: 473, loss is 0.009720256552100182\n",
      "epoch: 6 step: 474, loss is 0.026855051517486572\n",
      "epoch: 6 step: 475, loss is 0.0044145830906927586\n",
      "epoch: 6 step: 476, loss is 0.006297872867435217\n",
      "epoch: 6 step: 477, loss is 0.003139734035357833\n",
      "epoch: 6 step: 478, loss is 0.007588820531964302\n",
      "epoch: 6 step: 479, loss is 0.024254679679870605\n",
      "epoch: 6 step: 480, loss is 0.15512382984161377\n",
      "epoch: 6 step: 481, loss is 0.02493263967335224\n",
      "epoch: 6 step: 482, loss is 0.006752652581781149\n",
      "epoch: 6 step: 483, loss is 0.007410940248519182\n",
      "epoch: 6 step: 484, loss is 0.0038525110576301813\n",
      "epoch: 6 step: 485, loss is 0.007680207025259733\n",
      "epoch: 6 step: 486, loss is 0.007036829367280006\n",
      "epoch: 6 step: 487, loss is 0.2263759821653366\n",
      "epoch: 6 step: 488, loss is 0.0958196297287941\n",
      "epoch: 6 step: 489, loss is 0.059637658298015594\n",
      "epoch: 6 step: 490, loss is 0.0014609432546421885\n",
      "epoch: 6 step: 491, loss is 0.004991660825908184\n",
      "epoch: 6 step: 492, loss is 0.04335043951869011\n",
      "epoch: 6 step: 493, loss is 0.01938176527619362\n",
      "epoch: 6 step: 494, loss is 0.04489355906844139\n",
      "epoch: 6 step: 495, loss is 0.005203282460570335\n",
      "epoch: 6 step: 496, loss is 0.03019668348133564\n",
      "epoch: 6 step: 497, loss is 0.1636161506175995\n",
      "epoch: 6 step: 498, loss is 0.012933979742228985\n",
      "epoch: 6 step: 499, loss is 0.04901023581624031\n",
      "epoch: 6 step: 500, loss is 0.1394989788532257\n",
      "epoch: 6 step: 501, loss is 0.019124137237668037\n",
      "epoch: 6 step: 502, loss is 0.03949161246418953\n",
      "epoch: 6 step: 503, loss is 0.002242507180199027\n",
      "epoch: 6 step: 504, loss is 0.002932714531198144\n",
      "epoch: 6 step: 505, loss is 0.03529520705342293\n",
      "epoch: 6 step: 506, loss is 0.001902654767036438\n",
      "epoch: 6 step: 507, loss is 0.0939960926771164\n",
      "epoch: 6 step: 508, loss is 0.005410967860370874\n",
      "epoch: 6 step: 509, loss is 0.01736876368522644\n",
      "epoch: 6 step: 510, loss is 0.0023668892681598663\n",
      "epoch: 6 step: 511, loss is 0.0005591894150711596\n",
      "epoch: 6 step: 512, loss is 0.006129502318799496\n",
      "epoch: 6 step: 513, loss is 0.3000517785549164\n",
      "epoch: 6 step: 514, loss is 0.06373625248670578\n",
      "epoch: 6 step: 515, loss is 0.0029683231841772795\n",
      "epoch: 6 step: 516, loss is 0.003763316199183464\n",
      "epoch: 6 step: 517, loss is 0.13416653871536255\n",
      "epoch: 6 step: 518, loss is 0.014614286832511425\n",
      "epoch: 6 step: 519, loss is 0.033345747739076614\n",
      "epoch: 6 step: 520, loss is 0.052406761795282364\n",
      "epoch: 6 step: 521, loss is 0.08700694143772125\n",
      "epoch: 6 step: 522, loss is 0.03373154252767563\n",
      "epoch: 6 step: 523, loss is 0.21792364120483398\n",
      "epoch: 6 step: 524, loss is 0.053803764283657074\n",
      "epoch: 6 step: 525, loss is 0.009636436589062214\n",
      "epoch: 6 step: 526, loss is 0.07942447066307068\n",
      "epoch: 6 step: 527, loss is 0.03212299570441246\n",
      "epoch: 6 step: 528, loss is 0.01568693295121193\n",
      "epoch: 6 step: 529, loss is 0.003856513649225235\n",
      "epoch: 6 step: 530, loss is 0.0021605382207781076\n",
      "epoch: 6 step: 531, loss is 0.028745446354150772\n",
      "epoch: 6 step: 532, loss is 0.008696968667209148\n",
      "epoch: 6 step: 533, loss is 0.08968658000230789\n",
      "epoch: 6 step: 534, loss is 0.2507525682449341\n",
      "epoch: 6 step: 535, loss is 0.001287341001443565\n",
      "epoch: 6 step: 536, loss is 0.0007827788358554244\n",
      "epoch: 6 step: 537, loss is 0.17903639376163483\n",
      "epoch: 6 step: 538, loss is 0.0008368840790353715\n",
      "epoch: 6 step: 539, loss is 0.039531514048576355\n",
      "epoch: 6 step: 540, loss is 0.009432725608348846\n",
      "epoch: 6 step: 541, loss is 0.19785693287849426\n",
      "epoch: 6 step: 542, loss is 0.10982676595449448\n",
      "epoch: 6 step: 543, loss is 0.005622860509902239\n",
      "epoch: 6 step: 544, loss is 0.3026336431503296\n",
      "epoch: 6 step: 545, loss is 0.21607154607772827\n",
      "epoch: 6 step: 546, loss is 0.012131144292652607\n",
      "epoch: 6 step: 547, loss is 0.06806502491235733\n",
      "epoch: 6 step: 548, loss is 0.008738094940781593\n",
      "epoch: 6 step: 549, loss is 0.03781306743621826\n",
      "epoch: 6 step: 550, loss is 0.04087471216917038\n",
      "epoch: 6 step: 551, loss is 0.0016699085244908929\n",
      "epoch: 6 step: 552, loss is 0.012276986613869667\n",
      "epoch: 6 step: 553, loss is 0.004330375697463751\n",
      "epoch: 6 step: 554, loss is 0.02393423020839691\n",
      "epoch: 6 step: 555, loss is 0.010243825614452362\n",
      "epoch: 6 step: 556, loss is 0.0027480360586196184\n",
      "epoch: 6 step: 557, loss is 0.011896602809429169\n",
      "epoch: 6 step: 558, loss is 0.035410311073064804\n",
      "epoch: 6 step: 559, loss is 0.12351144850254059\n",
      "epoch: 6 step: 560, loss is 0.002407477702945471\n",
      "epoch: 6 step: 561, loss is 0.00580652616918087\n",
      "epoch: 6 step: 562, loss is 0.04660101607441902\n",
      "epoch: 6 step: 563, loss is 0.034542348235845566\n",
      "epoch: 6 step: 564, loss is 0.07477785646915436\n",
      "epoch: 6 step: 565, loss is 0.00919791404157877\n",
      "epoch: 6 step: 566, loss is 0.030117403715848923\n",
      "epoch: 6 step: 567, loss is 0.0022649159654974937\n",
      "epoch: 6 step: 568, loss is 0.019377442076802254\n",
      "epoch: 6 step: 569, loss is 0.0013876324519515038\n",
      "epoch: 6 step: 570, loss is 0.01304680947214365\n",
      "epoch: 6 step: 571, loss is 0.006760634947568178\n",
      "epoch: 6 step: 572, loss is 0.015076519921422005\n",
      "epoch: 6 step: 573, loss is 0.02137480303645134\n",
      "epoch: 6 step: 574, loss is 0.08417439460754395\n",
      "epoch: 6 step: 575, loss is 0.017025167122483253\n",
      "epoch: 6 step: 576, loss is 0.10234302282333374\n",
      "epoch: 6 step: 577, loss is 0.0050393156707286835\n",
      "epoch: 6 step: 578, loss is 0.002648772671818733\n",
      "epoch: 6 step: 579, loss is 0.004207300022244453\n",
      "epoch: 6 step: 580, loss is 0.17361415922641754\n",
      "epoch: 6 step: 581, loss is 0.012773874215781689\n",
      "epoch: 6 step: 582, loss is 0.1604422926902771\n",
      "epoch: 6 step: 583, loss is 0.00208698189817369\n",
      "epoch: 6 step: 584, loss is 0.014454040676355362\n",
      "epoch: 6 step: 585, loss is 0.04849771782755852\n",
      "epoch: 6 step: 586, loss is 0.014864714816212654\n",
      "epoch: 6 step: 587, loss is 0.1915379762649536\n",
      "epoch: 6 step: 588, loss is 0.003393275197595358\n",
      "epoch: 6 step: 589, loss is 0.009871409274637699\n",
      "epoch: 6 step: 590, loss is 0.05444600060582161\n",
      "epoch: 6 step: 591, loss is 0.014483689330518246\n",
      "epoch: 6 step: 592, loss is 0.0050280350260436535\n",
      "epoch: 6 step: 593, loss is 0.36499670147895813\n",
      "epoch: 6 step: 594, loss is 0.0030624577775597572\n",
      "epoch: 6 step: 595, loss is 0.02141331136226654\n",
      "epoch: 6 step: 596, loss is 0.017003806307911873\n",
      "epoch: 6 step: 597, loss is 0.12640759348869324\n",
      "epoch: 6 step: 598, loss is 0.01423615962266922\n",
      "epoch: 6 step: 599, loss is 0.006097625941038132\n",
      "epoch: 6 step: 600, loss is 0.004402169492095709\n",
      "epoch: 6 step: 601, loss is 0.008481817319989204\n",
      "epoch: 6 step: 602, loss is 0.013366132974624634\n",
      "epoch: 6 step: 603, loss is 0.022239703685045242\n",
      "epoch: 6 step: 604, loss is 0.0746118426322937\n",
      "epoch: 6 step: 605, loss is 0.00588986137881875\n",
      "epoch: 6 step: 606, loss is 0.0023160963319242\n",
      "epoch: 6 step: 607, loss is 0.007459135260432959\n",
      "epoch: 6 step: 608, loss is 0.0064206658862531185\n",
      "epoch: 6 step: 609, loss is 0.04824281483888626\n",
      "epoch: 6 step: 610, loss is 0.03184901922941208\n",
      "epoch: 6 step: 611, loss is 0.1969074308872223\n",
      "epoch: 6 step: 612, loss is 0.007536103017628193\n",
      "epoch: 6 step: 613, loss is 0.11616452783346176\n",
      "epoch: 6 step: 614, loss is 0.0009612115100026131\n",
      "epoch: 6 step: 615, loss is 0.0008528984035365283\n",
      "epoch: 6 step: 616, loss is 0.00020416731422301382\n",
      "epoch: 6 step: 617, loss is 0.10345402359962463\n",
      "epoch: 6 step: 618, loss is 0.0012769503518939018\n",
      "epoch: 6 step: 619, loss is 0.016802644357085228\n",
      "epoch: 6 step: 620, loss is 0.1096482127904892\n",
      "epoch: 6 step: 621, loss is 0.007734738755971193\n",
      "epoch: 6 step: 622, loss is 0.0032725248020142317\n",
      "epoch: 6 step: 623, loss is 0.1638464480638504\n",
      "epoch: 6 step: 624, loss is 0.09903636574745178\n",
      "epoch: 6 step: 625, loss is 0.050711739808321\n",
      "epoch: 6 step: 626, loss is 0.004713982343673706\n",
      "epoch: 6 step: 627, loss is 0.027395576238632202\n",
      "epoch: 6 step: 628, loss is 0.16533789038658142\n",
      "epoch: 6 step: 629, loss is 0.0662873387336731\n",
      "epoch: 6 step: 630, loss is 0.018948787823319435\n",
      "epoch: 6 step: 631, loss is 0.011023863218724728\n",
      "epoch: 6 step: 632, loss is 0.081405408680439\n",
      "epoch: 6 step: 633, loss is 0.0005425316630862653\n",
      "epoch: 6 step: 634, loss is 0.06550446897745132\n",
      "epoch: 6 step: 635, loss is 0.2382769137620926\n",
      "epoch: 6 step: 636, loss is 0.016046026721596718\n",
      "epoch: 6 step: 637, loss is 0.060327835381031036\n",
      "epoch: 6 step: 638, loss is 0.00884618703275919\n",
      "epoch: 6 step: 639, loss is 0.06376441568136215\n",
      "epoch: 6 step: 640, loss is 0.00617634691298008\n",
      "epoch: 6 step: 641, loss is 0.07043614983558655\n",
      "epoch: 6 step: 642, loss is 0.0708649531006813\n",
      "epoch: 6 step: 643, loss is 0.06433815509080887\n",
      "epoch: 6 step: 644, loss is 0.023526860401034355\n",
      "epoch: 6 step: 645, loss is 0.02081967703998089\n",
      "epoch: 6 step: 646, loss is 0.012775848619639874\n",
      "epoch: 6 step: 647, loss is 0.05629450082778931\n",
      "epoch: 6 step: 648, loss is 0.01630447246134281\n",
      "epoch: 6 step: 649, loss is 0.009015796706080437\n",
      "epoch: 6 step: 650, loss is 0.04108528420329094\n",
      "epoch: 6 step: 651, loss is 0.03258966654539108\n",
      "epoch: 6 step: 652, loss is 0.10385710000991821\n",
      "epoch: 6 step: 653, loss is 0.018911262974143028\n",
      "epoch: 6 step: 654, loss is 0.010161221958696842\n",
      "epoch: 6 step: 655, loss is 0.01379329152405262\n",
      "epoch: 6 step: 656, loss is 0.024293161928653717\n",
      "epoch: 6 step: 657, loss is 0.0032053100876510143\n",
      "epoch: 6 step: 658, loss is 0.05382338538765907\n",
      "epoch: 6 step: 659, loss is 0.08000189065933228\n",
      "epoch: 6 step: 660, loss is 0.00985795259475708\n",
      "epoch: 6 step: 661, loss is 0.10909067094326019\n",
      "epoch: 6 step: 662, loss is 0.1535966545343399\n",
      "epoch: 6 step: 663, loss is 0.004051441326737404\n",
      "epoch: 6 step: 664, loss is 0.015566913411021233\n",
      "epoch: 6 step: 665, loss is 0.1496390700340271\n",
      "epoch: 6 step: 666, loss is 0.0030124529730528593\n",
      "epoch: 6 step: 667, loss is 0.061466045677661896\n",
      "epoch: 6 step: 668, loss is 0.016758359968662262\n",
      "epoch: 6 step: 669, loss is 0.09210685640573502\n",
      "epoch: 6 step: 670, loss is 0.018569938838481903\n",
      "epoch: 6 step: 671, loss is 0.01840389519929886\n",
      "epoch: 6 step: 672, loss is 0.02742650918662548\n",
      "epoch: 6 step: 673, loss is 0.040925342589616776\n",
      "epoch: 6 step: 674, loss is 0.019414788112044334\n",
      "epoch: 6 step: 675, loss is 0.030393965542316437\n",
      "epoch: 6 step: 676, loss is 0.004675499629229307\n",
      "epoch: 6 step: 677, loss is 0.08249463886022568\n",
      "epoch: 6 step: 678, loss is 0.04641086235642433\n",
      "epoch: 6 step: 679, loss is 0.10103727132081985\n",
      "epoch: 6 step: 680, loss is 0.049989692866802216\n",
      "epoch: 6 step: 681, loss is 0.1544027030467987\n",
      "epoch: 6 step: 682, loss is 0.021860355511307716\n",
      "epoch: 6 step: 683, loss is 0.11355561763048172\n",
      "epoch: 6 step: 684, loss is 0.0015950626693665981\n",
      "epoch: 6 step: 685, loss is 0.009367924183607101\n",
      "epoch: 6 step: 686, loss is 0.007026894483715296\n",
      "epoch: 6 step: 687, loss is 0.019123559817671776\n",
      "epoch: 6 step: 688, loss is 0.08515581488609314\n",
      "epoch: 6 step: 689, loss is 0.015339307487010956\n",
      "epoch: 6 step: 690, loss is 0.002330755814909935\n",
      "epoch: 6 step: 691, loss is 0.02224510908126831\n",
      "epoch: 6 step: 692, loss is 0.02699517458677292\n",
      "epoch: 6 step: 693, loss is 0.023857396095991135\n",
      "epoch: 6 step: 694, loss is 0.014529288746416569\n",
      "epoch: 6 step: 695, loss is 0.013826319947838783\n",
      "epoch: 6 step: 696, loss is 0.01395730022341013\n",
      "epoch: 6 step: 697, loss is 0.0230732299387455\n",
      "epoch: 6 step: 698, loss is 0.01272738166153431\n",
      "epoch: 6 step: 699, loss is 0.002251277444884181\n",
      "epoch: 6 step: 700, loss is 0.010913529433310032\n",
      "epoch: 6 step: 701, loss is 0.005928412079811096\n",
      "epoch: 6 step: 702, loss is 0.087999626994133\n",
      "epoch: 6 step: 703, loss is 9.065197082236409e-05\n",
      "epoch: 6 step: 704, loss is 0.006075941491872072\n",
      "epoch: 6 step: 705, loss is 0.0037237736396491528\n",
      "epoch: 6 step: 706, loss is 0.061592355370521545\n",
      "epoch: 6 step: 707, loss is 0.12285074591636658\n",
      "epoch: 6 step: 708, loss is 0.009978917427361012\n",
      "epoch: 6 step: 709, loss is 0.005551384761929512\n",
      "epoch: 6 step: 710, loss is 0.03182193264365196\n",
      "epoch: 6 step: 711, loss is 0.0005057463422417641\n",
      "epoch: 6 step: 712, loss is 0.0005866860738024116\n",
      "epoch: 6 step: 713, loss is 0.03821263834834099\n",
      "epoch: 6 step: 714, loss is 0.011063200421631336\n",
      "epoch: 6 step: 715, loss is 0.022907469421625137\n",
      "epoch: 6 step: 716, loss is 0.007938239723443985\n",
      "epoch: 6 step: 717, loss is 0.00031423397012986243\n",
      "epoch: 6 step: 718, loss is 0.0030062426812946796\n",
      "epoch: 6 step: 719, loss is 0.018862834200263023\n",
      "epoch: 6 step: 720, loss is 0.03591261804103851\n",
      "epoch: 6 step: 721, loss is 0.01036139391362667\n",
      "epoch: 6 step: 722, loss is 0.006397091317921877\n",
      "epoch: 6 step: 723, loss is 0.1631612926721573\n",
      "epoch: 6 step: 724, loss is 0.11614630371332169\n",
      "epoch: 6 step: 725, loss is 0.03055831603705883\n",
      "epoch: 6 step: 726, loss is 0.007640829309821129\n",
      "epoch: 6 step: 727, loss is 0.1072394922375679\n",
      "epoch: 6 step: 728, loss is 0.23759184777736664\n",
      "epoch: 6 step: 729, loss is 0.009236969985067844\n",
      "epoch: 6 step: 730, loss is 0.008870960213243961\n",
      "epoch: 6 step: 731, loss is 0.005910592153668404\n",
      "epoch: 6 step: 732, loss is 0.0015877141850069165\n",
      "epoch: 6 step: 733, loss is 0.014078585430979729\n",
      "epoch: 6 step: 734, loss is 0.0007033285219222307\n",
      "epoch: 6 step: 735, loss is 0.3014075756072998\n",
      "epoch: 6 step: 736, loss is 0.022580336779356003\n",
      "epoch: 6 step: 737, loss is 0.16042481362819672\n",
      "epoch: 6 step: 738, loss is 0.0024600487668067217\n",
      "epoch: 6 step: 739, loss is 0.019061503931879997\n",
      "epoch: 6 step: 740, loss is 0.08094154298305511\n",
      "epoch: 6 step: 741, loss is 0.00904140155762434\n",
      "epoch: 6 step: 742, loss is 0.0003431937366258353\n",
      "epoch: 6 step: 743, loss is 0.012571724131703377\n",
      "epoch: 6 step: 744, loss is 0.015898453071713448\n",
      "epoch: 6 step: 745, loss is 0.050480715930461884\n",
      "epoch: 6 step: 746, loss is 0.015448887832462788\n",
      "epoch: 6 step: 747, loss is 0.009851839393377304\n",
      "epoch: 6 step: 748, loss is 0.004464940633624792\n",
      "epoch: 6 step: 749, loss is 0.07863757014274597\n",
      "epoch: 6 step: 750, loss is 0.004168866667896509\n",
      "epoch: 6 step: 751, loss is 0.10163243860006332\n",
      "epoch: 6 step: 752, loss is 0.004278602544218302\n",
      "epoch: 6 step: 753, loss is 0.04571157321333885\n",
      "epoch: 6 step: 754, loss is 0.1373370885848999\n",
      "epoch: 6 step: 755, loss is 0.006329172290861607\n",
      "epoch: 6 step: 756, loss is 0.00604249956086278\n",
      "epoch: 6 step: 757, loss is 0.043762750923633575\n",
      "epoch: 6 step: 758, loss is 0.04607553035020828\n",
      "epoch: 6 step: 759, loss is 0.03522511199116707\n",
      "epoch: 6 step: 760, loss is 0.017789341509342194\n",
      "epoch: 6 step: 761, loss is 0.0022207964211702347\n",
      "epoch: 6 step: 762, loss is 0.0341491773724556\n",
      "epoch: 6 step: 763, loss is 0.02649279125034809\n",
      "epoch: 6 step: 764, loss is 0.0036872404161840677\n",
      "epoch: 6 step: 765, loss is 0.025895962491631508\n",
      "epoch: 6 step: 766, loss is 0.007405768148601055\n",
      "epoch: 6 step: 767, loss is 0.0712902694940567\n",
      "epoch: 6 step: 768, loss is 0.03184300288558006\n",
      "epoch: 6 step: 769, loss is 0.06603710353374481\n",
      "epoch: 6 step: 770, loss is 0.04290425404906273\n",
      "epoch: 6 step: 771, loss is 0.03691136837005615\n",
      "epoch: 6 step: 772, loss is 0.017073435708880424\n",
      "epoch: 6 step: 773, loss is 0.023401755839586258\n",
      "epoch: 6 step: 774, loss is 0.16394269466400146\n",
      "epoch: 6 step: 775, loss is 0.011368520557880402\n",
      "epoch: 6 step: 776, loss is 0.013774562627077103\n",
      "epoch: 6 step: 777, loss is 0.010785859078168869\n",
      "epoch: 6 step: 778, loss is 0.2546353042125702\n",
      "epoch: 6 step: 779, loss is 0.003202434629201889\n",
      "epoch: 6 step: 780, loss is 0.0983002558350563\n",
      "epoch: 6 step: 781, loss is 0.004486473277211189\n",
      "epoch: 6 step: 782, loss is 0.004144884180277586\n",
      "epoch: 6 step: 783, loss is 0.020411357283592224\n",
      "epoch: 6 step: 784, loss is 0.10402542352676392\n",
      "epoch: 6 step: 785, loss is 0.09050556272268295\n",
      "epoch: 6 step: 786, loss is 0.003196490230038762\n",
      "epoch: 6 step: 787, loss is 0.13865803182125092\n",
      "epoch: 6 step: 788, loss is 0.1594943106174469\n",
      "epoch: 6 step: 789, loss is 0.024980872869491577\n",
      "epoch: 6 step: 790, loss is 0.0035978597588837147\n",
      "epoch: 6 step: 791, loss is 0.14665457606315613\n",
      "epoch: 6 step: 792, loss is 0.02013722062110901\n",
      "epoch: 6 step: 793, loss is 0.21473664045333862\n",
      "epoch: 6 step: 794, loss is 0.2283637672662735\n",
      "epoch: 6 step: 795, loss is 0.12362056225538254\n",
      "epoch: 6 step: 796, loss is 0.08637937158346176\n",
      "epoch: 6 step: 797, loss is 0.02125278115272522\n",
      "epoch: 6 step: 798, loss is 0.12166420370340347\n",
      "epoch: 6 step: 799, loss is 0.02226886712014675\n",
      "epoch: 6 step: 800, loss is 0.033925820142030716\n",
      "epoch: 6 step: 801, loss is 0.0029712338000535965\n",
      "epoch: 6 step: 802, loss is 0.0219836737960577\n",
      "epoch: 6 step: 803, loss is 0.021296996623277664\n",
      "epoch: 6 step: 804, loss is 0.03173812851309776\n",
      "epoch: 6 step: 805, loss is 0.0014357749605551362\n",
      "epoch: 6 step: 806, loss is 0.13126760721206665\n",
      "epoch: 6 step: 807, loss is 0.23372383415699005\n",
      "epoch: 6 step: 808, loss is 0.01665320247411728\n",
      "epoch: 6 step: 809, loss is 0.05977868288755417\n",
      "epoch: 6 step: 810, loss is 0.0035264042671769857\n",
      "epoch: 6 step: 811, loss is 0.004279328044503927\n",
      "epoch: 6 step: 812, loss is 0.0038289562799036503\n",
      "epoch: 6 step: 813, loss is 0.09636439383029938\n",
      "epoch: 6 step: 814, loss is 0.01822647824883461\n",
      "epoch: 6 step: 815, loss is 0.039555031806230545\n",
      "epoch: 6 step: 816, loss is 0.00201103906147182\n",
      "epoch: 6 step: 817, loss is 0.000537724990863353\n",
      "epoch: 6 step: 818, loss is 0.00636713532730937\n",
      "epoch: 6 step: 819, loss is 0.0011607143096625805\n",
      "epoch: 6 step: 820, loss is 0.06254775822162628\n",
      "epoch: 6 step: 821, loss is 0.005795818287879229\n",
      "epoch: 6 step: 822, loss is 0.005931430496275425\n",
      "epoch: 6 step: 823, loss is 0.09782768040895462\n",
      "epoch: 6 step: 824, loss is 0.06330526620149612\n",
      "epoch: 6 step: 825, loss is 0.03845231980085373\n",
      "epoch: 6 step: 826, loss is 0.018636509776115417\n",
      "epoch: 6 step: 827, loss is 0.007408049423247576\n",
      "epoch: 6 step: 828, loss is 0.025471875444054604\n",
      "epoch: 6 step: 829, loss is 0.17392979562282562\n",
      "epoch: 6 step: 830, loss is 0.009325827471911907\n",
      "epoch: 6 step: 831, loss is 0.11011645942926407\n",
      "epoch: 6 step: 832, loss is 0.019134413450956345\n",
      "epoch: 6 step: 833, loss is 0.014476481825113297\n",
      "epoch: 6 step: 834, loss is 0.025657735764980316\n",
      "epoch: 6 step: 835, loss is 0.05067862197756767\n",
      "epoch: 6 step: 836, loss is 0.009926467202603817\n",
      "epoch: 6 step: 837, loss is 0.04662211984395981\n",
      "epoch: 6 step: 838, loss is 0.006894922349601984\n",
      "epoch: 6 step: 839, loss is 0.0007467526593245566\n",
      "epoch: 6 step: 840, loss is 0.05067875608801842\n",
      "epoch: 6 step: 841, loss is 0.014953398145735264\n",
      "epoch: 6 step: 842, loss is 0.0017065992578864098\n",
      "epoch: 6 step: 843, loss is 0.010563316754996777\n",
      "epoch: 6 step: 844, loss is 0.011661605909466743\n",
      "epoch: 6 step: 845, loss is 0.0023586340248584747\n",
      "epoch: 6 step: 846, loss is 0.0036099934950470924\n",
      "epoch: 6 step: 847, loss is 0.011016198433935642\n",
      "epoch: 6 step: 848, loss is 0.030942613258957863\n",
      "epoch: 6 step: 849, loss is 0.0012153154239058495\n",
      "epoch: 6 step: 850, loss is 0.013159375637769699\n",
      "epoch: 6 step: 851, loss is 0.027126070111989975\n",
      "epoch: 6 step: 852, loss is 0.07156719267368317\n",
      "epoch: 6 step: 853, loss is 0.0021888664923608303\n",
      "epoch: 6 step: 854, loss is 0.0051679243333637714\n",
      "epoch: 6 step: 855, loss is 0.030810542404651642\n",
      "epoch: 6 step: 856, loss is 0.02229904942214489\n",
      "epoch: 6 step: 857, loss is 0.07344631850719452\n",
      "epoch: 6 step: 858, loss is 0.08304474502801895\n",
      "epoch: 6 step: 859, loss is 0.0023432443849742413\n",
      "epoch: 6 step: 860, loss is 0.0009346282458864152\n",
      "epoch: 6 step: 861, loss is 0.01613532565534115\n",
      "epoch: 6 step: 862, loss is 0.052507754415273666\n",
      "epoch: 6 step: 863, loss is 0.014531128108501434\n",
      "epoch: 6 step: 864, loss is 0.02581176906824112\n",
      "epoch: 6 step: 865, loss is 0.012098378501832485\n",
      "epoch: 6 step: 866, loss is 0.0916205570101738\n",
      "epoch: 6 step: 867, loss is 0.026410646736621857\n",
      "epoch: 6 step: 868, loss is 0.017120089381933212\n",
      "epoch: 6 step: 869, loss is 0.025144271552562714\n",
      "epoch: 6 step: 870, loss is 0.012883566319942474\n",
      "epoch: 6 step: 871, loss is 0.03960851952433586\n",
      "epoch: 6 step: 872, loss is 0.0047905342653393745\n",
      "epoch: 6 step: 873, loss is 0.0014195782132446766\n",
      "epoch: 6 step: 874, loss is 0.11019454896450043\n",
      "epoch: 6 step: 875, loss is 0.02146083116531372\n",
      "epoch: 6 step: 876, loss is 0.030161995440721512\n",
      "epoch: 6 step: 877, loss is 0.00809781439602375\n",
      "epoch: 6 step: 878, loss is 0.15525853633880615\n",
      "epoch: 6 step: 879, loss is 0.0012712075840681791\n",
      "epoch: 6 step: 880, loss is 0.006837229244410992\n",
      "epoch: 6 step: 881, loss is 0.08647316694259644\n",
      "epoch: 6 step: 882, loss is 0.021004756912589073\n",
      "epoch: 6 step: 883, loss is 0.0011820074869319797\n",
      "epoch: 6 step: 884, loss is 0.0006234981119632721\n",
      "epoch: 6 step: 885, loss is 0.008597156032919884\n",
      "epoch: 6 step: 886, loss is 0.02751314640045166\n",
      "epoch: 6 step: 887, loss is 0.15188315510749817\n",
      "epoch: 6 step: 888, loss is 0.003387227887287736\n",
      "epoch: 6 step: 889, loss is 0.011613999493420124\n",
      "epoch: 6 step: 890, loss is 0.05701242387294769\n",
      "epoch: 6 step: 891, loss is 0.04495663568377495\n",
      "epoch: 6 step: 892, loss is 0.22223350405693054\n",
      "epoch: 6 step: 893, loss is 0.02082192525267601\n",
      "epoch: 6 step: 894, loss is 0.018538638949394226\n",
      "epoch: 6 step: 895, loss is 0.0027140509337186813\n",
      "epoch: 6 step: 896, loss is 0.043829575181007385\n",
      "epoch: 6 step: 897, loss is 0.13865888118743896\n",
      "epoch: 6 step: 898, loss is 0.12083867192268372\n",
      "epoch: 6 step: 899, loss is 0.08557651191949844\n",
      "epoch: 6 step: 900, loss is 0.010808063670992851\n",
      "epoch: 6 step: 901, loss is 0.008847919292747974\n",
      "epoch: 6 step: 902, loss is 0.06637492775917053\n",
      "epoch: 6 step: 903, loss is 0.16759631037712097\n",
      "epoch: 6 step: 904, loss is 0.004561530891805887\n",
      "epoch: 6 step: 905, loss is 0.15506824851036072\n",
      "epoch: 6 step: 906, loss is 0.008277058601379395\n",
      "epoch: 6 step: 907, loss is 0.004306198563426733\n",
      "epoch: 6 step: 908, loss is 0.06536707282066345\n",
      "epoch: 6 step: 909, loss is 0.11629016697406769\n",
      "epoch: 6 step: 910, loss is 0.004222653340548277\n",
      "epoch: 6 step: 911, loss is 0.07764576375484467\n",
      "epoch: 6 step: 912, loss is 0.03673911839723587\n",
      "epoch: 6 step: 913, loss is 0.007316553499549627\n",
      "epoch: 6 step: 914, loss is 0.2397337555885315\n",
      "epoch: 6 step: 915, loss is 0.0409424789249897\n",
      "epoch: 6 step: 916, loss is 0.014664119109511375\n",
      "epoch: 6 step: 917, loss is 0.04686056822538376\n",
      "epoch: 6 step: 918, loss is 0.009652238339185715\n",
      "epoch: 6 step: 919, loss is 0.011637132614850998\n",
      "epoch: 6 step: 920, loss is 0.1219879686832428\n",
      "epoch: 6 step: 921, loss is 0.0066017392091453075\n",
      "epoch: 6 step: 922, loss is 0.03557621315121651\n",
      "epoch: 6 step: 923, loss is 0.012999458238482475\n",
      "epoch: 6 step: 924, loss is 0.031136581674218178\n",
      "epoch: 6 step: 925, loss is 0.057081639766693115\n",
      "epoch: 6 step: 926, loss is 0.03997752070426941\n",
      "epoch: 6 step: 927, loss is 0.020133858546614647\n",
      "epoch: 6 step: 928, loss is 0.026792598888278008\n",
      "epoch: 6 step: 929, loss is 0.02253234200179577\n",
      "epoch: 6 step: 930, loss is 0.0014290619874373078\n",
      "epoch: 6 step: 931, loss is 0.009374681860208511\n",
      "epoch: 6 step: 932, loss is 0.05467402562499046\n",
      "epoch: 6 step: 933, loss is 0.07732176035642624\n",
      "epoch: 6 step: 934, loss is 0.0030397074297070503\n",
      "epoch: 6 step: 935, loss is 0.016547106206417084\n",
      "epoch: 6 step: 936, loss is 0.017007920891046524\n",
      "epoch: 6 step: 937, loss is 0.010213322006165981\n",
      "epoch: 6 step: 938, loss is 0.002307669259607792\n",
      "epoch: 6 step: 939, loss is 0.00044953328324481845\n",
      "epoch: 6 step: 940, loss is 0.012902965769171715\n",
      "epoch: 6 step: 941, loss is 0.09007775038480759\n",
      "epoch: 6 step: 942, loss is 0.013908599503338337\n",
      "epoch: 6 step: 943, loss is 0.02493955008685589\n",
      "epoch: 6 step: 944, loss is 0.0018228242406621575\n",
      "epoch: 6 step: 945, loss is 0.031188858672976494\n",
      "epoch: 6 step: 946, loss is 0.0670260637998581\n",
      "epoch: 6 step: 947, loss is 0.01662699691951275\n",
      "epoch: 6 step: 948, loss is 0.007569797337055206\n",
      "epoch: 6 step: 949, loss is 0.01766122691333294\n",
      "epoch: 6 step: 950, loss is 0.010393993929028511\n",
      "epoch: 6 step: 951, loss is 0.0028977426700294018\n",
      "epoch: 6 step: 952, loss is 0.2500145733356476\n",
      "epoch: 6 step: 953, loss is 0.0013757767155766487\n",
      "epoch: 6 step: 954, loss is 0.0005873398622497916\n",
      "epoch: 6 step: 955, loss is 0.03738163411617279\n",
      "epoch: 6 step: 956, loss is 0.1967111974954605\n",
      "epoch: 6 step: 957, loss is 0.011476361192762852\n",
      "epoch: 6 step: 958, loss is 0.000435567315435037\n",
      "epoch: 6 step: 959, loss is 0.008826074190437794\n",
      "epoch: 6 step: 960, loss is 0.004879883956164122\n",
      "epoch: 6 step: 961, loss is 0.002809180412441492\n",
      "epoch: 6 step: 962, loss is 0.04542858898639679\n",
      "epoch: 6 step: 963, loss is 0.011277709156274796\n",
      "epoch: 6 step: 964, loss is 0.018328778445720673\n",
      "epoch: 6 step: 965, loss is 0.036532677710056305\n",
      "epoch: 6 step: 966, loss is 0.11361358314752579\n",
      "epoch: 6 step: 967, loss is 0.009906616061925888\n",
      "epoch: 6 step: 968, loss is 0.09070949256420135\n",
      "epoch: 6 step: 969, loss is 0.13619032502174377\n",
      "epoch: 6 step: 970, loss is 0.022891156375408173\n",
      "epoch: 6 step: 971, loss is 0.03740791603922844\n",
      "epoch: 6 step: 972, loss is 0.08284524083137512\n",
      "epoch: 6 step: 973, loss is 0.027694426476955414\n",
      "epoch: 6 step: 974, loss is 0.0016440900508314371\n",
      "epoch: 6 step: 975, loss is 0.17600314319133759\n",
      "epoch: 6 step: 976, loss is 0.1365126371383667\n",
      "epoch: 6 step: 977, loss is 0.23147696256637573\n",
      "epoch: 6 step: 978, loss is 0.04310234636068344\n",
      "epoch: 6 step: 979, loss is 0.08386927098035812\n",
      "epoch: 6 step: 980, loss is 0.009311280213296413\n",
      "epoch: 6 step: 981, loss is 0.032276835292577744\n",
      "epoch: 6 step: 982, loss is 0.004190947860479355\n",
      "epoch: 6 step: 983, loss is 0.03636802360415459\n",
      "epoch: 6 step: 984, loss is 0.0006376206874847412\n",
      "epoch: 6 step: 985, loss is 0.02870957925915718\n",
      "epoch: 6 step: 986, loss is 0.02414698339998722\n",
      "epoch: 6 step: 987, loss is 0.013923512771725655\n",
      "epoch: 6 step: 988, loss is 0.02654249407351017\n",
      "epoch: 6 step: 989, loss is 0.013710498809814453\n",
      "epoch: 6 step: 990, loss is 0.006875813473016024\n",
      "epoch: 6 step: 991, loss is 0.011922871693968773\n",
      "epoch: 6 step: 992, loss is 0.041309699416160583\n",
      "epoch: 6 step: 993, loss is 0.06228146329522133\n",
      "epoch: 6 step: 994, loss is 0.04221651330590248\n",
      "epoch: 6 step: 995, loss is 0.04255058243870735\n",
      "epoch: 6 step: 996, loss is 0.0039191399700939655\n",
      "epoch: 6 step: 997, loss is 0.1631929874420166\n",
      "epoch: 6 step: 998, loss is 0.014260665513575077\n",
      "epoch: 6 step: 999, loss is 0.0018799093086272478\n",
      "epoch: 6 step: 1000, loss is 0.1351640224456787\n",
      "epoch: 6 step: 1001, loss is 0.028633100911974907\n",
      "epoch: 6 step: 1002, loss is 0.010700086131691933\n",
      "epoch: 6 step: 1003, loss is 0.020593389868736267\n",
      "epoch: 6 step: 1004, loss is 0.21211481094360352\n",
      "epoch: 6 step: 1005, loss is 0.0008174488320946693\n",
      "epoch: 6 step: 1006, loss is 0.05929826572537422\n",
      "epoch: 6 step: 1007, loss is 0.03448222205042839\n",
      "epoch: 6 step: 1008, loss is 0.06425401568412781\n",
      "epoch: 6 step: 1009, loss is 0.059895068407058716\n",
      "epoch: 6 step: 1010, loss is 0.04208670184016228\n",
      "epoch: 6 step: 1011, loss is 0.04023702070116997\n",
      "epoch: 6 step: 1012, loss is 0.0006113462150096893\n",
      "epoch: 6 step: 1013, loss is 0.04012950137257576\n",
      "epoch: 6 step: 1014, loss is 8.324436203110963e-05\n",
      "epoch: 6 step: 1015, loss is 0.02923947386443615\n",
      "epoch: 6 step: 1016, loss is 0.01525815948843956\n",
      "epoch: 6 step: 1017, loss is 0.019933532923460007\n",
      "epoch: 6 step: 1018, loss is 0.00043842900777235627\n",
      "epoch: 6 step: 1019, loss is 0.0747554823756218\n",
      "epoch: 6 step: 1020, loss is 0.006508203223347664\n",
      "epoch: 6 step: 1021, loss is 0.006470984313637018\n",
      "epoch: 6 step: 1022, loss is 0.1421068161725998\n",
      "epoch: 6 step: 1023, loss is 0.10439839959144592\n",
      "epoch: 6 step: 1024, loss is 0.07096780091524124\n",
      "epoch: 6 step: 1025, loss is 0.012632882222533226\n",
      "epoch: 6 step: 1026, loss is 0.05241309478878975\n",
      "epoch: 6 step: 1027, loss is 0.05187372863292694\n",
      "epoch: 6 step: 1028, loss is 0.09240004420280457\n",
      "epoch: 6 step: 1029, loss is 0.02552461251616478\n",
      "epoch: 6 step: 1030, loss is 0.016905903816223145\n",
      "epoch: 6 step: 1031, loss is 0.023427758365869522\n",
      "epoch: 6 step: 1032, loss is 0.010326337069272995\n",
      "epoch: 6 step: 1033, loss is 0.07613670825958252\n",
      "epoch: 6 step: 1034, loss is 0.11406712979078293\n",
      "epoch: 6 step: 1035, loss is 0.03707095980644226\n",
      "epoch: 6 step: 1036, loss is 0.01682564616203308\n",
      "epoch: 6 step: 1037, loss is 0.04537693038582802\n",
      "epoch: 6 step: 1038, loss is 0.06435482203960419\n",
      "epoch: 6 step: 1039, loss is 0.019412875175476074\n",
      "epoch: 6 step: 1040, loss is 0.021392667666077614\n",
      "epoch: 6 step: 1041, loss is 0.01537102460861206\n",
      "epoch: 6 step: 1042, loss is 0.015019120648503304\n",
      "epoch: 6 step: 1043, loss is 0.029048094525933266\n",
      "epoch: 6 step: 1044, loss is 0.01853666640818119\n",
      "epoch: 6 step: 1045, loss is 0.02905385009944439\n",
      "epoch: 6 step: 1046, loss is 0.04616720229387283\n",
      "epoch: 6 step: 1047, loss is 0.028617121279239655\n",
      "epoch: 6 step: 1048, loss is 0.001131137483753264\n",
      "epoch: 6 step: 1049, loss is 0.007212105672806501\n",
      "epoch: 6 step: 1050, loss is 0.018012110143899918\n",
      "epoch: 6 step: 1051, loss is 0.0007737091509625316\n",
      "epoch: 6 step: 1052, loss is 0.001083094161003828\n",
      "epoch: 6 step: 1053, loss is 0.0055964188650250435\n",
      "epoch: 6 step: 1054, loss is 0.04934649169445038\n",
      "epoch: 6 step: 1055, loss is 0.027009818702936172\n",
      "epoch: 6 step: 1056, loss is 0.00021042395383119583\n",
      "epoch: 6 step: 1057, loss is 0.027942020446062088\n",
      "epoch: 6 step: 1058, loss is 0.10582426935434341\n",
      "epoch: 6 step: 1059, loss is 0.003624292090535164\n",
      "epoch: 6 step: 1060, loss is 0.016716541722416878\n",
      "epoch: 6 step: 1061, loss is 0.009287719614803791\n",
      "epoch: 6 step: 1062, loss is 0.0176404919475317\n",
      "epoch: 6 step: 1063, loss is 0.04118705540895462\n",
      "epoch: 6 step: 1064, loss is 0.03045753389596939\n",
      "epoch: 6 step: 1065, loss is 0.4846040904521942\n",
      "epoch: 6 step: 1066, loss is 0.027703896164894104\n",
      "epoch: 6 step: 1067, loss is 0.020709743723273277\n",
      "epoch: 6 step: 1068, loss is 0.0035486528649926186\n",
      "epoch: 6 step: 1069, loss is 0.04688284918665886\n",
      "epoch: 6 step: 1070, loss is 0.010835612192749977\n",
      "epoch: 6 step: 1071, loss is 0.017664575949311256\n",
      "epoch: 6 step: 1072, loss is 0.005526241380721331\n",
      "epoch: 6 step: 1073, loss is 0.10931292176246643\n",
      "epoch: 6 step: 1074, loss is 0.07628001272678375\n",
      "epoch: 6 step: 1075, loss is 0.20126090943813324\n",
      "epoch: 6 step: 1076, loss is 0.002941027283668518\n",
      "epoch: 6 step: 1077, loss is 0.005415880121290684\n",
      "epoch: 6 step: 1078, loss is 0.006538589484989643\n",
      "epoch: 6 step: 1079, loss is 0.046352602541446686\n",
      "epoch: 6 step: 1080, loss is 0.04378676787018776\n",
      "epoch: 6 step: 1081, loss is 0.06093255430459976\n",
      "epoch: 6 step: 1082, loss is 0.06384612619876862\n",
      "epoch: 6 step: 1083, loss is 0.03263888508081436\n",
      "epoch: 6 step: 1084, loss is 0.003767502261325717\n",
      "epoch: 6 step: 1085, loss is 0.003130456665530801\n",
      "epoch: 6 step: 1086, loss is 0.0417812317609787\n",
      "epoch: 6 step: 1087, loss is 0.006305386777967215\n",
      "epoch: 6 step: 1088, loss is 0.021252531558275223\n",
      "epoch: 6 step: 1089, loss is 0.028693633154034615\n",
      "epoch: 6 step: 1090, loss is 0.5232036113739014\n",
      "epoch: 6 step: 1091, loss is 0.14254365861415863\n",
      "epoch: 6 step: 1092, loss is 0.008900404907763004\n",
      "epoch: 6 step: 1093, loss is 0.15050694346427917\n",
      "epoch: 6 step: 1094, loss is 0.2416384518146515\n",
      "epoch: 6 step: 1095, loss is 0.0032600085251033306\n",
      "epoch: 6 step: 1096, loss is 0.02457709237933159\n",
      "epoch: 6 step: 1097, loss is 0.17062896490097046\n",
      "epoch: 6 step: 1098, loss is 0.00971755851060152\n",
      "epoch: 6 step: 1099, loss is 0.005143939051777124\n",
      "epoch: 6 step: 1100, loss is 0.01756267063319683\n",
      "epoch: 6 step: 1101, loss is 0.002506714081391692\n",
      "epoch: 6 step: 1102, loss is 0.004687681794166565\n",
      "epoch: 6 step: 1103, loss is 0.04836786165833473\n",
      "epoch: 6 step: 1104, loss is 0.1260654479265213\n",
      "epoch: 6 step: 1105, loss is 0.009867223910987377\n",
      "epoch: 6 step: 1106, loss is 0.07791955769062042\n",
      "epoch: 6 step: 1107, loss is 0.07518307119607925\n",
      "epoch: 6 step: 1108, loss is 0.06063678115606308\n",
      "epoch: 6 step: 1109, loss is 0.03839213401079178\n",
      "epoch: 6 step: 1110, loss is 0.007611870765686035\n",
      "epoch: 6 step: 1111, loss is 0.006218850612640381\n",
      "epoch: 6 step: 1112, loss is 0.13674525916576385\n",
      "epoch: 6 step: 1113, loss is 0.011269224807620049\n",
      "epoch: 6 step: 1114, loss is 0.15098892152309418\n",
      "epoch: 6 step: 1115, loss is 0.006024603266268969\n",
      "epoch: 6 step: 1116, loss is 0.010684587992727757\n",
      "epoch: 6 step: 1117, loss is 0.008681430481374264\n",
      "epoch: 6 step: 1118, loss is 0.004057597368955612\n",
      "epoch: 6 step: 1119, loss is 0.15392886102199554\n",
      "epoch: 6 step: 1120, loss is 0.11399845033884048\n",
      "epoch: 6 step: 1121, loss is 0.028336357325315475\n",
      "epoch: 6 step: 1122, loss is 0.0028604273684322834\n",
      "epoch: 6 step: 1123, loss is 0.060681410133838654\n",
      "epoch: 6 step: 1124, loss is 0.00816709827631712\n",
      "epoch: 6 step: 1125, loss is 0.005358744412660599\n",
      "epoch: 6 step: 1126, loss is 0.01167958602309227\n",
      "epoch: 6 step: 1127, loss is 0.1530054807662964\n",
      "epoch: 6 step: 1128, loss is 0.04421009495854378\n",
      "epoch: 6 step: 1129, loss is 0.14493903517723083\n",
      "epoch: 6 step: 1130, loss is 0.031138950958848\n",
      "epoch: 6 step: 1131, loss is 0.17227241396903992\n",
      "epoch: 6 step: 1132, loss is 0.0049309274181723595\n",
      "epoch: 6 step: 1133, loss is 0.02056339755654335\n",
      "epoch: 6 step: 1134, loss is 0.0861680880188942\n",
      "epoch: 6 step: 1135, loss is 0.008181266486644745\n",
      "epoch: 6 step: 1136, loss is 0.008715343661606312\n",
      "epoch: 6 step: 1137, loss is 0.12956364452838898\n",
      "epoch: 6 step: 1138, loss is 0.023681847378611565\n",
      "epoch: 6 step: 1139, loss is 0.05611639469861984\n",
      "epoch: 6 step: 1140, loss is 0.002778915921226144\n",
      "epoch: 6 step: 1141, loss is 0.02646060287952423\n",
      "epoch: 6 step: 1142, loss is 0.09113208204507828\n",
      "epoch: 6 step: 1143, loss is 0.03184118866920471\n",
      "epoch: 6 step: 1144, loss is 0.0018965360941365361\n",
      "epoch: 6 step: 1145, loss is 0.0066619450226426125\n",
      "epoch: 6 step: 1146, loss is 0.035509735345840454\n",
      "epoch: 6 step: 1147, loss is 0.00018380067194812\n",
      "epoch: 6 step: 1148, loss is 0.007166482508182526\n",
      "epoch: 6 step: 1149, loss is 0.02112644538283348\n",
      "epoch: 6 step: 1150, loss is 0.019692249596118927\n",
      "epoch: 6 step: 1151, loss is 0.03945751115679741\n",
      "epoch: 6 step: 1152, loss is 0.19393695890903473\n",
      "epoch: 6 step: 1153, loss is 0.0869680866599083\n",
      "epoch: 6 step: 1154, loss is 0.004778478294610977\n",
      "epoch: 6 step: 1155, loss is 0.04821939766407013\n",
      "epoch: 6 step: 1156, loss is 0.023204296827316284\n",
      "epoch: 6 step: 1157, loss is 0.00161771010607481\n",
      "epoch: 6 step: 1158, loss is 0.25564447045326233\n",
      "epoch: 6 step: 1159, loss is 0.0884394571185112\n",
      "epoch: 6 step: 1160, loss is 0.012691173702478409\n",
      "epoch: 6 step: 1161, loss is 0.05071588233113289\n",
      "epoch: 6 step: 1162, loss is 0.02842237986624241\n",
      "epoch: 6 step: 1163, loss is 0.12906844913959503\n",
      "epoch: 6 step: 1164, loss is 0.005411135498434305\n",
      "epoch: 6 step: 1165, loss is 0.019600795581936836\n",
      "epoch: 6 step: 1166, loss is 0.06153317913413048\n",
      "epoch: 6 step: 1167, loss is 0.0046874722465872765\n",
      "epoch: 6 step: 1168, loss is 0.01028358843177557\n",
      "epoch: 6 step: 1169, loss is 0.06890829652547836\n",
      "epoch: 6 step: 1170, loss is 0.09499156475067139\n",
      "epoch: 6 step: 1171, loss is 0.06951514631509781\n",
      "epoch: 6 step: 1172, loss is 0.0371001660823822\n",
      "epoch: 6 step: 1173, loss is 0.008640987798571587\n",
      "epoch: 6 step: 1174, loss is 0.0026478737127035856\n",
      "epoch: 6 step: 1175, loss is 0.019803987815976143\n",
      "epoch: 6 step: 1176, loss is 0.021206490695476532\n",
      "epoch: 6 step: 1177, loss is 0.1346353441476822\n",
      "epoch: 6 step: 1178, loss is 0.04482075944542885\n",
      "epoch: 6 step: 1179, loss is 0.003080652328208089\n",
      "epoch: 6 step: 1180, loss is 0.01988752745091915\n",
      "epoch: 6 step: 1181, loss is 0.006532181054353714\n",
      "epoch: 6 step: 1182, loss is 0.3222426772117615\n",
      "epoch: 6 step: 1183, loss is 0.000873624230735004\n",
      "epoch: 6 step: 1184, loss is 0.0030600472819060087\n",
      "epoch: 6 step: 1185, loss is 0.023746902123093605\n",
      "epoch: 6 step: 1186, loss is 0.012617280706763268\n",
      "epoch: 6 step: 1187, loss is 0.006367138586938381\n",
      "epoch: 6 step: 1188, loss is 0.010179571807384491\n",
      "epoch: 6 step: 1189, loss is 0.12990260124206543\n",
      "epoch: 6 step: 1190, loss is 0.13227900862693787\n",
      "epoch: 6 step: 1191, loss is 0.0030551853124052286\n",
      "epoch: 6 step: 1192, loss is 0.05297417938709259\n",
      "epoch: 6 step: 1193, loss is 0.0019018942257389426\n",
      "epoch: 6 step: 1194, loss is 0.011636736802756786\n",
      "epoch: 6 step: 1195, loss is 0.06757036596536636\n",
      "epoch: 6 step: 1196, loss is 0.0051957424730062485\n",
      "epoch: 6 step: 1197, loss is 0.0016124179819598794\n",
      "epoch: 6 step: 1198, loss is 0.20638695359230042\n",
      "epoch: 6 step: 1199, loss is 0.027093030512332916\n",
      "epoch: 6 step: 1200, loss is 0.15287111699581146\n",
      "epoch: 6 step: 1201, loss is 0.003856577444821596\n",
      "epoch: 6 step: 1202, loss is 0.16001799702644348\n",
      "epoch: 6 step: 1203, loss is 0.12913928925991058\n",
      "epoch: 6 step: 1204, loss is 0.07294945418834686\n",
      "epoch: 6 step: 1205, loss is 0.00890369713306427\n",
      "epoch: 6 step: 1206, loss is 0.004010360687971115\n",
      "epoch: 6 step: 1207, loss is 0.05366666987538338\n",
      "epoch: 6 step: 1208, loss is 0.027996085584163666\n",
      "epoch: 6 step: 1209, loss is 0.013142144307494164\n",
      "epoch: 6 step: 1210, loss is 0.048369064927101135\n",
      "epoch: 6 step: 1211, loss is 0.00504104420542717\n",
      "epoch: 6 step: 1212, loss is 0.0051614888943731785\n",
      "epoch: 6 step: 1213, loss is 0.019105400890111923\n",
      "epoch: 6 step: 1214, loss is 0.13611873984336853\n",
      "epoch: 6 step: 1215, loss is 0.04998297989368439\n",
      "epoch: 6 step: 1216, loss is 0.0010264384327456355\n",
      "epoch: 6 step: 1217, loss is 0.02738257683813572\n",
      "epoch: 6 step: 1218, loss is 0.05297107994556427\n",
      "epoch: 6 step: 1219, loss is 0.009557017125189304\n",
      "epoch: 6 step: 1220, loss is 0.08508142828941345\n",
      "epoch: 6 step: 1221, loss is 0.007578145246952772\n",
      "epoch: 6 step: 1222, loss is 0.009711170569062233\n",
      "epoch: 6 step: 1223, loss is 0.03251801058650017\n",
      "epoch: 6 step: 1224, loss is 0.003888930194079876\n",
      "epoch: 6 step: 1225, loss is 0.009578954428434372\n",
      "epoch: 6 step: 1226, loss is 0.1457529067993164\n",
      "epoch: 6 step: 1227, loss is 0.027395352721214294\n",
      "epoch: 6 step: 1228, loss is 0.06482281535863876\n",
      "epoch: 6 step: 1229, loss is 0.0853106677532196\n",
      "epoch: 6 step: 1230, loss is 0.0164469126611948\n",
      "epoch: 6 step: 1231, loss is 0.006260204594582319\n",
      "epoch: 6 step: 1232, loss is 0.0005542231374420226\n",
      "epoch: 6 step: 1233, loss is 0.001681900699622929\n",
      "epoch: 6 step: 1234, loss is 0.038327138870954514\n",
      "epoch: 6 step: 1235, loss is 0.05458550155162811\n",
      "epoch: 6 step: 1236, loss is 0.03217654302716255\n",
      "epoch: 6 step: 1237, loss is 0.18735025823116302\n",
      "epoch: 6 step: 1238, loss is 0.15512393414974213\n",
      "epoch: 6 step: 1239, loss is 0.00032897619530558586\n",
      "epoch: 6 step: 1240, loss is 0.08549103140830994\n",
      "epoch: 6 step: 1241, loss is 0.002420553471893072\n",
      "epoch: 6 step: 1242, loss is 0.008185611106455326\n",
      "epoch: 6 step: 1243, loss is 0.0789564922451973\n",
      "epoch: 6 step: 1244, loss is 0.07183616608381271\n",
      "epoch: 6 step: 1245, loss is 0.14090849459171295\n",
      "epoch: 6 step: 1246, loss is 0.004778087604790926\n",
      "epoch: 6 step: 1247, loss is 0.015624594874680042\n",
      "epoch: 6 step: 1248, loss is 0.02283109538257122\n",
      "epoch: 6 step: 1249, loss is 0.003850910346955061\n",
      "epoch: 6 step: 1250, loss is 0.017295081168413162\n",
      "epoch: 6 step: 1251, loss is 0.09907983243465424\n",
      "epoch: 6 step: 1252, loss is 0.007818957790732384\n",
      "epoch: 6 step: 1253, loss is 0.012510444968938828\n",
      "epoch: 6 step: 1254, loss is 0.017610864713788033\n",
      "epoch: 6 step: 1255, loss is 0.08463265001773834\n",
      "epoch: 6 step: 1256, loss is 0.1218762993812561\n",
      "epoch: 6 step: 1257, loss is 0.04630851373076439\n",
      "epoch: 6 step: 1258, loss is 0.00876157358288765\n",
      "epoch: 6 step: 1259, loss is 0.008358892984688282\n",
      "epoch: 6 step: 1260, loss is 0.014367112889885902\n",
      "epoch: 6 step: 1261, loss is 0.09657901525497437\n",
      "epoch: 6 step: 1262, loss is 0.0027489964850246906\n",
      "epoch: 6 step: 1263, loss is 0.01266980729997158\n",
      "epoch: 6 step: 1264, loss is 0.1462070643901825\n",
      "epoch: 6 step: 1265, loss is 0.02794903889298439\n",
      "epoch: 6 step: 1266, loss is 0.18420147895812988\n",
      "epoch: 6 step: 1267, loss is 0.002880832413211465\n",
      "epoch: 6 step: 1268, loss is 0.013184590265154839\n",
      "epoch: 6 step: 1269, loss is 0.11021320521831512\n",
      "epoch: 6 step: 1270, loss is 0.10001973062753677\n",
      "epoch: 6 step: 1271, loss is 0.08384133875370026\n",
      "epoch: 6 step: 1272, loss is 0.002104370156303048\n",
      "epoch: 6 step: 1273, loss is 0.028198959305882454\n",
      "epoch: 6 step: 1274, loss is 0.009167073294520378\n",
      "epoch: 6 step: 1275, loss is 0.009624973870813847\n",
      "epoch: 6 step: 1276, loss is 0.09083445370197296\n",
      "epoch: 6 step: 1277, loss is 0.018460584804415703\n",
      "epoch: 6 step: 1278, loss is 0.05015430226922035\n",
      "epoch: 6 step: 1279, loss is 0.039610132575035095\n",
      "epoch: 6 step: 1280, loss is 0.022969717159867287\n",
      "epoch: 6 step: 1281, loss is 0.020569171756505966\n",
      "epoch: 6 step: 1282, loss is 0.021601615473628044\n",
      "epoch: 6 step: 1283, loss is 0.00910189002752304\n",
      "epoch: 6 step: 1284, loss is 0.0033217137679457664\n",
      "epoch: 6 step: 1285, loss is 0.011720240116119385\n",
      "epoch: 6 step: 1286, loss is 0.04937583580613136\n",
      "epoch: 6 step: 1287, loss is 0.011793488636612892\n",
      "epoch: 6 step: 1288, loss is 0.006352393887937069\n",
      "epoch: 6 step: 1289, loss is 0.005816986318677664\n",
      "epoch: 6 step: 1290, loss is 0.05752211809158325\n",
      "epoch: 6 step: 1291, loss is 0.0074744983576238155\n",
      "epoch: 6 step: 1292, loss is 0.007972229272127151\n",
      "epoch: 6 step: 1293, loss is 0.01141294650733471\n",
      "epoch: 6 step: 1294, loss is 0.00395135348662734\n",
      "epoch: 6 step: 1295, loss is 0.007336812559515238\n",
      "epoch: 6 step: 1296, loss is 0.004001402761787176\n",
      "epoch: 6 step: 1297, loss is 0.0024894368834793568\n",
      "epoch: 6 step: 1298, loss is 0.09475822001695633\n",
      "epoch: 6 step: 1299, loss is 0.21581943333148956\n",
      "epoch: 6 step: 1300, loss is 0.019938409328460693\n",
      "epoch: 6 step: 1301, loss is 0.00883994996547699\n",
      "epoch: 6 step: 1302, loss is 0.016919372603297234\n",
      "epoch: 6 step: 1303, loss is 0.42148885130882263\n",
      "epoch: 6 step: 1304, loss is 0.005081322509795427\n",
      "epoch: 6 step: 1305, loss is 0.06854406744241714\n",
      "epoch: 6 step: 1306, loss is 0.000634213094599545\n",
      "epoch: 6 step: 1307, loss is 0.06392931938171387\n",
      "epoch: 6 step: 1308, loss is 0.06269299983978271\n",
      "epoch: 6 step: 1309, loss is 0.00621087895706296\n",
      "epoch: 6 step: 1310, loss is 0.0019668838940560818\n",
      "epoch: 6 step: 1311, loss is 0.006491494830697775\n",
      "epoch: 6 step: 1312, loss is 0.00960735883563757\n",
      "epoch: 6 step: 1313, loss is 0.06853209435939789\n",
      "epoch: 6 step: 1314, loss is 0.03225785121321678\n",
      "epoch: 6 step: 1315, loss is 0.2018849402666092\n",
      "epoch: 6 step: 1316, loss is 0.037303317338228226\n",
      "epoch: 6 step: 1317, loss is 0.10680516064167023\n",
      "epoch: 6 step: 1318, loss is 0.04863027110695839\n",
      "epoch: 6 step: 1319, loss is 0.06661289185285568\n",
      "epoch: 6 step: 1320, loss is 0.061895404011011124\n",
      "epoch: 6 step: 1321, loss is 0.09640225023031235\n",
      "epoch: 6 step: 1322, loss is 0.004673490300774574\n",
      "epoch: 6 step: 1323, loss is 0.094399094581604\n",
      "epoch: 6 step: 1324, loss is 0.09698446094989777\n",
      "epoch: 6 step: 1325, loss is 0.0022026728838682175\n",
      "epoch: 6 step: 1326, loss is 0.03474722430109978\n",
      "epoch: 6 step: 1327, loss is 0.006679742597043514\n",
      "epoch: 6 step: 1328, loss is 0.11514493077993393\n",
      "epoch: 6 step: 1329, loss is 0.0075699687004089355\n",
      "epoch: 6 step: 1330, loss is 0.002474845154210925\n",
      "epoch: 6 step: 1331, loss is 0.041232794523239136\n",
      "epoch: 6 step: 1332, loss is 0.0023986194282770157\n",
      "epoch: 6 step: 1333, loss is 0.06110604852437973\n",
      "epoch: 6 step: 1334, loss is 0.003015556139871478\n",
      "epoch: 6 step: 1335, loss is 0.04933439940214157\n",
      "epoch: 6 step: 1336, loss is 0.08103339374065399\n",
      "epoch: 6 step: 1337, loss is 0.007114610634744167\n",
      "epoch: 6 step: 1338, loss is 0.01631656475365162\n",
      "epoch: 6 step: 1339, loss is 0.03738148510456085\n",
      "epoch: 6 step: 1340, loss is 0.038512345403432846\n",
      "epoch: 6 step: 1341, loss is 0.018893403932452202\n",
      "epoch: 6 step: 1342, loss is 0.00408259266987443\n",
      "epoch: 6 step: 1343, loss is 0.006343986839056015\n",
      "epoch: 6 step: 1344, loss is 0.009602444246411324\n",
      "epoch: 6 step: 1345, loss is 0.01211143471300602\n",
      "epoch: 6 step: 1346, loss is 0.012613464146852493\n",
      "epoch: 6 step: 1347, loss is 0.02069404534995556\n",
      "epoch: 6 step: 1348, loss is 0.026786457747220993\n",
      "epoch: 6 step: 1349, loss is 0.0013769399374723434\n",
      "epoch: 6 step: 1350, loss is 0.004378529731184244\n",
      "epoch: 6 step: 1351, loss is 0.033550750464200974\n",
      "epoch: 6 step: 1352, loss is 0.01208809856325388\n",
      "epoch: 6 step: 1353, loss is 0.008861223235726357\n",
      "epoch: 6 step: 1354, loss is 0.05589050054550171\n",
      "epoch: 6 step: 1355, loss is 0.05369329825043678\n",
      "epoch: 6 step: 1356, loss is 0.017283376306295395\n",
      "epoch: 6 step: 1357, loss is 0.018587861210107803\n",
      "epoch: 6 step: 1358, loss is 0.024636443704366684\n",
      "epoch: 6 step: 1359, loss is 0.007730220444500446\n",
      "epoch: 6 step: 1360, loss is 0.004972469061613083\n",
      "epoch: 6 step: 1361, loss is 0.04219435900449753\n",
      "epoch: 6 step: 1362, loss is 0.002352224662899971\n",
      "epoch: 6 step: 1363, loss is 0.02616056054830551\n",
      "epoch: 6 step: 1364, loss is 0.024570239707827568\n",
      "epoch: 6 step: 1365, loss is 0.1351669579744339\n",
      "epoch: 6 step: 1366, loss is 0.05752060189843178\n",
      "epoch: 6 step: 1367, loss is 0.11786855757236481\n",
      "epoch: 6 step: 1368, loss is 0.06250253319740295\n",
      "epoch: 6 step: 1369, loss is 0.015356975607573986\n",
      "epoch: 6 step: 1370, loss is 0.0044965920969843864\n",
      "epoch: 6 step: 1371, loss is 0.02364434488117695\n",
      "epoch: 6 step: 1372, loss is 0.00578451668843627\n",
      "epoch: 6 step: 1373, loss is 0.009188554249703884\n",
      "epoch: 6 step: 1374, loss is 0.04154597967863083\n",
      "epoch: 6 step: 1375, loss is 0.03434117138385773\n",
      "epoch: 6 step: 1376, loss is 0.034682437777519226\n",
      "epoch: 6 step: 1377, loss is 0.059108369052410126\n",
      "epoch: 6 step: 1378, loss is 0.003454785095527768\n",
      "epoch: 6 step: 1379, loss is 0.010584860108792782\n",
      "epoch: 6 step: 1380, loss is 0.008405627682805061\n",
      "epoch: 6 step: 1381, loss is 0.1463506519794464\n",
      "epoch: 6 step: 1382, loss is 0.05874428525567055\n",
      "epoch: 6 step: 1383, loss is 0.04591866210103035\n",
      "epoch: 6 step: 1384, loss is 0.04333676025271416\n",
      "epoch: 6 step: 1385, loss is 0.0047483607195317745\n",
      "epoch: 6 step: 1386, loss is 0.034636929631233215\n",
      "epoch: 6 step: 1387, loss is 0.1904177963733673\n",
      "epoch: 6 step: 1388, loss is 0.07626346498727798\n",
      "epoch: 6 step: 1389, loss is 0.004664525389671326\n",
      "epoch: 6 step: 1390, loss is 0.27354171872138977\n",
      "epoch: 6 step: 1391, loss is 0.008834884501993656\n",
      "epoch: 6 step: 1392, loss is 0.035314884036779404\n",
      "epoch: 6 step: 1393, loss is 0.08793448656797409\n",
      "epoch: 6 step: 1394, loss is 0.005516574252396822\n",
      "epoch: 6 step: 1395, loss is 0.012742905877530575\n",
      "epoch: 6 step: 1396, loss is 0.0073799267411231995\n",
      "epoch: 6 step: 1397, loss is 0.03210437670350075\n",
      "epoch: 6 step: 1398, loss is 0.005073125008493662\n",
      "epoch: 6 step: 1399, loss is 0.006869341712445021\n",
      "epoch: 6 step: 1400, loss is 0.17350894212722778\n",
      "epoch: 6 step: 1401, loss is 0.01058376207947731\n",
      "epoch: 6 step: 1402, loss is 0.025037474930286407\n",
      "epoch: 6 step: 1403, loss is 0.21390382945537567\n",
      "epoch: 6 step: 1404, loss is 0.008107757195830345\n",
      "epoch: 6 step: 1405, loss is 0.002499820664525032\n",
      "epoch: 6 step: 1406, loss is 0.0032292220275849104\n",
      "epoch: 6 step: 1407, loss is 0.036878813058137894\n",
      "epoch: 6 step: 1408, loss is 0.007275890558958054\n",
      "epoch: 6 step: 1409, loss is 0.0015203850343823433\n",
      "epoch: 6 step: 1410, loss is 0.054958462715148926\n",
      "epoch: 6 step: 1411, loss is 0.012069595977663994\n",
      "epoch: 6 step: 1412, loss is 0.05413474142551422\n",
      "epoch: 6 step: 1413, loss is 0.03433085232973099\n",
      "epoch: 6 step: 1414, loss is 0.023837614804506302\n",
      "epoch: 6 step: 1415, loss is 0.2257203459739685\n",
      "epoch: 6 step: 1416, loss is 0.0019040980841964483\n",
      "epoch: 6 step: 1417, loss is 0.06421370804309845\n",
      "epoch: 6 step: 1418, loss is 0.009996389038860798\n",
      "epoch: 6 step: 1419, loss is 0.025115549564361572\n",
      "epoch: 6 step: 1420, loss is 0.004898954648524523\n",
      "epoch: 6 step: 1421, loss is 0.03146275877952576\n",
      "epoch: 6 step: 1422, loss is 0.004080642014741898\n",
      "epoch: 6 step: 1423, loss is 0.20012757182121277\n",
      "epoch: 6 step: 1424, loss is 0.0008512809872627258\n",
      "epoch: 6 step: 1425, loss is 0.3158091902732849\n",
      "epoch: 6 step: 1426, loss is 0.08490152657032013\n",
      "epoch: 6 step: 1427, loss is 0.009921598248183727\n",
      "epoch: 6 step: 1428, loss is 0.020387165248394012\n",
      "epoch: 6 step: 1429, loss is 0.14193029701709747\n",
      "epoch: 6 step: 1430, loss is 0.003182600950822234\n",
      "epoch: 6 step: 1431, loss is 0.03510211408138275\n",
      "epoch: 6 step: 1432, loss is 0.057912446558475494\n",
      "epoch: 6 step: 1433, loss is 0.041605088859796524\n",
      "epoch: 6 step: 1434, loss is 0.004368864931166172\n",
      "epoch: 6 step: 1435, loss is 0.15600089728832245\n",
      "epoch: 6 step: 1436, loss is 0.08060956746339798\n",
      "epoch: 6 step: 1437, loss is 0.06877325475215912\n",
      "epoch: 6 step: 1438, loss is 0.01055810134857893\n",
      "epoch: 6 step: 1439, loss is 0.014321794733405113\n",
      "epoch: 6 step: 1440, loss is 0.008637264370918274\n",
      "epoch: 6 step: 1441, loss is 0.07109274715185165\n",
      "epoch: 6 step: 1442, loss is 0.003671388141810894\n",
      "epoch: 6 step: 1443, loss is 0.012582083232700825\n",
      "epoch: 6 step: 1444, loss is 0.016861511394381523\n",
      "epoch: 6 step: 1445, loss is 0.0030114336404949427\n",
      "epoch: 6 step: 1446, loss is 0.04216235131025314\n",
      "epoch: 6 step: 1447, loss is 0.004813925363123417\n",
      "epoch: 6 step: 1448, loss is 0.01989809237420559\n",
      "epoch: 6 step: 1449, loss is 0.0873829647898674\n",
      "epoch: 6 step: 1450, loss is 0.021270116791129112\n",
      "epoch: 6 step: 1451, loss is 0.018705595284700394\n",
      "epoch: 6 step: 1452, loss is 0.12903909385204315\n",
      "epoch: 6 step: 1453, loss is 0.170154869556427\n",
      "epoch: 6 step: 1454, loss is 0.06969565153121948\n",
      "epoch: 6 step: 1455, loss is 0.002548809163272381\n",
      "epoch: 6 step: 1456, loss is 0.024911735206842422\n",
      "epoch: 6 step: 1457, loss is 0.10329485684633255\n",
      "epoch: 6 step: 1458, loss is 0.004142705351114273\n",
      "epoch: 6 step: 1459, loss is 0.09463323652744293\n",
      "epoch: 6 step: 1460, loss is 0.014820925891399384\n",
      "epoch: 6 step: 1461, loss is 0.007293362636119127\n",
      "epoch: 6 step: 1462, loss is 0.006781460251659155\n",
      "epoch: 6 step: 1463, loss is 0.004020850174129009\n",
      "epoch: 6 step: 1464, loss is 0.054215241223573685\n",
      "epoch: 6 step: 1465, loss is 0.005510249175131321\n",
      "epoch: 6 step: 1466, loss is 0.31004956364631653\n",
      "epoch: 6 step: 1467, loss is 0.021399391815066338\n",
      "epoch: 6 step: 1468, loss is 0.07808371633291245\n",
      "epoch: 6 step: 1469, loss is 0.07127539068460464\n",
      "epoch: 6 step: 1470, loss is 0.1550627499818802\n",
      "epoch: 6 step: 1471, loss is 0.15284772217273712\n",
      "epoch: 6 step: 1472, loss is 0.0056387996301054955\n",
      "epoch: 6 step: 1473, loss is 0.0029023734387010336\n",
      "epoch: 6 step: 1474, loss is 0.028426162898540497\n",
      "epoch: 6 step: 1475, loss is 0.1081262081861496\n",
      "epoch: 6 step: 1476, loss is 0.006462786812335253\n",
      "epoch: 6 step: 1477, loss is 0.011334971524775028\n",
      "epoch: 6 step: 1478, loss is 0.08936555683612823\n",
      "epoch: 6 step: 1479, loss is 0.004654543939977884\n",
      "epoch: 6 step: 1480, loss is 0.005342000164091587\n",
      "epoch: 6 step: 1481, loss is 0.042443983256816864\n",
      "epoch: 6 step: 1482, loss is 0.03469226509332657\n",
      "epoch: 6 step: 1483, loss is 0.09592553228139877\n",
      "epoch: 6 step: 1484, loss is 0.02431144192814827\n",
      "epoch: 6 step: 1485, loss is 0.11170297116041183\n",
      "epoch: 6 step: 1486, loss is 0.11747287958860397\n",
      "epoch: 6 step: 1487, loss is 0.09605562686920166\n",
      "epoch: 6 step: 1488, loss is 0.0030354501213878393\n",
      "epoch: 6 step: 1489, loss is 0.005040956195443869\n",
      "epoch: 6 step: 1490, loss is 0.06591945886611938\n",
      "epoch: 6 step: 1491, loss is 0.01570555754005909\n",
      "epoch: 6 step: 1492, loss is 0.0037121125496923923\n",
      "epoch: 6 step: 1493, loss is 0.027315570041537285\n",
      "epoch: 6 step: 1494, loss is 0.007526638451963663\n",
      "epoch: 6 step: 1495, loss is 0.012363983318209648\n",
      "epoch: 6 step: 1496, loss is 0.042693279683589935\n",
      "epoch: 6 step: 1497, loss is 0.007318371906876564\n",
      "epoch: 6 step: 1498, loss is 0.07798418402671814\n",
      "epoch: 6 step: 1499, loss is 0.034261662513017654\n",
      "epoch: 6 step: 1500, loss is 0.20642125606536865\n",
      "epoch: 6 step: 1501, loss is 0.2968423366546631\n",
      "epoch: 6 step: 1502, loss is 0.06437753885984421\n",
      "epoch: 6 step: 1503, loss is 0.00951888132840395\n",
      "epoch: 6 step: 1504, loss is 0.05019696056842804\n",
      "epoch: 6 step: 1505, loss is 0.008415997959673405\n",
      "epoch: 6 step: 1506, loss is 0.08462266623973846\n",
      "epoch: 6 step: 1507, loss is 0.08310847729444504\n",
      "epoch: 6 step: 1508, loss is 0.03505171090364456\n",
      "epoch: 6 step: 1509, loss is 0.06516973674297333\n",
      "epoch: 6 step: 1510, loss is 0.04650247469544411\n",
      "epoch: 6 step: 1511, loss is 0.016491198912262917\n",
      "epoch: 6 step: 1512, loss is 0.024162502959370613\n",
      "epoch: 6 step: 1513, loss is 0.0016091803554445505\n",
      "epoch: 6 step: 1514, loss is 0.03993441164493561\n",
      "epoch: 6 step: 1515, loss is 0.07551984488964081\n",
      "epoch: 6 step: 1516, loss is 0.006043102592229843\n",
      "epoch: 6 step: 1517, loss is 0.010093889199197292\n",
      "epoch: 6 step: 1518, loss is 0.00968193355947733\n",
      "epoch: 6 step: 1519, loss is 0.019597681239247322\n",
      "epoch: 6 step: 1520, loss is 0.11742100864648819\n",
      "epoch: 6 step: 1521, loss is 0.01253089401870966\n",
      "epoch: 6 step: 1522, loss is 0.0007937524933367968\n",
      "epoch: 6 step: 1523, loss is 0.2398635447025299\n",
      "epoch: 6 step: 1524, loss is 0.008839969523251057\n",
      "epoch: 6 step: 1525, loss is 0.074870266020298\n",
      "epoch: 6 step: 1526, loss is 0.005585773382335901\n",
      "epoch: 6 step: 1527, loss is 0.002946209628134966\n",
      "epoch: 6 step: 1528, loss is 0.08516526967287064\n",
      "epoch: 6 step: 1529, loss is 0.03628859296441078\n",
      "epoch: 6 step: 1530, loss is 0.01491815410554409\n",
      "epoch: 6 step: 1531, loss is 0.017757097259163857\n",
      "epoch: 6 step: 1532, loss is 0.11450635641813278\n",
      "epoch: 6 step: 1533, loss is 0.04479881748557091\n",
      "epoch: 6 step: 1534, loss is 0.04463091120123863\n",
      "epoch: 6 step: 1535, loss is 0.048198189586400986\n",
      "epoch: 6 step: 1536, loss is 0.013165346346795559\n",
      "epoch: 6 step: 1537, loss is 0.006342848762869835\n",
      "epoch: 6 step: 1538, loss is 0.012958047911524773\n",
      "epoch: 6 step: 1539, loss is 0.054360054433345795\n",
      "epoch: 6 step: 1540, loss is 0.030692683532834053\n",
      "epoch: 6 step: 1541, loss is 0.01356483530253172\n",
      "epoch: 6 step: 1542, loss is 0.04593176022171974\n",
      "epoch: 6 step: 1543, loss is 0.004424193874001503\n",
      "epoch: 6 step: 1544, loss is 0.055393755435943604\n",
      "epoch: 6 step: 1545, loss is 0.07004175335168839\n",
      "epoch: 6 step: 1546, loss is 0.07830394804477692\n",
      "epoch: 6 step: 1547, loss is 0.10322649776935577\n",
      "epoch: 6 step: 1548, loss is 0.004278244916349649\n",
      "epoch: 6 step: 1549, loss is 0.010376627556979656\n",
      "epoch: 6 step: 1550, loss is 0.03798823431134224\n",
      "epoch: 6 step: 1551, loss is 0.04095535725355148\n",
      "epoch: 6 step: 1552, loss is 0.05031822994351387\n",
      "epoch: 6 step: 1553, loss is 0.07435353100299835\n",
      "epoch: 6 step: 1554, loss is 0.005267615895718336\n",
      "epoch: 6 step: 1555, loss is 0.042435482144355774\n",
      "epoch: 6 step: 1556, loss is 0.030430544167757034\n",
      "epoch: 6 step: 1557, loss is 0.21449415385723114\n",
      "epoch: 6 step: 1558, loss is 0.04283773899078369\n",
      "epoch: 6 step: 1559, loss is 0.01872153952717781\n",
      "epoch: 6 step: 1560, loss is 0.00598132936283946\n",
      "epoch: 6 step: 1561, loss is 0.00021318075596354902\n",
      "epoch: 6 step: 1562, loss is 0.02526887319982052\n",
      "epoch: 6 step: 1563, loss is 0.11363821476697922\n",
      "epoch: 6 step: 1564, loss is 0.031275540590286255\n",
      "epoch: 6 step: 1565, loss is 0.003937439993023872\n",
      "epoch: 6 step: 1566, loss is 0.013050396926701069\n",
      "epoch: 6 step: 1567, loss is 0.12376213073730469\n",
      "epoch: 6 step: 1568, loss is 0.016559461131691933\n",
      "epoch: 6 step: 1569, loss is 0.1971743106842041\n",
      "epoch: 6 step: 1570, loss is 0.003418255364522338\n",
      "epoch: 6 step: 1571, loss is 0.006197396665811539\n",
      "epoch: 6 step: 1572, loss is 0.0008443294791504741\n",
      "epoch: 6 step: 1573, loss is 0.0029297107830643654\n",
      "epoch: 6 step: 1574, loss is 0.20441702008247375\n",
      "epoch: 6 step: 1575, loss is 0.006414517760276794\n",
      "epoch: 6 step: 1576, loss is 0.1223461851477623\n",
      "epoch: 6 step: 1577, loss is 0.010168083943426609\n",
      "epoch: 6 step: 1578, loss is 0.0011741907801479101\n",
      "epoch: 6 step: 1579, loss is 0.029797492548823357\n",
      "epoch: 6 step: 1580, loss is 0.11710689961910248\n",
      "epoch: 6 step: 1581, loss is 0.00011685257777571678\n",
      "epoch: 6 step: 1582, loss is 0.002098122611641884\n",
      "epoch: 6 step: 1583, loss is 0.007140627596527338\n",
      "epoch: 6 step: 1584, loss is 0.010181238874793053\n",
      "epoch: 6 step: 1585, loss is 0.00974256917834282\n",
      "epoch: 6 step: 1586, loss is 0.109885074198246\n",
      "epoch: 6 step: 1587, loss is 0.0786837711930275\n",
      "epoch: 6 step: 1588, loss is 0.007325818762183189\n",
      "epoch: 6 step: 1589, loss is 0.00273848045617342\n",
      "epoch: 6 step: 1590, loss is 0.023943714797496796\n",
      "epoch: 6 step: 1591, loss is 0.004468197003006935\n",
      "epoch: 6 step: 1592, loss is 0.023766672238707542\n",
      "epoch: 6 step: 1593, loss is 0.04628193750977516\n",
      "epoch: 6 step: 1594, loss is 0.03581301495432854\n",
      "epoch: 6 step: 1595, loss is 0.0008368666749447584\n",
      "epoch: 6 step: 1596, loss is 0.08668531477451324\n",
      "epoch: 6 step: 1597, loss is 0.007947482168674469\n",
      "epoch: 6 step: 1598, loss is 0.026096614077687263\n",
      "epoch: 6 step: 1599, loss is 0.004028914496302605\n",
      "epoch: 6 step: 1600, loss is 0.15030385553836823\n",
      "epoch: 6 step: 1601, loss is 0.07166972756385803\n",
      "epoch: 6 step: 1602, loss is 0.022314513102173805\n",
      "epoch: 6 step: 1603, loss is 0.12149757146835327\n",
      "epoch: 6 step: 1604, loss is 0.04634939879179001\n",
      "epoch: 6 step: 1605, loss is 0.0036208382807672024\n",
      "epoch: 6 step: 1606, loss is 0.04382183775305748\n",
      "epoch: 6 step: 1607, loss is 0.06143544986844063\n",
      "epoch: 6 step: 1608, loss is 0.0380699597299099\n",
      "epoch: 6 step: 1609, loss is 0.013415448367595673\n",
      "epoch: 6 step: 1610, loss is 0.03452784940600395\n",
      "epoch: 6 step: 1611, loss is 0.19308678805828094\n",
      "epoch: 6 step: 1612, loss is 0.3764292001724243\n",
      "epoch: 6 step: 1613, loss is 0.0024509290233254433\n",
      "epoch: 6 step: 1614, loss is 0.10074955970048904\n",
      "epoch: 6 step: 1615, loss is 0.06954656541347504\n",
      "epoch: 6 step: 1616, loss is 0.0054049380123615265\n",
      "epoch: 6 step: 1617, loss is 0.006985280197113752\n",
      "epoch: 6 step: 1618, loss is 0.130973681807518\n",
      "epoch: 6 step: 1619, loss is 0.00399271072819829\n",
      "epoch: 6 step: 1620, loss is 0.011390061117708683\n",
      "epoch: 6 step: 1621, loss is 0.0014789969427511096\n",
      "epoch: 6 step: 1622, loss is 0.018164176493883133\n",
      "epoch: 6 step: 1623, loss is 0.0023613222874701023\n",
      "epoch: 6 step: 1624, loss is 0.005053959786891937\n",
      "epoch: 6 step: 1625, loss is 0.029684051871299744\n",
      "epoch: 6 step: 1626, loss is 0.11600984632968903\n",
      "epoch: 6 step: 1627, loss is 0.0008731555426493287\n",
      "epoch: 6 step: 1628, loss is 0.02477715164422989\n",
      "epoch: 6 step: 1629, loss is 0.01120715495198965\n",
      "epoch: 6 step: 1630, loss is 0.2062162607908249\n",
      "epoch: 6 step: 1631, loss is 0.15598565340042114\n",
      "epoch: 6 step: 1632, loss is 0.0013785064220428467\n",
      "epoch: 6 step: 1633, loss is 0.003750668838620186\n",
      "epoch: 6 step: 1634, loss is 0.009215748868882656\n",
      "epoch: 6 step: 1635, loss is 0.011890135705471039\n",
      "epoch: 6 step: 1636, loss is 0.08182830363512039\n",
      "epoch: 6 step: 1637, loss is 0.025810880586504936\n",
      "epoch: 6 step: 1638, loss is 0.010479884222149849\n",
      "epoch: 6 step: 1639, loss is 0.0749838724732399\n",
      "epoch: 6 step: 1640, loss is 0.042154710739851\n",
      "epoch: 6 step: 1641, loss is 0.09547016769647598\n",
      "epoch: 6 step: 1642, loss is 0.04074031114578247\n",
      "epoch: 6 step: 1643, loss is 0.10496014356613159\n",
      "epoch: 6 step: 1644, loss is 0.004913597367703915\n",
      "epoch: 6 step: 1645, loss is 0.03301231935620308\n",
      "epoch: 6 step: 1646, loss is 0.014042488299310207\n",
      "epoch: 6 step: 1647, loss is 0.038327936083078384\n",
      "epoch: 6 step: 1648, loss is 0.08311057090759277\n",
      "epoch: 6 step: 1649, loss is 0.015649918466806412\n",
      "epoch: 6 step: 1650, loss is 0.014154208824038506\n",
      "epoch: 6 step: 1651, loss is 0.031789518892765045\n",
      "epoch: 6 step: 1652, loss is 0.001280670752748847\n",
      "epoch: 6 step: 1653, loss is 0.013268980197608471\n",
      "epoch: 6 step: 1654, loss is 0.045613937079906464\n",
      "epoch: 6 step: 1655, loss is 0.09498187899589539\n",
      "epoch: 6 step: 1656, loss is 0.15787483751773834\n",
      "epoch: 6 step: 1657, loss is 0.0009865174070000648\n",
      "epoch: 6 step: 1658, loss is 0.09976717084646225\n",
      "epoch: 6 step: 1659, loss is 0.015747595578432083\n",
      "epoch: 6 step: 1660, loss is 0.13975857198238373\n",
      "epoch: 6 step: 1661, loss is 0.040925879031419754\n",
      "epoch: 6 step: 1662, loss is 0.02188689261674881\n",
      "epoch: 6 step: 1663, loss is 0.017072316259145737\n",
      "epoch: 6 step: 1664, loss is 0.02064838819205761\n",
      "epoch: 6 step: 1665, loss is 0.003095222869887948\n",
      "epoch: 6 step: 1666, loss is 0.0009535497520118952\n",
      "epoch: 6 step: 1667, loss is 0.07502313703298569\n",
      "epoch: 6 step: 1668, loss is 0.009451699443161488\n",
      "epoch: 6 step: 1669, loss is 0.02105282060801983\n",
      "epoch: 6 step: 1670, loss is 0.004203123971819878\n",
      "epoch: 6 step: 1671, loss is 0.036108214408159256\n",
      "epoch: 6 step: 1672, loss is 0.03187581151723862\n",
      "epoch: 6 step: 1673, loss is 0.0218658410012722\n",
      "epoch: 6 step: 1674, loss is 0.029185762628912926\n",
      "epoch: 6 step: 1675, loss is 0.020797355100512505\n",
      "epoch: 6 step: 1676, loss is 0.0022416403517127037\n",
      "epoch: 6 step: 1677, loss is 0.0021674216259270906\n",
      "epoch: 6 step: 1678, loss is 0.009569695219397545\n",
      "epoch: 6 step: 1679, loss is 0.009570459835231304\n",
      "epoch: 6 step: 1680, loss is 0.005690034944564104\n",
      "epoch: 6 step: 1681, loss is 0.06070555001497269\n",
      "epoch: 6 step: 1682, loss is 0.021611493080854416\n",
      "epoch: 6 step: 1683, loss is 0.022944757714867592\n",
      "epoch: 6 step: 1684, loss is 0.0016085613751783967\n",
      "epoch: 6 step: 1685, loss is 0.013981472700834274\n",
      "epoch: 6 step: 1686, loss is 0.19921864569187164\n",
      "epoch: 6 step: 1687, loss is 0.1039171814918518\n",
      "epoch: 6 step: 1688, loss is 0.004112553782761097\n",
      "epoch: 6 step: 1689, loss is 0.011853459291160107\n",
      "epoch: 6 step: 1690, loss is 0.017946163192391396\n",
      "epoch: 6 step: 1691, loss is 0.0029594521038234234\n",
      "epoch: 6 step: 1692, loss is 0.05734725296497345\n",
      "epoch: 6 step: 1693, loss is 0.07346159964799881\n",
      "epoch: 6 step: 1694, loss is 0.019325191155076027\n",
      "epoch: 6 step: 1695, loss is 0.04447648674249649\n",
      "epoch: 6 step: 1696, loss is 0.06926730275154114\n",
      "epoch: 6 step: 1697, loss is 0.33056187629699707\n",
      "epoch: 6 step: 1698, loss is 0.12522342801094055\n",
      "epoch: 6 step: 1699, loss is 0.005764757748693228\n",
      "epoch: 6 step: 1700, loss is 0.07741856575012207\n",
      "epoch: 6 step: 1701, loss is 0.03863136097788811\n",
      "epoch: 6 step: 1702, loss is 0.036023423075675964\n",
      "epoch: 6 step: 1703, loss is 0.026941001415252686\n",
      "epoch: 6 step: 1704, loss is 0.017317796126008034\n",
      "epoch: 6 step: 1705, loss is 0.011842172592878342\n",
      "epoch: 6 step: 1706, loss is 0.2719237804412842\n",
      "epoch: 6 step: 1707, loss is 0.08216045796871185\n",
      "epoch: 6 step: 1708, loss is 0.0028520578052848577\n",
      "epoch: 6 step: 1709, loss is 0.12751829624176025\n",
      "epoch: 6 step: 1710, loss is 0.08797819912433624\n",
      "epoch: 6 step: 1711, loss is 0.010012393817305565\n",
      "epoch: 6 step: 1712, loss is 0.009658437222242355\n",
      "epoch: 6 step: 1713, loss is 0.2331264317035675\n",
      "epoch: 6 step: 1714, loss is 0.013268660753965378\n",
      "epoch: 6 step: 1715, loss is 0.21367688477039337\n",
      "epoch: 6 step: 1716, loss is 0.014409427531063557\n",
      "epoch: 6 step: 1717, loss is 0.2240072786808014\n",
      "epoch: 6 step: 1718, loss is 0.016068702563643456\n",
      "epoch: 6 step: 1719, loss is 0.06940269470214844\n",
      "epoch: 6 step: 1720, loss is 0.006711060181260109\n",
      "epoch: 6 step: 1721, loss is 0.029534347355365753\n",
      "epoch: 6 step: 1722, loss is 0.0019692108035087585\n",
      "epoch: 6 step: 1723, loss is 0.11465863883495331\n",
      "epoch: 6 step: 1724, loss is 0.09139052778482437\n",
      "epoch: 6 step: 1725, loss is 0.16894656419754028\n",
      "epoch: 6 step: 1726, loss is 0.025750642642378807\n",
      "epoch: 6 step: 1727, loss is 0.16748756170272827\n",
      "epoch: 6 step: 1728, loss is 0.03606502711772919\n",
      "epoch: 6 step: 1729, loss is 0.044482383877038956\n",
      "epoch: 6 step: 1730, loss is 0.021165654063224792\n",
      "epoch: 6 step: 1731, loss is 0.007081339135766029\n",
      "epoch: 6 step: 1732, loss is 0.005397127475589514\n",
      "epoch: 6 step: 1733, loss is 0.0021883200388401747\n",
      "epoch: 6 step: 1734, loss is 0.002577362349256873\n",
      "epoch: 6 step: 1735, loss is 0.00986169371753931\n",
      "epoch: 6 step: 1736, loss is 0.006301887799054384\n",
      "epoch: 6 step: 1737, loss is 0.028455819934606552\n",
      "epoch: 6 step: 1738, loss is 0.011142382398247719\n",
      "epoch: 6 step: 1739, loss is 0.19979673624038696\n",
      "epoch: 6 step: 1740, loss is 0.24304215610027313\n",
      "epoch: 6 step: 1741, loss is 0.007154087070375681\n",
      "epoch: 6 step: 1742, loss is 0.020130624994635582\n",
      "epoch: 6 step: 1743, loss is 0.013608918525278568\n",
      "epoch: 6 step: 1744, loss is 0.027809254825115204\n",
      "epoch: 6 step: 1745, loss is 0.006656050682067871\n",
      "epoch: 6 step: 1746, loss is 0.3340202271938324\n",
      "epoch: 6 step: 1747, loss is 0.0007538010249845684\n",
      "epoch: 6 step: 1748, loss is 0.0005809717695228755\n",
      "epoch: 6 step: 1749, loss is 0.005377477966248989\n",
      "epoch: 6 step: 1750, loss is 0.005428032483905554\n",
      "epoch: 6 step: 1751, loss is 0.001733148586936295\n",
      "epoch: 6 step: 1752, loss is 0.08021347969770432\n",
      "epoch: 6 step: 1753, loss is 0.02109406515955925\n",
      "epoch: 6 step: 1754, loss is 0.13591542840003967\n",
      "epoch: 6 step: 1755, loss is 0.0324813574552536\n",
      "epoch: 6 step: 1756, loss is 0.003533619688823819\n",
      "epoch: 6 step: 1757, loss is 0.016976844519376755\n",
      "epoch: 6 step: 1758, loss is 0.12842506170272827\n",
      "epoch: 6 step: 1759, loss is 0.004327977541834116\n",
      "epoch: 6 step: 1760, loss is 0.0026818416081368923\n",
      "epoch: 6 step: 1761, loss is 0.021108727902173996\n",
      "epoch: 6 step: 1762, loss is 0.0019366990309208632\n",
      "epoch: 6 step: 1763, loss is 0.029626403003931046\n",
      "epoch: 6 step: 1764, loss is 0.0279762651771307\n",
      "epoch: 6 step: 1765, loss is 0.04379599913954735\n",
      "epoch: 6 step: 1766, loss is 0.0035299723967909813\n",
      "epoch: 6 step: 1767, loss is 0.23253203928470612\n",
      "epoch: 6 step: 1768, loss is 0.0008119326084852219\n",
      "epoch: 6 step: 1769, loss is 0.06859304755926132\n",
      "epoch: 6 step: 1770, loss is 0.026904582977294922\n",
      "epoch: 6 step: 1771, loss is 0.001745364279486239\n",
      "epoch: 6 step: 1772, loss is 0.043899454176425934\n",
      "epoch: 6 step: 1773, loss is 0.0007674868684262037\n",
      "epoch: 6 step: 1774, loss is 0.005717808380723\n",
      "epoch: 6 step: 1775, loss is 0.003822584869340062\n",
      "epoch: 6 step: 1776, loss is 0.0011004500556737185\n",
      "epoch: 6 step: 1777, loss is 0.027413658797740936\n",
      "epoch: 6 step: 1778, loss is 0.0035370842088013887\n",
      "epoch: 6 step: 1779, loss is 0.05714442580938339\n",
      "epoch: 6 step: 1780, loss is 0.021726297214627266\n",
      "epoch: 6 step: 1781, loss is 0.07035817205905914\n",
      "epoch: 6 step: 1782, loss is 0.05617903172969818\n",
      "epoch: 6 step: 1783, loss is 0.11219504475593567\n",
      "epoch: 6 step: 1784, loss is 0.0029739616438746452\n",
      "epoch: 6 step: 1785, loss is 0.03851064667105675\n",
      "epoch: 6 step: 1786, loss is 0.011504166759550571\n",
      "epoch: 6 step: 1787, loss is 0.01930888369679451\n",
      "epoch: 6 step: 1788, loss is 0.04506793990731239\n",
      "epoch: 6 step: 1789, loss is 0.001457281643524766\n",
      "epoch: 6 step: 1790, loss is 0.005089404992759228\n",
      "epoch: 6 step: 1791, loss is 0.14688871800899506\n",
      "epoch: 6 step: 1792, loss is 0.008698884397745132\n",
      "epoch: 6 step: 1793, loss is 0.19942781329154968\n",
      "epoch: 6 step: 1794, loss is 0.05948222428560257\n",
      "epoch: 6 step: 1795, loss is 0.0034972564317286015\n",
      "epoch: 6 step: 1796, loss is 0.007127583492547274\n",
      "epoch: 6 step: 1797, loss is 0.4211994707584381\n",
      "epoch: 6 step: 1798, loss is 0.0017267484217882156\n",
      "epoch: 6 step: 1799, loss is 0.03258134424686432\n",
      "epoch: 6 step: 1800, loss is 0.005307454150170088\n",
      "epoch: 6 step: 1801, loss is 0.001280350610613823\n",
      "epoch: 6 step: 1802, loss is 0.2596568465232849\n",
      "epoch: 6 step: 1803, loss is 0.23706363141536713\n",
      "epoch: 6 step: 1804, loss is 0.0022511722054332495\n",
      "epoch: 6 step: 1805, loss is 0.3739594519138336\n",
      "epoch: 6 step: 1806, loss is 0.04981863871216774\n",
      "epoch: 6 step: 1807, loss is 0.015596031211316586\n",
      "epoch: 6 step: 1808, loss is 0.08488807082176208\n",
      "epoch: 6 step: 1809, loss is 0.05290985107421875\n",
      "epoch: 6 step: 1810, loss is 0.07820825278759003\n",
      "epoch: 6 step: 1811, loss is 0.042343415319919586\n",
      "epoch: 6 step: 1812, loss is 0.0053224400617182255\n",
      "epoch: 6 step: 1813, loss is 0.03558705374598503\n",
      "epoch: 6 step: 1814, loss is 0.005741059314459562\n",
      "epoch: 6 step: 1815, loss is 0.03402980417013168\n",
      "epoch: 6 step: 1816, loss is 0.02305331826210022\n",
      "epoch: 6 step: 1817, loss is 0.023497914895415306\n",
      "epoch: 6 step: 1818, loss is 0.1461644023656845\n",
      "epoch: 6 step: 1819, loss is 0.022218504920601845\n",
      "epoch: 6 step: 1820, loss is 0.009844781830906868\n",
      "epoch: 6 step: 1821, loss is 0.13825984299182892\n",
      "epoch: 6 step: 1822, loss is 0.0020133405923843384\n",
      "epoch: 6 step: 1823, loss is 0.12669335305690765\n",
      "epoch: 6 step: 1824, loss is 0.005804551765322685\n",
      "epoch: 6 step: 1825, loss is 0.019258344545960426\n",
      "epoch: 6 step: 1826, loss is 0.10616527497768402\n",
      "epoch: 6 step: 1827, loss is 0.10612421482801437\n",
      "epoch: 6 step: 1828, loss is 0.002870001597329974\n",
      "epoch: 6 step: 1829, loss is 0.01973305456340313\n",
      "epoch: 6 step: 1830, loss is 0.003112216480076313\n",
      "epoch: 6 step: 1831, loss is 0.1010802686214447\n",
      "epoch: 6 step: 1832, loss is 0.01142775360494852\n",
      "epoch: 6 step: 1833, loss is 0.0010980364168062806\n",
      "epoch: 6 step: 1834, loss is 0.1258743405342102\n",
      "epoch: 6 step: 1835, loss is 0.07297204434871674\n",
      "epoch: 6 step: 1836, loss is 0.005491705611348152\n",
      "epoch: 6 step: 1837, loss is 0.013367945328354836\n",
      "epoch: 6 step: 1838, loss is 0.02410702407360077\n",
      "epoch: 6 step: 1839, loss is 0.11364435404539108\n",
      "epoch: 6 step: 1840, loss is 0.0394960381090641\n",
      "epoch: 6 step: 1841, loss is 0.0758567601442337\n",
      "epoch: 6 step: 1842, loss is 0.2837395668029785\n",
      "epoch: 6 step: 1843, loss is 0.06338722258806229\n",
      "epoch: 6 step: 1844, loss is 0.02903890796005726\n",
      "epoch: 6 step: 1845, loss is 0.015619522891938686\n",
      "epoch: 6 step: 1846, loss is 0.10657603293657303\n",
      "epoch: 6 step: 1847, loss is 0.05804666876792908\n",
      "epoch: 6 step: 1848, loss is 0.264374315738678\n",
      "epoch: 6 step: 1849, loss is 0.0033782401587814093\n",
      "epoch: 6 step: 1850, loss is 0.0803542360663414\n",
      "epoch: 6 step: 1851, loss is 0.002505992539227009\n",
      "epoch: 6 step: 1852, loss is 0.0007559345103800297\n",
      "epoch: 6 step: 1853, loss is 0.018393905833363533\n",
      "epoch: 6 step: 1854, loss is 0.01833893172442913\n",
      "epoch: 6 step: 1855, loss is 0.012096481397747993\n",
      "epoch: 6 step: 1856, loss is 0.12423215061426163\n",
      "epoch: 6 step: 1857, loss is 0.10140285640954971\n",
      "epoch: 6 step: 1858, loss is 0.08266439288854599\n",
      "epoch: 6 step: 1859, loss is 0.004629739560186863\n",
      "epoch: 6 step: 1860, loss is 0.03369305282831192\n",
      "epoch: 6 step: 1861, loss is 0.005021562799811363\n",
      "epoch: 6 step: 1862, loss is 0.032941125333309174\n",
      "epoch: 6 step: 1863, loss is 0.009504024870693684\n",
      "epoch: 6 step: 1864, loss is 0.023873131722211838\n",
      "epoch: 6 step: 1865, loss is 0.0021924276370555162\n",
      "epoch: 6 step: 1866, loss is 0.01932435669004917\n",
      "epoch: 6 step: 1867, loss is 0.02677249349653721\n",
      "epoch: 6 step: 1868, loss is 0.2189108431339264\n",
      "epoch: 6 step: 1869, loss is 0.11544404923915863\n",
      "epoch: 6 step: 1870, loss is 0.023331481963396072\n",
      "epoch: 6 step: 1871, loss is 0.07065720856189728\n",
      "epoch: 6 step: 1872, loss is 0.01798918843269348\n",
      "epoch: 6 step: 1873, loss is 0.0018689108546823263\n",
      "epoch: 6 step: 1874, loss is 0.08978821337223053\n",
      "epoch: 6 step: 1875, loss is 0.00797949731349945\n",
      "epoch: 7 step: 1, loss is 0.006560071371495724\n",
      "epoch: 7 step: 2, loss is 0.016016831621527672\n",
      "epoch: 7 step: 3, loss is 0.0016741830622777343\n",
      "epoch: 7 step: 4, loss is 0.011178693734109402\n",
      "epoch: 7 step: 5, loss is 0.046954549849033356\n",
      "epoch: 7 step: 6, loss is 0.0019468790851533413\n",
      "epoch: 7 step: 7, loss is 0.004890116397291422\n",
      "epoch: 7 step: 8, loss is 0.0070181540213525295\n",
      "epoch: 7 step: 9, loss is 0.008469048887491226\n",
      "epoch: 7 step: 10, loss is 0.09872253984212875\n",
      "epoch: 7 step: 11, loss is 0.001973817590624094\n",
      "epoch: 7 step: 12, loss is 0.07169479876756668\n",
      "epoch: 7 step: 13, loss is 0.0025613014586269855\n",
      "epoch: 7 step: 14, loss is 0.008094798773527145\n",
      "epoch: 7 step: 15, loss is 0.05236569792032242\n",
      "epoch: 7 step: 16, loss is 0.07079412788152695\n",
      "epoch: 7 step: 17, loss is 0.019737476482987404\n",
      "epoch: 7 step: 18, loss is 0.09641715884208679\n",
      "epoch: 7 step: 19, loss is 0.03518831729888916\n",
      "epoch: 7 step: 20, loss is 0.022058477625250816\n",
      "epoch: 7 step: 21, loss is 0.004548229742795229\n",
      "epoch: 7 step: 22, loss is 0.020349152386188507\n",
      "epoch: 7 step: 23, loss is 0.03449099510908127\n",
      "epoch: 7 step: 24, loss is 0.0006529304664582014\n",
      "epoch: 7 step: 25, loss is 0.14846251904964447\n",
      "epoch: 7 step: 26, loss is 0.030225159600377083\n",
      "epoch: 7 step: 27, loss is 0.16098366677761078\n",
      "epoch: 7 step: 28, loss is 0.013815750367939472\n",
      "epoch: 7 step: 29, loss is 0.004818418528884649\n",
      "epoch: 7 step: 30, loss is 0.002591746859252453\n",
      "epoch: 7 step: 31, loss is 0.036915626376867294\n",
      "epoch: 7 step: 32, loss is 0.00015282153617590666\n",
      "epoch: 7 step: 33, loss is 0.010470036417245865\n",
      "epoch: 7 step: 34, loss is 0.3361692726612091\n",
      "epoch: 7 step: 35, loss is 0.003219746518880129\n",
      "epoch: 7 step: 36, loss is 0.08691585808992386\n",
      "epoch: 7 step: 37, loss is 0.003589239437133074\n",
      "epoch: 7 step: 38, loss is 0.0018387623131275177\n",
      "epoch: 7 step: 39, loss is 0.031878482550382614\n",
      "epoch: 7 step: 40, loss is 0.014324161224067211\n",
      "epoch: 7 step: 41, loss is 0.03033340908586979\n",
      "epoch: 7 step: 42, loss is 0.020948629826307297\n",
      "epoch: 7 step: 43, loss is 0.004737611394375563\n",
      "epoch: 7 step: 44, loss is 0.011229421943426132\n",
      "epoch: 7 step: 45, loss is 0.038409747183322906\n",
      "epoch: 7 step: 46, loss is 0.0009516170248389244\n",
      "epoch: 7 step: 47, loss is 0.07984539866447449\n",
      "epoch: 7 step: 48, loss is 0.13359925150871277\n",
      "epoch: 7 step: 49, loss is 0.015406443737447262\n",
      "epoch: 7 step: 50, loss is 0.04580991715192795\n",
      "epoch: 7 step: 51, loss is 0.01975458487868309\n",
      "epoch: 7 step: 52, loss is 0.0026438275817781687\n",
      "epoch: 7 step: 53, loss is 0.04459470510482788\n",
      "epoch: 7 step: 54, loss is 0.0007689404301345348\n",
      "epoch: 7 step: 55, loss is 0.0027881173882633448\n",
      "epoch: 7 step: 56, loss is 0.00377509742975235\n",
      "epoch: 7 step: 57, loss is 0.0053615267388522625\n",
      "epoch: 7 step: 58, loss is 0.019134366884827614\n",
      "epoch: 7 step: 59, loss is 0.04476341977715492\n",
      "epoch: 7 step: 60, loss is 0.014155523851513863\n",
      "epoch: 7 step: 61, loss is 0.022964471951127052\n",
      "epoch: 7 step: 62, loss is 0.11055745929479599\n",
      "epoch: 7 step: 63, loss is 0.04995459318161011\n",
      "epoch: 7 step: 64, loss is 0.007051299791783094\n",
      "epoch: 7 step: 65, loss is 0.012408602982759476\n",
      "epoch: 7 step: 66, loss is 0.004978753160685301\n",
      "epoch: 7 step: 67, loss is 0.0011219967855140567\n",
      "epoch: 7 step: 68, loss is 0.006824143696576357\n",
      "epoch: 7 step: 69, loss is 0.0015533174155279994\n",
      "epoch: 7 step: 70, loss is 0.009207057766616344\n",
      "epoch: 7 step: 71, loss is 0.04968634992837906\n",
      "epoch: 7 step: 72, loss is 0.033174067735672\n",
      "epoch: 7 step: 73, loss is 0.0012997845187783241\n",
      "epoch: 7 step: 74, loss is 0.015292505733668804\n",
      "epoch: 7 step: 75, loss is 0.06756758689880371\n",
      "epoch: 7 step: 76, loss is 0.0006765036378055811\n",
      "epoch: 7 step: 77, loss is 0.02725047804415226\n",
      "epoch: 7 step: 78, loss is 0.001505664549767971\n",
      "epoch: 7 step: 79, loss is 0.011853676289319992\n",
      "epoch: 7 step: 80, loss is 0.0638873279094696\n",
      "epoch: 7 step: 81, loss is 0.0019370520021766424\n",
      "epoch: 7 step: 82, loss is 0.03641543537378311\n",
      "epoch: 7 step: 83, loss is 0.06934358924627304\n",
      "epoch: 7 step: 84, loss is 0.019797727465629578\n",
      "epoch: 7 step: 85, loss is 0.007425290532410145\n",
      "epoch: 7 step: 86, loss is 0.12825064361095428\n",
      "epoch: 7 step: 87, loss is 0.0003078859008383006\n",
      "epoch: 7 step: 88, loss is 0.05684097856283188\n",
      "epoch: 7 step: 89, loss is 0.002470272360369563\n",
      "epoch: 7 step: 90, loss is 0.03905879706144333\n",
      "epoch: 7 step: 91, loss is 0.0007731167715974152\n",
      "epoch: 7 step: 92, loss is 0.007711914833635092\n",
      "epoch: 7 step: 93, loss is 0.0007995780906639993\n",
      "epoch: 7 step: 94, loss is 0.0753171369433403\n",
      "epoch: 7 step: 95, loss is 0.009295986965298653\n",
      "epoch: 7 step: 96, loss is 0.002398342126980424\n",
      "epoch: 7 step: 97, loss is 0.02004329487681389\n",
      "epoch: 7 step: 98, loss is 0.04069235548377037\n",
      "epoch: 7 step: 99, loss is 0.0035662937443703413\n",
      "epoch: 7 step: 100, loss is 0.0008094435324892402\n",
      "epoch: 7 step: 101, loss is 0.00593138113617897\n",
      "epoch: 7 step: 102, loss is 0.029732001945376396\n",
      "epoch: 7 step: 103, loss is 0.004483741242438555\n",
      "epoch: 7 step: 104, loss is 0.0016462813364341855\n",
      "epoch: 7 step: 105, loss is 0.0030647958628833294\n",
      "epoch: 7 step: 106, loss is 0.030334265902638435\n",
      "epoch: 7 step: 107, loss is 0.0010190948378294706\n",
      "epoch: 7 step: 108, loss is 0.0132868317887187\n",
      "epoch: 7 step: 109, loss is 0.1058695986866951\n",
      "epoch: 7 step: 110, loss is 0.0062671019695699215\n",
      "epoch: 7 step: 111, loss is 0.011457104235887527\n",
      "epoch: 7 step: 112, loss is 0.0032671464141458273\n",
      "epoch: 7 step: 113, loss is 0.001534180250018835\n",
      "epoch: 7 step: 114, loss is 0.0014656533021479845\n",
      "epoch: 7 step: 115, loss is 0.0015302968677133322\n",
      "epoch: 7 step: 116, loss is 0.010793082416057587\n",
      "epoch: 7 step: 117, loss is 0.011078957468271255\n",
      "epoch: 7 step: 118, loss is 0.004626287147402763\n",
      "epoch: 7 step: 119, loss is 0.010117895901203156\n",
      "epoch: 7 step: 120, loss is 0.04862021282315254\n",
      "epoch: 7 step: 121, loss is 0.02367061749100685\n",
      "epoch: 7 step: 122, loss is 0.01331525668501854\n",
      "epoch: 7 step: 123, loss is 0.04017927497625351\n",
      "epoch: 7 step: 124, loss is 0.0014486368745565414\n",
      "epoch: 7 step: 125, loss is 0.0018695296021178365\n",
      "epoch: 7 step: 126, loss is 0.07920725643634796\n",
      "epoch: 7 step: 127, loss is 0.003840336576104164\n",
      "epoch: 7 step: 128, loss is 0.0058343857526779175\n",
      "epoch: 7 step: 129, loss is 0.0024209036491811275\n",
      "epoch: 7 step: 130, loss is 0.0017378641059622169\n",
      "epoch: 7 step: 131, loss is 0.004029048606753349\n",
      "epoch: 7 step: 132, loss is 0.03915514796972275\n",
      "epoch: 7 step: 133, loss is 0.01629580371081829\n",
      "epoch: 7 step: 134, loss is 0.0021009771153330803\n",
      "epoch: 7 step: 135, loss is 0.00449522165581584\n",
      "epoch: 7 step: 136, loss is 0.0003965676878578961\n",
      "epoch: 7 step: 137, loss is 0.12259676307439804\n",
      "epoch: 7 step: 138, loss is 0.07924588024616241\n",
      "epoch: 7 step: 139, loss is 0.0031704786233603954\n",
      "epoch: 7 step: 140, loss is 0.011610821820795536\n",
      "epoch: 7 step: 141, loss is 0.00401033740490675\n",
      "epoch: 7 step: 142, loss is 0.046541567891836166\n",
      "epoch: 7 step: 143, loss is 0.03225024789571762\n",
      "epoch: 7 step: 144, loss is 0.017876476049423218\n",
      "epoch: 7 step: 145, loss is 0.0017447778955101967\n",
      "epoch: 7 step: 146, loss is 0.07604172825813293\n",
      "epoch: 7 step: 147, loss is 0.0037498127203434706\n",
      "epoch: 7 step: 148, loss is 0.004733471665531397\n",
      "epoch: 7 step: 149, loss is 0.01419648714363575\n",
      "epoch: 7 step: 150, loss is 0.0008360664360225201\n",
      "epoch: 7 step: 151, loss is 0.006968014873564243\n",
      "epoch: 7 step: 152, loss is 0.0027348848525434732\n",
      "epoch: 7 step: 153, loss is 0.05349656194448471\n",
      "epoch: 7 step: 154, loss is 0.008493693545460701\n",
      "epoch: 7 step: 155, loss is 0.003331552492454648\n",
      "epoch: 7 step: 156, loss is 0.10361411422491074\n",
      "epoch: 7 step: 157, loss is 0.001724832458421588\n",
      "epoch: 7 step: 158, loss is 0.21358118951320648\n",
      "epoch: 7 step: 159, loss is 0.00831011775881052\n",
      "epoch: 7 step: 160, loss is 0.08611255139112473\n",
      "epoch: 7 step: 161, loss is 0.01278583612293005\n",
      "epoch: 7 step: 162, loss is 0.06993776559829712\n",
      "epoch: 7 step: 163, loss is 0.025103703141212463\n",
      "epoch: 7 step: 164, loss is 0.1438513845205307\n",
      "epoch: 7 step: 165, loss is 0.00133659562561661\n",
      "epoch: 7 step: 166, loss is 0.01324950996786356\n",
      "epoch: 7 step: 167, loss is 0.039725806564092636\n",
      "epoch: 7 step: 168, loss is 0.004712941125035286\n",
      "epoch: 7 step: 169, loss is 0.018637893721461296\n",
      "epoch: 7 step: 170, loss is 0.031912535429000854\n",
      "epoch: 7 step: 171, loss is 0.008602662943303585\n",
      "epoch: 7 step: 172, loss is 0.1288633644580841\n",
      "epoch: 7 step: 173, loss is 0.017801757901906967\n",
      "epoch: 7 step: 174, loss is 0.0018898847047239542\n",
      "epoch: 7 step: 175, loss is 0.0049422248266637325\n",
      "epoch: 7 step: 176, loss is 0.023615900427103043\n",
      "epoch: 7 step: 177, loss is 0.008074288256466389\n",
      "epoch: 7 step: 178, loss is 0.0004837865417357534\n",
      "epoch: 7 step: 179, loss is 0.0002766090910881758\n",
      "epoch: 7 step: 180, loss is 0.03547239676117897\n",
      "epoch: 7 step: 181, loss is 0.0024807776790112257\n",
      "epoch: 7 step: 182, loss is 0.01378523651510477\n",
      "epoch: 7 step: 183, loss is 0.013188316486775875\n",
      "epoch: 7 step: 184, loss is 0.001504640793427825\n",
      "epoch: 7 step: 185, loss is 0.11078131198883057\n",
      "epoch: 7 step: 186, loss is 0.00553932273760438\n",
      "epoch: 7 step: 187, loss is 0.07116314023733139\n",
      "epoch: 7 step: 188, loss is 0.003339035902172327\n",
      "epoch: 7 step: 189, loss is 0.0026162280701100826\n",
      "epoch: 7 step: 190, loss is 0.0021432084031403065\n",
      "epoch: 7 step: 191, loss is 0.10150361806154251\n",
      "epoch: 7 step: 192, loss is 0.011186011135578156\n",
      "epoch: 7 step: 193, loss is 0.026225630193948746\n",
      "epoch: 7 step: 194, loss is 0.023994052782654762\n",
      "epoch: 7 step: 195, loss is 0.013944768346846104\n",
      "epoch: 7 step: 196, loss is 0.15261180698871613\n",
      "epoch: 7 step: 197, loss is 0.017169635742902756\n",
      "epoch: 7 step: 198, loss is 0.00409507704898715\n",
      "epoch: 7 step: 199, loss is 0.022869182750582695\n",
      "epoch: 7 step: 200, loss is 0.01501644216477871\n",
      "epoch: 7 step: 201, loss is 0.0023385044187307358\n",
      "epoch: 7 step: 202, loss is 0.005913028493523598\n",
      "epoch: 7 step: 203, loss is 0.02300529181957245\n",
      "epoch: 7 step: 204, loss is 0.01819891482591629\n",
      "epoch: 7 step: 205, loss is 0.0025489674881100655\n",
      "epoch: 7 step: 206, loss is 0.0028500373009592295\n",
      "epoch: 7 step: 207, loss is 0.05056161433458328\n",
      "epoch: 7 step: 208, loss is 0.0069075170904397964\n",
      "epoch: 7 step: 209, loss is 0.002983890939503908\n",
      "epoch: 7 step: 210, loss is 0.06833213567733765\n",
      "epoch: 7 step: 211, loss is 0.001754046301357448\n",
      "epoch: 7 step: 212, loss is 0.01201013382524252\n",
      "epoch: 7 step: 213, loss is 0.004411529283970594\n",
      "epoch: 7 step: 214, loss is 0.05208500474691391\n",
      "epoch: 7 step: 215, loss is 0.003292960813269019\n",
      "epoch: 7 step: 216, loss is 0.02757413499057293\n",
      "epoch: 7 step: 217, loss is 0.004828325472772121\n",
      "epoch: 7 step: 218, loss is 0.011349481530487537\n",
      "epoch: 7 step: 219, loss is 0.002772329840809107\n",
      "epoch: 7 step: 220, loss is 0.005485126283019781\n",
      "epoch: 7 step: 221, loss is 0.026191528886556625\n",
      "epoch: 7 step: 222, loss is 0.007428980898112059\n",
      "epoch: 7 step: 223, loss is 0.010033332742750645\n",
      "epoch: 7 step: 224, loss is 0.0241668950766325\n",
      "epoch: 7 step: 225, loss is 0.01855071261525154\n",
      "epoch: 7 step: 226, loss is 0.045355841517448425\n",
      "epoch: 7 step: 227, loss is 0.018537383526563644\n",
      "epoch: 7 step: 228, loss is 0.0006644077366217971\n",
      "epoch: 7 step: 229, loss is 0.08212818205356598\n",
      "epoch: 7 step: 230, loss is 0.002082060556858778\n",
      "epoch: 7 step: 231, loss is 0.07205775380134583\n",
      "epoch: 7 step: 232, loss is 0.0003889156214427203\n",
      "epoch: 7 step: 233, loss is 0.005426515359431505\n",
      "epoch: 7 step: 234, loss is 0.005014069844037294\n",
      "epoch: 7 step: 235, loss is 0.0031691694166511297\n",
      "epoch: 7 step: 236, loss is 0.0016641889233142138\n",
      "epoch: 7 step: 237, loss is 0.003960052039474249\n",
      "epoch: 7 step: 238, loss is 0.08478105813264847\n",
      "epoch: 7 step: 239, loss is 0.0038173343054950237\n",
      "epoch: 7 step: 240, loss is 0.11958644539117813\n",
      "epoch: 7 step: 241, loss is 0.05418820679187775\n",
      "epoch: 7 step: 242, loss is 0.0027397903613746166\n",
      "epoch: 7 step: 243, loss is 0.008591825142502785\n",
      "epoch: 7 step: 244, loss is 0.03938833996653557\n",
      "epoch: 7 step: 245, loss is 0.02047036960721016\n",
      "epoch: 7 step: 246, loss is 0.005952444858849049\n",
      "epoch: 7 step: 247, loss is 0.015726249665021896\n",
      "epoch: 7 step: 248, loss is 0.28299447894096375\n",
      "epoch: 7 step: 249, loss is 0.08472193777561188\n",
      "epoch: 7 step: 250, loss is 0.002188933314755559\n",
      "epoch: 7 step: 251, loss is 0.01284822728484869\n",
      "epoch: 7 step: 252, loss is 0.011650566011667252\n",
      "epoch: 7 step: 253, loss is 0.05592382699251175\n",
      "epoch: 7 step: 254, loss is 0.00238357437774539\n",
      "epoch: 7 step: 255, loss is 0.0006849080673418939\n",
      "epoch: 7 step: 256, loss is 0.0507328063249588\n",
      "epoch: 7 step: 257, loss is 0.014419140294194221\n",
      "epoch: 7 step: 258, loss is 0.10113557428121567\n",
      "epoch: 7 step: 259, loss is 0.008201926946640015\n",
      "epoch: 7 step: 260, loss is 0.004471955820918083\n",
      "epoch: 7 step: 261, loss is 0.00561808655038476\n",
      "epoch: 7 step: 262, loss is 0.025627663359045982\n",
      "epoch: 7 step: 263, loss is 0.0006598746404051781\n",
      "epoch: 7 step: 264, loss is 0.0017968887696042657\n",
      "epoch: 7 step: 265, loss is 0.03728276491165161\n",
      "epoch: 7 step: 266, loss is 0.0738939717411995\n",
      "epoch: 7 step: 267, loss is 0.05776336416602135\n",
      "epoch: 7 step: 268, loss is 0.06334427744150162\n",
      "epoch: 7 step: 269, loss is 0.01809552125632763\n",
      "epoch: 7 step: 270, loss is 0.21468184888362885\n",
      "epoch: 7 step: 271, loss is 0.004348477348685265\n",
      "epoch: 7 step: 272, loss is 0.046225983649492264\n",
      "epoch: 7 step: 273, loss is 0.13728421926498413\n",
      "epoch: 7 step: 274, loss is 0.007037536706775427\n",
      "epoch: 7 step: 275, loss is 0.016544559970498085\n",
      "epoch: 7 step: 276, loss is 0.040666721761226654\n",
      "epoch: 7 step: 277, loss is 0.05248972773551941\n",
      "epoch: 7 step: 278, loss is 0.0014807587722316384\n",
      "epoch: 7 step: 279, loss is 0.00411253422498703\n",
      "epoch: 7 step: 280, loss is 0.07152044773101807\n",
      "epoch: 7 step: 281, loss is 0.012493760325014591\n",
      "epoch: 7 step: 282, loss is 0.00019358220743015409\n",
      "epoch: 7 step: 283, loss is 0.0003920479502994567\n",
      "epoch: 7 step: 284, loss is 0.022256800904870033\n",
      "epoch: 7 step: 285, loss is 0.005505720619112253\n",
      "epoch: 7 step: 286, loss is 0.10724832862615585\n",
      "epoch: 7 step: 287, loss is 0.024071307852864265\n",
      "epoch: 7 step: 288, loss is 0.07515592873096466\n",
      "epoch: 7 step: 289, loss is 0.006266280077397823\n",
      "epoch: 7 step: 290, loss is 0.07370541989803314\n",
      "epoch: 7 step: 291, loss is 0.07538791000843048\n",
      "epoch: 7 step: 292, loss is 0.06904999166727066\n",
      "epoch: 7 step: 293, loss is 0.0003657449851743877\n",
      "epoch: 7 step: 294, loss is 0.11144591122865677\n",
      "epoch: 7 step: 295, loss is 0.008281083777546883\n",
      "epoch: 7 step: 296, loss is 0.02379809133708477\n",
      "epoch: 7 step: 297, loss is 0.0003995350270997733\n",
      "epoch: 7 step: 298, loss is 0.011419935151934624\n",
      "epoch: 7 step: 299, loss is 0.014210253022611141\n",
      "epoch: 7 step: 300, loss is 0.021409835666418076\n",
      "epoch: 7 step: 301, loss is 0.006050277501344681\n",
      "epoch: 7 step: 302, loss is 0.15745870769023895\n",
      "epoch: 7 step: 303, loss is 0.0037628747522830963\n",
      "epoch: 7 step: 304, loss is 0.08268412202596664\n",
      "epoch: 7 step: 305, loss is 0.06866985559463501\n",
      "epoch: 7 step: 306, loss is 0.013081398792564869\n",
      "epoch: 7 step: 307, loss is 0.003329031402245164\n",
      "epoch: 7 step: 308, loss is 0.00260682450607419\n",
      "epoch: 7 step: 309, loss is 0.005620297975838184\n",
      "epoch: 7 step: 310, loss is 0.10124541819095612\n",
      "epoch: 7 step: 311, loss is 0.07544483989477158\n",
      "epoch: 7 step: 312, loss is 0.0006747553125023842\n",
      "epoch: 7 step: 313, loss is 0.012421117164194584\n",
      "epoch: 7 step: 314, loss is 0.0749029815196991\n",
      "epoch: 7 step: 315, loss is 0.02505561336874962\n",
      "epoch: 7 step: 316, loss is 0.026402903720736504\n",
      "epoch: 7 step: 317, loss is 0.005051451735198498\n",
      "epoch: 7 step: 318, loss is 0.03220226615667343\n",
      "epoch: 7 step: 319, loss is 0.0026004030369222164\n",
      "epoch: 7 step: 320, loss is 0.008805710822343826\n",
      "epoch: 7 step: 321, loss is 0.009024673141539097\n",
      "epoch: 7 step: 322, loss is 0.0005218334845267236\n",
      "epoch: 7 step: 323, loss is 0.05233887583017349\n",
      "epoch: 7 step: 324, loss is 0.003774617100134492\n",
      "epoch: 7 step: 325, loss is 0.022424640133976936\n",
      "epoch: 7 step: 326, loss is 0.003782865358516574\n",
      "epoch: 7 step: 327, loss is 0.03601275011897087\n",
      "epoch: 7 step: 328, loss is 0.013339335098862648\n",
      "epoch: 7 step: 329, loss is 0.003572590183466673\n",
      "epoch: 7 step: 330, loss is 0.010343712754547596\n",
      "epoch: 7 step: 331, loss is 0.0015508049400523305\n",
      "epoch: 7 step: 332, loss is 0.10825109481811523\n",
      "epoch: 7 step: 333, loss is 0.016464952379465103\n",
      "epoch: 7 step: 334, loss is 0.013287598267197609\n",
      "epoch: 7 step: 335, loss is 0.0038288491778075695\n",
      "epoch: 7 step: 336, loss is 0.003195062978193164\n",
      "epoch: 7 step: 337, loss is 0.021979326382279396\n",
      "epoch: 7 step: 338, loss is 0.08691299706697464\n",
      "epoch: 7 step: 339, loss is 0.02971462719142437\n",
      "epoch: 7 step: 340, loss is 0.011718309484422207\n",
      "epoch: 7 step: 341, loss is 0.0003410637436900288\n",
      "epoch: 7 step: 342, loss is 0.016761507838964462\n",
      "epoch: 7 step: 343, loss is 0.03335397690534592\n",
      "epoch: 7 step: 344, loss is 0.0020100672263652086\n",
      "epoch: 7 step: 345, loss is 0.00406343350186944\n",
      "epoch: 7 step: 346, loss is 0.0009426087490282953\n",
      "epoch: 7 step: 347, loss is 0.04287921264767647\n",
      "epoch: 7 step: 348, loss is 0.0021844832226634026\n",
      "epoch: 7 step: 349, loss is 0.03199189156293869\n",
      "epoch: 7 step: 350, loss is 0.00015720294322818518\n",
      "epoch: 7 step: 351, loss is 0.0021655159071087837\n",
      "epoch: 7 step: 352, loss is 0.04259738326072693\n",
      "epoch: 7 step: 353, loss is 0.12144773453474045\n",
      "epoch: 7 step: 354, loss is 0.00889300461858511\n",
      "epoch: 7 step: 355, loss is 0.0007789142546243966\n",
      "epoch: 7 step: 356, loss is 0.014808266423642635\n",
      "epoch: 7 step: 357, loss is 0.08725336194038391\n",
      "epoch: 7 step: 358, loss is 0.021643545478582382\n",
      "epoch: 7 step: 359, loss is 0.0264432393014431\n",
      "epoch: 7 step: 360, loss is 0.010331402532756329\n",
      "epoch: 7 step: 361, loss is 0.004036400467157364\n",
      "epoch: 7 step: 362, loss is 0.015203568153083324\n",
      "epoch: 7 step: 363, loss is 0.016451960429549217\n",
      "epoch: 7 step: 364, loss is 0.027655750513076782\n",
      "epoch: 7 step: 365, loss is 0.01609412580728531\n",
      "epoch: 7 step: 366, loss is 0.0040910812094807625\n",
      "epoch: 7 step: 367, loss is 0.019428415223956108\n",
      "epoch: 7 step: 368, loss is 0.008948474191129208\n",
      "epoch: 7 step: 369, loss is 0.005916527938097715\n",
      "epoch: 7 step: 370, loss is 0.004799100570380688\n",
      "epoch: 7 step: 371, loss is 0.003258348908275366\n",
      "epoch: 7 step: 372, loss is 0.02476031519472599\n",
      "epoch: 7 step: 373, loss is 0.008201626129448414\n",
      "epoch: 7 step: 374, loss is 0.027347460389137268\n",
      "epoch: 7 step: 375, loss is 0.00013404512719716877\n",
      "epoch: 7 step: 376, loss is 0.02080097235739231\n",
      "epoch: 7 step: 377, loss is 0.003262265119701624\n",
      "epoch: 7 step: 378, loss is 0.10248303413391113\n",
      "epoch: 7 step: 379, loss is 0.04365946352481842\n",
      "epoch: 7 step: 380, loss is 0.010019014589488506\n",
      "epoch: 7 step: 381, loss is 0.09105060249567032\n",
      "epoch: 7 step: 382, loss is 0.14876335859298706\n",
      "epoch: 7 step: 383, loss is 0.015945101156830788\n",
      "epoch: 7 step: 384, loss is 0.028784019872546196\n",
      "epoch: 7 step: 385, loss is 0.002345167100429535\n",
      "epoch: 7 step: 386, loss is 0.008324084803462029\n",
      "epoch: 7 step: 387, loss is 0.0003236047923564911\n",
      "epoch: 7 step: 388, loss is 0.0036115916445851326\n",
      "epoch: 7 step: 389, loss is 0.007166678551584482\n",
      "epoch: 7 step: 390, loss is 0.027115866541862488\n",
      "epoch: 7 step: 391, loss is 0.013624155893921852\n",
      "epoch: 7 step: 392, loss is 0.013440337032079697\n",
      "epoch: 7 step: 393, loss is 0.0010171947069466114\n",
      "epoch: 7 step: 394, loss is 0.020848317071795464\n",
      "epoch: 7 step: 395, loss is 0.001334739732556045\n",
      "epoch: 7 step: 396, loss is 0.0026040906086564064\n",
      "epoch: 7 step: 397, loss is 0.0024926546029746532\n",
      "epoch: 7 step: 398, loss is 0.007069674786180258\n",
      "epoch: 7 step: 399, loss is 0.13131152093410492\n",
      "epoch: 7 step: 400, loss is 0.007238909136503935\n",
      "epoch: 7 step: 401, loss is 0.0018538732547312975\n",
      "epoch: 7 step: 402, loss is 0.0020692585967481136\n",
      "epoch: 7 step: 403, loss is 0.17296941578388214\n",
      "epoch: 7 step: 404, loss is 0.0158194862306118\n",
      "epoch: 7 step: 405, loss is 0.014050583355128765\n",
      "epoch: 7 step: 406, loss is 0.003625093260779977\n",
      "epoch: 7 step: 407, loss is 0.006352539639919996\n",
      "epoch: 7 step: 408, loss is 0.1428285837173462\n",
      "epoch: 7 step: 409, loss is 0.022513557225465775\n",
      "epoch: 7 step: 410, loss is 0.022994045168161392\n",
      "epoch: 7 step: 411, loss is 0.008852727711200714\n",
      "epoch: 7 step: 412, loss is 0.006541013717651367\n",
      "epoch: 7 step: 413, loss is 0.004763839766383171\n",
      "epoch: 7 step: 414, loss is 0.20846985280513763\n",
      "epoch: 7 step: 415, loss is 0.0215239766985178\n",
      "epoch: 7 step: 416, loss is 0.0028161450754851103\n",
      "epoch: 7 step: 417, loss is 0.007463016547262669\n",
      "epoch: 7 step: 418, loss is 0.024125218391418457\n",
      "epoch: 7 step: 419, loss is 0.001071485341526568\n",
      "epoch: 7 step: 420, loss is 0.0012218391057103872\n",
      "epoch: 7 step: 421, loss is 0.0095438901335001\n",
      "epoch: 7 step: 422, loss is 0.0005558101693168283\n",
      "epoch: 7 step: 423, loss is 0.2164638787508011\n",
      "epoch: 7 step: 424, loss is 0.08250822871923447\n",
      "epoch: 7 step: 425, loss is 0.14261697232723236\n",
      "epoch: 7 step: 426, loss is 0.05298646166920662\n",
      "epoch: 7 step: 427, loss is 0.0007046666578389704\n",
      "epoch: 7 step: 428, loss is 0.009858598932623863\n",
      "epoch: 7 step: 429, loss is 0.003176816739141941\n",
      "epoch: 7 step: 430, loss is 0.055253513157367706\n",
      "epoch: 7 step: 431, loss is 0.1647990643978119\n",
      "epoch: 7 step: 432, loss is 0.05915109068155289\n",
      "epoch: 7 step: 433, loss is 0.006659612059593201\n",
      "epoch: 7 step: 434, loss is 0.011708351783454418\n",
      "epoch: 7 step: 435, loss is 0.04377257823944092\n",
      "epoch: 7 step: 436, loss is 0.13553880155086517\n",
      "epoch: 7 step: 437, loss is 0.004372043535113335\n",
      "epoch: 7 step: 438, loss is 0.005249646492302418\n",
      "epoch: 7 step: 439, loss is 0.013697675429284573\n",
      "epoch: 7 step: 440, loss is 0.002126614097505808\n",
      "epoch: 7 step: 441, loss is 0.04987088218331337\n",
      "epoch: 7 step: 442, loss is 0.018439093604683876\n",
      "epoch: 7 step: 443, loss is 0.09683959186077118\n",
      "epoch: 7 step: 444, loss is 0.01209142617881298\n",
      "epoch: 7 step: 445, loss is 0.012292573228478432\n",
      "epoch: 7 step: 446, loss is 0.0005797533667646348\n",
      "epoch: 7 step: 447, loss is 0.2594257891178131\n",
      "epoch: 7 step: 448, loss is 0.006939860060811043\n",
      "epoch: 7 step: 449, loss is 0.006622649729251862\n",
      "epoch: 7 step: 450, loss is 0.007503543980419636\n",
      "epoch: 7 step: 451, loss is 0.040356412529945374\n",
      "epoch: 7 step: 452, loss is 0.03424844518303871\n",
      "epoch: 7 step: 453, loss is 0.0004874903825111687\n",
      "epoch: 7 step: 454, loss is 0.0020644147880375385\n",
      "epoch: 7 step: 455, loss is 0.052271295338869095\n",
      "epoch: 7 step: 456, loss is 0.02337058074772358\n",
      "epoch: 7 step: 457, loss is 0.004576406441628933\n",
      "epoch: 7 step: 458, loss is 0.014930715784430504\n",
      "epoch: 7 step: 459, loss is 0.006340362131595612\n",
      "epoch: 7 step: 460, loss is 0.03782753273844719\n",
      "epoch: 7 step: 461, loss is 0.0024682253133505583\n",
      "epoch: 7 step: 462, loss is 0.0010038233594968915\n",
      "epoch: 7 step: 463, loss is 0.05778513476252556\n",
      "epoch: 7 step: 464, loss is 0.043885327875614166\n",
      "epoch: 7 step: 465, loss is 0.2607150375843048\n",
      "epoch: 7 step: 466, loss is 0.16131916642189026\n",
      "epoch: 7 step: 467, loss is 0.0007532188319601119\n",
      "epoch: 7 step: 468, loss is 0.00466224504634738\n",
      "epoch: 7 step: 469, loss is 0.07405094802379608\n",
      "epoch: 7 step: 470, loss is 0.010422268882393837\n",
      "epoch: 7 step: 471, loss is 0.008287199772894382\n",
      "epoch: 7 step: 472, loss is 0.007154333870857954\n",
      "epoch: 7 step: 473, loss is 0.07905390858650208\n",
      "epoch: 7 step: 474, loss is 0.00043203693348914385\n",
      "epoch: 7 step: 475, loss is 0.020491423085331917\n",
      "epoch: 7 step: 476, loss is 0.008251342922449112\n",
      "epoch: 7 step: 477, loss is 0.053602397441864014\n",
      "epoch: 7 step: 478, loss is 0.009058807045221329\n",
      "epoch: 7 step: 479, loss is 0.004175565671175718\n",
      "epoch: 7 step: 480, loss is 0.13579384982585907\n",
      "epoch: 7 step: 481, loss is 0.05964311957359314\n",
      "epoch: 7 step: 482, loss is 0.08744523674249649\n",
      "epoch: 7 step: 483, loss is 0.03205231949687004\n",
      "epoch: 7 step: 484, loss is 0.0008033594931475818\n",
      "epoch: 7 step: 485, loss is 0.058598842471838\n",
      "epoch: 7 step: 486, loss is 0.003178132465109229\n",
      "epoch: 7 step: 487, loss is 0.002630809787660837\n",
      "epoch: 7 step: 488, loss is 0.027153359726071358\n",
      "epoch: 7 step: 489, loss is 0.021357130259275436\n",
      "epoch: 7 step: 490, loss is 0.011358939111232758\n",
      "epoch: 7 step: 491, loss is 0.2944137156009674\n",
      "epoch: 7 step: 492, loss is 0.0039750682190060616\n",
      "epoch: 7 step: 493, loss is 0.001429565716534853\n",
      "epoch: 7 step: 494, loss is 0.008305605500936508\n",
      "epoch: 7 step: 495, loss is 0.001131986966356635\n",
      "epoch: 7 step: 496, loss is 0.05531153455376625\n",
      "epoch: 7 step: 497, loss is 0.0029304283671081066\n",
      "epoch: 7 step: 498, loss is 0.015120988711714745\n",
      "epoch: 7 step: 499, loss is 0.0006544279749505222\n",
      "epoch: 7 step: 500, loss is 0.013954441994428635\n",
      "epoch: 7 step: 501, loss is 0.07817891985177994\n",
      "epoch: 7 step: 502, loss is 0.042353030294179916\n",
      "epoch: 7 step: 503, loss is 0.005833492614328861\n",
      "epoch: 7 step: 504, loss is 0.02199714258313179\n",
      "epoch: 7 step: 505, loss is 0.0061195483431220055\n",
      "epoch: 7 step: 506, loss is 0.04672570526599884\n",
      "epoch: 7 step: 507, loss is 0.02974460832774639\n",
      "epoch: 7 step: 508, loss is 0.020090380683541298\n",
      "epoch: 7 step: 509, loss is 0.22044268250465393\n",
      "epoch: 7 step: 510, loss is 0.0036167032085359097\n",
      "epoch: 7 step: 511, loss is 0.07701052725315094\n",
      "epoch: 7 step: 512, loss is 0.02966289035975933\n",
      "epoch: 7 step: 513, loss is 0.006649762857705355\n",
      "epoch: 7 step: 514, loss is 0.06397846341133118\n",
      "epoch: 7 step: 515, loss is 0.06477692723274231\n",
      "epoch: 7 step: 516, loss is 0.052374016493558884\n",
      "epoch: 7 step: 517, loss is 0.012534026056528091\n",
      "epoch: 7 step: 518, loss is 0.024713823571801186\n",
      "epoch: 7 step: 519, loss is 0.028964178636670113\n",
      "epoch: 7 step: 520, loss is 0.10171746462583542\n",
      "epoch: 7 step: 521, loss is 0.04608285054564476\n",
      "epoch: 7 step: 522, loss is 0.011711392551660538\n",
      "epoch: 7 step: 523, loss is 0.008901202119886875\n",
      "epoch: 7 step: 524, loss is 0.14779157936573029\n",
      "epoch: 7 step: 525, loss is 0.002246716758236289\n",
      "epoch: 7 step: 526, loss is 0.019770683720707893\n",
      "epoch: 7 step: 527, loss is 0.05115329101681709\n",
      "epoch: 7 step: 528, loss is 0.05747488886117935\n",
      "epoch: 7 step: 529, loss is 0.010494515299797058\n",
      "epoch: 7 step: 530, loss is 0.0020503897685557604\n",
      "epoch: 7 step: 531, loss is 0.023174671456217766\n",
      "epoch: 7 step: 532, loss is 0.0005139147397130728\n",
      "epoch: 7 step: 533, loss is 0.04924903064966202\n",
      "epoch: 7 step: 534, loss is 0.10461467504501343\n",
      "epoch: 7 step: 535, loss is 0.0656076967716217\n",
      "epoch: 7 step: 536, loss is 0.0017924342537298799\n",
      "epoch: 7 step: 537, loss is 0.35753750801086426\n",
      "epoch: 7 step: 538, loss is 0.007972818799316883\n",
      "epoch: 7 step: 539, loss is 0.05901820585131645\n",
      "epoch: 7 step: 540, loss is 0.08467292785644531\n",
      "epoch: 7 step: 541, loss is 0.010693981312215328\n",
      "epoch: 7 step: 542, loss is 0.09891042113304138\n",
      "epoch: 7 step: 543, loss is 0.0057496577501297\n",
      "epoch: 7 step: 544, loss is 0.060774438083171844\n",
      "epoch: 7 step: 545, loss is 0.004713446367532015\n",
      "epoch: 7 step: 546, loss is 0.026547782123088837\n",
      "epoch: 7 step: 547, loss is 0.028796765953302383\n",
      "epoch: 7 step: 548, loss is 0.00015622233331669122\n",
      "epoch: 7 step: 549, loss is 0.012622935697436333\n",
      "epoch: 7 step: 550, loss is 0.13468986749649048\n",
      "epoch: 7 step: 551, loss is 0.022840583696961403\n",
      "epoch: 7 step: 552, loss is 0.04961542785167694\n",
      "epoch: 7 step: 553, loss is 0.14597059786319733\n",
      "epoch: 7 step: 554, loss is 0.009441845118999481\n",
      "epoch: 7 step: 555, loss is 0.12245545536279678\n",
      "epoch: 7 step: 556, loss is 0.05733025446534157\n",
      "epoch: 7 step: 557, loss is 0.06035051867365837\n",
      "epoch: 7 step: 558, loss is 0.06464000046253204\n",
      "epoch: 7 step: 559, loss is 0.0007649005856364965\n",
      "epoch: 7 step: 560, loss is 0.010648845694959164\n",
      "epoch: 7 step: 561, loss is 0.006999120581895113\n",
      "epoch: 7 step: 562, loss is 0.0018527740612626076\n",
      "epoch: 7 step: 563, loss is 0.0027343600522726774\n",
      "epoch: 7 step: 564, loss is 0.006380648817867041\n",
      "epoch: 7 step: 565, loss is 0.03127342090010643\n",
      "epoch: 7 step: 566, loss is 0.07094493508338928\n",
      "epoch: 7 step: 567, loss is 0.003528294852003455\n",
      "epoch: 7 step: 568, loss is 0.20469920337200165\n",
      "epoch: 7 step: 569, loss is 0.00735921086743474\n",
      "epoch: 7 step: 570, loss is 0.010993204079568386\n",
      "epoch: 7 step: 571, loss is 0.006204111501574516\n",
      "epoch: 7 step: 572, loss is 0.03231321647763252\n",
      "epoch: 7 step: 573, loss is 0.033422645181417465\n",
      "epoch: 7 step: 574, loss is 0.013489488512277603\n",
      "epoch: 7 step: 575, loss is 0.013571186922490597\n",
      "epoch: 7 step: 576, loss is 0.07616232335567474\n",
      "epoch: 7 step: 577, loss is 0.12671370804309845\n",
      "epoch: 7 step: 578, loss is 0.010720502585172653\n",
      "epoch: 7 step: 579, loss is 0.029495421797037125\n",
      "epoch: 7 step: 580, loss is 0.0029915007762610912\n",
      "epoch: 7 step: 581, loss is 0.0018033308442682028\n",
      "epoch: 7 step: 582, loss is 0.021224651485681534\n",
      "epoch: 7 step: 583, loss is 0.03279118239879608\n",
      "epoch: 7 step: 584, loss is 0.1412256807088852\n",
      "epoch: 7 step: 585, loss is 0.0027510286308825016\n",
      "epoch: 7 step: 586, loss is 0.025180675089359283\n",
      "epoch: 7 step: 587, loss is 0.10504482686519623\n",
      "epoch: 7 step: 588, loss is 0.015207628719508648\n",
      "epoch: 7 step: 589, loss is 0.06760376691818237\n",
      "epoch: 7 step: 590, loss is 0.003074672305956483\n",
      "epoch: 7 step: 591, loss is 0.044092029333114624\n",
      "epoch: 7 step: 592, loss is 0.022895148023962975\n",
      "epoch: 7 step: 593, loss is 0.004972475580871105\n",
      "epoch: 7 step: 594, loss is 0.07504294812679291\n",
      "epoch: 7 step: 595, loss is 0.07635100185871124\n",
      "epoch: 7 step: 596, loss is 0.003921696450561285\n",
      "epoch: 7 step: 597, loss is 0.023250043392181396\n",
      "epoch: 7 step: 598, loss is 0.09429420530796051\n",
      "epoch: 7 step: 599, loss is 0.04190051183104515\n",
      "epoch: 7 step: 600, loss is 0.008131001144647598\n",
      "epoch: 7 step: 601, loss is 0.09452459961175919\n",
      "epoch: 7 step: 602, loss is 0.0017523723654448986\n",
      "epoch: 7 step: 603, loss is 0.03836628422141075\n",
      "epoch: 7 step: 604, loss is 0.006751507055014372\n",
      "epoch: 7 step: 605, loss is 0.04214749485254288\n",
      "epoch: 7 step: 606, loss is 0.0009188258554786444\n",
      "epoch: 7 step: 607, loss is 0.008911440148949623\n",
      "epoch: 7 step: 608, loss is 0.007388841826468706\n",
      "epoch: 7 step: 609, loss is 0.1679786741733551\n",
      "epoch: 7 step: 610, loss is 0.0112372487783432\n",
      "epoch: 7 step: 611, loss is 0.060719095170497894\n",
      "epoch: 7 step: 612, loss is 0.006321245804429054\n",
      "epoch: 7 step: 613, loss is 0.05055971071124077\n",
      "epoch: 7 step: 614, loss is 0.01658961921930313\n",
      "epoch: 7 step: 615, loss is 0.015000881627202034\n",
      "epoch: 7 step: 616, loss is 0.01424606516957283\n",
      "epoch: 7 step: 617, loss is 0.0040406156331300735\n",
      "epoch: 7 step: 618, loss is 0.02540711499750614\n",
      "epoch: 7 step: 619, loss is 0.0008141370490193367\n",
      "epoch: 7 step: 620, loss is 0.05510002747178078\n",
      "epoch: 7 step: 621, loss is 0.0283772312104702\n",
      "epoch: 7 step: 622, loss is 0.01290458906441927\n",
      "epoch: 7 step: 623, loss is 0.010655727237462997\n",
      "epoch: 7 step: 624, loss is 0.008177435956895351\n",
      "epoch: 7 step: 625, loss is 0.051493361592292786\n",
      "epoch: 7 step: 626, loss is 0.01835104450583458\n",
      "epoch: 7 step: 627, loss is 0.0006226465338841081\n",
      "epoch: 7 step: 628, loss is 0.18357162177562714\n",
      "epoch: 7 step: 629, loss is 0.011811597272753716\n",
      "epoch: 7 step: 630, loss is 0.020823359489440918\n",
      "epoch: 7 step: 631, loss is 0.018494773656129837\n",
      "epoch: 7 step: 632, loss is 0.012922859750688076\n",
      "epoch: 7 step: 633, loss is 0.21023079752922058\n",
      "epoch: 7 step: 634, loss is 0.008919785730540752\n",
      "epoch: 7 step: 635, loss is 0.17888586223125458\n",
      "epoch: 7 step: 636, loss is 0.0616200752556324\n",
      "epoch: 7 step: 637, loss is 0.04053214192390442\n",
      "epoch: 7 step: 638, loss is 0.0025283468421548605\n",
      "epoch: 7 step: 639, loss is 0.21304181218147278\n",
      "epoch: 7 step: 640, loss is 0.00599070405587554\n",
      "epoch: 7 step: 641, loss is 0.029040031135082245\n",
      "epoch: 7 step: 642, loss is 0.003911627922207117\n",
      "epoch: 7 step: 643, loss is 0.16084815561771393\n",
      "epoch: 7 step: 644, loss is 0.09480726718902588\n",
      "epoch: 7 step: 645, loss is 0.0031283306889235973\n",
      "epoch: 7 step: 646, loss is 0.2628666162490845\n",
      "epoch: 7 step: 647, loss is 0.009411315433681011\n",
      "epoch: 7 step: 648, loss is 0.0009485375485382974\n",
      "epoch: 7 step: 649, loss is 0.005047924350947142\n",
      "epoch: 7 step: 650, loss is 0.21504725515842438\n",
      "epoch: 7 step: 651, loss is 0.10415302217006683\n",
      "epoch: 7 step: 652, loss is 0.13320611417293549\n",
      "epoch: 7 step: 653, loss is 0.10581189393997192\n",
      "epoch: 7 step: 654, loss is 0.010865285992622375\n",
      "epoch: 7 step: 655, loss is 0.017522264271974564\n",
      "epoch: 7 step: 656, loss is 0.029372652992606163\n",
      "epoch: 7 step: 657, loss is 0.12132550776004791\n",
      "epoch: 7 step: 658, loss is 0.19264636933803558\n",
      "epoch: 7 step: 659, loss is 0.023896779865026474\n",
      "epoch: 7 step: 660, loss is 0.010150214657187462\n",
      "epoch: 7 step: 661, loss is 0.003957356326282024\n",
      "epoch: 7 step: 662, loss is 0.006149835418909788\n",
      "epoch: 7 step: 663, loss is 0.02004714123904705\n",
      "epoch: 7 step: 664, loss is 0.1788168102502823\n",
      "epoch: 7 step: 665, loss is 0.1067432388663292\n",
      "epoch: 7 step: 666, loss is 0.01642875373363495\n",
      "epoch: 7 step: 667, loss is 0.004578559193760157\n",
      "epoch: 7 step: 668, loss is 0.01021252479404211\n",
      "epoch: 7 step: 669, loss is 0.09489677846431732\n",
      "epoch: 7 step: 670, loss is 0.040860366076231\n",
      "epoch: 7 step: 671, loss is 0.02105378545820713\n",
      "epoch: 7 step: 672, loss is 0.017984667792916298\n",
      "epoch: 7 step: 673, loss is 0.003255111165344715\n",
      "epoch: 7 step: 674, loss is 0.009582028724253178\n",
      "epoch: 7 step: 675, loss is 0.003370033809915185\n",
      "epoch: 7 step: 676, loss is 0.13940361142158508\n",
      "epoch: 7 step: 677, loss is 0.019099365919828415\n",
      "epoch: 7 step: 678, loss is 0.0031192703172564507\n",
      "epoch: 7 step: 679, loss is 0.004023566376417875\n",
      "epoch: 7 step: 680, loss is 0.11933386325836182\n",
      "epoch: 7 step: 681, loss is 0.0829668864607811\n",
      "epoch: 7 step: 682, loss is 0.0732705220580101\n",
      "epoch: 7 step: 683, loss is 0.025781985372304916\n",
      "epoch: 7 step: 684, loss is 0.018938938155770302\n",
      "epoch: 7 step: 685, loss is 0.011538301594555378\n",
      "epoch: 7 step: 686, loss is 0.36307233572006226\n",
      "epoch: 7 step: 687, loss is 0.1317787617444992\n",
      "epoch: 7 step: 688, loss is 0.000871559779625386\n",
      "epoch: 7 step: 689, loss is 0.008558595553040504\n",
      "epoch: 7 step: 690, loss is 0.08421268314123154\n",
      "epoch: 7 step: 691, loss is 0.07093542069196701\n",
      "epoch: 7 step: 692, loss is 0.0058708516880869865\n",
      "epoch: 7 step: 693, loss is 0.06498129665851593\n",
      "epoch: 7 step: 694, loss is 0.011920928955078125\n",
      "epoch: 7 step: 695, loss is 0.0017685728380456567\n",
      "epoch: 7 step: 696, loss is 0.015834476798772812\n",
      "epoch: 7 step: 697, loss is 0.005773911718279123\n",
      "epoch: 7 step: 698, loss is 0.004700225777924061\n",
      "epoch: 7 step: 699, loss is 0.005697089247405529\n",
      "epoch: 7 step: 700, loss is 0.003677746979519725\n",
      "epoch: 7 step: 701, loss is 0.029059527441859245\n",
      "epoch: 7 step: 702, loss is 0.015824614092707634\n",
      "epoch: 7 step: 703, loss is 0.022873099893331528\n",
      "epoch: 7 step: 704, loss is 0.027353471145033836\n",
      "epoch: 7 step: 705, loss is 0.03445429727435112\n",
      "epoch: 7 step: 706, loss is 0.031736765056848526\n",
      "epoch: 7 step: 707, loss is 0.018968066200613976\n",
      "epoch: 7 step: 708, loss is 0.014930503442883492\n",
      "epoch: 7 step: 709, loss is 0.05437250807881355\n",
      "epoch: 7 step: 710, loss is 0.05620000138878822\n",
      "epoch: 7 step: 711, loss is 0.06910502165555954\n",
      "epoch: 7 step: 712, loss is 0.007526244502514601\n",
      "epoch: 7 step: 713, loss is 0.11896920204162598\n",
      "epoch: 7 step: 714, loss is 0.1255408078432083\n",
      "epoch: 7 step: 715, loss is 0.016365544870495796\n",
      "epoch: 7 step: 716, loss is 0.1947959065437317\n",
      "epoch: 7 step: 717, loss is 0.03209640830755234\n",
      "epoch: 7 step: 718, loss is 0.02461712621152401\n",
      "epoch: 7 step: 719, loss is 0.004773561842739582\n",
      "epoch: 7 step: 720, loss is 0.0822349339723587\n",
      "epoch: 7 step: 721, loss is 0.003167800372466445\n",
      "epoch: 7 step: 722, loss is 0.07415303587913513\n",
      "epoch: 7 step: 723, loss is 0.19863730669021606\n",
      "epoch: 7 step: 724, loss is 0.048121046274900436\n",
      "epoch: 7 step: 725, loss is 0.11177217960357666\n",
      "epoch: 7 step: 726, loss is 0.0036146428901702166\n",
      "epoch: 7 step: 727, loss is 0.013018227182328701\n",
      "epoch: 7 step: 728, loss is 0.014653947204351425\n",
      "epoch: 7 step: 729, loss is 0.010237867943942547\n",
      "epoch: 7 step: 730, loss is 0.00013396533904597163\n",
      "epoch: 7 step: 731, loss is 0.20604310929775238\n",
      "epoch: 7 step: 732, loss is 0.0045334333553910255\n",
      "epoch: 7 step: 733, loss is 0.03490540012717247\n",
      "epoch: 7 step: 734, loss is 0.0651797205209732\n",
      "epoch: 7 step: 735, loss is 0.06248822063207626\n",
      "epoch: 7 step: 736, loss is 0.13576720654964447\n",
      "epoch: 7 step: 737, loss is 0.014795911498367786\n",
      "epoch: 7 step: 738, loss is 0.019426394253969193\n",
      "epoch: 7 step: 739, loss is 0.06191840395331383\n",
      "epoch: 7 step: 740, loss is 0.0012364748399704695\n",
      "epoch: 7 step: 741, loss is 0.0008908441523090005\n",
      "epoch: 7 step: 742, loss is 0.05153167247772217\n",
      "epoch: 7 step: 743, loss is 0.15000873804092407\n",
      "epoch: 7 step: 744, loss is 0.002724576508626342\n",
      "epoch: 7 step: 745, loss is 0.021190928295254707\n",
      "epoch: 7 step: 746, loss is 0.007813462987542152\n",
      "epoch: 7 step: 747, loss is 0.26696503162384033\n",
      "epoch: 7 step: 748, loss is 0.026282940059900284\n",
      "epoch: 7 step: 749, loss is 0.0006967908702790737\n",
      "epoch: 7 step: 750, loss is 0.005979865789413452\n",
      "epoch: 7 step: 751, loss is 0.002081328770145774\n",
      "epoch: 7 step: 752, loss is 0.0004986927378922701\n",
      "epoch: 7 step: 753, loss is 0.17213088274002075\n",
      "epoch: 7 step: 754, loss is 0.03633532300591469\n",
      "epoch: 7 step: 755, loss is 0.007682086434215307\n",
      "epoch: 7 step: 756, loss is 0.0033375127241015434\n",
      "epoch: 7 step: 757, loss is 0.011285565793514252\n",
      "epoch: 7 step: 758, loss is 0.056030984967947006\n",
      "epoch: 7 step: 759, loss is 0.015749122947454453\n",
      "epoch: 7 step: 760, loss is 0.029592685401439667\n",
      "epoch: 7 step: 761, loss is 0.001096842810511589\n",
      "epoch: 7 step: 762, loss is 0.006122784689068794\n",
      "epoch: 7 step: 763, loss is 0.05395248532295227\n",
      "epoch: 7 step: 764, loss is 0.1762983798980713\n",
      "epoch: 7 step: 765, loss is 0.1934313178062439\n",
      "epoch: 7 step: 766, loss is 0.07623845338821411\n",
      "epoch: 7 step: 767, loss is 0.010557794943451881\n",
      "epoch: 7 step: 768, loss is 0.05941053852438927\n",
      "epoch: 7 step: 769, loss is 0.009549400769174099\n",
      "epoch: 7 step: 770, loss is 0.0005978501285426319\n",
      "epoch: 7 step: 771, loss is 0.0030019243713468313\n",
      "epoch: 7 step: 772, loss is 0.09922120720148087\n",
      "epoch: 7 step: 773, loss is 0.22313708066940308\n",
      "epoch: 7 step: 774, loss is 0.005841891746968031\n",
      "epoch: 7 step: 775, loss is 0.1350131332874298\n",
      "epoch: 7 step: 776, loss is 0.00035246970946900547\n",
      "epoch: 7 step: 777, loss is 0.0559886135160923\n",
      "epoch: 7 step: 778, loss is 0.0028121513314545155\n",
      "epoch: 7 step: 779, loss is 0.02335256338119507\n",
      "epoch: 7 step: 780, loss is 0.04427831992506981\n",
      "epoch: 7 step: 781, loss is 0.0012100047897547483\n",
      "epoch: 7 step: 782, loss is 0.005570563953369856\n",
      "epoch: 7 step: 783, loss is 0.0064099631272256374\n",
      "epoch: 7 step: 784, loss is 0.08280690014362335\n",
      "epoch: 7 step: 785, loss is 0.014677999541163445\n",
      "epoch: 7 step: 786, loss is 0.000707642175257206\n",
      "epoch: 7 step: 787, loss is 0.05023420974612236\n",
      "epoch: 7 step: 788, loss is 0.07382502406835556\n",
      "epoch: 7 step: 789, loss is 0.0023313153069466352\n",
      "epoch: 7 step: 790, loss is 0.02214740216732025\n",
      "epoch: 7 step: 791, loss is 0.14076924324035645\n",
      "epoch: 7 step: 792, loss is 0.0015438442351296544\n",
      "epoch: 7 step: 793, loss is 0.03738134726881981\n",
      "epoch: 7 step: 794, loss is 0.04692552611231804\n",
      "epoch: 7 step: 795, loss is 0.0031825280748307705\n",
      "epoch: 7 step: 796, loss is 0.007050683256238699\n",
      "epoch: 7 step: 797, loss is 0.11313784122467041\n",
      "epoch: 7 step: 798, loss is 0.008137568831443787\n",
      "epoch: 7 step: 799, loss is 0.0005898806848563254\n",
      "epoch: 7 step: 800, loss is 0.0069814096204936504\n",
      "epoch: 7 step: 801, loss is 0.004455700516700745\n",
      "epoch: 7 step: 802, loss is 0.005151237361133099\n",
      "epoch: 7 step: 803, loss is 0.001581427757628262\n",
      "epoch: 7 step: 804, loss is 0.04599376395344734\n",
      "epoch: 7 step: 805, loss is 0.026560550555586815\n",
      "epoch: 7 step: 806, loss is 0.009225426241755486\n",
      "epoch: 7 step: 807, loss is 0.00539533793926239\n",
      "epoch: 7 step: 808, loss is 0.011815553531050682\n",
      "epoch: 7 step: 809, loss is 0.015580820851027966\n",
      "epoch: 7 step: 810, loss is 0.0313849039375782\n",
      "epoch: 7 step: 811, loss is 0.005781854502856731\n",
      "epoch: 7 step: 812, loss is 0.0014793372247368097\n",
      "epoch: 7 step: 813, loss is 0.0009612235007807612\n",
      "epoch: 7 step: 814, loss is 0.008302602916955948\n",
      "epoch: 7 step: 815, loss is 0.0006643030792474747\n",
      "epoch: 7 step: 816, loss is 0.0007098241476342082\n",
      "epoch: 7 step: 817, loss is 0.0018228725530207157\n",
      "epoch: 7 step: 818, loss is 0.017100844532251358\n",
      "epoch: 7 step: 819, loss is 0.11420698463916779\n",
      "epoch: 7 step: 820, loss is 0.007603982463479042\n",
      "epoch: 7 step: 821, loss is 0.02800861932337284\n",
      "epoch: 7 step: 822, loss is 0.2365906536579132\n",
      "epoch: 7 step: 823, loss is 0.0047609033063054085\n",
      "epoch: 7 step: 824, loss is 0.0018426995957270265\n",
      "epoch: 7 step: 825, loss is 0.009805374778807163\n",
      "epoch: 7 step: 826, loss is 0.23330087959766388\n",
      "epoch: 7 step: 827, loss is 0.0148087739944458\n",
      "epoch: 7 step: 828, loss is 0.15941573679447174\n",
      "epoch: 7 step: 829, loss is 0.13195188343524933\n",
      "epoch: 7 step: 830, loss is 0.14298796653747559\n",
      "epoch: 7 step: 831, loss is 0.01828387938439846\n",
      "epoch: 7 step: 832, loss is 0.005869089160114527\n",
      "epoch: 7 step: 833, loss is 0.017665382474660873\n",
      "epoch: 7 step: 834, loss is 0.028900988399982452\n",
      "epoch: 7 step: 835, loss is 0.01569414511322975\n",
      "epoch: 7 step: 836, loss is 0.07979670912027359\n",
      "epoch: 7 step: 837, loss is 0.04568403214216232\n",
      "epoch: 7 step: 838, loss is 0.1707174926996231\n",
      "epoch: 7 step: 839, loss is 0.0019119182834401727\n",
      "epoch: 7 step: 840, loss is 0.10598333179950714\n",
      "epoch: 7 step: 841, loss is 0.007431453093886375\n",
      "epoch: 7 step: 842, loss is 0.21996967494487762\n",
      "epoch: 7 step: 843, loss is 0.03958454355597496\n",
      "epoch: 7 step: 844, loss is 0.011672243475914001\n",
      "epoch: 7 step: 845, loss is 0.026258165016770363\n",
      "epoch: 7 step: 846, loss is 0.0003780645492952317\n",
      "epoch: 7 step: 847, loss is 0.029903097078204155\n",
      "epoch: 7 step: 848, loss is 0.08004380017518997\n",
      "epoch: 7 step: 849, loss is 0.006641216576099396\n",
      "epoch: 7 step: 850, loss is 0.0032552843913435936\n",
      "epoch: 7 step: 851, loss is 0.15499205887317657\n",
      "epoch: 7 step: 852, loss is 0.005222107749432325\n",
      "epoch: 7 step: 853, loss is 0.03406812995672226\n",
      "epoch: 7 step: 854, loss is 0.17764200270175934\n",
      "epoch: 7 step: 855, loss is 0.03818643465638161\n",
      "epoch: 7 step: 856, loss is 0.14030449092388153\n",
      "epoch: 7 step: 857, loss is 0.001415034756064415\n",
      "epoch: 7 step: 858, loss is 0.0018849975895136595\n",
      "epoch: 7 step: 859, loss is 0.050195224583148956\n",
      "epoch: 7 step: 860, loss is 0.0404660701751709\n",
      "epoch: 7 step: 861, loss is 0.017109757289290428\n",
      "epoch: 7 step: 862, loss is 0.050654325634241104\n",
      "epoch: 7 step: 863, loss is 0.005861009005457163\n",
      "epoch: 7 step: 864, loss is 0.021417470648884773\n",
      "epoch: 7 step: 865, loss is 0.008628236129879951\n",
      "epoch: 7 step: 866, loss is 0.0033929895143955946\n",
      "epoch: 7 step: 867, loss is 0.016397984698414803\n",
      "epoch: 7 step: 868, loss is 0.03276967629790306\n",
      "epoch: 7 step: 869, loss is 0.001885417615994811\n",
      "epoch: 7 step: 870, loss is 0.002014906145632267\n",
      "epoch: 7 step: 871, loss is 0.017782723531126976\n",
      "epoch: 7 step: 872, loss is 0.00212903693318367\n",
      "epoch: 7 step: 873, loss is 0.017964210361242294\n",
      "epoch: 7 step: 874, loss is 0.0035853718873113394\n",
      "epoch: 7 step: 875, loss is 0.20463238656520844\n",
      "epoch: 7 step: 876, loss is 0.0015042949235066772\n",
      "epoch: 7 step: 877, loss is 0.0019098911434412003\n",
      "epoch: 7 step: 878, loss is 0.169432133436203\n",
      "epoch: 7 step: 879, loss is 0.10790111124515533\n",
      "epoch: 7 step: 880, loss is 0.002733986359089613\n",
      "epoch: 7 step: 881, loss is 0.002996182069182396\n",
      "epoch: 7 step: 882, loss is 0.02389252930879593\n",
      "epoch: 7 step: 883, loss is 0.14162462949752808\n",
      "epoch: 7 step: 884, loss is 0.027864230796694756\n",
      "epoch: 7 step: 885, loss is 0.16477492451667786\n",
      "epoch: 7 step: 886, loss is 0.007423520088195801\n",
      "epoch: 7 step: 887, loss is 0.001173784607090056\n",
      "epoch: 7 step: 888, loss is 0.0035277639981359243\n",
      "epoch: 7 step: 889, loss is 0.03404324874281883\n",
      "epoch: 7 step: 890, loss is 0.0030879974365234375\n",
      "epoch: 7 step: 891, loss is 0.011942376382648945\n",
      "epoch: 7 step: 892, loss is 0.08210955560207367\n",
      "epoch: 7 step: 893, loss is 0.0034276782535016537\n",
      "epoch: 7 step: 894, loss is 0.03466324880719185\n",
      "epoch: 7 step: 895, loss is 0.008268140256404877\n",
      "epoch: 7 step: 896, loss is 0.01169951818883419\n",
      "epoch: 7 step: 897, loss is 0.013443414121866226\n",
      "epoch: 7 step: 898, loss is 0.10931989550590515\n",
      "epoch: 7 step: 899, loss is 0.012642760761082172\n",
      "epoch: 7 step: 900, loss is 0.013105056248605251\n",
      "epoch: 7 step: 901, loss is 0.0027635102160274982\n",
      "epoch: 7 step: 902, loss is 0.0065026357769966125\n",
      "epoch: 7 step: 903, loss is 0.022536559030413628\n",
      "epoch: 7 step: 904, loss is 0.007150860968977213\n",
      "epoch: 7 step: 905, loss is 0.014562923461198807\n",
      "epoch: 7 step: 906, loss is 0.07871642708778381\n",
      "epoch: 7 step: 907, loss is 0.00040434891707263887\n",
      "epoch: 7 step: 908, loss is 0.000705694081261754\n",
      "epoch: 7 step: 909, loss is 0.02705978974699974\n",
      "epoch: 7 step: 910, loss is 0.014083608984947205\n",
      "epoch: 7 step: 911, loss is 0.006764730904251337\n",
      "epoch: 7 step: 912, loss is 0.00614725099876523\n",
      "epoch: 7 step: 913, loss is 0.0007414116989821196\n",
      "epoch: 7 step: 914, loss is 0.11014048010110855\n",
      "epoch: 7 step: 915, loss is 0.0032945843413472176\n",
      "epoch: 7 step: 916, loss is 0.005540952552109957\n",
      "epoch: 7 step: 917, loss is 0.006771009415388107\n",
      "epoch: 7 step: 918, loss is 0.08599258959293365\n",
      "epoch: 7 step: 919, loss is 0.0014298250898718834\n",
      "epoch: 7 step: 920, loss is 0.0043033272959291935\n",
      "epoch: 7 step: 921, loss is 0.008015463128685951\n",
      "epoch: 7 step: 922, loss is 0.03603048995137215\n",
      "epoch: 7 step: 923, loss is 0.0012533089611679316\n",
      "epoch: 7 step: 924, loss is 0.025717074051499367\n",
      "epoch: 7 step: 925, loss is 0.07541073858737946\n",
      "epoch: 7 step: 926, loss is 0.01073466520756483\n",
      "epoch: 7 step: 927, loss is 0.0036873547360301018\n",
      "epoch: 7 step: 928, loss is 0.024023128673434258\n",
      "epoch: 7 step: 929, loss is 0.002858813153579831\n",
      "epoch: 7 step: 930, loss is 0.002826447132974863\n",
      "epoch: 7 step: 931, loss is 0.19252146780490875\n",
      "epoch: 7 step: 932, loss is 0.0011651711538434029\n",
      "epoch: 7 step: 933, loss is 0.0008339795749634504\n",
      "epoch: 7 step: 934, loss is 0.036543454974889755\n",
      "epoch: 7 step: 935, loss is 0.011185400187969208\n",
      "epoch: 7 step: 936, loss is 0.10270855575799942\n",
      "epoch: 7 step: 937, loss is 0.009705822914838791\n",
      "epoch: 7 step: 938, loss is 0.04534245654940605\n",
      "epoch: 7 step: 939, loss is 0.05072877183556557\n",
      "epoch: 7 step: 940, loss is 0.09552368521690369\n",
      "epoch: 7 step: 941, loss is 0.05767051503062248\n",
      "epoch: 7 step: 942, loss is 0.0104964105412364\n",
      "epoch: 7 step: 943, loss is 0.10349640250205994\n",
      "epoch: 7 step: 944, loss is 0.014953956007957458\n",
      "epoch: 7 step: 945, loss is 0.004637896083295345\n",
      "epoch: 7 step: 946, loss is 0.021078910678625107\n",
      "epoch: 7 step: 947, loss is 0.003149814670905471\n",
      "epoch: 7 step: 948, loss is 0.0005616527050733566\n",
      "epoch: 7 step: 949, loss is 0.0013663111021742225\n",
      "epoch: 7 step: 950, loss is 0.021242476999759674\n",
      "epoch: 7 step: 951, loss is 0.1043192520737648\n",
      "epoch: 7 step: 952, loss is 0.13709759712219238\n",
      "epoch: 7 step: 953, loss is 0.005127869546413422\n",
      "epoch: 7 step: 954, loss is 0.016290893778204918\n",
      "epoch: 7 step: 955, loss is 0.003722206922248006\n",
      "epoch: 7 step: 956, loss is 0.00034141968353651464\n",
      "epoch: 7 step: 957, loss is 0.004954901523888111\n",
      "epoch: 7 step: 958, loss is 0.006863842718303204\n",
      "epoch: 7 step: 959, loss is 0.038436997681856155\n",
      "epoch: 7 step: 960, loss is 0.04642924666404724\n",
      "epoch: 7 step: 961, loss is 0.01371274795383215\n",
      "epoch: 7 step: 962, loss is 0.1707538664340973\n",
      "epoch: 7 step: 963, loss is 0.04284632205963135\n",
      "epoch: 7 step: 964, loss is 0.031632181257009506\n",
      "epoch: 7 step: 965, loss is 0.08148805797100067\n",
      "epoch: 7 step: 966, loss is 0.005484031047672033\n",
      "epoch: 7 step: 967, loss is 0.022969471290707588\n",
      "epoch: 7 step: 968, loss is 0.007060685195028782\n",
      "epoch: 7 step: 969, loss is 0.006410794332623482\n",
      "epoch: 7 step: 970, loss is 0.05068880692124367\n",
      "epoch: 7 step: 971, loss is 0.18622982501983643\n",
      "epoch: 7 step: 972, loss is 0.05878681316971779\n",
      "epoch: 7 step: 973, loss is 0.02939772419631481\n",
      "epoch: 7 step: 974, loss is 0.004634394776076078\n",
      "epoch: 7 step: 975, loss is 0.026313308626413345\n",
      "epoch: 7 step: 976, loss is 0.057564977556467056\n",
      "epoch: 7 step: 977, loss is 0.003058922942727804\n",
      "epoch: 7 step: 978, loss is 0.010580269619822502\n",
      "epoch: 7 step: 979, loss is 0.02398301288485527\n",
      "epoch: 7 step: 980, loss is 0.11581522226333618\n",
      "epoch: 7 step: 981, loss is 0.0011141914874315262\n",
      "epoch: 7 step: 982, loss is 0.01563849300146103\n",
      "epoch: 7 step: 983, loss is 0.0018750595627352595\n",
      "epoch: 7 step: 984, loss is 0.005140912719070911\n",
      "epoch: 7 step: 985, loss is 0.13233524560928345\n",
      "epoch: 7 step: 986, loss is 0.035765912383794785\n",
      "epoch: 7 step: 987, loss is 0.4113195538520813\n",
      "epoch: 7 step: 988, loss is 0.03889260068535805\n",
      "epoch: 7 step: 989, loss is 0.018703840672969818\n",
      "epoch: 7 step: 990, loss is 0.03672924265265465\n",
      "epoch: 7 step: 991, loss is 0.0023241762537509203\n",
      "epoch: 7 step: 992, loss is 0.003958700690418482\n",
      "epoch: 7 step: 993, loss is 0.008894896134734154\n",
      "epoch: 7 step: 994, loss is 0.016600556671619415\n",
      "epoch: 7 step: 995, loss is 0.03960075229406357\n",
      "epoch: 7 step: 996, loss is 0.0009673215099610388\n",
      "epoch: 7 step: 997, loss is 0.0019419946474954486\n",
      "epoch: 7 step: 998, loss is 0.018317552283406258\n",
      "epoch: 7 step: 999, loss is 0.006250469945371151\n",
      "epoch: 7 step: 1000, loss is 0.047402311116456985\n",
      "epoch: 7 step: 1001, loss is 0.0010785041376948357\n",
      "epoch: 7 step: 1002, loss is 0.0038722623139619827\n",
      "epoch: 7 step: 1003, loss is 0.012084176763892174\n",
      "epoch: 7 step: 1004, loss is 0.08795452862977982\n",
      "epoch: 7 step: 1005, loss is 0.03330688551068306\n",
      "epoch: 7 step: 1006, loss is 0.005745941307395697\n",
      "epoch: 7 step: 1007, loss is 0.0007375971181318164\n",
      "epoch: 7 step: 1008, loss is 0.01044969167560339\n",
      "epoch: 7 step: 1009, loss is 0.00830051489174366\n",
      "epoch: 7 step: 1010, loss is 0.013621741905808449\n",
      "epoch: 7 step: 1011, loss is 0.02685982547700405\n",
      "epoch: 7 step: 1012, loss is 0.014539689756929874\n",
      "epoch: 7 step: 1013, loss is 0.14677473902702332\n",
      "epoch: 7 step: 1014, loss is 0.0004318594583310187\n",
      "epoch: 7 step: 1015, loss is 0.003569838125258684\n",
      "epoch: 7 step: 1016, loss is 0.0452081635594368\n",
      "epoch: 7 step: 1017, loss is 0.06786961853504181\n",
      "epoch: 7 step: 1018, loss is 0.004248500801622868\n",
      "epoch: 7 step: 1019, loss is 0.024105386808514595\n",
      "epoch: 7 step: 1020, loss is 0.008462583646178246\n",
      "epoch: 7 step: 1021, loss is 0.0007822277839295566\n",
      "epoch: 7 step: 1022, loss is 0.07386984676122665\n",
      "epoch: 7 step: 1023, loss is 0.011380743235349655\n",
      "epoch: 7 step: 1024, loss is 0.0030054764356464148\n",
      "epoch: 7 step: 1025, loss is 0.004437276627868414\n",
      "epoch: 7 step: 1026, loss is 0.011445951648056507\n",
      "epoch: 7 step: 1027, loss is 0.003216419368982315\n",
      "epoch: 7 step: 1028, loss is 0.007567384280264378\n",
      "epoch: 7 step: 1029, loss is 0.052812617272138596\n",
      "epoch: 7 step: 1030, loss is 0.0213620625436306\n",
      "epoch: 7 step: 1031, loss is 0.008394824340939522\n",
      "epoch: 7 step: 1032, loss is 0.01954849250614643\n",
      "epoch: 7 step: 1033, loss is 0.00678662396967411\n",
      "epoch: 7 step: 1034, loss is 0.0009932118700817227\n",
      "epoch: 7 step: 1035, loss is 0.0049692378379404545\n",
      "epoch: 7 step: 1036, loss is 0.07781599462032318\n",
      "epoch: 7 step: 1037, loss is 0.0015808084281161427\n",
      "epoch: 7 step: 1038, loss is 0.03365795686841011\n",
      "epoch: 7 step: 1039, loss is 0.0034951800480484962\n",
      "epoch: 7 step: 1040, loss is 0.011465940624475479\n",
      "epoch: 7 step: 1041, loss is 0.015575739555060863\n",
      "epoch: 7 step: 1042, loss is 0.006162555888295174\n",
      "epoch: 7 step: 1043, loss is 0.040240660309791565\n",
      "epoch: 7 step: 1044, loss is 0.008362893015146255\n",
      "epoch: 7 step: 1045, loss is 0.03560308739542961\n",
      "epoch: 7 step: 1046, loss is 0.06895551830530167\n",
      "epoch: 7 step: 1047, loss is 0.01443447731435299\n",
      "epoch: 7 step: 1048, loss is 0.009772449731826782\n",
      "epoch: 7 step: 1049, loss is 0.09690038114786148\n",
      "epoch: 7 step: 1050, loss is 0.051360078155994415\n",
      "epoch: 7 step: 1051, loss is 0.007988360710442066\n",
      "epoch: 7 step: 1052, loss is 0.008688284084200859\n",
      "epoch: 7 step: 1053, loss is 0.03673635795712471\n",
      "epoch: 7 step: 1054, loss is 0.0013452944112941623\n",
      "epoch: 7 step: 1055, loss is 0.002285606460645795\n",
      "epoch: 7 step: 1056, loss is 0.04203936457633972\n",
      "epoch: 7 step: 1057, loss is 0.21981190145015717\n",
      "epoch: 7 step: 1058, loss is 0.0026216215919703245\n",
      "epoch: 7 step: 1059, loss is 0.00698212580755353\n",
      "epoch: 7 step: 1060, loss is 0.054816022515296936\n",
      "epoch: 7 step: 1061, loss is 0.04331474378705025\n",
      "epoch: 7 step: 1062, loss is 0.0030912235379219055\n",
      "epoch: 7 step: 1063, loss is 0.0001258261181646958\n",
      "epoch: 7 step: 1064, loss is 0.034888748079538345\n",
      "epoch: 7 step: 1065, loss is 0.008051229640841484\n",
      "epoch: 7 step: 1066, loss is 0.003264164784923196\n",
      "epoch: 7 step: 1067, loss is 0.08099345862865448\n",
      "epoch: 7 step: 1068, loss is 0.02342630736529827\n",
      "epoch: 7 step: 1069, loss is 0.00439848005771637\n",
      "epoch: 7 step: 1070, loss is 0.15033647418022156\n",
      "epoch: 7 step: 1071, loss is 0.02555098384618759\n",
      "epoch: 7 step: 1072, loss is 0.08238982409238815\n",
      "epoch: 7 step: 1073, loss is 0.01873661018908024\n",
      "epoch: 7 step: 1074, loss is 0.00394537253305316\n",
      "epoch: 7 step: 1075, loss is 0.04119905084371567\n",
      "epoch: 7 step: 1076, loss is 0.0010586414719000459\n",
      "epoch: 7 step: 1077, loss is 0.014138163067400455\n",
      "epoch: 7 step: 1078, loss is 0.10576605051755905\n",
      "epoch: 7 step: 1079, loss is 0.0037058682646602392\n",
      "epoch: 7 step: 1080, loss is 0.07059020549058914\n",
      "epoch: 7 step: 1081, loss is 0.00846141204237938\n",
      "epoch: 7 step: 1082, loss is 0.0063872188329696655\n",
      "epoch: 7 step: 1083, loss is 0.012023193761706352\n",
      "epoch: 7 step: 1084, loss is 0.0010948538547381759\n",
      "epoch: 7 step: 1085, loss is 0.005838207900524139\n",
      "epoch: 7 step: 1086, loss is 0.048397246748209\n",
      "epoch: 7 step: 1087, loss is 0.02004227787256241\n",
      "epoch: 7 step: 1088, loss is 0.2066217064857483\n",
      "epoch: 7 step: 1089, loss is 0.009535582736134529\n",
      "epoch: 7 step: 1090, loss is 0.14687100052833557\n",
      "epoch: 7 step: 1091, loss is 0.11567249149084091\n",
      "epoch: 7 step: 1092, loss is 0.007913581095635891\n",
      "epoch: 7 step: 1093, loss is 0.017589179798960686\n",
      "epoch: 7 step: 1094, loss is 0.009227842092514038\n",
      "epoch: 7 step: 1095, loss is 0.008854895830154419\n",
      "epoch: 7 step: 1096, loss is 0.10293955355882645\n",
      "epoch: 7 step: 1097, loss is 0.04641910642385483\n",
      "epoch: 7 step: 1098, loss is 0.0012025446631014347\n",
      "epoch: 7 step: 1099, loss is 0.06428075581789017\n",
      "epoch: 7 step: 1100, loss is 0.03889675438404083\n",
      "epoch: 7 step: 1101, loss is 0.015473855659365654\n",
      "epoch: 7 step: 1102, loss is 0.01996370032429695\n",
      "epoch: 7 step: 1103, loss is 0.008849408477544785\n",
      "epoch: 7 step: 1104, loss is 0.08211836963891983\n",
      "epoch: 7 step: 1105, loss is 0.031214142218232155\n",
      "epoch: 7 step: 1106, loss is 0.015428630635142326\n",
      "epoch: 7 step: 1107, loss is 0.005696382839232683\n",
      "epoch: 7 step: 1108, loss is 0.016321634873747826\n",
      "epoch: 7 step: 1109, loss is 0.02940130978822708\n",
      "epoch: 7 step: 1110, loss is 0.01688028872013092\n",
      "epoch: 7 step: 1111, loss is 0.009944528341293335\n",
      "epoch: 7 step: 1112, loss is 0.006135637406259775\n",
      "epoch: 7 step: 1113, loss is 0.26080846786499023\n",
      "epoch: 7 step: 1114, loss is 0.007651688996702433\n",
      "epoch: 7 step: 1115, loss is 0.006005191244184971\n",
      "epoch: 7 step: 1116, loss is 0.003450989257544279\n",
      "epoch: 7 step: 1117, loss is 0.03203004598617554\n",
      "epoch: 7 step: 1118, loss is 0.019467996433377266\n",
      "epoch: 7 step: 1119, loss is 0.008657166734337807\n",
      "epoch: 7 step: 1120, loss is 0.02428261749446392\n",
      "epoch: 7 step: 1121, loss is 0.027896221727132797\n",
      "epoch: 7 step: 1122, loss is 0.043669600039720535\n",
      "epoch: 7 step: 1123, loss is 0.04278891161084175\n",
      "epoch: 7 step: 1124, loss is 0.03592924028635025\n",
      "epoch: 7 step: 1125, loss is 0.05776020884513855\n",
      "epoch: 7 step: 1126, loss is 0.0016526166582480073\n",
      "epoch: 7 step: 1127, loss is 0.018487732857465744\n",
      "epoch: 7 step: 1128, loss is 0.01254992000758648\n",
      "epoch: 7 step: 1129, loss is 0.0026397332549095154\n",
      "epoch: 7 step: 1130, loss is 0.020134635269641876\n",
      "epoch: 7 step: 1131, loss is 0.015208929777145386\n",
      "epoch: 7 step: 1132, loss is 0.0851895809173584\n",
      "epoch: 7 step: 1133, loss is 0.10409687459468842\n",
      "epoch: 7 step: 1134, loss is 5.6922337535070255e-05\n",
      "epoch: 7 step: 1135, loss is 0.014635903760790825\n",
      "epoch: 7 step: 1136, loss is 0.009885950945317745\n",
      "epoch: 7 step: 1137, loss is 0.006479742471128702\n",
      "epoch: 7 step: 1138, loss is 0.003914457280188799\n",
      "epoch: 7 step: 1139, loss is 0.03277689591050148\n",
      "epoch: 7 step: 1140, loss is 0.07377094775438309\n",
      "epoch: 7 step: 1141, loss is 0.2865673303604126\n",
      "epoch: 7 step: 1142, loss is 0.033854685723781586\n",
      "epoch: 7 step: 1143, loss is 0.013300411403179169\n",
      "epoch: 7 step: 1144, loss is 0.01088008750230074\n",
      "epoch: 7 step: 1145, loss is 0.12359408289194107\n",
      "epoch: 7 step: 1146, loss is 0.07712268084287643\n",
      "epoch: 7 step: 1147, loss is 0.05712760612368584\n",
      "epoch: 7 step: 1148, loss is 0.04591025784611702\n",
      "epoch: 7 step: 1149, loss is 0.01624719239771366\n",
      "epoch: 7 step: 1150, loss is 0.0026033453177660704\n",
      "epoch: 7 step: 1151, loss is 0.014612950384616852\n",
      "epoch: 7 step: 1152, loss is 0.15778157114982605\n",
      "epoch: 7 step: 1153, loss is 0.009362805634737015\n",
      "epoch: 7 step: 1154, loss is 0.007045987993478775\n",
      "epoch: 7 step: 1155, loss is 0.0014260633615776896\n",
      "epoch: 7 step: 1156, loss is 0.01532675325870514\n",
      "epoch: 7 step: 1157, loss is 0.0027856132946908474\n",
      "epoch: 7 step: 1158, loss is 0.04037773236632347\n",
      "epoch: 7 step: 1159, loss is 0.10578961670398712\n",
      "epoch: 7 step: 1160, loss is 0.0015854822704568505\n",
      "epoch: 7 step: 1161, loss is 0.0030467903707176447\n",
      "epoch: 7 step: 1162, loss is 0.07410930097103119\n",
      "epoch: 7 step: 1163, loss is 0.21634702384471893\n",
      "epoch: 7 step: 1164, loss is 0.07759272307157516\n",
      "epoch: 7 step: 1165, loss is 0.08688780665397644\n",
      "epoch: 7 step: 1166, loss is 0.20703458786010742\n",
      "epoch: 7 step: 1167, loss is 0.008867406286299229\n",
      "epoch: 7 step: 1168, loss is 0.016444562003016472\n",
      "epoch: 7 step: 1169, loss is 0.04542814940214157\n",
      "epoch: 7 step: 1170, loss is 0.011149564757943153\n",
      "epoch: 7 step: 1171, loss is 0.11748136579990387\n",
      "epoch: 7 step: 1172, loss is 0.000980716897174716\n",
      "epoch: 7 step: 1173, loss is 0.053344227373600006\n",
      "epoch: 7 step: 1174, loss is 0.06259825825691223\n",
      "epoch: 7 step: 1175, loss is 0.021311968564987183\n",
      "epoch: 7 step: 1176, loss is 0.12279719114303589\n",
      "epoch: 7 step: 1177, loss is 0.05804295465350151\n",
      "epoch: 7 step: 1178, loss is 0.0061014071106910706\n",
      "epoch: 7 step: 1179, loss is 0.018082022666931152\n",
      "epoch: 7 step: 1180, loss is 0.002824035007506609\n",
      "epoch: 7 step: 1181, loss is 0.06047297269105911\n",
      "epoch: 7 step: 1182, loss is 0.016417674720287323\n",
      "epoch: 7 step: 1183, loss is 0.005129882134497166\n",
      "epoch: 7 step: 1184, loss is 0.15145859122276306\n",
      "epoch: 7 step: 1185, loss is 0.07133349776268005\n",
      "epoch: 7 step: 1186, loss is 0.016184015199542046\n",
      "epoch: 7 step: 1187, loss is 0.06087828055024147\n",
      "epoch: 7 step: 1188, loss is 0.05589575693011284\n",
      "epoch: 7 step: 1189, loss is 0.01992751657962799\n",
      "epoch: 7 step: 1190, loss is 0.004584998358041048\n",
      "epoch: 7 step: 1191, loss is 0.08780308067798615\n",
      "epoch: 7 step: 1192, loss is 0.08359143137931824\n",
      "epoch: 7 step: 1193, loss is 0.006045025307685137\n",
      "epoch: 7 step: 1194, loss is 0.5271259546279907\n",
      "epoch: 7 step: 1195, loss is 0.041450925171375275\n",
      "epoch: 7 step: 1196, loss is 0.00320630706846714\n",
      "epoch: 7 step: 1197, loss is 0.039468321949243546\n",
      "epoch: 7 step: 1198, loss is 0.12138167023658752\n",
      "epoch: 7 step: 1199, loss is 0.0055077108554542065\n",
      "epoch: 7 step: 1200, loss is 0.00904126651585102\n",
      "epoch: 7 step: 1201, loss is 0.03221530839800835\n",
      "epoch: 7 step: 1202, loss is 0.002825689036399126\n",
      "epoch: 7 step: 1203, loss is 0.20585055649280548\n",
      "epoch: 7 step: 1204, loss is 0.06495021283626556\n",
      "epoch: 7 step: 1205, loss is 0.017195481806993484\n",
      "epoch: 7 step: 1206, loss is 0.009635810740292072\n",
      "epoch: 7 step: 1207, loss is 0.0002329259878024459\n",
      "epoch: 7 step: 1208, loss is 0.006970180198550224\n",
      "epoch: 7 step: 1209, loss is 0.0013116735499352217\n",
      "epoch: 7 step: 1210, loss is 0.048169542104005814\n",
      "epoch: 7 step: 1211, loss is 0.005401645787060261\n",
      "epoch: 7 step: 1212, loss is 0.0018143586348742247\n",
      "epoch: 7 step: 1213, loss is 0.043529439717531204\n",
      "epoch: 7 step: 1214, loss is 0.0015294820768758655\n",
      "epoch: 7 step: 1215, loss is 0.0755908340215683\n",
      "epoch: 7 step: 1216, loss is 0.009408035315573215\n",
      "epoch: 7 step: 1217, loss is 0.05655127763748169\n",
      "epoch: 7 step: 1218, loss is 0.0002669122186489403\n",
      "epoch: 7 step: 1219, loss is 0.037130679935216904\n",
      "epoch: 7 step: 1220, loss is 0.012599457055330276\n",
      "epoch: 7 step: 1221, loss is 0.0347527340054512\n",
      "epoch: 7 step: 1222, loss is 0.26809367537498474\n",
      "epoch: 7 step: 1223, loss is 0.0018790166359394789\n",
      "epoch: 7 step: 1224, loss is 0.000442264397861436\n",
      "epoch: 7 step: 1225, loss is 0.003673339495435357\n",
      "epoch: 7 step: 1226, loss is 0.0047567798756062984\n",
      "epoch: 7 step: 1227, loss is 0.0428384505212307\n",
      "epoch: 7 step: 1228, loss is 0.0457490049302578\n",
      "epoch: 7 step: 1229, loss is 0.01775592938065529\n",
      "epoch: 7 step: 1230, loss is 0.00021574244601652026\n",
      "epoch: 7 step: 1231, loss is 0.0011711244005709887\n",
      "epoch: 7 step: 1232, loss is 0.0025262823328375816\n",
      "epoch: 7 step: 1233, loss is 0.01119556836783886\n",
      "epoch: 7 step: 1234, loss is 0.02848302386701107\n",
      "epoch: 7 step: 1235, loss is 0.04389197379350662\n",
      "epoch: 7 step: 1236, loss is 0.07483018189668655\n",
      "epoch: 7 step: 1237, loss is 0.08683881908655167\n",
      "epoch: 7 step: 1238, loss is 0.07010751962661743\n",
      "epoch: 7 step: 1239, loss is 0.008072097785770893\n",
      "epoch: 7 step: 1240, loss is 0.03545932471752167\n",
      "epoch: 7 step: 1241, loss is 0.026448749005794525\n",
      "epoch: 7 step: 1242, loss is 0.002919501392170787\n",
      "epoch: 7 step: 1243, loss is 0.18052831292152405\n",
      "epoch: 7 step: 1244, loss is 0.006050110328942537\n",
      "epoch: 7 step: 1245, loss is 0.08265271037817001\n",
      "epoch: 7 step: 1246, loss is 0.003971015568822622\n",
      "epoch: 7 step: 1247, loss is 0.06252294778823853\n",
      "epoch: 7 step: 1248, loss is 0.10047289729118347\n",
      "epoch: 7 step: 1249, loss is 0.06576057523488998\n",
      "epoch: 7 step: 1250, loss is 0.08882207423448563\n",
      "epoch: 7 step: 1251, loss is 8.021265966817737e-05\n",
      "epoch: 7 step: 1252, loss is 0.03856997191905975\n",
      "epoch: 7 step: 1253, loss is 0.013615768402814865\n",
      "epoch: 7 step: 1254, loss is 0.00013424608914647251\n",
      "epoch: 7 step: 1255, loss is 0.10070820152759552\n",
      "epoch: 7 step: 1256, loss is 0.08400153368711472\n",
      "epoch: 7 step: 1257, loss is 0.011994992382824421\n",
      "epoch: 7 step: 1258, loss is 0.06497744470834732\n",
      "epoch: 7 step: 1259, loss is 0.005929635837674141\n",
      "epoch: 7 step: 1260, loss is 0.033361200243234634\n",
      "epoch: 7 step: 1261, loss is 0.001973014557734132\n",
      "epoch: 7 step: 1262, loss is 0.020490901544690132\n",
      "epoch: 7 step: 1263, loss is 0.12032972276210785\n",
      "epoch: 7 step: 1264, loss is 0.0006770681357011199\n",
      "epoch: 7 step: 1265, loss is 0.1668039858341217\n",
      "epoch: 7 step: 1266, loss is 0.012228022329509258\n",
      "epoch: 7 step: 1267, loss is 0.0021354048512876034\n",
      "epoch: 7 step: 1268, loss is 0.06844811141490936\n",
      "epoch: 7 step: 1269, loss is 0.0006965097272768617\n",
      "epoch: 7 step: 1270, loss is 0.015614181756973267\n",
      "epoch: 7 step: 1271, loss is 0.004330634605139494\n",
      "epoch: 7 step: 1272, loss is 0.0046148705296218395\n",
      "epoch: 7 step: 1273, loss is 0.00844052154570818\n",
      "epoch: 7 step: 1274, loss is 0.002936897100880742\n",
      "epoch: 7 step: 1275, loss is 0.015204040333628654\n",
      "epoch: 7 step: 1276, loss is 0.08544114232063293\n",
      "epoch: 7 step: 1277, loss is 0.6325791478157043\n",
      "epoch: 7 step: 1278, loss is 0.02098715305328369\n",
      "epoch: 7 step: 1279, loss is 0.004249099642038345\n",
      "epoch: 7 step: 1280, loss is 0.011839322745800018\n",
      "epoch: 7 step: 1281, loss is 0.0023801836650818586\n",
      "epoch: 7 step: 1282, loss is 0.012628396041691303\n",
      "epoch: 7 step: 1283, loss is 0.002257496817037463\n",
      "epoch: 7 step: 1284, loss is 0.13513034582138062\n",
      "epoch: 7 step: 1285, loss is 0.015109460800886154\n",
      "epoch: 7 step: 1286, loss is 0.03522619605064392\n",
      "epoch: 7 step: 1287, loss is 0.034058067947626114\n",
      "epoch: 7 step: 1288, loss is 0.025194713845849037\n",
      "epoch: 7 step: 1289, loss is 0.01748804934322834\n",
      "epoch: 7 step: 1290, loss is 0.007292541209608316\n",
      "epoch: 7 step: 1291, loss is 0.005636266898363829\n",
      "epoch: 7 step: 1292, loss is 0.0035208333283662796\n",
      "epoch: 7 step: 1293, loss is 0.1617605835199356\n",
      "epoch: 7 step: 1294, loss is 0.0005714904400520027\n",
      "epoch: 7 step: 1295, loss is 0.07728544622659683\n",
      "epoch: 7 step: 1296, loss is 0.025264307856559753\n",
      "epoch: 7 step: 1297, loss is 0.015516105107963085\n",
      "epoch: 7 step: 1298, loss is 0.035356007516384125\n",
      "epoch: 7 step: 1299, loss is 0.08063545823097229\n",
      "epoch: 7 step: 1300, loss is 0.03161618113517761\n",
      "epoch: 7 step: 1301, loss is 0.09062507003545761\n",
      "epoch: 7 step: 1302, loss is 0.17024989426136017\n",
      "epoch: 7 step: 1303, loss is 0.10224859416484833\n",
      "epoch: 7 step: 1304, loss is 0.00237088231369853\n",
      "epoch: 7 step: 1305, loss is 0.03510310500860214\n",
      "epoch: 7 step: 1306, loss is 0.017031529918313026\n",
      "epoch: 7 step: 1307, loss is 0.07729927450418472\n",
      "epoch: 7 step: 1308, loss is 0.24926169216632843\n",
      "epoch: 7 step: 1309, loss is 0.00792225357145071\n",
      "epoch: 7 step: 1310, loss is 0.09304061532020569\n",
      "epoch: 7 step: 1311, loss is 0.07936778664588928\n",
      "epoch: 7 step: 1312, loss is 0.006503652315586805\n",
      "epoch: 7 step: 1313, loss is 0.04721924290060997\n",
      "epoch: 7 step: 1314, loss is 0.0088191544637084\n",
      "epoch: 7 step: 1315, loss is 0.00082879897672683\n",
      "epoch: 7 step: 1316, loss is 0.027506962418556213\n",
      "epoch: 7 step: 1317, loss is 0.028747377917170525\n",
      "epoch: 7 step: 1318, loss is 0.011854581534862518\n",
      "epoch: 7 step: 1319, loss is 0.008004415780305862\n",
      "epoch: 7 step: 1320, loss is 0.014574753120541573\n",
      "epoch: 7 step: 1321, loss is 0.007757228333503008\n",
      "epoch: 7 step: 1322, loss is 0.003169771982356906\n",
      "epoch: 7 step: 1323, loss is 0.0012195021845400333\n",
      "epoch: 7 step: 1324, loss is 0.01418625470250845\n",
      "epoch: 7 step: 1325, loss is 0.0028921375051140785\n",
      "epoch: 7 step: 1326, loss is 0.008803275413811207\n",
      "epoch: 7 step: 1327, loss is 0.007675067521631718\n",
      "epoch: 7 step: 1328, loss is 0.026041077449917793\n",
      "epoch: 7 step: 1329, loss is 0.1110987514257431\n",
      "epoch: 7 step: 1330, loss is 0.01812840811908245\n",
      "epoch: 7 step: 1331, loss is 0.2019517719745636\n",
      "epoch: 7 step: 1332, loss is 0.0006582465721294284\n",
      "epoch: 7 step: 1333, loss is 0.14098693430423737\n",
      "epoch: 7 step: 1334, loss is 0.055188216269016266\n",
      "epoch: 7 step: 1335, loss is 0.05360942706465721\n",
      "epoch: 7 step: 1336, loss is 0.008270440623164177\n",
      "epoch: 7 step: 1337, loss is 0.12131866067647934\n",
      "epoch: 7 step: 1338, loss is 0.00019426042854320258\n",
      "epoch: 7 step: 1339, loss is 0.008223484270274639\n",
      "epoch: 7 step: 1340, loss is 0.0004619299725163728\n",
      "epoch: 7 step: 1341, loss is 0.05442722141742706\n",
      "epoch: 7 step: 1342, loss is 0.02934443950653076\n",
      "epoch: 7 step: 1343, loss is 0.07241135090589523\n",
      "epoch: 7 step: 1344, loss is 0.007935566827654839\n",
      "epoch: 7 step: 1345, loss is 0.021110091358423233\n",
      "epoch: 7 step: 1346, loss is 0.0020038068760186434\n",
      "epoch: 7 step: 1347, loss is 0.09291691333055496\n",
      "epoch: 7 step: 1348, loss is 0.0025948958937078714\n",
      "epoch: 7 step: 1349, loss is 0.004539011977612972\n",
      "epoch: 7 step: 1350, loss is 0.1091470792889595\n",
      "epoch: 7 step: 1351, loss is 0.004810437560081482\n",
      "epoch: 7 step: 1352, loss is 0.005344905890524387\n",
      "epoch: 7 step: 1353, loss is 0.003963940776884556\n",
      "epoch: 7 step: 1354, loss is 0.057028889656066895\n",
      "epoch: 7 step: 1355, loss is 0.06223118305206299\n",
      "epoch: 7 step: 1356, loss is 0.0474526509642601\n",
      "epoch: 7 step: 1357, loss is 0.0011380532523617148\n",
      "epoch: 7 step: 1358, loss is 0.005796596873551607\n",
      "epoch: 7 step: 1359, loss is 0.04888182878494263\n",
      "epoch: 7 step: 1360, loss is 0.11191844940185547\n",
      "epoch: 7 step: 1361, loss is 0.007531208451837301\n",
      "epoch: 7 step: 1362, loss is 0.2596273720264435\n",
      "epoch: 7 step: 1363, loss is 0.10852303355932236\n",
      "epoch: 7 step: 1364, loss is 0.015702875331044197\n",
      "epoch: 7 step: 1365, loss is 0.006191186141222715\n",
      "epoch: 7 step: 1366, loss is 0.1594526618719101\n",
      "epoch: 7 step: 1367, loss is 0.029702041298151016\n",
      "epoch: 7 step: 1368, loss is 0.005764373112469912\n",
      "epoch: 7 step: 1369, loss is 0.011364884674549103\n",
      "epoch: 7 step: 1370, loss is 0.008228454738855362\n",
      "epoch: 7 step: 1371, loss is 0.005316011607646942\n",
      "epoch: 7 step: 1372, loss is 0.00019279604020994157\n",
      "epoch: 7 step: 1373, loss is 0.0036740382201969624\n",
      "epoch: 7 step: 1374, loss is 0.0007154320483095944\n",
      "epoch: 7 step: 1375, loss is 0.0025425665080547333\n",
      "epoch: 7 step: 1376, loss is 0.022139381617307663\n",
      "epoch: 7 step: 1377, loss is 0.12189291417598724\n",
      "epoch: 7 step: 1378, loss is 0.0007191200857050717\n",
      "epoch: 7 step: 1379, loss is 0.0893695205450058\n",
      "epoch: 7 step: 1380, loss is 0.0011914066271856427\n",
      "epoch: 7 step: 1381, loss is 0.0026552926283329725\n",
      "epoch: 7 step: 1382, loss is 0.12397395074367523\n",
      "epoch: 7 step: 1383, loss is 0.1079106330871582\n",
      "epoch: 7 step: 1384, loss is 0.07385418564081192\n",
      "epoch: 7 step: 1385, loss is 0.03309784084558487\n",
      "epoch: 7 step: 1386, loss is 0.029150275513529778\n",
      "epoch: 7 step: 1387, loss is 0.018736789003014565\n",
      "epoch: 7 step: 1388, loss is 0.012037709355354309\n",
      "epoch: 7 step: 1389, loss is 0.07463493943214417\n",
      "epoch: 7 step: 1390, loss is 0.0853135883808136\n",
      "epoch: 7 step: 1391, loss is 0.00565857719630003\n",
      "epoch: 7 step: 1392, loss is 0.08716544508934021\n",
      "epoch: 7 step: 1393, loss is 0.03213170915842056\n",
      "epoch: 7 step: 1394, loss is 0.0071813310496509075\n",
      "epoch: 7 step: 1395, loss is 0.1499837189912796\n",
      "epoch: 7 step: 1396, loss is 0.04455351456999779\n",
      "epoch: 7 step: 1397, loss is 0.006592877674847841\n",
      "epoch: 7 step: 1398, loss is 0.05123109742999077\n",
      "epoch: 7 step: 1399, loss is 0.04062937945127487\n",
      "epoch: 7 step: 1400, loss is 0.11284222453832626\n",
      "epoch: 7 step: 1401, loss is 0.11476285010576248\n",
      "epoch: 7 step: 1402, loss is 0.01586449146270752\n",
      "epoch: 7 step: 1403, loss is 0.004074521828442812\n",
      "epoch: 7 step: 1404, loss is 0.08369390666484833\n",
      "epoch: 7 step: 1405, loss is 0.031662750989198685\n",
      "epoch: 7 step: 1406, loss is 0.020609907805919647\n",
      "epoch: 7 step: 1407, loss is 0.006028729025274515\n",
      "epoch: 7 step: 1408, loss is 0.15433210134506226\n",
      "epoch: 7 step: 1409, loss is 0.0013340505538508296\n",
      "epoch: 7 step: 1410, loss is 0.16045792400836945\n",
      "epoch: 7 step: 1411, loss is 0.002804775256663561\n",
      "epoch: 7 step: 1412, loss is 0.0005370285362005234\n",
      "epoch: 7 step: 1413, loss is 0.0061738137155771255\n",
      "epoch: 7 step: 1414, loss is 0.005475552286952734\n",
      "epoch: 7 step: 1415, loss is 0.04328780993819237\n",
      "epoch: 7 step: 1416, loss is 0.0007333316025324166\n",
      "epoch: 7 step: 1417, loss is 0.0634387731552124\n",
      "epoch: 7 step: 1418, loss is 0.000986858387477696\n",
      "epoch: 7 step: 1419, loss is 0.0004305079928599298\n",
      "epoch: 7 step: 1420, loss is 0.0016008636448532343\n",
      "epoch: 7 step: 1421, loss is 0.008575431071221828\n",
      "epoch: 7 step: 1422, loss is 0.04478049650788307\n",
      "epoch: 7 step: 1423, loss is 0.07180626690387726\n",
      "epoch: 7 step: 1424, loss is 0.044397078454494476\n",
      "epoch: 7 step: 1425, loss is 0.010425849817693233\n",
      "epoch: 7 step: 1426, loss is 0.1313103586435318\n",
      "epoch: 7 step: 1427, loss is 0.04987790435552597\n",
      "epoch: 7 step: 1428, loss is 0.07614990323781967\n",
      "epoch: 7 step: 1429, loss is 0.002030980307608843\n",
      "epoch: 7 step: 1430, loss is 0.030806610360741615\n",
      "epoch: 7 step: 1431, loss is 0.0062073711305856705\n",
      "epoch: 7 step: 1432, loss is 0.00891775730997324\n",
      "epoch: 7 step: 1433, loss is 0.1898943930864334\n",
      "epoch: 7 step: 1434, loss is 0.02133418433368206\n",
      "epoch: 7 step: 1435, loss is 0.0053567527793347836\n",
      "epoch: 7 step: 1436, loss is 0.022045517340302467\n",
      "epoch: 7 step: 1437, loss is 0.002179165603592992\n",
      "epoch: 7 step: 1438, loss is 0.08011753112077713\n",
      "epoch: 7 step: 1439, loss is 0.004379527643322945\n",
      "epoch: 7 step: 1440, loss is 0.05007127672433853\n",
      "epoch: 7 step: 1441, loss is 0.05236881598830223\n",
      "epoch: 7 step: 1442, loss is 0.0008344328962266445\n",
      "epoch: 7 step: 1443, loss is 0.14330020546913147\n",
      "epoch: 7 step: 1444, loss is 0.013003173284232616\n",
      "epoch: 7 step: 1445, loss is 0.08862261474132538\n",
      "epoch: 7 step: 1446, loss is 0.003315478563308716\n",
      "epoch: 7 step: 1447, loss is 0.10061311721801758\n",
      "epoch: 7 step: 1448, loss is 0.030900809913873672\n",
      "epoch: 7 step: 1449, loss is 0.1515335738658905\n",
      "epoch: 7 step: 1450, loss is 0.004600544460117817\n",
      "epoch: 7 step: 1451, loss is 0.01566346548497677\n",
      "epoch: 7 step: 1452, loss is 0.060181375592947006\n",
      "epoch: 7 step: 1453, loss is 0.17607033252716064\n",
      "epoch: 7 step: 1454, loss is 0.006918145343661308\n",
      "epoch: 7 step: 1455, loss is 0.1390780359506607\n",
      "epoch: 7 step: 1456, loss is 0.17688307166099548\n",
      "epoch: 7 step: 1457, loss is 0.10677709430456161\n",
      "epoch: 7 step: 1458, loss is 0.04466283693909645\n",
      "epoch: 7 step: 1459, loss is 0.015030143782496452\n",
      "epoch: 7 step: 1460, loss is 0.14223872125148773\n",
      "epoch: 7 step: 1461, loss is 0.0012679190840572119\n",
      "epoch: 7 step: 1462, loss is 0.07874532788991928\n",
      "epoch: 7 step: 1463, loss is 0.005799470003694296\n",
      "epoch: 7 step: 1464, loss is 0.12717382609844208\n",
      "epoch: 7 step: 1465, loss is 0.011182831600308418\n",
      "epoch: 7 step: 1466, loss is 0.013627070002257824\n",
      "epoch: 7 step: 1467, loss is 0.009848538786172867\n",
      "epoch: 7 step: 1468, loss is 0.10230087488889694\n",
      "epoch: 7 step: 1469, loss is 0.0006996134179644287\n",
      "epoch: 7 step: 1470, loss is 0.005756141617894173\n",
      "epoch: 7 step: 1471, loss is 0.0003462257736828178\n",
      "epoch: 7 step: 1472, loss is 0.10348442196846008\n",
      "epoch: 7 step: 1473, loss is 0.03207289054989815\n",
      "epoch: 7 step: 1474, loss is 0.0034966175444424152\n",
      "epoch: 7 step: 1475, loss is 0.0069921561516821384\n",
      "epoch: 7 step: 1476, loss is 0.018234267830848694\n",
      "epoch: 7 step: 1477, loss is 0.006983091123402119\n",
      "epoch: 7 step: 1478, loss is 0.020018063485622406\n",
      "epoch: 7 step: 1479, loss is 0.019230995327234268\n",
      "epoch: 7 step: 1480, loss is 0.054357659071683884\n",
      "epoch: 7 step: 1481, loss is 0.017268521711230278\n",
      "epoch: 7 step: 1482, loss is 0.007260509766638279\n",
      "epoch: 7 step: 1483, loss is 0.19957977533340454\n",
      "epoch: 7 step: 1484, loss is 0.003180725732818246\n",
      "epoch: 7 step: 1485, loss is 0.014919915236532688\n",
      "epoch: 7 step: 1486, loss is 0.004115112591534853\n",
      "epoch: 7 step: 1487, loss is 0.09117931127548218\n",
      "epoch: 7 step: 1488, loss is 0.22020989656448364\n",
      "epoch: 7 step: 1489, loss is 0.0074308752082288265\n",
      "epoch: 7 step: 1490, loss is 0.0019408544758334756\n",
      "epoch: 7 step: 1491, loss is 0.041520897299051285\n",
      "epoch: 7 step: 1492, loss is 0.033451490104198456\n",
      "epoch: 7 step: 1493, loss is 0.06977467238903046\n",
      "epoch: 7 step: 1494, loss is 0.012755940668284893\n",
      "epoch: 7 step: 1495, loss is 0.10334714502096176\n",
      "epoch: 7 step: 1496, loss is 0.003574185539036989\n",
      "epoch: 7 step: 1497, loss is 0.000981535529717803\n",
      "epoch: 7 step: 1498, loss is 0.003231689566746354\n",
      "epoch: 7 step: 1499, loss is 0.009356996975839138\n",
      "epoch: 7 step: 1500, loss is 0.0034885567147284746\n",
      "epoch: 7 step: 1501, loss is 0.017973346635699272\n",
      "epoch: 7 step: 1502, loss is 0.004294681828469038\n",
      "epoch: 7 step: 1503, loss is 0.18660368025302887\n",
      "epoch: 7 step: 1504, loss is 0.02622179500758648\n",
      "epoch: 7 step: 1505, loss is 0.24935856461524963\n",
      "epoch: 7 step: 1506, loss is 0.2893230617046356\n",
      "epoch: 7 step: 1507, loss is 0.06712178140878677\n",
      "epoch: 7 step: 1508, loss is 0.09730559587478638\n",
      "epoch: 7 step: 1509, loss is 0.010625731199979782\n",
      "epoch: 7 step: 1510, loss is 0.025821702554821968\n",
      "epoch: 7 step: 1511, loss is 0.0189176257699728\n",
      "epoch: 7 step: 1512, loss is 0.0933593362569809\n",
      "epoch: 7 step: 1513, loss is 0.0031039377208799124\n",
      "epoch: 7 step: 1514, loss is 0.016258403658866882\n",
      "epoch: 7 step: 1515, loss is 0.06133990362286568\n",
      "epoch: 7 step: 1516, loss is 0.028936538845300674\n",
      "epoch: 7 step: 1517, loss is 0.0043358951807022095\n",
      "epoch: 7 step: 1518, loss is 0.06967967003583908\n",
      "epoch: 7 step: 1519, loss is 0.07969366759061813\n",
      "epoch: 7 step: 1520, loss is 0.0008768588304519653\n",
      "epoch: 7 step: 1521, loss is 0.0819624736905098\n",
      "epoch: 7 step: 1522, loss is 0.04196770861744881\n",
      "epoch: 7 step: 1523, loss is 0.04069380462169647\n",
      "epoch: 7 step: 1524, loss is 0.04377228021621704\n",
      "epoch: 7 step: 1525, loss is 0.0017114320071414113\n",
      "epoch: 7 step: 1526, loss is 0.03620447590947151\n",
      "epoch: 7 step: 1527, loss is 0.007600737735629082\n",
      "epoch: 7 step: 1528, loss is 0.12460244446992874\n",
      "epoch: 7 step: 1529, loss is 0.014457227662205696\n",
      "epoch: 7 step: 1530, loss is 0.04405341297388077\n",
      "epoch: 7 step: 1531, loss is 0.0674562156200409\n",
      "epoch: 7 step: 1532, loss is 0.08948466926813126\n",
      "epoch: 7 step: 1533, loss is 0.010031010955572128\n",
      "epoch: 7 step: 1534, loss is 0.04208860173821449\n",
      "epoch: 7 step: 1535, loss is 0.02564193680882454\n",
      "epoch: 7 step: 1536, loss is 0.00777824129909277\n",
      "epoch: 7 step: 1537, loss is 0.0070066992193460464\n",
      "epoch: 7 step: 1538, loss is 0.006454091984778643\n",
      "epoch: 7 step: 1539, loss is 0.10026219487190247\n",
      "epoch: 7 step: 1540, loss is 0.008372913114726543\n",
      "epoch: 7 step: 1541, loss is 0.009842535480856895\n",
      "epoch: 7 step: 1542, loss is 0.0004572231264319271\n",
      "epoch: 7 step: 1543, loss is 0.002133897738531232\n",
      "epoch: 7 step: 1544, loss is 0.08105092495679855\n",
      "epoch: 7 step: 1545, loss is 0.006359632592648268\n",
      "epoch: 7 step: 1546, loss is 0.0010197553783655167\n",
      "epoch: 7 step: 1547, loss is 0.0077048735693097115\n",
      "epoch: 7 step: 1548, loss is 0.00976024940609932\n",
      "epoch: 7 step: 1549, loss is 0.0003956378495786339\n",
      "epoch: 7 step: 1550, loss is 0.012737048789858818\n",
      "epoch: 7 step: 1551, loss is 0.028409255668520927\n",
      "epoch: 7 step: 1552, loss is 0.18346364796161652\n",
      "epoch: 7 step: 1553, loss is 0.006210329942405224\n",
      "epoch: 7 step: 1554, loss is 0.03945012390613556\n",
      "epoch: 7 step: 1555, loss is 0.008856507018208504\n",
      "epoch: 7 step: 1556, loss is 0.024209771305322647\n",
      "epoch: 7 step: 1557, loss is 0.016029303893446922\n",
      "epoch: 7 step: 1558, loss is 0.01604268327355385\n",
      "epoch: 7 step: 1559, loss is 0.07661544531583786\n",
      "epoch: 7 step: 1560, loss is 0.066379114985466\n",
      "epoch: 7 step: 1561, loss is 0.010346322320401669\n",
      "epoch: 7 step: 1562, loss is 0.002139214426279068\n",
      "epoch: 7 step: 1563, loss is 0.002716389251872897\n",
      "epoch: 7 step: 1564, loss is 0.0032543656416237354\n",
      "epoch: 7 step: 1565, loss is 0.0007066394318826497\n",
      "epoch: 7 step: 1566, loss is 0.07315651327371597\n",
      "epoch: 7 step: 1567, loss is 0.004351082257926464\n",
      "epoch: 7 step: 1568, loss is 0.008382266387343407\n",
      "epoch: 7 step: 1569, loss is 0.03992433845996857\n",
      "epoch: 7 step: 1570, loss is 0.0014572885120287538\n",
      "epoch: 7 step: 1571, loss is 0.004726302344352007\n",
      "epoch: 7 step: 1572, loss is 0.0946805328130722\n",
      "epoch: 7 step: 1573, loss is 0.04074264317750931\n",
      "epoch: 7 step: 1574, loss is 0.01461521815508604\n",
      "epoch: 7 step: 1575, loss is 0.09600085765123367\n",
      "epoch: 7 step: 1576, loss is 0.15064789354801178\n",
      "epoch: 7 step: 1577, loss is 0.0028812347445636988\n",
      "epoch: 7 step: 1578, loss is 0.001686207135207951\n",
      "epoch: 7 step: 1579, loss is 0.2538956105709076\n",
      "epoch: 7 step: 1580, loss is 0.06001146510243416\n",
      "epoch: 7 step: 1581, loss is 0.005072931759059429\n",
      "epoch: 7 step: 1582, loss is 0.03126712143421173\n",
      "epoch: 7 step: 1583, loss is 0.1407906413078308\n",
      "epoch: 7 step: 1584, loss is 0.03779952600598335\n",
      "epoch: 7 step: 1585, loss is 0.017390195280313492\n",
      "epoch: 7 step: 1586, loss is 0.002749738283455372\n",
      "epoch: 7 step: 1587, loss is 0.0009468927164562047\n",
      "epoch: 7 step: 1588, loss is 0.03834979236125946\n",
      "epoch: 7 step: 1589, loss is 0.05803452432155609\n",
      "epoch: 7 step: 1590, loss is 0.008028801530599594\n",
      "epoch: 7 step: 1591, loss is 0.002555175917223096\n",
      "epoch: 7 step: 1592, loss is 0.1333259493112564\n",
      "epoch: 7 step: 1593, loss is 0.003914297558367252\n",
      "epoch: 7 step: 1594, loss is 0.011274571530520916\n",
      "epoch: 7 step: 1595, loss is 0.0028830240480601788\n",
      "epoch: 7 step: 1596, loss is 0.06918365508317947\n",
      "epoch: 7 step: 1597, loss is 0.0016937705222517252\n",
      "epoch: 7 step: 1598, loss is 0.07289690524339676\n",
      "epoch: 7 step: 1599, loss is 0.020781829953193665\n",
      "epoch: 7 step: 1600, loss is 0.08623462170362473\n",
      "epoch: 7 step: 1601, loss is 0.013765370473265648\n",
      "epoch: 7 step: 1602, loss is 0.0014459429075941443\n",
      "epoch: 7 step: 1603, loss is 0.0012059068540111184\n",
      "epoch: 7 step: 1604, loss is 0.005438822787255049\n",
      "epoch: 7 step: 1605, loss is 0.012421156279742718\n",
      "epoch: 7 step: 1606, loss is 0.01276857778429985\n",
      "epoch: 7 step: 1607, loss is 0.0033909145276993513\n",
      "epoch: 7 step: 1608, loss is 0.08367948234081268\n",
      "epoch: 7 step: 1609, loss is 0.06365520507097244\n",
      "epoch: 7 step: 1610, loss is 0.001523121609352529\n",
      "epoch: 7 step: 1611, loss is 0.017546240240335464\n",
      "epoch: 7 step: 1612, loss is 0.022174490615725517\n",
      "epoch: 7 step: 1613, loss is 0.21100440621376038\n",
      "epoch: 7 step: 1614, loss is 0.0959789976477623\n",
      "epoch: 7 step: 1615, loss is 0.0006126748630777001\n",
      "epoch: 7 step: 1616, loss is 0.0013209846802055836\n",
      "epoch: 7 step: 1617, loss is 0.09008772671222687\n",
      "epoch: 7 step: 1618, loss is 0.009172957390546799\n",
      "epoch: 7 step: 1619, loss is 0.01583893410861492\n",
      "epoch: 7 step: 1620, loss is 0.006899163126945496\n",
      "epoch: 7 step: 1621, loss is 0.012823643162846565\n",
      "epoch: 7 step: 1622, loss is 0.08216346055269241\n",
      "epoch: 7 step: 1623, loss is 0.0007265189196914434\n",
      "epoch: 7 step: 1624, loss is 0.03647823631763458\n",
      "epoch: 7 step: 1625, loss is 0.043695732951164246\n",
      "epoch: 7 step: 1626, loss is 0.007905848324298859\n",
      "epoch: 7 step: 1627, loss is 0.06621121615171432\n",
      "epoch: 7 step: 1628, loss is 0.29174724221229553\n",
      "epoch: 7 step: 1629, loss is 0.0013692914508283138\n",
      "epoch: 7 step: 1630, loss is 0.03456949442625046\n",
      "epoch: 7 step: 1631, loss is 0.19227087497711182\n",
      "epoch: 7 step: 1632, loss is 0.011850218288600445\n",
      "epoch: 7 step: 1633, loss is 0.007452176418155432\n",
      "epoch: 7 step: 1634, loss is 0.005266091786324978\n",
      "epoch: 7 step: 1635, loss is 0.027493365108966827\n",
      "epoch: 7 step: 1636, loss is 0.04187588021159172\n",
      "epoch: 7 step: 1637, loss is 0.02799612656235695\n",
      "epoch: 7 step: 1638, loss is 0.06958509981632233\n",
      "epoch: 7 step: 1639, loss is 0.03696572408080101\n",
      "epoch: 7 step: 1640, loss is 0.015821637585759163\n",
      "epoch: 7 step: 1641, loss is 0.014983784407377243\n",
      "epoch: 7 step: 1642, loss is 0.004360885825008154\n",
      "epoch: 7 step: 1643, loss is 0.06212795153260231\n",
      "epoch: 7 step: 1644, loss is 0.006901601795107126\n",
      "epoch: 7 step: 1645, loss is 0.10958943516016006\n",
      "epoch: 7 step: 1646, loss is 0.013134591281414032\n",
      "epoch: 7 step: 1647, loss is 0.01676039770245552\n",
      "epoch: 7 step: 1648, loss is 0.14928723871707916\n",
      "epoch: 7 step: 1649, loss is 0.2069045752286911\n",
      "epoch: 7 step: 1650, loss is 0.0016710932832211256\n",
      "epoch: 7 step: 1651, loss is 0.026268193498253822\n",
      "epoch: 7 step: 1652, loss is 0.01271516177803278\n",
      "epoch: 7 step: 1653, loss is 0.02238413318991661\n",
      "epoch: 7 step: 1654, loss is 0.00136620516423136\n",
      "epoch: 7 step: 1655, loss is 0.002151699271053076\n",
      "epoch: 7 step: 1656, loss is 0.009988801553845406\n",
      "epoch: 7 step: 1657, loss is 0.07834535837173462\n",
      "epoch: 7 step: 1658, loss is 0.021492771804332733\n",
      "epoch: 7 step: 1659, loss is 0.04328702762722969\n",
      "epoch: 7 step: 1660, loss is 0.00015024446474853903\n",
      "epoch: 7 step: 1661, loss is 0.108799047768116\n",
      "epoch: 7 step: 1662, loss is 0.08850476145744324\n",
      "epoch: 7 step: 1663, loss is 0.006314274854958057\n",
      "epoch: 7 step: 1664, loss is 0.15776154398918152\n",
      "epoch: 7 step: 1665, loss is 0.0029737118165940046\n",
      "epoch: 7 step: 1666, loss is 0.029768653213977814\n",
      "epoch: 7 step: 1667, loss is 0.025581195950508118\n",
      "epoch: 7 step: 1668, loss is 0.0026814984157681465\n",
      "epoch: 7 step: 1669, loss is 0.0187262911349535\n",
      "epoch: 7 step: 1670, loss is 0.00528096966445446\n",
      "epoch: 7 step: 1671, loss is 0.0008310336852446198\n",
      "epoch: 7 step: 1672, loss is 0.10385750979185104\n",
      "epoch: 7 step: 1673, loss is 0.2458006590604782\n",
      "epoch: 7 step: 1674, loss is 0.013290801085531712\n",
      "epoch: 7 step: 1675, loss is 0.008594922721385956\n",
      "epoch: 7 step: 1676, loss is 0.027845950797200203\n",
      "epoch: 7 step: 1677, loss is 0.25016599893569946\n",
      "epoch: 7 step: 1678, loss is 0.011375732719898224\n",
      "epoch: 7 step: 1679, loss is 0.0015181797789409757\n",
      "epoch: 7 step: 1680, loss is 0.043309248983860016\n",
      "epoch: 7 step: 1681, loss is 0.027674047276377678\n",
      "epoch: 7 step: 1682, loss is 0.011648253537714481\n",
      "epoch: 7 step: 1683, loss is 0.08555704355239868\n",
      "epoch: 7 step: 1684, loss is 0.04692310467362404\n",
      "epoch: 7 step: 1685, loss is 0.0008743219659663737\n",
      "epoch: 7 step: 1686, loss is 0.07584844529628754\n",
      "epoch: 7 step: 1687, loss is 0.002637171186506748\n",
      "epoch: 7 step: 1688, loss is 0.08878225088119507\n",
      "epoch: 7 step: 1689, loss is 0.13922755420207977\n",
      "epoch: 7 step: 1690, loss is 0.29978498816490173\n",
      "epoch: 7 step: 1691, loss is 0.018676117062568665\n",
      "epoch: 7 step: 1692, loss is 0.10198817402124405\n",
      "epoch: 7 step: 1693, loss is 0.2723439037799835\n",
      "epoch: 7 step: 1694, loss is 0.06108161807060242\n",
      "epoch: 7 step: 1695, loss is 0.03656797111034393\n",
      "epoch: 7 step: 1696, loss is 0.006746332161128521\n",
      "epoch: 7 step: 1697, loss is 0.007165426854044199\n",
      "epoch: 7 step: 1698, loss is 0.003609734121710062\n",
      "epoch: 7 step: 1699, loss is 0.0029915878549218178\n",
      "epoch: 7 step: 1700, loss is 0.16666162014007568\n",
      "epoch: 7 step: 1701, loss is 0.006447276100516319\n",
      "epoch: 7 step: 1702, loss is 0.03749286010861397\n",
      "epoch: 7 step: 1703, loss is 0.021118368953466415\n",
      "epoch: 7 step: 1704, loss is 0.04839315265417099\n",
      "epoch: 7 step: 1705, loss is 0.03647392988204956\n",
      "epoch: 7 step: 1706, loss is 0.07162115722894669\n",
      "epoch: 7 step: 1707, loss is 0.06220662221312523\n",
      "epoch: 7 step: 1708, loss is 0.01563749834895134\n",
      "epoch: 7 step: 1709, loss is 0.036872949451208115\n",
      "epoch: 7 step: 1710, loss is 0.1762773096561432\n",
      "epoch: 7 step: 1711, loss is 0.1232220008969307\n",
      "epoch: 7 step: 1712, loss is 0.010039245709776878\n",
      "epoch: 7 step: 1713, loss is 0.014932063408195972\n",
      "epoch: 7 step: 1714, loss is 0.0317561961710453\n",
      "epoch: 7 step: 1715, loss is 0.04164357855916023\n",
      "epoch: 7 step: 1716, loss is 0.01081305742263794\n",
      "epoch: 7 step: 1717, loss is 0.0046804919838905334\n",
      "epoch: 7 step: 1718, loss is 0.01716490648686886\n",
      "epoch: 7 step: 1719, loss is 0.009442506358027458\n",
      "epoch: 7 step: 1720, loss is 0.000921296130400151\n",
      "epoch: 7 step: 1721, loss is 0.0025495567824691534\n",
      "epoch: 7 step: 1722, loss is 0.014902428723871708\n",
      "epoch: 7 step: 1723, loss is 0.06949542462825775\n",
      "epoch: 7 step: 1724, loss is 0.1555604189634323\n",
      "epoch: 7 step: 1725, loss is 0.04612831771373749\n",
      "epoch: 7 step: 1726, loss is 0.0016566484700888395\n",
      "epoch: 7 step: 1727, loss is 0.016543712466955185\n",
      "epoch: 7 step: 1728, loss is 0.019553417339920998\n",
      "epoch: 7 step: 1729, loss is 0.018673034384846687\n",
      "epoch: 7 step: 1730, loss is 0.0043998099863529205\n",
      "epoch: 7 step: 1731, loss is 0.1268676221370697\n",
      "epoch: 7 step: 1732, loss is 0.00155858404468745\n",
      "epoch: 7 step: 1733, loss is 0.04808536171913147\n",
      "epoch: 7 step: 1734, loss is 0.014809801243245602\n",
      "epoch: 7 step: 1735, loss is 0.03076327219605446\n",
      "epoch: 7 step: 1736, loss is 0.006265938747674227\n",
      "epoch: 7 step: 1737, loss is 0.016233010217547417\n",
      "epoch: 7 step: 1738, loss is 0.0036646684166043997\n",
      "epoch: 7 step: 1739, loss is 0.02419574372470379\n",
      "epoch: 7 step: 1740, loss is 0.006867909803986549\n",
      "epoch: 7 step: 1741, loss is 0.018817614763975143\n",
      "epoch: 7 step: 1742, loss is 0.07047490775585175\n",
      "epoch: 7 step: 1743, loss is 0.038515686988830566\n",
      "epoch: 7 step: 1744, loss is 0.0049232495948672295\n",
      "epoch: 7 step: 1745, loss is 0.02583565004169941\n",
      "epoch: 7 step: 1746, loss is 0.007112571969628334\n",
      "epoch: 7 step: 1747, loss is 0.018647337332367897\n",
      "epoch: 7 step: 1748, loss is 0.0027603995986282825\n",
      "epoch: 7 step: 1749, loss is 0.02086978778243065\n",
      "epoch: 7 step: 1750, loss is 0.002175777917727828\n",
      "epoch: 7 step: 1751, loss is 0.06575627624988556\n",
      "epoch: 7 step: 1752, loss is 0.00840998999774456\n",
      "epoch: 7 step: 1753, loss is 0.0008982065482996404\n",
      "epoch: 7 step: 1754, loss is 0.04786240682005882\n",
      "epoch: 7 step: 1755, loss is 0.0052894167602062225\n",
      "epoch: 7 step: 1756, loss is 0.03186087682843208\n",
      "epoch: 7 step: 1757, loss is 0.0032612544018775225\n",
      "epoch: 7 step: 1758, loss is 0.002105527790263295\n",
      "epoch: 7 step: 1759, loss is 0.0009169962722808123\n",
      "epoch: 7 step: 1760, loss is 0.004355609882622957\n",
      "epoch: 7 step: 1761, loss is 0.04113565385341644\n",
      "epoch: 7 step: 1762, loss is 0.0011953632347285748\n",
      "epoch: 7 step: 1763, loss is 0.00047960085794329643\n",
      "epoch: 7 step: 1764, loss is 0.07093337923288345\n",
      "epoch: 7 step: 1765, loss is 0.0020141899585723877\n",
      "epoch: 7 step: 1766, loss is 0.0011327068787068129\n",
      "epoch: 7 step: 1767, loss is 0.026993289589881897\n",
      "epoch: 7 step: 1768, loss is 0.005655872635543346\n",
      "epoch: 7 step: 1769, loss is 0.012052210047841072\n",
      "epoch: 7 step: 1770, loss is 0.0005551284411922097\n",
      "epoch: 7 step: 1771, loss is 0.004128338303416967\n",
      "epoch: 7 step: 1772, loss is 0.01095910370349884\n",
      "epoch: 7 step: 1773, loss is 0.004052458796650171\n",
      "epoch: 7 step: 1774, loss is 0.021224772557616234\n",
      "epoch: 7 step: 1775, loss is 0.06017032265663147\n",
      "epoch: 7 step: 1776, loss is 0.002060917904600501\n",
      "epoch: 7 step: 1777, loss is 0.0023576952517032623\n",
      "epoch: 7 step: 1778, loss is 0.07617869228124619\n",
      "epoch: 7 step: 1779, loss is 0.0036662619095295668\n",
      "epoch: 7 step: 1780, loss is 0.04137616232037544\n",
      "epoch: 7 step: 1781, loss is 0.025122594088315964\n",
      "epoch: 7 step: 1782, loss is 0.07910130172967911\n",
      "epoch: 7 step: 1783, loss is 0.04437907785177231\n",
      "epoch: 7 step: 1784, loss is 0.0014566084137186408\n",
      "epoch: 7 step: 1785, loss is 0.023599229753017426\n",
      "epoch: 7 step: 1786, loss is 0.11229013651609421\n",
      "epoch: 7 step: 1787, loss is 0.007491430267691612\n",
      "epoch: 7 step: 1788, loss is 0.0047803400084376335\n",
      "epoch: 7 step: 1789, loss is 0.04163547605276108\n",
      "epoch: 7 step: 1790, loss is 0.004737000912427902\n",
      "epoch: 7 step: 1791, loss is 0.09613267332315445\n",
      "epoch: 7 step: 1792, loss is 0.010658957064151764\n",
      "epoch: 7 step: 1793, loss is 0.033429838716983795\n",
      "epoch: 7 step: 1794, loss is 0.0776919424533844\n",
      "epoch: 7 step: 1795, loss is 0.012729671783745289\n",
      "epoch: 7 step: 1796, loss is 0.03638759255409241\n",
      "epoch: 7 step: 1797, loss is 0.004938931670039892\n",
      "epoch: 7 step: 1798, loss is 0.0038392147980630398\n",
      "epoch: 7 step: 1799, loss is 0.053268756717443466\n",
      "epoch: 7 step: 1800, loss is 0.007366107311099768\n",
      "epoch: 7 step: 1801, loss is 0.021551914513111115\n",
      "epoch: 7 step: 1802, loss is 0.008470187894999981\n",
      "epoch: 7 step: 1803, loss is 0.07906760275363922\n",
      "epoch: 7 step: 1804, loss is 0.004067943897098303\n",
      "epoch: 7 step: 1805, loss is 0.001018739421851933\n",
      "epoch: 7 step: 1806, loss is 0.0636591911315918\n",
      "epoch: 7 step: 1807, loss is 0.020390352234244347\n",
      "epoch: 7 step: 1808, loss is 0.02503359504044056\n",
      "epoch: 7 step: 1809, loss is 0.09459558874368668\n",
      "epoch: 7 step: 1810, loss is 0.002908836817368865\n",
      "epoch: 7 step: 1811, loss is 0.0004918997292406857\n",
      "epoch: 7 step: 1812, loss is 0.005587373860180378\n",
      "epoch: 7 step: 1813, loss is 0.1326943188905716\n",
      "epoch: 7 step: 1814, loss is 0.011933124624192715\n",
      "epoch: 7 step: 1815, loss is 0.013166887685656548\n",
      "epoch: 7 step: 1816, loss is 0.010071535594761372\n",
      "epoch: 7 step: 1817, loss is 0.018907027319073677\n",
      "epoch: 7 step: 1818, loss is 0.014150584116578102\n",
      "epoch: 7 step: 1819, loss is 0.0017616692930459976\n",
      "epoch: 7 step: 1820, loss is 0.010394446551799774\n",
      "epoch: 7 step: 1821, loss is 0.036978304386138916\n",
      "epoch: 7 step: 1822, loss is 0.018963422626256943\n",
      "epoch: 7 step: 1823, loss is 0.003867247374728322\n",
      "epoch: 7 step: 1824, loss is 0.05194070190191269\n",
      "epoch: 7 step: 1825, loss is 0.24277709424495697\n",
      "epoch: 7 step: 1826, loss is 0.010360841639339924\n",
      "epoch: 7 step: 1827, loss is 0.025568628683686256\n",
      "epoch: 7 step: 1828, loss is 0.010406051762402058\n",
      "epoch: 7 step: 1829, loss is 0.0035011221189051867\n",
      "epoch: 7 step: 1830, loss is 0.000984623096883297\n",
      "epoch: 7 step: 1831, loss is 0.004412256646901369\n",
      "epoch: 7 step: 1832, loss is 0.02968594618141651\n",
      "epoch: 7 step: 1833, loss is 0.010381026193499565\n",
      "epoch: 7 step: 1834, loss is 0.014053626917302608\n",
      "epoch: 7 step: 1835, loss is 0.04197140783071518\n",
      "epoch: 7 step: 1836, loss is 0.04288661479949951\n",
      "epoch: 7 step: 1837, loss is 0.06297320127487183\n",
      "epoch: 7 step: 1838, loss is 0.05215074494481087\n",
      "epoch: 7 step: 1839, loss is 0.0011767062824219465\n",
      "epoch: 7 step: 1840, loss is 0.013990052975714207\n",
      "epoch: 7 step: 1841, loss is 0.002285306341946125\n",
      "epoch: 7 step: 1842, loss is 0.04247695580124855\n",
      "epoch: 7 step: 1843, loss is 0.041112445294857025\n",
      "epoch: 7 step: 1844, loss is 0.004132890608161688\n",
      "epoch: 7 step: 1845, loss is 0.11374980956315994\n",
      "epoch: 7 step: 1846, loss is 0.0020872410386800766\n",
      "epoch: 7 step: 1847, loss is 0.002616771264001727\n",
      "epoch: 7 step: 1848, loss is 0.0005200915038585663\n",
      "epoch: 7 step: 1849, loss is 0.002316777827218175\n",
      "epoch: 7 step: 1850, loss is 0.16707201302051544\n",
      "epoch: 7 step: 1851, loss is 0.0003917099966201931\n",
      "epoch: 7 step: 1852, loss is 0.0479961559176445\n",
      "epoch: 7 step: 1853, loss is 0.03281537443399429\n",
      "epoch: 7 step: 1854, loss is 0.005022196564823389\n",
      "epoch: 7 step: 1855, loss is 0.1279095709323883\n",
      "epoch: 7 step: 1856, loss is 0.0021320439409464598\n",
      "epoch: 7 step: 1857, loss is 0.05197308212518692\n",
      "epoch: 7 step: 1858, loss is 0.0017455930355936289\n",
      "epoch: 7 step: 1859, loss is 0.3192594647407532\n",
      "epoch: 7 step: 1860, loss is 0.038022007793188095\n",
      "epoch: 7 step: 1861, loss is 0.0027749608270823956\n",
      "epoch: 7 step: 1862, loss is 0.06455110758543015\n",
      "epoch: 7 step: 1863, loss is 0.029392734169960022\n",
      "epoch: 7 step: 1864, loss is 0.00040763759170658886\n",
      "epoch: 7 step: 1865, loss is 0.24936255812644958\n",
      "epoch: 7 step: 1866, loss is 0.07943499088287354\n",
      "epoch: 7 step: 1867, loss is 0.010110300034284592\n",
      "epoch: 7 step: 1868, loss is 0.20510229468345642\n",
      "epoch: 7 step: 1869, loss is 0.1164739727973938\n",
      "epoch: 7 step: 1870, loss is 0.0023035139311105013\n",
      "epoch: 7 step: 1871, loss is 0.006357964593917131\n",
      "epoch: 7 step: 1872, loss is 0.036471664905548096\n",
      "epoch: 7 step: 1873, loss is 0.005315196700394154\n",
      "epoch: 7 step: 1874, loss is 0.011169117875397205\n",
      "epoch: 7 step: 1875, loss is 0.0043760319240391254\n",
      "epoch: 8 step: 1, loss is 0.008398138917982578\n",
      "epoch: 8 step: 2, loss is 0.02476055920124054\n",
      "epoch: 8 step: 3, loss is 0.0017035171622410417\n",
      "epoch: 8 step: 4, loss is 0.004604659508913755\n",
      "epoch: 8 step: 5, loss is 0.1000213623046875\n",
      "epoch: 8 step: 6, loss is 0.002163965255022049\n",
      "epoch: 8 step: 7, loss is 0.018553655594587326\n",
      "epoch: 8 step: 8, loss is 0.030033018440008163\n",
      "epoch: 8 step: 9, loss is 0.002766052959486842\n",
      "epoch: 8 step: 10, loss is 0.10748598724603653\n",
      "epoch: 8 step: 11, loss is 0.005773481912910938\n",
      "epoch: 8 step: 12, loss is 0.007891501300036907\n",
      "epoch: 8 step: 13, loss is 0.0691257044672966\n",
      "epoch: 8 step: 14, loss is 0.02189609594643116\n",
      "epoch: 8 step: 15, loss is 0.0026630444917827845\n",
      "epoch: 8 step: 16, loss is 0.000778913963586092\n",
      "epoch: 8 step: 17, loss is 0.004119065590202808\n",
      "epoch: 8 step: 18, loss is 0.005287493113428354\n",
      "epoch: 8 step: 19, loss is 0.006264780182391405\n",
      "epoch: 8 step: 20, loss is 0.0004479140625335276\n",
      "epoch: 8 step: 21, loss is 0.012867344543337822\n",
      "epoch: 8 step: 22, loss is 0.004768718034029007\n",
      "epoch: 8 step: 23, loss is 0.005799019243568182\n",
      "epoch: 8 step: 24, loss is 0.004512409679591656\n",
      "epoch: 8 step: 25, loss is 0.10054237395524979\n",
      "epoch: 8 step: 26, loss is 0.06034412607550621\n",
      "epoch: 8 step: 27, loss is 0.08944413810968399\n",
      "epoch: 8 step: 28, loss is 0.17251619696617126\n",
      "epoch: 8 step: 29, loss is 0.0031852214597165585\n",
      "epoch: 8 step: 30, loss is 0.0009437834960408509\n",
      "epoch: 8 step: 31, loss is 0.0004490530991461128\n",
      "epoch: 8 step: 32, loss is 0.00400425074622035\n",
      "epoch: 8 step: 33, loss is 0.21401678025722504\n",
      "epoch: 8 step: 34, loss is 0.09779296815395355\n",
      "epoch: 8 step: 35, loss is 0.021229395642876625\n",
      "epoch: 8 step: 36, loss is 0.001029281411319971\n",
      "epoch: 8 step: 37, loss is 0.0039824978448450565\n",
      "epoch: 8 step: 38, loss is 0.011766082607209682\n",
      "epoch: 8 step: 39, loss is 0.001571499276906252\n",
      "epoch: 8 step: 40, loss is 0.014709969982504845\n",
      "epoch: 8 step: 41, loss is 0.03360642492771149\n",
      "epoch: 8 step: 42, loss is 0.0012870630016550422\n",
      "epoch: 8 step: 43, loss is 0.007916415110230446\n",
      "epoch: 8 step: 44, loss is 0.08293189108371735\n",
      "epoch: 8 step: 45, loss is 0.04985755681991577\n",
      "epoch: 8 step: 46, loss is 0.031176306307315826\n",
      "epoch: 8 step: 47, loss is 0.09032689779996872\n",
      "epoch: 8 step: 48, loss is 0.006940171122550964\n",
      "epoch: 8 step: 49, loss is 0.0031305623706430197\n",
      "epoch: 8 step: 50, loss is 0.003763985587283969\n",
      "epoch: 8 step: 51, loss is 0.0030299853533506393\n",
      "epoch: 8 step: 52, loss is 0.018335696309804916\n",
      "epoch: 8 step: 53, loss is 0.003073644358664751\n",
      "epoch: 8 step: 54, loss is 0.0014317795867100358\n",
      "epoch: 8 step: 55, loss is 0.04268878698348999\n",
      "epoch: 8 step: 56, loss is 0.0014351463178172708\n",
      "epoch: 8 step: 57, loss is 0.0039165932685136795\n",
      "epoch: 8 step: 58, loss is 0.032280486077070236\n",
      "epoch: 8 step: 59, loss is 0.007612133864313364\n",
      "epoch: 8 step: 60, loss is 0.00263385777361691\n",
      "epoch: 8 step: 61, loss is 0.006922732573002577\n",
      "epoch: 8 step: 62, loss is 0.0018637486500665545\n",
      "epoch: 8 step: 63, loss is 0.005990331992506981\n",
      "epoch: 8 step: 64, loss is 0.15479788184165955\n",
      "epoch: 8 step: 65, loss is 0.02387819066643715\n",
      "epoch: 8 step: 66, loss is 0.04385644197463989\n",
      "epoch: 8 step: 67, loss is 0.10011593997478485\n",
      "epoch: 8 step: 68, loss is 0.018818333745002747\n",
      "epoch: 8 step: 69, loss is 0.0032433115411549807\n",
      "epoch: 8 step: 70, loss is 0.007959920912981033\n",
      "epoch: 8 step: 71, loss is 0.0025770398788154125\n",
      "epoch: 8 step: 72, loss is 0.012079165317118168\n",
      "epoch: 8 step: 73, loss is 0.01294983271509409\n",
      "epoch: 8 step: 74, loss is 0.006202814169228077\n",
      "epoch: 8 step: 75, loss is 0.03304967284202576\n",
      "epoch: 8 step: 76, loss is 0.03481338173151016\n",
      "epoch: 8 step: 77, loss is 0.002224568510428071\n",
      "epoch: 8 step: 78, loss is 0.0054346779361367226\n",
      "epoch: 8 step: 79, loss is 0.0020237602293491364\n",
      "epoch: 8 step: 80, loss is 0.010688459500670433\n",
      "epoch: 8 step: 81, loss is 0.04996239393949509\n",
      "epoch: 8 step: 82, loss is 0.03339032456278801\n",
      "epoch: 8 step: 83, loss is 0.006296847946941853\n",
      "epoch: 8 step: 84, loss is 0.004447978921234608\n",
      "epoch: 8 step: 85, loss is 0.0021876960527151823\n",
      "epoch: 8 step: 86, loss is 0.049991726875305176\n",
      "epoch: 8 step: 87, loss is 0.042996570467948914\n",
      "epoch: 8 step: 88, loss is 0.006354177370667458\n",
      "epoch: 8 step: 89, loss is 0.00028867454966530204\n",
      "epoch: 8 step: 90, loss is 0.00812176987528801\n",
      "epoch: 8 step: 91, loss is 0.05487194284796715\n",
      "epoch: 8 step: 92, loss is 0.007330227177590132\n",
      "epoch: 8 step: 93, loss is 0.04312102124094963\n",
      "epoch: 8 step: 94, loss is 0.001126294955611229\n",
      "epoch: 8 step: 95, loss is 0.02314462885260582\n",
      "epoch: 8 step: 96, loss is 0.004048104397952557\n",
      "epoch: 8 step: 97, loss is 0.01069298479706049\n",
      "epoch: 8 step: 98, loss is 0.0262270700186491\n",
      "epoch: 8 step: 99, loss is 0.007274516858160496\n",
      "epoch: 8 step: 100, loss is 0.0005043206037953496\n",
      "epoch: 8 step: 101, loss is 0.20888760685920715\n",
      "epoch: 8 step: 102, loss is 0.002053367905318737\n",
      "epoch: 8 step: 103, loss is 0.017005430534482002\n",
      "epoch: 8 step: 104, loss is 0.011761536821722984\n",
      "epoch: 8 step: 105, loss is 0.03126269578933716\n",
      "epoch: 8 step: 106, loss is 0.0019205032149329782\n",
      "epoch: 8 step: 107, loss is 0.007882684469223022\n",
      "epoch: 8 step: 108, loss is 0.04396146908402443\n",
      "epoch: 8 step: 109, loss is 0.023221252486109734\n",
      "epoch: 8 step: 110, loss is 0.006011340767145157\n",
      "epoch: 8 step: 111, loss is 0.0015643453225493431\n",
      "epoch: 8 step: 112, loss is 0.007107458543032408\n",
      "epoch: 8 step: 113, loss is 0.006384264212101698\n",
      "epoch: 8 step: 114, loss is 0.1096482202410698\n",
      "epoch: 8 step: 115, loss is 0.07338128238916397\n",
      "epoch: 8 step: 116, loss is 0.01607154682278633\n",
      "epoch: 8 step: 117, loss is 0.0013158649671822786\n",
      "epoch: 8 step: 118, loss is 0.06380374729633331\n",
      "epoch: 8 step: 119, loss is 0.003924199845641851\n",
      "epoch: 8 step: 120, loss is 0.014597980305552483\n",
      "epoch: 8 step: 121, loss is 0.013141071423888206\n",
      "epoch: 8 step: 122, loss is 0.008078860118985176\n",
      "epoch: 8 step: 123, loss is 0.0005656607099808753\n",
      "epoch: 8 step: 124, loss is 0.012898579239845276\n",
      "epoch: 8 step: 125, loss is 0.002533745253458619\n",
      "epoch: 8 step: 126, loss is 0.008148277178406715\n",
      "epoch: 8 step: 127, loss is 0.0026270162779837847\n",
      "epoch: 8 step: 128, loss is 0.03725588321685791\n",
      "epoch: 8 step: 129, loss is 0.0779552012681961\n",
      "epoch: 8 step: 130, loss is 0.006383900996297598\n",
      "epoch: 8 step: 131, loss is 0.013397577218711376\n",
      "epoch: 8 step: 132, loss is 0.009339344687759876\n",
      "epoch: 8 step: 133, loss is 0.002379452344030142\n",
      "epoch: 8 step: 134, loss is 0.005698234774172306\n",
      "epoch: 8 step: 135, loss is 0.004505479242652655\n",
      "epoch: 8 step: 136, loss is 0.002672750037163496\n",
      "epoch: 8 step: 137, loss is 0.020400771871209145\n",
      "epoch: 8 step: 138, loss is 0.08271186053752899\n",
      "epoch: 8 step: 139, loss is 0.003251343499869108\n",
      "epoch: 8 step: 140, loss is 0.0013503202935680747\n",
      "epoch: 8 step: 141, loss is 0.02754061296582222\n",
      "epoch: 8 step: 142, loss is 0.003282868769019842\n",
      "epoch: 8 step: 143, loss is 0.021436065435409546\n",
      "epoch: 8 step: 144, loss is 0.011759593151509762\n",
      "epoch: 8 step: 145, loss is 0.002558565465733409\n",
      "epoch: 8 step: 146, loss is 0.000644434941932559\n",
      "epoch: 8 step: 147, loss is 0.005708684213459492\n",
      "epoch: 8 step: 148, loss is 0.007311437278985977\n",
      "epoch: 8 step: 149, loss is 0.0009520146995782852\n",
      "epoch: 8 step: 150, loss is 0.023961659520864487\n",
      "epoch: 8 step: 151, loss is 0.02330232784152031\n",
      "epoch: 8 step: 152, loss is 0.016689162701368332\n",
      "epoch: 8 step: 153, loss is 0.02070837840437889\n",
      "epoch: 8 step: 154, loss is 0.039370082318782806\n",
      "epoch: 8 step: 155, loss is 0.0038369717076420784\n",
      "epoch: 8 step: 156, loss is 0.0008355024037882686\n",
      "epoch: 8 step: 157, loss is 0.07601500302553177\n",
      "epoch: 8 step: 158, loss is 0.004407771863043308\n",
      "epoch: 8 step: 159, loss is 0.15250059962272644\n",
      "epoch: 8 step: 160, loss is 0.018355868756771088\n",
      "epoch: 8 step: 161, loss is 0.036981843411922455\n",
      "epoch: 8 step: 162, loss is 0.002380151068791747\n",
      "epoch: 8 step: 163, loss is 0.15261797606945038\n",
      "epoch: 8 step: 164, loss is 0.0019370772643014789\n",
      "epoch: 8 step: 165, loss is 0.004368508234620094\n",
      "epoch: 8 step: 166, loss is 0.014484691433608532\n",
      "epoch: 8 step: 167, loss is 0.004294462036341429\n",
      "epoch: 8 step: 168, loss is 0.0613209493458271\n",
      "epoch: 8 step: 169, loss is 0.01799275539815426\n",
      "epoch: 8 step: 170, loss is 0.006331312470138073\n",
      "epoch: 8 step: 171, loss is 0.018963037058711052\n",
      "epoch: 8 step: 172, loss is 0.005075663328170776\n",
      "epoch: 8 step: 173, loss is 3.63898288924247e-05\n",
      "epoch: 8 step: 174, loss is 0.005220002494752407\n",
      "epoch: 8 step: 175, loss is 0.0016852536937221885\n",
      "epoch: 8 step: 176, loss is 0.03539036214351654\n",
      "epoch: 8 step: 177, loss is 0.005255018826574087\n",
      "epoch: 8 step: 178, loss is 0.00015064318722579628\n",
      "epoch: 8 step: 179, loss is 0.04049311578273773\n",
      "epoch: 8 step: 180, loss is 0.005762207321822643\n",
      "epoch: 8 step: 181, loss is 0.001073626452125609\n",
      "epoch: 8 step: 182, loss is 0.04577925056219101\n",
      "epoch: 8 step: 183, loss is 0.0008133624214679003\n",
      "epoch: 8 step: 184, loss is 0.02329566702246666\n",
      "epoch: 8 step: 185, loss is 0.005711148027330637\n",
      "epoch: 8 step: 186, loss is 0.03889961540699005\n",
      "epoch: 8 step: 187, loss is 0.00024987859069369733\n",
      "epoch: 8 step: 188, loss is 0.00584026612341404\n",
      "epoch: 8 step: 189, loss is 0.037249527871608734\n",
      "epoch: 8 step: 190, loss is 0.05552436038851738\n",
      "epoch: 8 step: 191, loss is 0.0031991612631827593\n",
      "epoch: 8 step: 192, loss is 0.0005254149436950684\n",
      "epoch: 8 step: 193, loss is 0.004688967484980822\n",
      "epoch: 8 step: 194, loss is 0.002411099150776863\n",
      "epoch: 8 step: 195, loss is 0.020971326157450676\n",
      "epoch: 8 step: 196, loss is 0.0004854662693105638\n",
      "epoch: 8 step: 197, loss is 0.061330605298280716\n",
      "epoch: 8 step: 198, loss is 0.03870537877082825\n",
      "epoch: 8 step: 199, loss is 0.003415429499000311\n",
      "epoch: 8 step: 200, loss is 0.003585757687687874\n",
      "epoch: 8 step: 201, loss is 0.016978099942207336\n",
      "epoch: 8 step: 202, loss is 0.0008156504482030869\n",
      "epoch: 8 step: 203, loss is 0.01249249093234539\n",
      "epoch: 8 step: 204, loss is 0.005809870548546314\n",
      "epoch: 8 step: 205, loss is 0.02267783135175705\n",
      "epoch: 8 step: 206, loss is 0.053025949746370316\n",
      "epoch: 8 step: 207, loss is 0.06530430167913437\n",
      "epoch: 8 step: 208, loss is 0.03422372788190842\n",
      "epoch: 8 step: 209, loss is 0.0009135983418673277\n",
      "epoch: 8 step: 210, loss is 0.020133348181843758\n",
      "epoch: 8 step: 211, loss is 0.003561983583495021\n",
      "epoch: 8 step: 212, loss is 0.0004078464990016073\n",
      "epoch: 8 step: 213, loss is 0.0015178907196968794\n",
      "epoch: 8 step: 214, loss is 0.0027252258732914925\n",
      "epoch: 8 step: 215, loss is 0.08100070804357529\n",
      "epoch: 8 step: 216, loss is 0.004765817429870367\n",
      "epoch: 8 step: 217, loss is 0.0030202807392925024\n",
      "epoch: 8 step: 218, loss is 0.0083044758066535\n",
      "epoch: 8 step: 219, loss is 0.011323216371238232\n",
      "epoch: 8 step: 220, loss is 0.011537405662238598\n",
      "epoch: 8 step: 221, loss is 0.003391954582184553\n",
      "epoch: 8 step: 222, loss is 0.0059343078173696995\n",
      "epoch: 8 step: 223, loss is 0.022949520498514175\n",
      "epoch: 8 step: 224, loss is 0.009414882399141788\n",
      "epoch: 8 step: 225, loss is 0.000615728902630508\n",
      "epoch: 8 step: 226, loss is 0.07619800418615341\n",
      "epoch: 8 step: 227, loss is 0.09937428683042526\n",
      "epoch: 8 step: 228, loss is 0.050141558051109314\n",
      "epoch: 8 step: 229, loss is 0.00160508812405169\n",
      "epoch: 8 step: 230, loss is 0.2844533324241638\n",
      "epoch: 8 step: 231, loss is 0.017721548676490784\n",
      "epoch: 8 step: 232, loss is 0.02110270783305168\n",
      "epoch: 8 step: 233, loss is 0.038705915212631226\n",
      "epoch: 8 step: 234, loss is 0.0036153404507786036\n",
      "epoch: 8 step: 235, loss is 0.002629250753670931\n",
      "epoch: 8 step: 236, loss is 0.0004354068951215595\n",
      "epoch: 8 step: 237, loss is 0.0002932010102085769\n",
      "epoch: 8 step: 238, loss is 0.0010657638777047396\n",
      "epoch: 8 step: 239, loss is 0.09226936101913452\n",
      "epoch: 8 step: 240, loss is 0.18583017587661743\n",
      "epoch: 8 step: 241, loss is 0.006023490335792303\n",
      "epoch: 8 step: 242, loss is 0.004458911716938019\n",
      "epoch: 8 step: 243, loss is 0.0013499213382601738\n",
      "epoch: 8 step: 244, loss is 0.022563576698303223\n",
      "epoch: 8 step: 245, loss is 0.010303542949259281\n",
      "epoch: 8 step: 246, loss is 0.04976142197847366\n",
      "epoch: 8 step: 247, loss is 0.018125610426068306\n",
      "epoch: 8 step: 248, loss is 0.005626054480671883\n",
      "epoch: 8 step: 249, loss is 0.026708992198109627\n",
      "epoch: 8 step: 250, loss is 0.0026864984538406134\n",
      "epoch: 8 step: 251, loss is 0.03472370281815529\n",
      "epoch: 8 step: 252, loss is 0.010448992252349854\n",
      "epoch: 8 step: 253, loss is 0.003153495490550995\n",
      "epoch: 8 step: 254, loss is 0.0510244257748127\n",
      "epoch: 8 step: 255, loss is 0.010226491838693619\n",
      "epoch: 8 step: 256, loss is 0.1350935995578766\n",
      "epoch: 8 step: 257, loss is 0.005036460235714912\n",
      "epoch: 8 step: 258, loss is 0.02870970405638218\n",
      "epoch: 8 step: 259, loss is 0.003249833593145013\n",
      "epoch: 8 step: 260, loss is 0.003512016963213682\n",
      "epoch: 8 step: 261, loss is 0.009647967293858528\n",
      "epoch: 8 step: 262, loss is 0.02584000863134861\n",
      "epoch: 8 step: 263, loss is 0.008793676272034645\n",
      "epoch: 8 step: 264, loss is 0.019402500241994858\n",
      "epoch: 8 step: 265, loss is 0.0014276853762567043\n",
      "epoch: 8 step: 266, loss is 0.0013907485408708453\n",
      "epoch: 8 step: 267, loss is 0.06829048693180084\n",
      "epoch: 8 step: 268, loss is 0.021861135959625244\n",
      "epoch: 8 step: 269, loss is 0.0007540773367509246\n",
      "epoch: 8 step: 270, loss is 0.1704852432012558\n",
      "epoch: 8 step: 271, loss is 0.012302410788834095\n",
      "epoch: 8 step: 272, loss is 0.002687564119696617\n",
      "epoch: 8 step: 273, loss is 0.004705826751887798\n",
      "epoch: 8 step: 274, loss is 0.00158737285528332\n",
      "epoch: 8 step: 275, loss is 0.16210633516311646\n",
      "epoch: 8 step: 276, loss is 0.0015959780430421233\n",
      "epoch: 8 step: 277, loss is 0.0011950904736295342\n",
      "epoch: 8 step: 278, loss is 0.047658711671829224\n",
      "epoch: 8 step: 279, loss is 0.11689499020576477\n",
      "epoch: 8 step: 280, loss is 0.005493673030287027\n",
      "epoch: 8 step: 281, loss is 0.012902585789561272\n",
      "epoch: 8 step: 282, loss is 0.0010683041764423251\n",
      "epoch: 8 step: 283, loss is 0.003156485967338085\n",
      "epoch: 8 step: 284, loss is 0.022584516555070877\n",
      "epoch: 8 step: 285, loss is 0.10756729543209076\n",
      "epoch: 8 step: 286, loss is 0.04906151443719864\n",
      "epoch: 8 step: 287, loss is 0.0014507463201880455\n",
      "epoch: 8 step: 288, loss is 0.010761703364551067\n",
      "epoch: 8 step: 289, loss is 0.03542941063642502\n",
      "epoch: 8 step: 290, loss is 0.00017797391046769917\n",
      "epoch: 8 step: 291, loss is 0.0014121981803327799\n",
      "epoch: 8 step: 292, loss is 0.027474824339151382\n",
      "epoch: 8 step: 293, loss is 0.024868499487638474\n",
      "epoch: 8 step: 294, loss is 0.19550444185733795\n",
      "epoch: 8 step: 295, loss is 0.002391417743638158\n",
      "epoch: 8 step: 296, loss is 0.002854689722880721\n",
      "epoch: 8 step: 297, loss is 0.0012318127555772662\n",
      "epoch: 8 step: 298, loss is 0.0010926685063168406\n",
      "epoch: 8 step: 299, loss is 0.02042606845498085\n",
      "epoch: 8 step: 300, loss is 0.07914792746305466\n",
      "epoch: 8 step: 301, loss is 0.07499939203262329\n",
      "epoch: 8 step: 302, loss is 0.0019920982886105776\n",
      "epoch: 8 step: 303, loss is 0.00949283130466938\n",
      "epoch: 8 step: 304, loss is 0.011908840388059616\n",
      "epoch: 8 step: 305, loss is 0.012491189874708652\n",
      "epoch: 8 step: 306, loss is 0.04292885214090347\n",
      "epoch: 8 step: 307, loss is 0.02911129780113697\n",
      "epoch: 8 step: 308, loss is 0.007751914206892252\n",
      "epoch: 8 step: 309, loss is 0.0018012747168540955\n",
      "epoch: 8 step: 310, loss is 0.02849198877811432\n",
      "epoch: 8 step: 311, loss is 0.006190589629113674\n",
      "epoch: 8 step: 312, loss is 0.030412452295422554\n",
      "epoch: 8 step: 313, loss is 0.15467149019241333\n",
      "epoch: 8 step: 314, loss is 0.06098804250359535\n",
      "epoch: 8 step: 315, loss is 0.0021466738544404507\n",
      "epoch: 8 step: 316, loss is 0.24022285640239716\n",
      "epoch: 8 step: 317, loss is 0.034879736602306366\n",
      "epoch: 8 step: 318, loss is 0.0022857757285237312\n",
      "epoch: 8 step: 319, loss is 0.007485158741474152\n",
      "epoch: 8 step: 320, loss is 0.0005521176499314606\n",
      "epoch: 8 step: 321, loss is 0.1093863993883133\n",
      "epoch: 8 step: 322, loss is 0.14555710554122925\n",
      "epoch: 8 step: 323, loss is 0.0007965038530528545\n",
      "epoch: 8 step: 324, loss is 0.027176037430763245\n",
      "epoch: 8 step: 325, loss is 0.015834001824259758\n",
      "epoch: 8 step: 326, loss is 0.09039191156625748\n",
      "epoch: 8 step: 327, loss is 0.16453905403614044\n",
      "epoch: 8 step: 328, loss is 0.0001250241621164605\n",
      "epoch: 8 step: 329, loss is 0.002143026562407613\n",
      "epoch: 8 step: 330, loss is 0.0003425571194384247\n",
      "epoch: 8 step: 331, loss is 0.0007963177631609142\n",
      "epoch: 8 step: 332, loss is 0.04402457922697067\n",
      "epoch: 8 step: 333, loss is 0.040514107793569565\n",
      "epoch: 8 step: 334, loss is 0.10826880484819412\n",
      "epoch: 8 step: 335, loss is 0.00455576041713357\n",
      "epoch: 8 step: 336, loss is 0.0405622236430645\n",
      "epoch: 8 step: 337, loss is 0.0023972513154149055\n",
      "epoch: 8 step: 338, loss is 0.0010801840107887983\n",
      "epoch: 8 step: 339, loss is 0.004111150745302439\n",
      "epoch: 8 step: 340, loss is 0.008451245725154877\n",
      "epoch: 8 step: 341, loss is 0.06034985929727554\n",
      "epoch: 8 step: 342, loss is 0.0016670250333845615\n",
      "epoch: 8 step: 343, loss is 0.0038435589522123337\n",
      "epoch: 8 step: 344, loss is 0.0027735806070268154\n",
      "epoch: 8 step: 345, loss is 0.1800594925880432\n",
      "epoch: 8 step: 346, loss is 0.0008392573217861354\n",
      "epoch: 8 step: 347, loss is 0.008322002366185188\n",
      "epoch: 8 step: 348, loss is 0.006325049791485071\n",
      "epoch: 8 step: 349, loss is 0.004282541573047638\n",
      "epoch: 8 step: 350, loss is 0.04081409424543381\n",
      "epoch: 8 step: 351, loss is 0.001308742444962263\n",
      "epoch: 8 step: 352, loss is 0.012389393523335457\n",
      "epoch: 8 step: 353, loss is 0.006777807138860226\n",
      "epoch: 8 step: 354, loss is 0.0205739364027977\n",
      "epoch: 8 step: 355, loss is 0.02250191941857338\n",
      "epoch: 8 step: 356, loss is 0.2517518699169159\n",
      "epoch: 8 step: 357, loss is 0.002010060241445899\n",
      "epoch: 8 step: 358, loss is 0.022730043157935143\n",
      "epoch: 8 step: 359, loss is 0.0013974811881780624\n",
      "epoch: 8 step: 360, loss is 0.07981675863265991\n",
      "epoch: 8 step: 361, loss is 0.0024376059882342815\n",
      "epoch: 8 step: 362, loss is 0.1507837474346161\n",
      "epoch: 8 step: 363, loss is 0.4418225586414337\n",
      "epoch: 8 step: 364, loss is 0.002300309482961893\n",
      "epoch: 8 step: 365, loss is 0.012424702756106853\n",
      "epoch: 8 step: 366, loss is 0.0030229573603719473\n",
      "epoch: 8 step: 367, loss is 0.0020605684258043766\n",
      "epoch: 8 step: 368, loss is 0.0013591271126642823\n",
      "epoch: 8 step: 369, loss is 0.0009018254349939525\n",
      "epoch: 8 step: 370, loss is 0.004021497908979654\n",
      "epoch: 8 step: 371, loss is 0.04281694442033768\n",
      "epoch: 8 step: 372, loss is 0.02455141395330429\n",
      "epoch: 8 step: 373, loss is 0.05094565823674202\n",
      "epoch: 8 step: 374, loss is 0.20712725818157196\n",
      "epoch: 8 step: 375, loss is 0.025256767868995667\n",
      "epoch: 8 step: 376, loss is 0.014458496123552322\n",
      "epoch: 8 step: 377, loss is 0.0012540246825665236\n",
      "epoch: 8 step: 378, loss is 0.004666798748075962\n",
      "epoch: 8 step: 379, loss is 0.04884501174092293\n",
      "epoch: 8 step: 380, loss is 0.03796631470322609\n",
      "epoch: 8 step: 381, loss is 0.000991487642750144\n",
      "epoch: 8 step: 382, loss is 0.012413079850375652\n",
      "epoch: 8 step: 383, loss is 0.0026389192789793015\n",
      "epoch: 8 step: 384, loss is 0.043413080275058746\n",
      "epoch: 8 step: 385, loss is 0.013184884563088417\n",
      "epoch: 8 step: 386, loss is 0.03936280310153961\n",
      "epoch: 8 step: 387, loss is 0.016735684126615524\n",
      "epoch: 8 step: 388, loss is 0.03301893174648285\n",
      "epoch: 8 step: 389, loss is 0.00767851946875453\n",
      "epoch: 8 step: 390, loss is 0.0011467494769021869\n",
      "epoch: 8 step: 391, loss is 0.024217169731855392\n",
      "epoch: 8 step: 392, loss is 0.04220989719033241\n",
      "epoch: 8 step: 393, loss is 0.1298469752073288\n",
      "epoch: 8 step: 394, loss is 0.01616174913942814\n",
      "epoch: 8 step: 395, loss is 0.010540994815528393\n",
      "epoch: 8 step: 396, loss is 0.0040173097513616085\n",
      "epoch: 8 step: 397, loss is 0.005248356610536575\n",
      "epoch: 8 step: 398, loss is 0.034766923636198044\n",
      "epoch: 8 step: 399, loss is 0.0019765375182032585\n",
      "epoch: 8 step: 400, loss is 0.004408673848956823\n",
      "epoch: 8 step: 401, loss is 0.01414809376001358\n",
      "epoch: 8 step: 402, loss is 0.056756455451250076\n",
      "epoch: 8 step: 403, loss is 0.029975594952702522\n",
      "epoch: 8 step: 404, loss is 0.0030081714503467083\n",
      "epoch: 8 step: 405, loss is 0.1097927838563919\n",
      "epoch: 8 step: 406, loss is 0.07736307382583618\n",
      "epoch: 8 step: 407, loss is 0.003801392624154687\n",
      "epoch: 8 step: 408, loss is 0.001470808987505734\n",
      "epoch: 8 step: 409, loss is 0.0014621857553720474\n",
      "epoch: 8 step: 410, loss is 0.0019504204392433167\n",
      "epoch: 8 step: 411, loss is 0.004208880476653576\n",
      "epoch: 8 step: 412, loss is 0.07888666540384293\n",
      "epoch: 8 step: 413, loss is 0.01005703303962946\n",
      "epoch: 8 step: 414, loss is 0.04197065904736519\n",
      "epoch: 8 step: 415, loss is 0.06588830798864365\n",
      "epoch: 8 step: 416, loss is 0.24347703158855438\n",
      "epoch: 8 step: 417, loss is 0.005155297927558422\n",
      "epoch: 8 step: 418, loss is 0.0011404466349631548\n",
      "epoch: 8 step: 419, loss is 0.015875346958637238\n",
      "epoch: 8 step: 420, loss is 0.07470302283763885\n",
      "epoch: 8 step: 421, loss is 0.005685633514076471\n",
      "epoch: 8 step: 422, loss is 0.0007915272726677358\n",
      "epoch: 8 step: 423, loss is 0.013547329232096672\n",
      "epoch: 8 step: 424, loss is 0.01818813383579254\n",
      "epoch: 8 step: 425, loss is 0.002330077113583684\n",
      "epoch: 8 step: 426, loss is 0.01126091368496418\n",
      "epoch: 8 step: 427, loss is 0.02431533671915531\n",
      "epoch: 8 step: 428, loss is 0.0002689965767785907\n",
      "epoch: 8 step: 429, loss is 0.08834511786699295\n",
      "epoch: 8 step: 430, loss is 0.07373031228780746\n",
      "epoch: 8 step: 431, loss is 0.0015002532163634896\n",
      "epoch: 8 step: 432, loss is 0.01021922193467617\n",
      "epoch: 8 step: 433, loss is 0.013021747581660748\n",
      "epoch: 8 step: 434, loss is 0.006223523057997227\n",
      "epoch: 8 step: 435, loss is 0.05383933335542679\n",
      "epoch: 8 step: 436, loss is 0.0021151278633624315\n",
      "epoch: 8 step: 437, loss is 0.00861783605068922\n",
      "epoch: 8 step: 438, loss is 0.08324164152145386\n",
      "epoch: 8 step: 439, loss is 0.019936518743634224\n",
      "epoch: 8 step: 440, loss is 0.02073710970580578\n",
      "epoch: 8 step: 441, loss is 0.02771787717938423\n",
      "epoch: 8 step: 442, loss is 0.0055953143164515495\n",
      "epoch: 8 step: 443, loss is 0.002167167142033577\n",
      "epoch: 8 step: 444, loss is 0.15329299867153168\n",
      "epoch: 8 step: 445, loss is 0.16411155462265015\n",
      "epoch: 8 step: 446, loss is 0.005830657668411732\n",
      "epoch: 8 step: 447, loss is 0.026110848411917686\n",
      "epoch: 8 step: 448, loss is 0.007493052631616592\n",
      "epoch: 8 step: 449, loss is 0.0023180395364761353\n",
      "epoch: 8 step: 450, loss is 0.003908982500433922\n",
      "epoch: 8 step: 451, loss is 0.0007864324725233018\n",
      "epoch: 8 step: 452, loss is 0.11459498852491379\n",
      "epoch: 8 step: 453, loss is 0.09927589446306229\n",
      "epoch: 8 step: 454, loss is 0.006796999368816614\n",
      "epoch: 8 step: 455, loss is 0.008495599962770939\n",
      "epoch: 8 step: 456, loss is 0.0034459384623914957\n",
      "epoch: 8 step: 457, loss is 0.002741086995229125\n",
      "epoch: 8 step: 458, loss is 0.0032606786116957664\n",
      "epoch: 8 step: 459, loss is 0.0008248086669482291\n",
      "epoch: 8 step: 460, loss is 0.02128923498094082\n",
      "epoch: 8 step: 461, loss is 0.025595899671316147\n",
      "epoch: 8 step: 462, loss is 0.0015115165151655674\n",
      "epoch: 8 step: 463, loss is 0.004769491963088512\n",
      "epoch: 8 step: 464, loss is 0.02132648415863514\n",
      "epoch: 8 step: 465, loss is 0.0008496665977872908\n",
      "epoch: 8 step: 466, loss is 0.003917188849300146\n",
      "epoch: 8 step: 467, loss is 0.003784944536164403\n",
      "epoch: 8 step: 468, loss is 0.0703575536608696\n",
      "epoch: 8 step: 469, loss is 0.12856504321098328\n",
      "epoch: 8 step: 470, loss is 0.046036820858716965\n",
      "epoch: 8 step: 471, loss is 0.004782392643392086\n",
      "epoch: 8 step: 472, loss is 0.003174755722284317\n",
      "epoch: 8 step: 473, loss is 0.0034003902692347765\n",
      "epoch: 8 step: 474, loss is 0.0034210076555609703\n",
      "epoch: 8 step: 475, loss is 0.15631045401096344\n",
      "epoch: 8 step: 476, loss is 0.05891673266887665\n",
      "epoch: 8 step: 477, loss is 0.033883098512887955\n",
      "epoch: 8 step: 478, loss is 0.004821224603801966\n",
      "epoch: 8 step: 479, loss is 0.014591913670301437\n",
      "epoch: 8 step: 480, loss is 0.3235989809036255\n",
      "epoch: 8 step: 481, loss is 0.0012908768840134144\n",
      "epoch: 8 step: 482, loss is 0.029788650572299957\n",
      "epoch: 8 step: 483, loss is 0.12136469781398773\n",
      "epoch: 8 step: 484, loss is 0.010979810729622841\n",
      "epoch: 8 step: 485, loss is 0.0029312276747077703\n",
      "epoch: 8 step: 486, loss is 0.011820808053016663\n",
      "epoch: 8 step: 487, loss is 0.010513202287256718\n",
      "epoch: 8 step: 488, loss is 0.004848220851272345\n",
      "epoch: 8 step: 489, loss is 0.02111329883337021\n",
      "epoch: 8 step: 490, loss is 0.005295142065733671\n",
      "epoch: 8 step: 491, loss is 0.006195218302309513\n",
      "epoch: 8 step: 492, loss is 0.007520864252001047\n",
      "epoch: 8 step: 493, loss is 0.017260968685150146\n",
      "epoch: 8 step: 494, loss is 0.002266879193484783\n",
      "epoch: 8 step: 495, loss is 0.0003845524333883077\n",
      "epoch: 8 step: 496, loss is 0.020657362416386604\n",
      "epoch: 8 step: 497, loss is 0.03849758207798004\n",
      "epoch: 8 step: 498, loss is 0.06394816935062408\n",
      "epoch: 8 step: 499, loss is 0.0005229792441241443\n",
      "epoch: 8 step: 500, loss is 0.01467398926615715\n",
      "epoch: 8 step: 501, loss is 0.038237277418375015\n",
      "epoch: 8 step: 502, loss is 0.09542708098888397\n",
      "epoch: 8 step: 503, loss is 0.0010255258530378342\n",
      "epoch: 8 step: 504, loss is 0.05798379331827164\n",
      "epoch: 8 step: 505, loss is 0.03976618871092796\n",
      "epoch: 8 step: 506, loss is 0.0010599122615531087\n",
      "epoch: 8 step: 507, loss is 0.006402762606739998\n",
      "epoch: 8 step: 508, loss is 0.018778003752231598\n",
      "epoch: 8 step: 509, loss is 0.00813792273402214\n",
      "epoch: 8 step: 510, loss is 0.09121009707450867\n",
      "epoch: 8 step: 511, loss is 0.0016646613366901875\n",
      "epoch: 8 step: 512, loss is 0.09173538535833359\n",
      "epoch: 8 step: 513, loss is 0.1417844444513321\n",
      "epoch: 8 step: 514, loss is 0.0016940411878749728\n",
      "epoch: 8 step: 515, loss is 0.006811688654124737\n",
      "epoch: 8 step: 516, loss is 0.06546211242675781\n",
      "epoch: 8 step: 517, loss is 0.004740671720355749\n",
      "epoch: 8 step: 518, loss is 0.0011005222331732512\n",
      "epoch: 8 step: 519, loss is 0.017727212980389595\n",
      "epoch: 8 step: 520, loss is 0.012510293163359165\n",
      "epoch: 8 step: 521, loss is 0.03231988847255707\n",
      "epoch: 8 step: 522, loss is 0.029090607538819313\n",
      "epoch: 8 step: 523, loss is 0.013720317743718624\n",
      "epoch: 8 step: 524, loss is 0.008925182744860649\n",
      "epoch: 8 step: 525, loss is 0.04184790328145027\n",
      "epoch: 8 step: 526, loss is 0.15722231566905975\n",
      "epoch: 8 step: 527, loss is 0.003313373075798154\n",
      "epoch: 8 step: 528, loss is 0.0008041150285862386\n",
      "epoch: 8 step: 529, loss is 0.029035743325948715\n",
      "epoch: 8 step: 530, loss is 0.0045083449222147465\n",
      "epoch: 8 step: 531, loss is 0.003742675995454192\n",
      "epoch: 8 step: 532, loss is 0.0009477774146944284\n",
      "epoch: 8 step: 533, loss is 0.001247418811544776\n",
      "epoch: 8 step: 534, loss is 0.0514918714761734\n",
      "epoch: 8 step: 535, loss is 0.01050048042088747\n",
      "epoch: 8 step: 536, loss is 0.0011609301436692476\n",
      "epoch: 8 step: 537, loss is 0.0005023118574172258\n",
      "epoch: 8 step: 538, loss is 0.031117483973503113\n",
      "epoch: 8 step: 539, loss is 0.021738944575190544\n",
      "epoch: 8 step: 540, loss is 0.004348420538008213\n",
      "epoch: 8 step: 541, loss is 0.03585614264011383\n",
      "epoch: 8 step: 542, loss is 0.046454619616270065\n",
      "epoch: 8 step: 543, loss is 0.006138444412499666\n",
      "epoch: 8 step: 544, loss is 0.000689141743350774\n",
      "epoch: 8 step: 545, loss is 0.002430906519293785\n",
      "epoch: 8 step: 546, loss is 0.0083883386105299\n",
      "epoch: 8 step: 547, loss is 0.05471506342291832\n",
      "epoch: 8 step: 548, loss is 0.00017746658704709262\n",
      "epoch: 8 step: 549, loss is 0.052083056420087814\n",
      "epoch: 8 step: 550, loss is 0.001128199975937605\n",
      "epoch: 8 step: 551, loss is 0.001384846749715507\n",
      "epoch: 8 step: 552, loss is 0.003971281927078962\n",
      "epoch: 8 step: 553, loss is 0.005391645710915327\n",
      "epoch: 8 step: 554, loss is 0.00704180495813489\n",
      "epoch: 8 step: 555, loss is 0.0011158445850014687\n",
      "epoch: 8 step: 556, loss is 0.01196155697107315\n",
      "epoch: 8 step: 557, loss is 0.006259281653910875\n",
      "epoch: 8 step: 558, loss is 0.008550669066607952\n",
      "epoch: 8 step: 559, loss is 0.1227918490767479\n",
      "epoch: 8 step: 560, loss is 0.012867097742855549\n",
      "epoch: 8 step: 561, loss is 0.013945936225354671\n",
      "epoch: 8 step: 562, loss is 0.012677155435085297\n",
      "epoch: 8 step: 563, loss is 0.0009403937729075551\n",
      "epoch: 8 step: 564, loss is 0.0185436699539423\n",
      "epoch: 8 step: 565, loss is 0.024982700124382973\n",
      "epoch: 8 step: 566, loss is 0.06115208938717842\n",
      "epoch: 8 step: 567, loss is 0.19773338735103607\n",
      "epoch: 8 step: 568, loss is 0.05473896861076355\n",
      "epoch: 8 step: 569, loss is 0.05045869201421738\n",
      "epoch: 8 step: 570, loss is 0.09588470309972763\n",
      "epoch: 8 step: 571, loss is 0.02741245925426483\n",
      "epoch: 8 step: 572, loss is 0.04385392367839813\n",
      "epoch: 8 step: 573, loss is 0.008057181723415852\n",
      "epoch: 8 step: 574, loss is 0.004941343795508146\n",
      "epoch: 8 step: 575, loss is 0.18187786638736725\n",
      "epoch: 8 step: 576, loss is 0.0007230882765725255\n",
      "epoch: 8 step: 577, loss is 0.3166220486164093\n",
      "epoch: 8 step: 578, loss is 0.011441335082054138\n",
      "epoch: 8 step: 579, loss is 0.033632006496191025\n",
      "epoch: 8 step: 580, loss is 0.02839227020740509\n",
      "epoch: 8 step: 581, loss is 0.0025693553034216166\n",
      "epoch: 8 step: 582, loss is 0.016411401331424713\n",
      "epoch: 8 step: 583, loss is 0.0026452091988176107\n",
      "epoch: 8 step: 584, loss is 0.024707267060875893\n",
      "epoch: 8 step: 585, loss is 0.0399460606276989\n",
      "epoch: 8 step: 586, loss is 0.041616275906562805\n",
      "epoch: 8 step: 587, loss is 0.10140016674995422\n",
      "epoch: 8 step: 588, loss is 0.012334784492850304\n",
      "epoch: 8 step: 589, loss is 0.1508166640996933\n",
      "epoch: 8 step: 590, loss is 0.0472407303750515\n",
      "epoch: 8 step: 591, loss is 0.12300536781549454\n",
      "epoch: 8 step: 592, loss is 0.018216192722320557\n",
      "epoch: 8 step: 593, loss is 0.0061722127720713615\n",
      "epoch: 8 step: 594, loss is 0.034753117710351944\n",
      "epoch: 8 step: 595, loss is 0.0374777689576149\n",
      "epoch: 8 step: 596, loss is 0.014164433814585209\n",
      "epoch: 8 step: 597, loss is 0.02463991753757\n",
      "epoch: 8 step: 598, loss is 0.0031184672843664885\n",
      "epoch: 8 step: 599, loss is 0.011363454163074493\n",
      "epoch: 8 step: 600, loss is 0.04321327805519104\n",
      "epoch: 8 step: 601, loss is 0.10813596844673157\n",
      "epoch: 8 step: 602, loss is 0.026535119861364365\n",
      "epoch: 8 step: 603, loss is 0.024702126160264015\n",
      "epoch: 8 step: 604, loss is 0.0006656965706497431\n",
      "epoch: 8 step: 605, loss is 0.007611083332449198\n",
      "epoch: 8 step: 606, loss is 0.0009325634455308318\n",
      "epoch: 8 step: 607, loss is 0.006709065288305283\n",
      "epoch: 8 step: 608, loss is 0.12912073731422424\n",
      "epoch: 8 step: 609, loss is 0.24828587472438812\n",
      "epoch: 8 step: 610, loss is 0.09661797434091568\n",
      "epoch: 8 step: 611, loss is 0.028554730117321014\n",
      "epoch: 8 step: 612, loss is 0.08891832083463669\n",
      "epoch: 8 step: 613, loss is 0.004325809422880411\n",
      "epoch: 8 step: 614, loss is 0.012066463939845562\n",
      "epoch: 8 step: 615, loss is 0.0030687879770994186\n",
      "epoch: 8 step: 616, loss is 0.011549280025064945\n",
      "epoch: 8 step: 617, loss is 0.036252234131097794\n",
      "epoch: 8 step: 618, loss is 0.005390096455812454\n",
      "epoch: 8 step: 619, loss is 0.003358371090143919\n",
      "epoch: 8 step: 620, loss is 0.0022716496605426073\n",
      "epoch: 8 step: 621, loss is 0.013861097395420074\n",
      "epoch: 8 step: 622, loss is 0.12080243229866028\n",
      "epoch: 8 step: 623, loss is 0.012936951592564583\n",
      "epoch: 8 step: 624, loss is 0.05428728833794594\n",
      "epoch: 8 step: 625, loss is 0.006203826051205397\n",
      "epoch: 8 step: 626, loss is 0.017820322886109352\n",
      "epoch: 8 step: 627, loss is 0.0014923126436769962\n",
      "epoch: 8 step: 628, loss is 0.004160866141319275\n",
      "epoch: 8 step: 629, loss is 0.0017617832636460662\n",
      "epoch: 8 step: 630, loss is 0.0480058453977108\n",
      "epoch: 8 step: 631, loss is 0.0004867106326855719\n",
      "epoch: 8 step: 632, loss is 0.13256436586380005\n",
      "epoch: 8 step: 633, loss is 0.004234188701957464\n",
      "epoch: 8 step: 634, loss is 0.0015191741986200213\n",
      "epoch: 8 step: 635, loss is 0.00973073672503233\n",
      "epoch: 8 step: 636, loss is 0.05898439511656761\n",
      "epoch: 8 step: 637, loss is 0.0088205486536026\n",
      "epoch: 8 step: 638, loss is 0.08889364451169968\n",
      "epoch: 8 step: 639, loss is 0.0033087842166423798\n",
      "epoch: 8 step: 640, loss is 0.0025568122509866953\n",
      "epoch: 8 step: 641, loss is 0.008417487144470215\n",
      "epoch: 8 step: 642, loss is 0.009898972697556019\n",
      "epoch: 8 step: 643, loss is 0.08042527735233307\n",
      "epoch: 8 step: 644, loss is 0.13276374340057373\n",
      "epoch: 8 step: 645, loss is 0.03438388183712959\n",
      "epoch: 8 step: 646, loss is 0.0036308688577264547\n",
      "epoch: 8 step: 647, loss is 0.0022669280879199505\n",
      "epoch: 8 step: 648, loss is 0.0029394428711384535\n",
      "epoch: 8 step: 649, loss is 0.11039091646671295\n",
      "epoch: 8 step: 650, loss is 0.0025754065718501806\n",
      "epoch: 8 step: 651, loss is 0.0030754138715565205\n",
      "epoch: 8 step: 652, loss is 0.0011386071564629674\n",
      "epoch: 8 step: 653, loss is 0.0025970865972340107\n",
      "epoch: 8 step: 654, loss is 0.011282116174697876\n",
      "epoch: 8 step: 655, loss is 0.23852252960205078\n",
      "epoch: 8 step: 656, loss is 0.11318447440862656\n",
      "epoch: 8 step: 657, loss is 0.018888942897319794\n",
      "epoch: 8 step: 658, loss is 0.0015854627126827836\n",
      "epoch: 8 step: 659, loss is 0.0006670024595223367\n",
      "epoch: 8 step: 660, loss is 0.16146649420261383\n",
      "epoch: 8 step: 661, loss is 0.09395253658294678\n",
      "epoch: 8 step: 662, loss is 0.0041490099392831326\n",
      "epoch: 8 step: 663, loss is 0.0354427695274353\n",
      "epoch: 8 step: 664, loss is 0.10955245792865753\n",
      "epoch: 8 step: 665, loss is 0.0601811520755291\n",
      "epoch: 8 step: 666, loss is 0.0018082191236317158\n",
      "epoch: 8 step: 667, loss is 0.028531702235341072\n",
      "epoch: 8 step: 668, loss is 0.08768048137426376\n",
      "epoch: 8 step: 669, loss is 0.002570914337411523\n",
      "epoch: 8 step: 670, loss is 0.10309012979269028\n",
      "epoch: 8 step: 671, loss is 0.10961450636386871\n",
      "epoch: 8 step: 672, loss is 0.02028505504131317\n",
      "epoch: 8 step: 673, loss is 0.024096213281154633\n",
      "epoch: 8 step: 674, loss is 0.1194990798830986\n",
      "epoch: 8 step: 675, loss is 0.11814681440591812\n",
      "epoch: 8 step: 676, loss is 0.001016126829199493\n",
      "epoch: 8 step: 677, loss is 0.0045243739150464535\n",
      "epoch: 8 step: 678, loss is 0.03194858133792877\n",
      "epoch: 8 step: 679, loss is 0.09018678963184357\n",
      "epoch: 8 step: 680, loss is 0.001192158437334001\n",
      "epoch: 8 step: 681, loss is 0.05374899134039879\n",
      "epoch: 8 step: 682, loss is 0.12746602296829224\n",
      "epoch: 8 step: 683, loss is 0.1389230340719223\n",
      "epoch: 8 step: 684, loss is 0.0018211184069514275\n",
      "epoch: 8 step: 685, loss is 0.0007528765127062798\n",
      "epoch: 8 step: 686, loss is 0.06675916910171509\n",
      "epoch: 8 step: 687, loss is 0.1222313791513443\n",
      "epoch: 8 step: 688, loss is 0.06095295399427414\n",
      "epoch: 8 step: 689, loss is 0.0034453291445970535\n",
      "epoch: 8 step: 690, loss is 0.09299881756305695\n",
      "epoch: 8 step: 691, loss is 0.0010716505348682404\n",
      "epoch: 8 step: 692, loss is 0.10374337434768677\n",
      "epoch: 8 step: 693, loss is 0.01678520068526268\n",
      "epoch: 8 step: 694, loss is 0.03717723861336708\n",
      "epoch: 8 step: 695, loss is 0.009912041015923023\n",
      "epoch: 8 step: 696, loss is 0.2125311642885208\n",
      "epoch: 8 step: 697, loss is 0.04041760787367821\n",
      "epoch: 8 step: 698, loss is 0.00040183510282076895\n",
      "epoch: 8 step: 699, loss is 0.40753883123397827\n",
      "epoch: 8 step: 700, loss is 0.030192162841558456\n",
      "epoch: 8 step: 701, loss is 0.12967966496944427\n",
      "epoch: 8 step: 702, loss is 0.0378175787627697\n",
      "epoch: 8 step: 703, loss is 0.003197768237441778\n",
      "epoch: 8 step: 704, loss is 0.007284070830792189\n",
      "epoch: 8 step: 705, loss is 0.010660817846655846\n",
      "epoch: 8 step: 706, loss is 0.0002081660059047863\n",
      "epoch: 8 step: 707, loss is 0.05958666279911995\n",
      "epoch: 8 step: 708, loss is 0.04243791103363037\n",
      "epoch: 8 step: 709, loss is 0.0023371921852231026\n",
      "epoch: 8 step: 710, loss is 0.0011595040559768677\n",
      "epoch: 8 step: 711, loss is 0.0007231561467051506\n",
      "epoch: 8 step: 712, loss is 0.0004582793335430324\n",
      "epoch: 8 step: 713, loss is 0.012230689637362957\n",
      "epoch: 8 step: 714, loss is 0.00288930581882596\n",
      "epoch: 8 step: 715, loss is 0.06864669173955917\n",
      "epoch: 8 step: 716, loss is 0.0256692823022604\n",
      "epoch: 8 step: 717, loss is 0.009616062976419926\n",
      "epoch: 8 step: 718, loss is 0.019239790737628937\n",
      "epoch: 8 step: 719, loss is 0.1323607861995697\n",
      "epoch: 8 step: 720, loss is 0.0011508187744766474\n",
      "epoch: 8 step: 721, loss is 0.0013137019705027342\n",
      "epoch: 8 step: 722, loss is 0.00167881662491709\n",
      "epoch: 8 step: 723, loss is 0.0032355913426727057\n",
      "epoch: 8 step: 724, loss is 0.0366102010011673\n",
      "epoch: 8 step: 725, loss is 0.006743122823536396\n",
      "epoch: 8 step: 726, loss is 0.016144828870892525\n",
      "epoch: 8 step: 727, loss is 0.01279802992939949\n",
      "epoch: 8 step: 728, loss is 0.09879433363676071\n",
      "epoch: 8 step: 729, loss is 0.0003496029239613563\n",
      "epoch: 8 step: 730, loss is 0.12180331349372864\n",
      "epoch: 8 step: 731, loss is 0.26812538504600525\n",
      "epoch: 8 step: 732, loss is 0.00025050787371583283\n",
      "epoch: 8 step: 733, loss is 0.0011977877002209425\n",
      "epoch: 8 step: 734, loss is 0.11069021373987198\n",
      "epoch: 8 step: 735, loss is 0.09012384712696075\n",
      "epoch: 8 step: 736, loss is 0.012564401142299175\n",
      "epoch: 8 step: 737, loss is 0.04293297231197357\n",
      "epoch: 8 step: 738, loss is 0.0012044486356899142\n",
      "epoch: 8 step: 739, loss is 0.00912657380104065\n",
      "epoch: 8 step: 740, loss is 0.06248587742447853\n",
      "epoch: 8 step: 741, loss is 0.024624204263091087\n",
      "epoch: 8 step: 742, loss is 0.007001696154475212\n",
      "epoch: 8 step: 743, loss is 0.0011204067850485444\n",
      "epoch: 8 step: 744, loss is 0.004049237817525864\n",
      "epoch: 8 step: 745, loss is 0.09587949514389038\n",
      "epoch: 8 step: 746, loss is 0.036037128418684006\n",
      "epoch: 8 step: 747, loss is 0.08561128377914429\n",
      "epoch: 8 step: 748, loss is 0.009532290510833263\n",
      "epoch: 8 step: 749, loss is 0.0057197557762265205\n",
      "epoch: 8 step: 750, loss is 0.00045348593266680837\n",
      "epoch: 8 step: 751, loss is 0.010837597772479057\n",
      "epoch: 8 step: 752, loss is 0.010882973670959473\n",
      "epoch: 8 step: 753, loss is 0.051382701843976974\n",
      "epoch: 8 step: 754, loss is 0.011914188042283058\n",
      "epoch: 8 step: 755, loss is 0.023967448621988297\n",
      "epoch: 8 step: 756, loss is 0.04450348764657974\n",
      "epoch: 8 step: 757, loss is 0.008499696850776672\n",
      "epoch: 8 step: 758, loss is 0.02056039683520794\n",
      "epoch: 8 step: 759, loss is 0.04684329032897949\n",
      "epoch: 8 step: 760, loss is 0.0006179191404953599\n",
      "epoch: 8 step: 761, loss is 0.014965809881687164\n",
      "epoch: 8 step: 762, loss is 0.22068051993846893\n",
      "epoch: 8 step: 763, loss is 0.0011457770597189665\n",
      "epoch: 8 step: 764, loss is 0.01121785119175911\n",
      "epoch: 8 step: 765, loss is 0.0012943912297487259\n",
      "epoch: 8 step: 766, loss is 0.010503137484192848\n",
      "epoch: 8 step: 767, loss is 0.0003724095586221665\n",
      "epoch: 8 step: 768, loss is 0.006026447284966707\n",
      "epoch: 8 step: 769, loss is 0.010501719079911709\n",
      "epoch: 8 step: 770, loss is 0.09575285017490387\n",
      "epoch: 8 step: 771, loss is 0.0018468272173777223\n",
      "epoch: 8 step: 772, loss is 0.008438329212367535\n",
      "epoch: 8 step: 773, loss is 0.08937177062034607\n",
      "epoch: 8 step: 774, loss is 0.0014149732887744904\n",
      "epoch: 8 step: 775, loss is 0.007356167770922184\n",
      "epoch: 8 step: 776, loss is 0.08922354131937027\n",
      "epoch: 8 step: 777, loss is 0.04257237911224365\n",
      "epoch: 8 step: 778, loss is 0.0027197953313589096\n",
      "epoch: 8 step: 779, loss is 0.02515902742743492\n",
      "epoch: 8 step: 780, loss is 0.029525432735681534\n",
      "epoch: 8 step: 781, loss is 0.0638805404305458\n",
      "epoch: 8 step: 782, loss is 0.0075773438438773155\n",
      "epoch: 8 step: 783, loss is 0.1218935027718544\n",
      "epoch: 8 step: 784, loss is 0.00153084727935493\n",
      "epoch: 8 step: 785, loss is 0.03804761543869972\n",
      "epoch: 8 step: 786, loss is 0.07255882769823074\n",
      "epoch: 8 step: 787, loss is 0.0050575342029333115\n",
      "epoch: 8 step: 788, loss is 0.03812645748257637\n",
      "epoch: 8 step: 789, loss is 0.0440274178981781\n",
      "epoch: 8 step: 790, loss is 0.006356390193104744\n",
      "epoch: 8 step: 791, loss is 0.051315709948539734\n",
      "epoch: 8 step: 792, loss is 0.0035780000034719706\n",
      "epoch: 8 step: 793, loss is 0.0022529992274940014\n",
      "epoch: 8 step: 794, loss is 0.02481485716998577\n",
      "epoch: 8 step: 795, loss is 0.02480394020676613\n",
      "epoch: 8 step: 796, loss is 0.001573292538523674\n",
      "epoch: 8 step: 797, loss is 0.0020693622063845396\n",
      "epoch: 8 step: 798, loss is 0.017823025584220886\n",
      "epoch: 8 step: 799, loss is 0.18268804252147675\n",
      "epoch: 8 step: 800, loss is 0.027393048629164696\n",
      "epoch: 8 step: 801, loss is 0.05613906681537628\n",
      "epoch: 8 step: 802, loss is 0.0071048252284526825\n",
      "epoch: 8 step: 803, loss is 0.011135089211165905\n",
      "epoch: 8 step: 804, loss is 0.033289968967437744\n",
      "epoch: 8 step: 805, loss is 0.0006346165318973362\n",
      "epoch: 8 step: 806, loss is 0.18270151317119598\n",
      "epoch: 8 step: 807, loss is 0.021472668275237083\n",
      "epoch: 8 step: 808, loss is 0.016657650470733643\n",
      "epoch: 8 step: 809, loss is 0.244777649641037\n",
      "epoch: 8 step: 810, loss is 0.0031653204932808876\n",
      "epoch: 8 step: 811, loss is 0.16762672364711761\n",
      "epoch: 8 step: 812, loss is 0.005831780377775431\n",
      "epoch: 8 step: 813, loss is 0.07672585546970367\n",
      "epoch: 8 step: 814, loss is 0.00596400024369359\n",
      "epoch: 8 step: 815, loss is 0.020522555336356163\n",
      "epoch: 8 step: 816, loss is 0.0019186475547030568\n",
      "epoch: 8 step: 817, loss is 0.0014458385994657874\n",
      "epoch: 8 step: 818, loss is 0.00515145156532526\n",
      "epoch: 8 step: 819, loss is 0.010833305306732655\n",
      "epoch: 8 step: 820, loss is 0.04696214199066162\n",
      "epoch: 8 step: 821, loss is 0.17301103472709656\n",
      "epoch: 8 step: 822, loss is 0.00045086865429766476\n",
      "epoch: 8 step: 823, loss is 0.12310653924942017\n",
      "epoch: 8 step: 824, loss is 0.025451060384511948\n",
      "epoch: 8 step: 825, loss is 0.003377673216164112\n",
      "epoch: 8 step: 826, loss is 0.004489257000386715\n",
      "epoch: 8 step: 827, loss is 0.06341772526502609\n",
      "epoch: 8 step: 828, loss is 0.16256645321846008\n",
      "epoch: 8 step: 829, loss is 0.050813715904951096\n",
      "epoch: 8 step: 830, loss is 0.007526400964707136\n",
      "epoch: 8 step: 831, loss is 0.07694646716117859\n",
      "epoch: 8 step: 832, loss is 0.10566892474889755\n",
      "epoch: 8 step: 833, loss is 0.10590898245573044\n",
      "epoch: 8 step: 834, loss is 0.0017770903650671244\n",
      "epoch: 8 step: 835, loss is 0.005939263850450516\n",
      "epoch: 8 step: 836, loss is 0.0028443303890526295\n",
      "epoch: 8 step: 837, loss is 0.0005842051468789577\n",
      "epoch: 8 step: 838, loss is 0.028149578720331192\n",
      "epoch: 8 step: 839, loss is 0.12594471871852875\n",
      "epoch: 8 step: 840, loss is 0.003954291343688965\n",
      "epoch: 8 step: 841, loss is 0.0011089795734733343\n",
      "epoch: 8 step: 842, loss is 0.02349127270281315\n",
      "epoch: 8 step: 843, loss is 0.06173701956868172\n",
      "epoch: 8 step: 844, loss is 0.0005522918654605746\n",
      "epoch: 8 step: 845, loss is 0.09077689796686172\n",
      "epoch: 8 step: 846, loss is 0.007159751374274492\n",
      "epoch: 8 step: 847, loss is 0.06713714450597763\n",
      "epoch: 8 step: 848, loss is 0.00841031689196825\n",
      "epoch: 8 step: 849, loss is 0.06314367055892944\n",
      "epoch: 8 step: 850, loss is 0.01989859901368618\n",
      "epoch: 8 step: 851, loss is 0.2304387092590332\n",
      "epoch: 8 step: 852, loss is 0.04241010174155235\n",
      "epoch: 8 step: 853, loss is 0.3743654787540436\n",
      "epoch: 8 step: 854, loss is 0.005393165163695812\n",
      "epoch: 8 step: 855, loss is 0.004988023079931736\n",
      "epoch: 8 step: 856, loss is 0.0860467180609703\n",
      "epoch: 8 step: 857, loss is 0.014072668738663197\n",
      "epoch: 8 step: 858, loss is 0.010527823120355606\n",
      "epoch: 8 step: 859, loss is 0.3251691460609436\n",
      "epoch: 8 step: 860, loss is 0.00603290181607008\n",
      "epoch: 8 step: 861, loss is 0.007765281014144421\n",
      "epoch: 8 step: 862, loss is 0.01941162720322609\n",
      "epoch: 8 step: 863, loss is 0.06369002908468246\n",
      "epoch: 8 step: 864, loss is 0.042301326990127563\n",
      "epoch: 8 step: 865, loss is 0.0009035090333782136\n",
      "epoch: 8 step: 866, loss is 0.002851970726624131\n",
      "epoch: 8 step: 867, loss is 0.0003561495977919549\n",
      "epoch: 8 step: 868, loss is 0.10407282412052155\n",
      "epoch: 8 step: 869, loss is 0.06567125767469406\n",
      "epoch: 8 step: 870, loss is 0.11236010491847992\n",
      "epoch: 8 step: 871, loss is 0.007201695814728737\n",
      "epoch: 8 step: 872, loss is 0.07274685800075531\n",
      "epoch: 8 step: 873, loss is 0.00041846968815661967\n",
      "epoch: 8 step: 874, loss is 0.05205608159303665\n",
      "epoch: 8 step: 875, loss is 0.043264809995889664\n",
      "epoch: 8 step: 876, loss is 0.0017194461543112993\n",
      "epoch: 8 step: 877, loss is 0.24959711730480194\n",
      "epoch: 8 step: 878, loss is 0.03452792763710022\n",
      "epoch: 8 step: 879, loss is 0.0006590053671970963\n",
      "epoch: 8 step: 880, loss is 0.015992918983101845\n",
      "epoch: 8 step: 881, loss is 0.23463155329227448\n",
      "epoch: 8 step: 882, loss is 0.017662305384874344\n",
      "epoch: 8 step: 883, loss is 0.008775229565799236\n",
      "epoch: 8 step: 884, loss is 0.02591429278254509\n",
      "epoch: 8 step: 885, loss is 0.0021346192806959152\n",
      "epoch: 8 step: 886, loss is 0.0007730855140835047\n",
      "epoch: 8 step: 887, loss is 0.015569171868264675\n",
      "epoch: 8 step: 888, loss is 0.005210916977375746\n",
      "epoch: 8 step: 889, loss is 0.22449295222759247\n",
      "epoch: 8 step: 890, loss is 0.04035304859280586\n",
      "epoch: 8 step: 891, loss is 0.1460038274526596\n",
      "epoch: 8 step: 892, loss is 0.022595791146159172\n",
      "epoch: 8 step: 893, loss is 0.024452760815620422\n",
      "epoch: 8 step: 894, loss is 0.0025341170839965343\n",
      "epoch: 8 step: 895, loss is 0.005594083573669195\n",
      "epoch: 8 step: 896, loss is 0.027675891295075417\n",
      "epoch: 8 step: 897, loss is 0.09862522780895233\n",
      "epoch: 8 step: 898, loss is 0.09476758539676666\n",
      "epoch: 8 step: 899, loss is 0.013416503556072712\n",
      "epoch: 8 step: 900, loss is 0.1594194620847702\n",
      "epoch: 8 step: 901, loss is 0.011864780448377132\n",
      "epoch: 8 step: 902, loss is 0.019520651549100876\n",
      "epoch: 8 step: 903, loss is 0.001648679026402533\n",
      "epoch: 8 step: 904, loss is 0.1518281102180481\n",
      "epoch: 8 step: 905, loss is 0.00896643940359354\n",
      "epoch: 8 step: 906, loss is 0.011659717187285423\n",
      "epoch: 8 step: 907, loss is 0.00027440348640084267\n",
      "epoch: 8 step: 908, loss is 0.03460060432553291\n",
      "epoch: 8 step: 909, loss is 0.03925545513629913\n",
      "epoch: 8 step: 910, loss is 0.021099228411912918\n",
      "epoch: 8 step: 911, loss is 0.006399997044354677\n",
      "epoch: 8 step: 912, loss is 0.013188851997256279\n",
      "epoch: 8 step: 913, loss is 0.01798618771135807\n",
      "epoch: 8 step: 914, loss is 0.007904724217951298\n",
      "epoch: 8 step: 915, loss is 0.17700229585170746\n",
      "epoch: 8 step: 916, loss is 0.06747960299253464\n",
      "epoch: 8 step: 917, loss is 0.004337903577834368\n",
      "epoch: 8 step: 918, loss is 0.017432861030101776\n",
      "epoch: 8 step: 919, loss is 0.03579503297805786\n",
      "epoch: 8 step: 920, loss is 0.0050066509284079075\n",
      "epoch: 8 step: 921, loss is 0.05525840073823929\n",
      "epoch: 8 step: 922, loss is 0.004876034799963236\n",
      "epoch: 8 step: 923, loss is 0.005655272398144007\n",
      "epoch: 8 step: 924, loss is 0.012412810698151588\n",
      "epoch: 8 step: 925, loss is 0.02157004177570343\n",
      "epoch: 8 step: 926, loss is 0.02602408640086651\n",
      "epoch: 8 step: 927, loss is 0.007812644354999065\n",
      "epoch: 8 step: 928, loss is 0.015144268050789833\n",
      "epoch: 8 step: 929, loss is 0.0014349498087540269\n",
      "epoch: 8 step: 930, loss is 0.019640587270259857\n",
      "epoch: 8 step: 931, loss is 0.04178744554519653\n",
      "epoch: 8 step: 932, loss is 0.004370270296931267\n",
      "epoch: 8 step: 933, loss is 0.026531659066677094\n",
      "epoch: 8 step: 934, loss is 0.020465295761823654\n",
      "epoch: 8 step: 935, loss is 0.054182268679142\n",
      "epoch: 8 step: 936, loss is 0.011912696994841099\n",
      "epoch: 8 step: 937, loss is 0.002432091161608696\n",
      "epoch: 8 step: 938, loss is 0.0036998677533119917\n",
      "epoch: 8 step: 939, loss is 0.004795572720468044\n",
      "epoch: 8 step: 940, loss is 0.07277584075927734\n",
      "epoch: 8 step: 941, loss is 0.004091721028089523\n",
      "epoch: 8 step: 942, loss is 0.0008661295869387686\n",
      "epoch: 8 step: 943, loss is 0.0892028734087944\n",
      "epoch: 8 step: 944, loss is 0.04300633445382118\n",
      "epoch: 8 step: 945, loss is 0.010271461680531502\n",
      "epoch: 8 step: 946, loss is 0.0018381130648776889\n",
      "epoch: 8 step: 947, loss is 0.0083578210324049\n",
      "epoch: 8 step: 948, loss is 0.12327130138874054\n",
      "epoch: 8 step: 949, loss is 0.04186522960662842\n",
      "epoch: 8 step: 950, loss is 0.06544585525989532\n",
      "epoch: 8 step: 951, loss is 0.019124710932374\n",
      "epoch: 8 step: 952, loss is 0.0026192504446953535\n",
      "epoch: 8 step: 953, loss is 0.03357895091176033\n",
      "epoch: 8 step: 954, loss is 0.002960989950224757\n",
      "epoch: 8 step: 955, loss is 0.0010564958211034536\n",
      "epoch: 8 step: 956, loss is 0.002958261175081134\n",
      "epoch: 8 step: 957, loss is 0.0011555826058611274\n",
      "epoch: 8 step: 958, loss is 0.03735136613249779\n",
      "epoch: 8 step: 959, loss is 0.0005202742759138346\n",
      "epoch: 8 step: 960, loss is 0.027083978056907654\n",
      "epoch: 8 step: 961, loss is 0.11285248398780823\n",
      "epoch: 8 step: 962, loss is 0.002395657589659095\n",
      "epoch: 8 step: 963, loss is 0.026477277278900146\n",
      "epoch: 8 step: 964, loss is 0.05636581405997276\n",
      "epoch: 8 step: 965, loss is 0.006870986893773079\n",
      "epoch: 8 step: 966, loss is 0.001388420001603663\n",
      "epoch: 8 step: 967, loss is 0.022721119225025177\n",
      "epoch: 8 step: 968, loss is 0.06960845738649368\n",
      "epoch: 8 step: 969, loss is 0.0006631456199102104\n",
      "epoch: 8 step: 970, loss is 0.19504287838935852\n",
      "epoch: 8 step: 971, loss is 0.09994175285100937\n",
      "epoch: 8 step: 972, loss is 0.010717794299125671\n",
      "epoch: 8 step: 973, loss is 0.04193311929702759\n",
      "epoch: 8 step: 974, loss is 0.016367748379707336\n",
      "epoch: 8 step: 975, loss is 0.0016250176122412086\n",
      "epoch: 8 step: 976, loss is 0.09891286492347717\n",
      "epoch: 8 step: 977, loss is 0.0018649306148290634\n",
      "epoch: 8 step: 978, loss is 0.05715358257293701\n",
      "epoch: 8 step: 979, loss is 0.08822830021381378\n",
      "epoch: 8 step: 980, loss is 0.005797970574349165\n",
      "epoch: 8 step: 981, loss is 0.0010495621245354414\n",
      "epoch: 8 step: 982, loss is 0.0007112803286872804\n",
      "epoch: 8 step: 983, loss is 0.007040271069854498\n",
      "epoch: 8 step: 984, loss is 0.0006981609622016549\n",
      "epoch: 8 step: 985, loss is 0.01231291051954031\n",
      "epoch: 8 step: 986, loss is 0.05741465464234352\n",
      "epoch: 8 step: 987, loss is 0.05171727389097214\n",
      "epoch: 8 step: 988, loss is 0.009537889622151852\n",
      "epoch: 8 step: 989, loss is 0.0019545999821275473\n",
      "epoch: 8 step: 990, loss is 0.0057577285915613174\n",
      "epoch: 8 step: 991, loss is 0.004827775061130524\n",
      "epoch: 8 step: 992, loss is 0.0012771270703524351\n",
      "epoch: 8 step: 993, loss is 0.001928364159539342\n",
      "epoch: 8 step: 994, loss is 0.008566784672439098\n",
      "epoch: 8 step: 995, loss is 0.07734938710927963\n",
      "epoch: 8 step: 996, loss is 0.0019434450659900904\n",
      "epoch: 8 step: 997, loss is 0.0016020386246964335\n",
      "epoch: 8 step: 998, loss is 0.00196593115106225\n",
      "epoch: 8 step: 999, loss is 0.08517517149448395\n",
      "epoch: 8 step: 1000, loss is 0.12523627281188965\n",
      "epoch: 8 step: 1001, loss is 0.04633915424346924\n",
      "epoch: 8 step: 1002, loss is 0.0008313958533108234\n",
      "epoch: 8 step: 1003, loss is 0.003393080085515976\n",
      "epoch: 8 step: 1004, loss is 0.0008985137101262808\n",
      "epoch: 8 step: 1005, loss is 0.013699549250304699\n",
      "epoch: 8 step: 1006, loss is 0.046107880771160126\n",
      "epoch: 8 step: 1007, loss is 0.06697339564561844\n",
      "epoch: 8 step: 1008, loss is 0.0009880932047963142\n",
      "epoch: 8 step: 1009, loss is 0.0449998676776886\n",
      "epoch: 8 step: 1010, loss is 0.03674762323498726\n",
      "epoch: 8 step: 1011, loss is 0.0017515860963612795\n",
      "epoch: 8 step: 1012, loss is 0.003244213992729783\n",
      "epoch: 8 step: 1013, loss is 0.0017843061359599233\n",
      "epoch: 8 step: 1014, loss is 0.02545740082859993\n",
      "epoch: 8 step: 1015, loss is 0.005711943842470646\n",
      "epoch: 8 step: 1016, loss is 0.005968826822936535\n",
      "epoch: 8 step: 1017, loss is 0.0012387075694277883\n",
      "epoch: 8 step: 1018, loss is 0.012362738139927387\n",
      "epoch: 8 step: 1019, loss is 0.029220404103398323\n",
      "epoch: 8 step: 1020, loss is 0.06703062355518341\n",
      "epoch: 8 step: 1021, loss is 0.005492118187248707\n",
      "epoch: 8 step: 1022, loss is 0.002759879920631647\n",
      "epoch: 8 step: 1023, loss is 0.10880035161972046\n",
      "epoch: 8 step: 1024, loss is 0.00012500709271989763\n",
      "epoch: 8 step: 1025, loss is 0.13349497318267822\n",
      "epoch: 8 step: 1026, loss is 0.019664131104946136\n",
      "epoch: 8 step: 1027, loss is 0.00045134968240745366\n",
      "epoch: 8 step: 1028, loss is 0.026729067787528038\n",
      "epoch: 8 step: 1029, loss is 0.022757120430469513\n",
      "epoch: 8 step: 1030, loss is 0.010839460417628288\n",
      "epoch: 8 step: 1031, loss is 0.02803783491253853\n",
      "epoch: 8 step: 1032, loss is 0.01226757187396288\n",
      "epoch: 8 step: 1033, loss is 0.01631835475564003\n",
      "epoch: 8 step: 1034, loss is 0.14394541084766388\n",
      "epoch: 8 step: 1035, loss is 0.001429876429028809\n",
      "epoch: 8 step: 1036, loss is 0.0023696310818195343\n",
      "epoch: 8 step: 1037, loss is 0.005812671035528183\n",
      "epoch: 8 step: 1038, loss is 0.01850108429789543\n",
      "epoch: 8 step: 1039, loss is 0.014298680238425732\n",
      "epoch: 8 step: 1040, loss is 0.006694810464978218\n",
      "epoch: 8 step: 1041, loss is 0.05847892537713051\n",
      "epoch: 8 step: 1042, loss is 0.00502411462366581\n",
      "epoch: 8 step: 1043, loss is 0.023908233270049095\n",
      "epoch: 8 step: 1044, loss is 0.0021614849101752043\n",
      "epoch: 8 step: 1045, loss is 0.0015250399010255933\n",
      "epoch: 8 step: 1046, loss is 0.002071164548397064\n",
      "epoch: 8 step: 1047, loss is 0.0013841637410223484\n",
      "epoch: 8 step: 1048, loss is 0.04547899216413498\n",
      "epoch: 8 step: 1049, loss is 0.006427745334804058\n",
      "epoch: 8 step: 1050, loss is 0.005538990255445242\n",
      "epoch: 8 step: 1051, loss is 0.000651978887617588\n",
      "epoch: 8 step: 1052, loss is 0.0023845017421990633\n",
      "epoch: 8 step: 1053, loss is 0.11149057000875473\n",
      "epoch: 8 step: 1054, loss is 0.0011947345919907093\n",
      "epoch: 8 step: 1055, loss is 0.027373507618904114\n",
      "epoch: 8 step: 1056, loss is 0.08140678703784943\n",
      "epoch: 8 step: 1057, loss is 0.0005401296075433493\n",
      "epoch: 8 step: 1058, loss is 0.06375499069690704\n",
      "epoch: 8 step: 1059, loss is 0.05001315847039223\n",
      "epoch: 8 step: 1060, loss is 0.0011229857336729765\n",
      "epoch: 8 step: 1061, loss is 0.0013493843143805861\n",
      "epoch: 8 step: 1062, loss is 0.08515431731939316\n",
      "epoch: 8 step: 1063, loss is 0.12968511879444122\n",
      "epoch: 8 step: 1064, loss is 0.005573096685111523\n",
      "epoch: 8 step: 1065, loss is 0.0026747395750135183\n",
      "epoch: 8 step: 1066, loss is 0.005679360590875149\n",
      "epoch: 8 step: 1067, loss is 0.029315810650587082\n",
      "epoch: 8 step: 1068, loss is 0.021861623972654343\n",
      "epoch: 8 step: 1069, loss is 0.008030369877815247\n",
      "epoch: 8 step: 1070, loss is 0.009550845250487328\n",
      "epoch: 8 step: 1071, loss is 0.0009692581952549517\n",
      "epoch: 8 step: 1072, loss is 0.003919017966836691\n",
      "epoch: 8 step: 1073, loss is 0.0373433381319046\n",
      "epoch: 8 step: 1074, loss is 0.29659855365753174\n",
      "epoch: 8 step: 1075, loss is 0.001421333639882505\n",
      "epoch: 8 step: 1076, loss is 0.05936190485954285\n",
      "epoch: 8 step: 1077, loss is 0.017235223203897476\n",
      "epoch: 8 step: 1078, loss is 0.0033507586922496557\n",
      "epoch: 8 step: 1079, loss is 0.0014932336052879691\n",
      "epoch: 8 step: 1080, loss is 0.3120659291744232\n",
      "epoch: 8 step: 1081, loss is 0.0021900420542806387\n",
      "epoch: 8 step: 1082, loss is 0.0027871790807694197\n",
      "epoch: 8 step: 1083, loss is 0.0006342410924844444\n",
      "epoch: 8 step: 1084, loss is 0.006855615880340338\n",
      "epoch: 8 step: 1085, loss is 0.10373241454362869\n",
      "epoch: 8 step: 1086, loss is 0.006244801916182041\n",
      "epoch: 8 step: 1087, loss is 0.03338395804166794\n",
      "epoch: 8 step: 1088, loss is 0.0007326591294258833\n",
      "epoch: 8 step: 1089, loss is 0.02227271907031536\n",
      "epoch: 8 step: 1090, loss is 0.0017882628599181771\n",
      "epoch: 8 step: 1091, loss is 0.019064228981733322\n",
      "epoch: 8 step: 1092, loss is 0.20527464151382446\n",
      "epoch: 8 step: 1093, loss is 0.15133997797966003\n",
      "epoch: 8 step: 1094, loss is 0.008214930072426796\n",
      "epoch: 8 step: 1095, loss is 0.16327740252017975\n",
      "epoch: 8 step: 1096, loss is 0.03147123381495476\n",
      "epoch: 8 step: 1097, loss is 0.018485477194190025\n",
      "epoch: 8 step: 1098, loss is 0.02469775266945362\n",
      "epoch: 8 step: 1099, loss is 0.0016003542114049196\n",
      "epoch: 8 step: 1100, loss is 0.03564790263772011\n",
      "epoch: 8 step: 1101, loss is 0.028248170390725136\n",
      "epoch: 8 step: 1102, loss is 0.25073379278182983\n",
      "epoch: 8 step: 1103, loss is 0.0026961613912135363\n",
      "epoch: 8 step: 1104, loss is 0.007082092110067606\n",
      "epoch: 8 step: 1105, loss is 0.0029558446258306503\n",
      "epoch: 8 step: 1106, loss is 0.0030412969645112753\n",
      "epoch: 8 step: 1107, loss is 0.09587197750806808\n",
      "epoch: 8 step: 1108, loss is 0.025915198028087616\n",
      "epoch: 8 step: 1109, loss is 0.003196301171556115\n",
      "epoch: 8 step: 1110, loss is 0.0006430992507375777\n",
      "epoch: 8 step: 1111, loss is 0.00030028665787540376\n",
      "epoch: 8 step: 1112, loss is 0.003985612187534571\n",
      "epoch: 8 step: 1113, loss is 0.0031163699459284544\n",
      "epoch: 8 step: 1114, loss is 0.1701311618089676\n",
      "epoch: 8 step: 1115, loss is 0.006345695350319147\n",
      "epoch: 8 step: 1116, loss is 0.12104938924312592\n",
      "epoch: 8 step: 1117, loss is 0.0012761414982378483\n",
      "epoch: 8 step: 1118, loss is 0.05765262618660927\n",
      "epoch: 8 step: 1119, loss is 0.25129663944244385\n",
      "epoch: 8 step: 1120, loss is 0.050746724009513855\n",
      "epoch: 8 step: 1121, loss is 0.0009093123953789473\n",
      "epoch: 8 step: 1122, loss is 0.0007665215525776148\n",
      "epoch: 8 step: 1123, loss is 0.040435295552015305\n",
      "epoch: 8 step: 1124, loss is 0.003181734587997198\n",
      "epoch: 8 step: 1125, loss is 0.055361632257699966\n",
      "epoch: 8 step: 1126, loss is 0.02605826035141945\n",
      "epoch: 8 step: 1127, loss is 0.09000235795974731\n",
      "epoch: 8 step: 1128, loss is 0.025752278044819832\n",
      "epoch: 8 step: 1129, loss is 0.3135426640510559\n",
      "epoch: 8 step: 1130, loss is 0.006423288956284523\n",
      "epoch: 8 step: 1131, loss is 0.03471606969833374\n",
      "epoch: 8 step: 1132, loss is 0.0002600551233626902\n",
      "epoch: 8 step: 1133, loss is 0.01086144894361496\n",
      "epoch: 8 step: 1134, loss is 0.329110711812973\n",
      "epoch: 8 step: 1135, loss is 0.015364671126008034\n",
      "epoch: 8 step: 1136, loss is 0.03826254606246948\n",
      "epoch: 8 step: 1137, loss is 0.019598238170146942\n",
      "epoch: 8 step: 1138, loss is 0.017717326059937477\n",
      "epoch: 8 step: 1139, loss is 0.0009183643269352615\n",
      "epoch: 8 step: 1140, loss is 0.002393733011558652\n",
      "epoch: 8 step: 1141, loss is 0.010696163401007652\n",
      "epoch: 8 step: 1142, loss is 0.0012174785370007157\n",
      "epoch: 8 step: 1143, loss is 0.012046229094266891\n",
      "epoch: 8 step: 1144, loss is 0.012764404527842999\n",
      "epoch: 8 step: 1145, loss is 0.0008316394523717463\n",
      "epoch: 8 step: 1146, loss is 0.0016498096520081162\n",
      "epoch: 8 step: 1147, loss is 0.012060381472110748\n",
      "epoch: 8 step: 1148, loss is 0.02501118928194046\n",
      "epoch: 8 step: 1149, loss is 0.004767933394759893\n",
      "epoch: 8 step: 1150, loss is 0.07003987580537796\n",
      "epoch: 8 step: 1151, loss is 0.0073474920354783535\n",
      "epoch: 8 step: 1152, loss is 0.0135948546230793\n",
      "epoch: 8 step: 1153, loss is 0.09109155088663101\n",
      "epoch: 8 step: 1154, loss is 0.10652637481689453\n",
      "epoch: 8 step: 1155, loss is 0.0022263459395617247\n",
      "epoch: 8 step: 1156, loss is 0.001849560416303575\n",
      "epoch: 8 step: 1157, loss is 0.04894963651895523\n",
      "epoch: 8 step: 1158, loss is 0.0017374176532030106\n",
      "epoch: 8 step: 1159, loss is 0.03725671023130417\n",
      "epoch: 8 step: 1160, loss is 0.00474602822214365\n",
      "epoch: 8 step: 1161, loss is 0.061350882053375244\n",
      "epoch: 8 step: 1162, loss is 0.004491187632083893\n",
      "epoch: 8 step: 1163, loss is 0.13994254171848297\n",
      "epoch: 8 step: 1164, loss is 0.0011433837935328484\n",
      "epoch: 8 step: 1165, loss is 0.18038630485534668\n",
      "epoch: 8 step: 1166, loss is 0.005437280982732773\n",
      "epoch: 8 step: 1167, loss is 0.10492342710494995\n",
      "epoch: 8 step: 1168, loss is 0.04231388494372368\n",
      "epoch: 8 step: 1169, loss is 0.0037285685539245605\n",
      "epoch: 8 step: 1170, loss is 0.053604934364557266\n",
      "epoch: 8 step: 1171, loss is 0.06800220906734467\n",
      "epoch: 8 step: 1172, loss is 0.011976965703070164\n",
      "epoch: 8 step: 1173, loss is 0.005495346151292324\n",
      "epoch: 8 step: 1174, loss is 0.012864508666098118\n",
      "epoch: 8 step: 1175, loss is 0.0004009195836260915\n",
      "epoch: 8 step: 1176, loss is 0.025681262835860252\n",
      "epoch: 8 step: 1177, loss is 0.0056439898908138275\n",
      "epoch: 8 step: 1178, loss is 0.06552767753601074\n",
      "epoch: 8 step: 1179, loss is 0.007962784729897976\n",
      "epoch: 8 step: 1180, loss is 0.23127157986164093\n",
      "epoch: 8 step: 1181, loss is 0.0018596244044601917\n",
      "epoch: 8 step: 1182, loss is 0.005377775989472866\n",
      "epoch: 8 step: 1183, loss is 0.0014344941591843963\n",
      "epoch: 8 step: 1184, loss is 0.06382746249437332\n",
      "epoch: 8 step: 1185, loss is 0.05688301846385002\n",
      "epoch: 8 step: 1186, loss is 0.048336584120988846\n",
      "epoch: 8 step: 1187, loss is 0.0009369748295284808\n",
      "epoch: 8 step: 1188, loss is 0.013049885630607605\n",
      "epoch: 8 step: 1189, loss is 0.0058914474211633205\n",
      "epoch: 8 step: 1190, loss is 0.01344392355531454\n",
      "epoch: 8 step: 1191, loss is 0.17223632335662842\n",
      "epoch: 8 step: 1192, loss is 0.0022911615669727325\n",
      "epoch: 8 step: 1193, loss is 0.04196036607027054\n",
      "epoch: 8 step: 1194, loss is 0.011636011302471161\n",
      "epoch: 8 step: 1195, loss is 0.010531898587942123\n",
      "epoch: 8 step: 1196, loss is 0.21518927812576294\n",
      "epoch: 8 step: 1197, loss is 0.024314766749739647\n",
      "epoch: 8 step: 1198, loss is 0.06882228702306747\n",
      "epoch: 8 step: 1199, loss is 0.027832308784127235\n",
      "epoch: 8 step: 1200, loss is 0.03900958225131035\n",
      "epoch: 8 step: 1201, loss is 0.002303139306604862\n",
      "epoch: 8 step: 1202, loss is 0.03301791101694107\n",
      "epoch: 8 step: 1203, loss is 0.21576954424381256\n",
      "epoch: 8 step: 1204, loss is 0.018755104392766953\n",
      "epoch: 8 step: 1205, loss is 0.005887215491384268\n",
      "epoch: 8 step: 1206, loss is 0.001529934350401163\n",
      "epoch: 8 step: 1207, loss is 0.009200677275657654\n",
      "epoch: 8 step: 1208, loss is 0.014782404527068138\n",
      "epoch: 8 step: 1209, loss is 0.01693391613662243\n",
      "epoch: 8 step: 1210, loss is 0.009134030900895596\n",
      "epoch: 8 step: 1211, loss is 0.0026763679925352335\n",
      "epoch: 8 step: 1212, loss is 0.015711722895503044\n",
      "epoch: 8 step: 1213, loss is 0.01953529380261898\n",
      "epoch: 8 step: 1214, loss is 0.1077834740281105\n",
      "epoch: 8 step: 1215, loss is 0.01615293323993683\n",
      "epoch: 8 step: 1216, loss is 0.01468740962445736\n",
      "epoch: 8 step: 1217, loss is 0.03836189582943916\n",
      "epoch: 8 step: 1218, loss is 0.00505111226812005\n",
      "epoch: 8 step: 1219, loss is 0.07326314598321915\n",
      "epoch: 8 step: 1220, loss is 0.04388519749045372\n",
      "epoch: 8 step: 1221, loss is 0.011529508978128433\n",
      "epoch: 8 step: 1222, loss is 0.011279383674263954\n",
      "epoch: 8 step: 1223, loss is 0.022696450352668762\n",
      "epoch: 8 step: 1224, loss is 0.004090452566742897\n",
      "epoch: 8 step: 1225, loss is 0.009717922657728195\n",
      "epoch: 8 step: 1226, loss is 0.057693351060152054\n",
      "epoch: 8 step: 1227, loss is 0.023275161162018776\n",
      "epoch: 8 step: 1228, loss is 0.05743050575256348\n",
      "epoch: 8 step: 1229, loss is 0.004583741072565317\n",
      "epoch: 8 step: 1230, loss is 0.09921474754810333\n",
      "epoch: 8 step: 1231, loss is 0.02644624374806881\n",
      "epoch: 8 step: 1232, loss is 0.001635148306377232\n",
      "epoch: 8 step: 1233, loss is 0.08738754689693451\n",
      "epoch: 8 step: 1234, loss is 0.05408905819058418\n",
      "epoch: 8 step: 1235, loss is 0.0011928451713174582\n",
      "epoch: 8 step: 1236, loss is 0.007412209175527096\n",
      "epoch: 8 step: 1237, loss is 0.019434938207268715\n",
      "epoch: 8 step: 1238, loss is 0.0003970974066760391\n",
      "epoch: 8 step: 1239, loss is 0.018271831795573235\n",
      "epoch: 8 step: 1240, loss is 0.11578397452831268\n",
      "epoch: 8 step: 1241, loss is 0.010626036673784256\n",
      "epoch: 8 step: 1242, loss is 0.02348097413778305\n",
      "epoch: 8 step: 1243, loss is 0.06348226219415665\n",
      "epoch: 8 step: 1244, loss is 0.06860772520303726\n",
      "epoch: 8 step: 1245, loss is 0.05372891575098038\n",
      "epoch: 8 step: 1246, loss is 0.009945504367351532\n",
      "epoch: 8 step: 1247, loss is 0.002605502726510167\n",
      "epoch: 8 step: 1248, loss is 0.18634667992591858\n",
      "epoch: 8 step: 1249, loss is 0.20530682802200317\n",
      "epoch: 8 step: 1250, loss is 0.009990169666707516\n",
      "epoch: 8 step: 1251, loss is 0.05694536492228508\n",
      "epoch: 8 step: 1252, loss is 0.012183617800474167\n",
      "epoch: 8 step: 1253, loss is 0.004038605373352766\n",
      "epoch: 8 step: 1254, loss is 0.0051230681128799915\n",
      "epoch: 8 step: 1255, loss is 0.0037163267843425274\n",
      "epoch: 8 step: 1256, loss is 0.016246706247329712\n",
      "epoch: 8 step: 1257, loss is 0.010752255097031593\n",
      "epoch: 8 step: 1258, loss is 0.049666937440633774\n",
      "epoch: 8 step: 1259, loss is 0.20979437232017517\n",
      "epoch: 8 step: 1260, loss is 0.18050354719161987\n",
      "epoch: 8 step: 1261, loss is 0.012599951587617397\n",
      "epoch: 8 step: 1262, loss is 0.06124112755060196\n",
      "epoch: 8 step: 1263, loss is 0.028368029743433\n",
      "epoch: 8 step: 1264, loss is 0.06159619614481926\n",
      "epoch: 8 step: 1265, loss is 0.003304483601823449\n",
      "epoch: 8 step: 1266, loss is 0.016854260116815567\n",
      "epoch: 8 step: 1267, loss is 0.00030807897564955056\n",
      "epoch: 8 step: 1268, loss is 0.0452529713511467\n",
      "epoch: 8 step: 1269, loss is 0.012720885686576366\n",
      "epoch: 8 step: 1270, loss is 0.06267018616199493\n",
      "epoch: 8 step: 1271, loss is 0.0022349620703607798\n",
      "epoch: 8 step: 1272, loss is 0.00322397006675601\n",
      "epoch: 8 step: 1273, loss is 0.0011500787222757936\n",
      "epoch: 8 step: 1274, loss is 0.057212281972169876\n",
      "epoch: 8 step: 1275, loss is 0.03503478690981865\n",
      "epoch: 8 step: 1276, loss is 0.0002151775115635246\n",
      "epoch: 8 step: 1277, loss is 0.0036453253123909235\n",
      "epoch: 8 step: 1278, loss is 0.00111426564399153\n",
      "epoch: 8 step: 1279, loss is 0.01865077205002308\n",
      "epoch: 8 step: 1280, loss is 0.0010373976547271013\n",
      "epoch: 8 step: 1281, loss is 0.0937328189611435\n",
      "epoch: 8 step: 1282, loss is 0.032049600034952164\n",
      "epoch: 8 step: 1283, loss is 0.006043393164873123\n",
      "epoch: 8 step: 1284, loss is 0.18822813034057617\n",
      "epoch: 8 step: 1285, loss is 0.0006784789729863405\n",
      "epoch: 8 step: 1286, loss is 0.0008697787416167557\n",
      "epoch: 8 step: 1287, loss is 0.010635612532496452\n",
      "epoch: 8 step: 1288, loss is 0.07933653146028519\n",
      "epoch: 8 step: 1289, loss is 0.06002891808748245\n",
      "epoch: 8 step: 1290, loss is 0.06610646098852158\n",
      "epoch: 8 step: 1291, loss is 0.001553695066832006\n",
      "epoch: 8 step: 1292, loss is 0.01982928439974785\n",
      "epoch: 8 step: 1293, loss is 0.041578635573387146\n",
      "epoch: 8 step: 1294, loss is 0.002269358839839697\n",
      "epoch: 8 step: 1295, loss is 0.009108385071158409\n",
      "epoch: 8 step: 1296, loss is 0.04408447816967964\n",
      "epoch: 8 step: 1297, loss is 0.04321329668164253\n",
      "epoch: 8 step: 1298, loss is 0.004299849737435579\n",
      "epoch: 8 step: 1299, loss is 0.16930077970027924\n",
      "epoch: 8 step: 1300, loss is 0.036589138209819794\n",
      "epoch: 8 step: 1301, loss is 0.0706731379032135\n",
      "epoch: 8 step: 1302, loss is 0.006282635498791933\n",
      "epoch: 8 step: 1303, loss is 0.00275758421048522\n",
      "epoch: 8 step: 1304, loss is 0.17803847789764404\n",
      "epoch: 8 step: 1305, loss is 0.09529213607311249\n",
      "epoch: 8 step: 1306, loss is 0.043588560074567795\n",
      "epoch: 8 step: 1307, loss is 0.036512408405542374\n",
      "epoch: 8 step: 1308, loss is 0.07440587878227234\n",
      "epoch: 8 step: 1309, loss is 0.007678372319787741\n",
      "epoch: 8 step: 1310, loss is 0.15102097392082214\n",
      "epoch: 8 step: 1311, loss is 0.001990454038605094\n",
      "epoch: 8 step: 1312, loss is 0.0033564704935997725\n",
      "epoch: 8 step: 1313, loss is 0.13696536421775818\n",
      "epoch: 8 step: 1314, loss is 0.07941965758800507\n",
      "epoch: 8 step: 1315, loss is 0.001457122270949185\n",
      "epoch: 8 step: 1316, loss is 0.02259996347129345\n",
      "epoch: 8 step: 1317, loss is 0.029886307194828987\n",
      "epoch: 8 step: 1318, loss is 0.008109960705041885\n",
      "epoch: 8 step: 1319, loss is 0.0021567498333752155\n",
      "epoch: 8 step: 1320, loss is 0.057315632700920105\n",
      "epoch: 8 step: 1321, loss is 0.030017752200365067\n",
      "epoch: 8 step: 1322, loss is 0.014732998795807362\n",
      "epoch: 8 step: 1323, loss is 0.010338719002902508\n",
      "epoch: 8 step: 1324, loss is 0.004474953282624483\n",
      "epoch: 8 step: 1325, loss is 0.030497979372739792\n",
      "epoch: 8 step: 1326, loss is 0.04081560671329498\n",
      "epoch: 8 step: 1327, loss is 0.010651021264493465\n",
      "epoch: 8 step: 1328, loss is 0.028270799666643143\n",
      "epoch: 8 step: 1329, loss is 0.002508984412997961\n",
      "epoch: 8 step: 1330, loss is 0.0014681469183415174\n",
      "epoch: 8 step: 1331, loss is 0.01098642498254776\n",
      "epoch: 8 step: 1332, loss is 0.07341886311769485\n",
      "epoch: 8 step: 1333, loss is 0.06922120600938797\n",
      "epoch: 8 step: 1334, loss is 0.0034451852552592754\n",
      "epoch: 8 step: 1335, loss is 0.006508528720587492\n",
      "epoch: 8 step: 1336, loss is 0.004594271536916494\n",
      "epoch: 8 step: 1337, loss is 0.022764798253774643\n",
      "epoch: 8 step: 1338, loss is 0.05262051150202751\n",
      "epoch: 8 step: 1339, loss is 0.04806150123476982\n",
      "epoch: 8 step: 1340, loss is 0.1552196741104126\n",
      "epoch: 8 step: 1341, loss is 0.005792061798274517\n",
      "epoch: 8 step: 1342, loss is 0.05702061951160431\n",
      "epoch: 8 step: 1343, loss is 0.0008180646691471338\n",
      "epoch: 8 step: 1344, loss is 0.09266985207796097\n",
      "epoch: 8 step: 1345, loss is 0.03633479028940201\n",
      "epoch: 8 step: 1346, loss is 0.15949156880378723\n",
      "epoch: 8 step: 1347, loss is 0.0015702315140515566\n",
      "epoch: 8 step: 1348, loss is 0.0009042072924785316\n",
      "epoch: 8 step: 1349, loss is 0.013924453407526016\n",
      "epoch: 8 step: 1350, loss is 0.006068829912692308\n",
      "epoch: 8 step: 1351, loss is 0.0006814228254370391\n",
      "epoch: 8 step: 1352, loss is 0.07380849868059158\n",
      "epoch: 8 step: 1353, loss is 0.015512429177761078\n",
      "epoch: 8 step: 1354, loss is 0.0533963106572628\n",
      "epoch: 8 step: 1355, loss is 0.025825325399637222\n",
      "epoch: 8 step: 1356, loss is 0.009379814378917217\n",
      "epoch: 8 step: 1357, loss is 0.008672812953591347\n",
      "epoch: 8 step: 1358, loss is 0.06092531979084015\n",
      "epoch: 8 step: 1359, loss is 0.004090884234756231\n",
      "epoch: 8 step: 1360, loss is 0.036063533276319504\n",
      "epoch: 8 step: 1361, loss is 0.010136988945305347\n",
      "epoch: 8 step: 1362, loss is 0.0015217354521155357\n",
      "epoch: 8 step: 1363, loss is 0.08005119860172272\n",
      "epoch: 8 step: 1364, loss is 0.015895677730441093\n",
      "epoch: 8 step: 1365, loss is 0.0002822254609782249\n",
      "epoch: 8 step: 1366, loss is 0.047940053045749664\n",
      "epoch: 8 step: 1367, loss is 0.007496730424463749\n",
      "epoch: 8 step: 1368, loss is 0.024803079664707184\n",
      "epoch: 8 step: 1369, loss is 0.05383056402206421\n",
      "epoch: 8 step: 1370, loss is 0.02356557361781597\n",
      "epoch: 8 step: 1371, loss is 0.020837292075157166\n",
      "epoch: 8 step: 1372, loss is 0.001727161929011345\n",
      "epoch: 8 step: 1373, loss is 0.0024774447083473206\n",
      "epoch: 8 step: 1374, loss is 0.03642357885837555\n",
      "epoch: 8 step: 1375, loss is 0.07421349734067917\n",
      "epoch: 8 step: 1376, loss is 0.0018090757075697184\n",
      "epoch: 8 step: 1377, loss is 0.006796679925173521\n",
      "epoch: 8 step: 1378, loss is 0.04591252654790878\n",
      "epoch: 8 step: 1379, loss is 0.007975796237587929\n",
      "epoch: 8 step: 1380, loss is 0.026360444724559784\n",
      "epoch: 8 step: 1381, loss is 0.0006217500776983798\n",
      "epoch: 8 step: 1382, loss is 0.009042857214808464\n",
      "epoch: 8 step: 1383, loss is 0.002953927032649517\n",
      "epoch: 8 step: 1384, loss is 0.012049492448568344\n",
      "epoch: 8 step: 1385, loss is 0.15963514149188995\n",
      "epoch: 8 step: 1386, loss is 0.0033302505034953356\n",
      "epoch: 8 step: 1387, loss is 0.0731906071305275\n",
      "epoch: 8 step: 1388, loss is 0.0029609487392008305\n",
      "epoch: 8 step: 1389, loss is 0.001134185935370624\n",
      "epoch: 8 step: 1390, loss is 0.10334537923336029\n",
      "epoch: 8 step: 1391, loss is 0.004111431073397398\n",
      "epoch: 8 step: 1392, loss is 0.10130740702152252\n",
      "epoch: 8 step: 1393, loss is 0.023222416639328003\n",
      "epoch: 8 step: 1394, loss is 0.04065610095858574\n",
      "epoch: 8 step: 1395, loss is 0.19287078082561493\n",
      "epoch: 8 step: 1396, loss is 0.015487883239984512\n",
      "epoch: 8 step: 1397, loss is 0.06638563424348831\n",
      "epoch: 8 step: 1398, loss is 0.20579709112644196\n",
      "epoch: 8 step: 1399, loss is 0.02303679846227169\n",
      "epoch: 8 step: 1400, loss is 0.051798056811094284\n",
      "epoch: 8 step: 1401, loss is 0.008560048416256905\n",
      "epoch: 8 step: 1402, loss is 0.05784667655825615\n",
      "epoch: 8 step: 1403, loss is 0.031669460237026215\n",
      "epoch: 8 step: 1404, loss is 0.06330082565546036\n",
      "epoch: 8 step: 1405, loss is 0.003910343628376722\n",
      "epoch: 8 step: 1406, loss is 0.014969746582210064\n",
      "epoch: 8 step: 1407, loss is 0.09203457087278366\n",
      "epoch: 8 step: 1408, loss is 0.000822767848148942\n",
      "epoch: 8 step: 1409, loss is 0.0006715630879625678\n",
      "epoch: 8 step: 1410, loss is 0.0005930675542913377\n",
      "epoch: 8 step: 1411, loss is 0.0004259187262505293\n",
      "epoch: 8 step: 1412, loss is 0.001853219699114561\n",
      "epoch: 8 step: 1413, loss is 0.003495109500363469\n",
      "epoch: 8 step: 1414, loss is 0.11571172624826431\n",
      "epoch: 8 step: 1415, loss is 0.0002823679824359715\n",
      "epoch: 8 step: 1416, loss is 0.0047630132175982\n",
      "epoch: 8 step: 1417, loss is 0.0629836767911911\n",
      "epoch: 8 step: 1418, loss is 0.004286647774279118\n",
      "epoch: 8 step: 1419, loss is 0.046094510704278946\n",
      "epoch: 8 step: 1420, loss is 0.07315672934055328\n",
      "epoch: 8 step: 1421, loss is 0.0023616922553628683\n",
      "epoch: 8 step: 1422, loss is 0.04559557884931564\n",
      "epoch: 8 step: 1423, loss is 0.001977083273231983\n",
      "epoch: 8 step: 1424, loss is 0.001378745073452592\n",
      "epoch: 8 step: 1425, loss is 0.0008101373095996678\n",
      "epoch: 8 step: 1426, loss is 0.0012939014704898\n",
      "epoch: 8 step: 1427, loss is 0.007712157443165779\n",
      "epoch: 8 step: 1428, loss is 0.06654591858386993\n",
      "epoch: 8 step: 1429, loss is 0.00371940853074193\n",
      "epoch: 8 step: 1430, loss is 0.000332767580403015\n",
      "epoch: 8 step: 1431, loss is 0.00292076519690454\n",
      "epoch: 8 step: 1432, loss is 0.02765297330915928\n",
      "epoch: 8 step: 1433, loss is 0.005567645188421011\n",
      "epoch: 8 step: 1434, loss is 0.045379336923360825\n",
      "epoch: 8 step: 1435, loss is 0.1259612888097763\n",
      "epoch: 8 step: 1436, loss is 0.0002711451379582286\n",
      "epoch: 8 step: 1437, loss is 0.00010848076635738835\n",
      "epoch: 8 step: 1438, loss is 0.08347789198160172\n",
      "epoch: 8 step: 1439, loss is 0.0033484171144664288\n",
      "epoch: 8 step: 1440, loss is 0.0012568449601531029\n",
      "epoch: 8 step: 1441, loss is 0.040355753153562546\n",
      "epoch: 8 step: 1442, loss is 0.16077639162540436\n",
      "epoch: 8 step: 1443, loss is 0.008443702943623066\n",
      "epoch: 8 step: 1444, loss is 0.032799817621707916\n",
      "epoch: 8 step: 1445, loss is 0.0034480507019907236\n",
      "epoch: 8 step: 1446, loss is 0.07062724977731705\n",
      "epoch: 8 step: 1447, loss is 0.02137138321995735\n",
      "epoch: 8 step: 1448, loss is 0.014471608214080334\n",
      "epoch: 8 step: 1449, loss is 0.06578969210386276\n",
      "epoch: 8 step: 1450, loss is 0.0037813398521393538\n",
      "epoch: 8 step: 1451, loss is 0.1687926948070526\n",
      "epoch: 8 step: 1452, loss is 0.017736490815877914\n",
      "epoch: 8 step: 1453, loss is 0.17925924062728882\n",
      "epoch: 8 step: 1454, loss is 0.0027315772604197264\n",
      "epoch: 8 step: 1455, loss is 0.047042813152074814\n",
      "epoch: 8 step: 1456, loss is 0.004930721130222082\n",
      "epoch: 8 step: 1457, loss is 0.0022879731841385365\n",
      "epoch: 8 step: 1458, loss is 0.036593858152627945\n",
      "epoch: 8 step: 1459, loss is 0.012920900247991085\n",
      "epoch: 8 step: 1460, loss is 0.000324308784911409\n",
      "epoch: 8 step: 1461, loss is 0.0036612285766750574\n",
      "epoch: 8 step: 1462, loss is 0.005541844759136438\n",
      "epoch: 8 step: 1463, loss is 0.0737147107720375\n",
      "epoch: 8 step: 1464, loss is 0.025436105206608772\n",
      "epoch: 8 step: 1465, loss is 0.0031203615944832563\n",
      "epoch: 8 step: 1466, loss is 0.049020830541849136\n",
      "epoch: 8 step: 1467, loss is 0.020303070545196533\n",
      "epoch: 8 step: 1468, loss is 0.014604059979319572\n",
      "epoch: 8 step: 1469, loss is 0.015891270712018013\n",
      "epoch: 8 step: 1470, loss is 0.0025575316976755857\n",
      "epoch: 8 step: 1471, loss is 0.0013731835642829537\n",
      "epoch: 8 step: 1472, loss is 0.09208256006240845\n",
      "epoch: 8 step: 1473, loss is 0.003555215196684003\n",
      "epoch: 8 step: 1474, loss is 0.006469986401498318\n",
      "epoch: 8 step: 1475, loss is 0.013927550055086613\n",
      "epoch: 8 step: 1476, loss is 0.0047929901629686356\n",
      "epoch: 8 step: 1477, loss is 0.005646298173815012\n",
      "epoch: 8 step: 1478, loss is 0.014716739766299725\n",
      "epoch: 8 step: 1479, loss is 0.09925845265388489\n",
      "epoch: 8 step: 1480, loss is 0.025332914665341377\n",
      "epoch: 8 step: 1481, loss is 0.04281771183013916\n",
      "epoch: 8 step: 1482, loss is 0.05452865734696388\n",
      "epoch: 8 step: 1483, loss is 0.004204957280308008\n",
      "epoch: 8 step: 1484, loss is 0.05700630694627762\n",
      "epoch: 8 step: 1485, loss is 0.021928709000349045\n",
      "epoch: 8 step: 1486, loss is 0.001435119309462607\n",
      "epoch: 8 step: 1487, loss is 0.04759502783417702\n",
      "epoch: 8 step: 1488, loss is 0.037567488849163055\n",
      "epoch: 8 step: 1489, loss is 0.02503395266830921\n",
      "epoch: 8 step: 1490, loss is 0.11878861486911774\n",
      "epoch: 8 step: 1491, loss is 0.0264902263879776\n",
      "epoch: 8 step: 1492, loss is 0.00046623125672340393\n",
      "epoch: 8 step: 1493, loss is 0.0044989269226789474\n",
      "epoch: 8 step: 1494, loss is 0.022117726504802704\n",
      "epoch: 8 step: 1495, loss is 0.020166879519820213\n",
      "epoch: 8 step: 1496, loss is 0.007873079739511013\n",
      "epoch: 8 step: 1497, loss is 0.01923130638897419\n",
      "epoch: 8 step: 1498, loss is 0.04157962277531624\n",
      "epoch: 8 step: 1499, loss is 0.004725905135273933\n",
      "epoch: 8 step: 1500, loss is 0.10244039446115494\n",
      "epoch: 8 step: 1501, loss is 0.009987988509237766\n",
      "epoch: 8 step: 1502, loss is 0.0021808564197272062\n",
      "epoch: 8 step: 1503, loss is 0.04232393205165863\n",
      "epoch: 8 step: 1504, loss is 0.010039910674095154\n",
      "epoch: 8 step: 1505, loss is 0.0006431589717976749\n",
      "epoch: 8 step: 1506, loss is 0.019720451906323433\n",
      "epoch: 8 step: 1507, loss is 0.03296104073524475\n",
      "epoch: 8 step: 1508, loss is 0.010298460721969604\n",
      "epoch: 8 step: 1509, loss is 0.0009168024407699704\n",
      "epoch: 8 step: 1510, loss is 0.2126799374818802\n",
      "epoch: 8 step: 1511, loss is 0.004104218911379576\n",
      "epoch: 8 step: 1512, loss is 0.04143282398581505\n",
      "epoch: 8 step: 1513, loss is 0.07485206425189972\n",
      "epoch: 8 step: 1514, loss is 0.04655489698052406\n",
      "epoch: 8 step: 1515, loss is 0.003886620281264186\n",
      "epoch: 8 step: 1516, loss is 0.030271589756011963\n",
      "epoch: 8 step: 1517, loss is 0.0019972063601017\n",
      "epoch: 8 step: 1518, loss is 0.0019644862040877342\n",
      "epoch: 8 step: 1519, loss is 0.012039183638989925\n",
      "epoch: 8 step: 1520, loss is 0.0011773835867643356\n",
      "epoch: 8 step: 1521, loss is 0.028473222628235817\n",
      "epoch: 8 step: 1522, loss is 0.006270614452660084\n",
      "epoch: 8 step: 1523, loss is 0.00012316557695157826\n",
      "epoch: 8 step: 1524, loss is 0.001439928077161312\n",
      "epoch: 8 step: 1525, loss is 0.0061595989391207695\n",
      "epoch: 8 step: 1526, loss is 0.007869093678891659\n",
      "epoch: 8 step: 1527, loss is 0.005899626761674881\n",
      "epoch: 8 step: 1528, loss is 0.1166735589504242\n",
      "epoch: 8 step: 1529, loss is 0.04059695079922676\n",
      "epoch: 8 step: 1530, loss is 0.12928402423858643\n",
      "epoch: 8 step: 1531, loss is 0.04358717054128647\n",
      "epoch: 8 step: 1532, loss is 0.003936212044209242\n",
      "epoch: 8 step: 1533, loss is 0.011334807612001896\n",
      "epoch: 8 step: 1534, loss is 0.006729333195835352\n",
      "epoch: 8 step: 1535, loss is 0.012350247241556644\n",
      "epoch: 8 step: 1536, loss is 0.0009269287693314254\n",
      "epoch: 8 step: 1537, loss is 0.0013645379804074764\n",
      "epoch: 8 step: 1538, loss is 0.0030963411554694176\n",
      "epoch: 8 step: 1539, loss is 0.002960101468488574\n",
      "epoch: 8 step: 1540, loss is 0.0059013646095991135\n",
      "epoch: 8 step: 1541, loss is 0.03217780217528343\n",
      "epoch: 8 step: 1542, loss is 0.012066863477230072\n",
      "epoch: 8 step: 1543, loss is 0.0052138520404696465\n",
      "epoch: 8 step: 1544, loss is 0.08690209686756134\n",
      "epoch: 8 step: 1545, loss is 0.008315346203744411\n",
      "epoch: 8 step: 1546, loss is 0.003010154003277421\n",
      "epoch: 8 step: 1547, loss is 0.002642796840518713\n",
      "epoch: 8 step: 1548, loss is 0.017043640837073326\n",
      "epoch: 8 step: 1549, loss is 0.0011630174703896046\n",
      "epoch: 8 step: 1550, loss is 0.02995109371840954\n",
      "epoch: 8 step: 1551, loss is 0.008881272748112679\n",
      "epoch: 8 step: 1552, loss is 0.0011434354819357395\n",
      "epoch: 8 step: 1553, loss is 0.1040855124592781\n",
      "epoch: 8 step: 1554, loss is 0.000294465891784057\n",
      "epoch: 8 step: 1555, loss is 0.050541818141937256\n",
      "epoch: 8 step: 1556, loss is 0.014444059692323208\n",
      "epoch: 8 step: 1557, loss is 0.0017714606365188956\n",
      "epoch: 8 step: 1558, loss is 0.0005600554286502302\n",
      "epoch: 8 step: 1559, loss is 0.01290417741984129\n",
      "epoch: 8 step: 1560, loss is 0.0024175175931304693\n",
      "epoch: 8 step: 1561, loss is 0.0006300600944086909\n",
      "epoch: 8 step: 1562, loss is 0.10926533490419388\n",
      "epoch: 8 step: 1563, loss is 0.04733220860362053\n",
      "epoch: 8 step: 1564, loss is 0.0010041341884061694\n",
      "epoch: 8 step: 1565, loss is 0.00011806065595010296\n",
      "epoch: 8 step: 1566, loss is 0.022376056760549545\n",
      "epoch: 8 step: 1567, loss is 0.00502850441262126\n",
      "epoch: 8 step: 1568, loss is 0.16773183643817902\n",
      "epoch: 8 step: 1569, loss is 0.02937711589038372\n",
      "epoch: 8 step: 1570, loss is 0.039983998984098434\n",
      "epoch: 8 step: 1571, loss is 0.004414246417582035\n",
      "epoch: 8 step: 1572, loss is 0.003948991186916828\n",
      "epoch: 8 step: 1573, loss is 0.19193391501903534\n",
      "epoch: 8 step: 1574, loss is 0.0132265854626894\n",
      "epoch: 8 step: 1575, loss is 0.01619887910783291\n",
      "epoch: 8 step: 1576, loss is 0.035758405923843384\n",
      "epoch: 8 step: 1577, loss is 0.00491444393992424\n",
      "epoch: 8 step: 1578, loss is 0.016580339521169662\n",
      "epoch: 8 step: 1579, loss is 0.08359405398368835\n",
      "epoch: 8 step: 1580, loss is 0.010830552317202091\n",
      "epoch: 8 step: 1581, loss is 0.04456701874732971\n",
      "epoch: 8 step: 1582, loss is 0.001302950200624764\n",
      "epoch: 8 step: 1583, loss is 0.14099247753620148\n",
      "epoch: 8 step: 1584, loss is 0.0006021615117788315\n",
      "epoch: 8 step: 1585, loss is 0.00032625533640384674\n",
      "epoch: 8 step: 1586, loss is 0.11517390608787537\n",
      "epoch: 8 step: 1587, loss is 0.1516612321138382\n",
      "epoch: 8 step: 1588, loss is 0.00033130808151327074\n",
      "epoch: 8 step: 1589, loss is 0.0007288647466339171\n",
      "epoch: 8 step: 1590, loss is 0.002432790584862232\n",
      "epoch: 8 step: 1591, loss is 0.10307396203279495\n",
      "epoch: 8 step: 1592, loss is 0.031446199864149094\n",
      "epoch: 8 step: 1593, loss is 0.0591248944401741\n",
      "epoch: 8 step: 1594, loss is 0.038069743663072586\n",
      "epoch: 8 step: 1595, loss is 0.02014334127306938\n",
      "epoch: 8 step: 1596, loss is 0.031567350029945374\n",
      "epoch: 8 step: 1597, loss is 0.004206713289022446\n",
      "epoch: 8 step: 1598, loss is 0.0012518796138465405\n",
      "epoch: 8 step: 1599, loss is 0.05485856533050537\n",
      "epoch: 8 step: 1600, loss is 0.04545450210571289\n",
      "epoch: 8 step: 1601, loss is 0.10201475024223328\n",
      "epoch: 8 step: 1602, loss is 0.0031135883182287216\n",
      "epoch: 8 step: 1603, loss is 0.002532046288251877\n",
      "epoch: 8 step: 1604, loss is 0.0008565483731217682\n",
      "epoch: 8 step: 1605, loss is 0.061524372547864914\n",
      "epoch: 8 step: 1606, loss is 0.005673577543348074\n",
      "epoch: 8 step: 1607, loss is 0.004467065911740065\n",
      "epoch: 8 step: 1608, loss is 0.03640417382121086\n",
      "epoch: 8 step: 1609, loss is 0.00967271439731121\n",
      "epoch: 8 step: 1610, loss is 0.0024887414183467627\n",
      "epoch: 8 step: 1611, loss is 0.01103427167981863\n",
      "epoch: 8 step: 1612, loss is 0.014861609786748886\n",
      "epoch: 8 step: 1613, loss is 0.07831663638353348\n",
      "epoch: 8 step: 1614, loss is 0.05063362419605255\n",
      "epoch: 8 step: 1615, loss is 0.058905746787786484\n",
      "epoch: 8 step: 1616, loss is 0.004538323730230331\n",
      "epoch: 8 step: 1617, loss is 0.03236309811472893\n",
      "epoch: 8 step: 1618, loss is 0.2967625558376312\n",
      "epoch: 8 step: 1619, loss is 0.03653736412525177\n",
      "epoch: 8 step: 1620, loss is 0.001088268356397748\n",
      "epoch: 8 step: 1621, loss is 0.009890535846352577\n",
      "epoch: 8 step: 1622, loss is 0.0013998969225212932\n",
      "epoch: 8 step: 1623, loss is 0.0007958367932587862\n",
      "epoch: 8 step: 1624, loss is 0.024547316133975983\n",
      "epoch: 8 step: 1625, loss is 0.015474592335522175\n",
      "epoch: 8 step: 1626, loss is 0.04860607162117958\n",
      "epoch: 8 step: 1627, loss is 0.000670896377414465\n",
      "epoch: 8 step: 1628, loss is 0.017778806388378143\n",
      "epoch: 8 step: 1629, loss is 0.014677771367132664\n",
      "epoch: 8 step: 1630, loss is 0.05852404236793518\n",
      "epoch: 8 step: 1631, loss is 0.08705068379640579\n",
      "epoch: 8 step: 1632, loss is 0.09854917973279953\n",
      "epoch: 8 step: 1633, loss is 0.01133057288825512\n",
      "epoch: 8 step: 1634, loss is 0.06409227848052979\n",
      "epoch: 8 step: 1635, loss is 0.0012227724073454738\n",
      "epoch: 8 step: 1636, loss is 0.01921037957072258\n",
      "epoch: 8 step: 1637, loss is 0.07909706234931946\n",
      "epoch: 8 step: 1638, loss is 0.014002352952957153\n",
      "epoch: 8 step: 1639, loss is 0.16946886479854584\n",
      "epoch: 8 step: 1640, loss is 0.12394686043262482\n",
      "epoch: 8 step: 1641, loss is 0.5032060742378235\n",
      "epoch: 8 step: 1642, loss is 0.24036076664924622\n",
      "epoch: 8 step: 1643, loss is 0.022769026458263397\n",
      "epoch: 8 step: 1644, loss is 0.02353517711162567\n",
      "epoch: 8 step: 1645, loss is 0.02674820087850094\n",
      "epoch: 8 step: 1646, loss is 0.06797202676534653\n",
      "epoch: 8 step: 1647, loss is 0.013011871837079525\n",
      "epoch: 8 step: 1648, loss is 0.0015084660844877362\n",
      "epoch: 8 step: 1649, loss is 0.006820935755968094\n",
      "epoch: 8 step: 1650, loss is 0.0026027716230601072\n",
      "epoch: 8 step: 1651, loss is 0.0046287039294838905\n",
      "epoch: 8 step: 1652, loss is 0.11299578100442886\n",
      "epoch: 8 step: 1653, loss is 0.12257851660251617\n",
      "epoch: 8 step: 1654, loss is 0.012534329667687416\n",
      "epoch: 8 step: 1655, loss is 0.005408789962530136\n",
      "epoch: 8 step: 1656, loss is 0.016239959746599197\n",
      "epoch: 8 step: 1657, loss is 0.0007947359699755907\n",
      "epoch: 8 step: 1658, loss is 0.008314059115946293\n",
      "epoch: 8 step: 1659, loss is 0.0022517074830830097\n",
      "epoch: 8 step: 1660, loss is 0.019724592566490173\n",
      "epoch: 8 step: 1661, loss is 0.016592834144830704\n",
      "epoch: 8 step: 1662, loss is 0.00010177271178690717\n",
      "epoch: 8 step: 1663, loss is 0.10743065178394318\n",
      "epoch: 8 step: 1664, loss is 0.0017063984414562583\n",
      "epoch: 8 step: 1665, loss is 0.020411236211657524\n",
      "epoch: 8 step: 1666, loss is 0.001998310908675194\n",
      "epoch: 8 step: 1667, loss is 0.017766635864973068\n",
      "epoch: 8 step: 1668, loss is 0.001324545475654304\n",
      "epoch: 8 step: 1669, loss is 0.015132712200284004\n",
      "epoch: 8 step: 1670, loss is 0.013255764730274677\n",
      "epoch: 8 step: 1671, loss is 0.028011759743094444\n",
      "epoch: 8 step: 1672, loss is 0.007180133368819952\n",
      "epoch: 8 step: 1673, loss is 0.006952343974262476\n",
      "epoch: 8 step: 1674, loss is 0.07923655956983566\n",
      "epoch: 8 step: 1675, loss is 0.02484819106757641\n",
      "epoch: 8 step: 1676, loss is 0.00576324388384819\n",
      "epoch: 8 step: 1677, loss is 0.009653654880821705\n",
      "epoch: 8 step: 1678, loss is 0.010100318118929863\n",
      "epoch: 8 step: 1679, loss is 0.19064882397651672\n",
      "epoch: 8 step: 1680, loss is 0.004759260453283787\n",
      "epoch: 8 step: 1681, loss is 0.06577472388744354\n",
      "epoch: 8 step: 1682, loss is 0.0050152200274169445\n",
      "epoch: 8 step: 1683, loss is 0.010678076185286045\n",
      "epoch: 8 step: 1684, loss is 0.16683948040008545\n",
      "epoch: 8 step: 1685, loss is 0.001584107056260109\n",
      "epoch: 8 step: 1686, loss is 0.09628482162952423\n",
      "epoch: 8 step: 1687, loss is 0.12420640885829926\n",
      "epoch: 8 step: 1688, loss is 0.0019902486819773912\n",
      "epoch: 8 step: 1689, loss is 0.027419449761509895\n",
      "epoch: 8 step: 1690, loss is 0.031037483364343643\n",
      "epoch: 8 step: 1691, loss is 0.02070765569806099\n",
      "epoch: 8 step: 1692, loss is 0.06558913737535477\n",
      "epoch: 8 step: 1693, loss is 0.003442042274400592\n",
      "epoch: 8 step: 1694, loss is 0.03140471130609512\n",
      "epoch: 8 step: 1695, loss is 0.044659558683633804\n",
      "epoch: 8 step: 1696, loss is 0.0015987183433026075\n",
      "epoch: 8 step: 1697, loss is 0.007669036742299795\n",
      "epoch: 8 step: 1698, loss is 0.0084789814427495\n",
      "epoch: 8 step: 1699, loss is 0.0025919200852513313\n",
      "epoch: 8 step: 1700, loss is 0.0007527540437877178\n",
      "epoch: 8 step: 1701, loss is 0.004863155074417591\n",
      "epoch: 8 step: 1702, loss is 0.003909030929207802\n",
      "epoch: 8 step: 1703, loss is 0.0031491650734096766\n",
      "epoch: 8 step: 1704, loss is 0.00011390815780032426\n",
      "epoch: 8 step: 1705, loss is 0.002776344772428274\n",
      "epoch: 8 step: 1706, loss is 0.011444634757936\n",
      "epoch: 8 step: 1707, loss is 0.019680790603160858\n",
      "epoch: 8 step: 1708, loss is 0.19899584352970123\n",
      "epoch: 8 step: 1709, loss is 0.0012723002582788467\n",
      "epoch: 8 step: 1710, loss is 0.017926445230841637\n",
      "epoch: 8 step: 1711, loss is 0.15718266367912292\n",
      "epoch: 8 step: 1712, loss is 0.016042882576584816\n",
      "epoch: 8 step: 1713, loss is 0.006199419032782316\n",
      "epoch: 8 step: 1714, loss is 0.058073822408914566\n",
      "epoch: 8 step: 1715, loss is 0.0002733782457653433\n",
      "epoch: 8 step: 1716, loss is 0.017346514388918877\n",
      "epoch: 8 step: 1717, loss is 0.07934774458408356\n",
      "epoch: 8 step: 1718, loss is 0.006768336519598961\n",
      "epoch: 8 step: 1719, loss is 0.01736181229352951\n",
      "epoch: 8 step: 1720, loss is 0.0760374516248703\n",
      "epoch: 8 step: 1721, loss is 0.025285005569458008\n",
      "epoch: 8 step: 1722, loss is 0.001366986078210175\n",
      "epoch: 8 step: 1723, loss is 0.22564394772052765\n",
      "epoch: 8 step: 1724, loss is 0.006022004410624504\n",
      "epoch: 8 step: 1725, loss is 0.002986678620800376\n",
      "epoch: 8 step: 1726, loss is 0.018724916502833366\n",
      "epoch: 8 step: 1727, loss is 0.014994609169661999\n",
      "epoch: 8 step: 1728, loss is 0.028315789997577667\n",
      "epoch: 8 step: 1729, loss is 0.006228063255548477\n",
      "epoch: 8 step: 1730, loss is 0.047506123781204224\n",
      "epoch: 8 step: 1731, loss is 0.03009907715022564\n",
      "epoch: 8 step: 1732, loss is 0.0016600238159298897\n",
      "epoch: 8 step: 1733, loss is 0.21764659881591797\n",
      "epoch: 8 step: 1734, loss is 0.002471423940733075\n",
      "epoch: 8 step: 1735, loss is 0.0012506138300523162\n",
      "epoch: 8 step: 1736, loss is 0.09326391667127609\n",
      "epoch: 8 step: 1737, loss is 0.004281781148165464\n",
      "epoch: 8 step: 1738, loss is 0.0075743598863482475\n",
      "epoch: 8 step: 1739, loss is 0.023404832929372787\n",
      "epoch: 8 step: 1740, loss is 0.06645935773849487\n",
      "epoch: 8 step: 1741, loss is 0.010305133648216724\n",
      "epoch: 8 step: 1742, loss is 0.007442323025316\n",
      "epoch: 8 step: 1743, loss is 0.011954940855503082\n",
      "epoch: 8 step: 1744, loss is 0.13208192586898804\n",
      "epoch: 8 step: 1745, loss is 0.05496062710881233\n",
      "epoch: 8 step: 1746, loss is 0.06039413437247276\n",
      "epoch: 8 step: 1747, loss is 0.0005300014745444059\n",
      "epoch: 8 step: 1748, loss is 0.004831518512219191\n",
      "epoch: 8 step: 1749, loss is 0.0012456648983061314\n",
      "epoch: 8 step: 1750, loss is 0.08443494141101837\n",
      "epoch: 8 step: 1751, loss is 0.001265581464394927\n",
      "epoch: 8 step: 1752, loss is 0.02388175204396248\n",
      "epoch: 8 step: 1753, loss is 0.02504296414554119\n",
      "epoch: 8 step: 1754, loss is 0.001422749599441886\n",
      "epoch: 8 step: 1755, loss is 0.04209806025028229\n",
      "epoch: 8 step: 1756, loss is 0.0022572202142328024\n",
      "epoch: 8 step: 1757, loss is 0.004340128041803837\n",
      "epoch: 8 step: 1758, loss is 0.017221445217728615\n",
      "epoch: 8 step: 1759, loss is 0.01371394656598568\n",
      "epoch: 8 step: 1760, loss is 0.003499984508380294\n",
      "epoch: 8 step: 1761, loss is 0.03947599604725838\n",
      "epoch: 8 step: 1762, loss is 0.05818801373243332\n",
      "epoch: 8 step: 1763, loss is 0.00019203859847038984\n",
      "epoch: 8 step: 1764, loss is 0.004856669809669256\n",
      "epoch: 8 step: 1765, loss is 0.008034722879529\n",
      "epoch: 8 step: 1766, loss is 0.010071389377117157\n",
      "epoch: 8 step: 1767, loss is 0.027789557352662086\n",
      "epoch: 8 step: 1768, loss is 0.0002715829177759588\n",
      "epoch: 8 step: 1769, loss is 0.14154624938964844\n",
      "epoch: 8 step: 1770, loss is 0.17920659482479095\n",
      "epoch: 8 step: 1771, loss is 0.005335777997970581\n",
      "epoch: 8 step: 1772, loss is 0.017732765525579453\n",
      "epoch: 8 step: 1773, loss is 0.028221240267157555\n",
      "epoch: 8 step: 1774, loss is 0.0789327397942543\n",
      "epoch: 8 step: 1775, loss is 0.039196960628032684\n",
      "epoch: 8 step: 1776, loss is 0.026394082233309746\n",
      "epoch: 8 step: 1777, loss is 0.3199642300605774\n",
      "epoch: 8 step: 1778, loss is 0.0034627187997102737\n",
      "epoch: 8 step: 1779, loss is 0.05869480222463608\n",
      "epoch: 8 step: 1780, loss is 0.10127124935388565\n",
      "epoch: 8 step: 1781, loss is 0.0009057221468538046\n",
      "epoch: 8 step: 1782, loss is 0.0006798506947234273\n",
      "epoch: 8 step: 1783, loss is 0.0011446012649685144\n",
      "epoch: 8 step: 1784, loss is 0.0054647657088935375\n",
      "epoch: 8 step: 1785, loss is 0.003113342681899667\n",
      "epoch: 8 step: 1786, loss is 0.003946956247091293\n",
      "epoch: 8 step: 1787, loss is 0.07990746945142746\n",
      "epoch: 8 step: 1788, loss is 0.002094388473778963\n",
      "epoch: 8 step: 1789, loss is 0.009496824815869331\n",
      "epoch: 8 step: 1790, loss is 0.007671266794204712\n",
      "epoch: 8 step: 1791, loss is 0.0009052525856532156\n",
      "epoch: 8 step: 1792, loss is 0.012273930944502354\n",
      "epoch: 8 step: 1793, loss is 0.04413674771785736\n",
      "epoch: 8 step: 1794, loss is 0.031284037977457047\n",
      "epoch: 8 step: 1795, loss is 0.030221667140722275\n",
      "epoch: 8 step: 1796, loss is 0.06657888740301132\n",
      "epoch: 8 step: 1797, loss is 0.031907010823488235\n",
      "epoch: 8 step: 1798, loss is 0.0021124689374119043\n",
      "epoch: 8 step: 1799, loss is 0.010359419509768486\n",
      "epoch: 8 step: 1800, loss is 0.01858040690422058\n",
      "epoch: 8 step: 1801, loss is 0.002612662734463811\n",
      "epoch: 8 step: 1802, loss is 0.06065014377236366\n",
      "epoch: 8 step: 1803, loss is 0.0019125392427667975\n",
      "epoch: 8 step: 1804, loss is 0.06916799396276474\n",
      "epoch: 8 step: 1805, loss is 0.03804226592183113\n",
      "epoch: 8 step: 1806, loss is 0.03412818908691406\n",
      "epoch: 8 step: 1807, loss is 0.001269565662369132\n",
      "epoch: 8 step: 1808, loss is 0.0018103196052834392\n",
      "epoch: 8 step: 1809, loss is 0.005432159639894962\n",
      "epoch: 8 step: 1810, loss is 0.0065323831513524055\n",
      "epoch: 8 step: 1811, loss is 0.029431717470288277\n",
      "epoch: 8 step: 1812, loss is 0.004596486687660217\n",
      "epoch: 8 step: 1813, loss is 0.002072806004434824\n",
      "epoch: 8 step: 1814, loss is 0.006330891977995634\n",
      "epoch: 8 step: 1815, loss is 0.000842551002278924\n",
      "epoch: 8 step: 1816, loss is 0.05749054253101349\n",
      "epoch: 8 step: 1817, loss is 0.006306102965027094\n",
      "epoch: 8 step: 1818, loss is 0.05452428758144379\n",
      "epoch: 8 step: 1819, loss is 0.010312441736459732\n",
      "epoch: 8 step: 1820, loss is 0.0003712537873070687\n",
      "epoch: 8 step: 1821, loss is 0.05034760758280754\n",
      "epoch: 8 step: 1822, loss is 0.10326236486434937\n",
      "epoch: 8 step: 1823, loss is 0.0040148403495550156\n",
      "epoch: 8 step: 1824, loss is 0.021137068048119545\n",
      "epoch: 8 step: 1825, loss is 0.009110073558986187\n",
      "epoch: 8 step: 1826, loss is 0.007464982103556395\n",
      "epoch: 8 step: 1827, loss is 0.029918247833848\n",
      "epoch: 8 step: 1828, loss is 0.035926349461078644\n",
      "epoch: 8 step: 1829, loss is 0.08304622769355774\n",
      "epoch: 8 step: 1830, loss is 0.01121707446873188\n",
      "epoch: 8 step: 1831, loss is 0.0034613641910254955\n",
      "epoch: 8 step: 1832, loss is 0.010981197468936443\n",
      "epoch: 8 step: 1833, loss is 0.08012094348669052\n",
      "epoch: 8 step: 1834, loss is 0.041195984929800034\n",
      "epoch: 8 step: 1835, loss is 0.001461594132706523\n",
      "epoch: 8 step: 1836, loss is 0.002751459600403905\n",
      "epoch: 8 step: 1837, loss is 0.023762192577123642\n",
      "epoch: 8 step: 1838, loss is 0.00974414311349392\n",
      "epoch: 8 step: 1839, loss is 0.0011104046134278178\n",
      "epoch: 8 step: 1840, loss is 0.0003993896534666419\n",
      "epoch: 8 step: 1841, loss is 0.0862933099269867\n",
      "epoch: 8 step: 1842, loss is 0.15578706562519073\n",
      "epoch: 8 step: 1843, loss is 0.0020675670821219683\n",
      "epoch: 8 step: 1844, loss is 0.05398896709084511\n",
      "epoch: 8 step: 1845, loss is 0.07821361720561981\n",
      "epoch: 8 step: 1846, loss is 0.007329682819545269\n",
      "epoch: 8 step: 1847, loss is 0.003397303633391857\n",
      "epoch: 8 step: 1848, loss is 0.044572971761226654\n",
      "epoch: 8 step: 1849, loss is 0.18403509259223938\n",
      "epoch: 8 step: 1850, loss is 0.0828513577580452\n",
      "epoch: 8 step: 1851, loss is 0.11230455338954926\n",
      "epoch: 8 step: 1852, loss is 0.06459694355726242\n",
      "epoch: 8 step: 1853, loss is 0.05822233855724335\n",
      "epoch: 8 step: 1854, loss is 0.0033058756962418556\n",
      "epoch: 8 step: 1855, loss is 0.004558485932648182\n",
      "epoch: 8 step: 1856, loss is 0.05676668882369995\n",
      "epoch: 8 step: 1857, loss is 0.0030946899205446243\n",
      "epoch: 8 step: 1858, loss is 0.003997498657554388\n",
      "epoch: 8 step: 1859, loss is 0.059590280055999756\n",
      "epoch: 8 step: 1860, loss is 0.00048450211761519313\n",
      "epoch: 8 step: 1861, loss is 0.002024014014750719\n",
      "epoch: 8 step: 1862, loss is 0.13462045788764954\n",
      "epoch: 8 step: 1863, loss is 0.0006077333819121122\n",
      "epoch: 8 step: 1864, loss is 0.10022829473018646\n",
      "epoch: 8 step: 1865, loss is 0.0016867701197043061\n",
      "epoch: 8 step: 1866, loss is 0.0065838126465678215\n",
      "epoch: 8 step: 1867, loss is 0.004047810100018978\n",
      "epoch: 8 step: 1868, loss is 0.13344737887382507\n",
      "epoch: 8 step: 1869, loss is 0.11883920431137085\n",
      "epoch: 8 step: 1870, loss is 0.1301807165145874\n",
      "epoch: 8 step: 1871, loss is 0.005728147458285093\n",
      "epoch: 8 step: 1872, loss is 0.0014743274077773094\n",
      "epoch: 8 step: 1873, loss is 0.005343755707144737\n",
      "epoch: 8 step: 1874, loss is 0.017327187582850456\n",
      "epoch: 8 step: 1875, loss is 0.07655203342437744\n",
      "epoch: 9 step: 1, loss is 0.04292656108736992\n",
      "epoch: 9 step: 2, loss is 0.009674707427620888\n",
      "epoch: 9 step: 3, loss is 0.010440628044307232\n",
      "epoch: 9 step: 4, loss is 0.0544879250228405\n",
      "epoch: 9 step: 5, loss is 0.011444862000644207\n",
      "epoch: 9 step: 6, loss is 0.009721319191157818\n",
      "epoch: 9 step: 7, loss is 0.003061086405068636\n",
      "epoch: 9 step: 8, loss is 0.026271367445588112\n",
      "epoch: 9 step: 9, loss is 0.0029004509560763836\n",
      "epoch: 9 step: 10, loss is 0.0018736658385023475\n",
      "epoch: 9 step: 11, loss is 0.04384394362568855\n",
      "epoch: 9 step: 12, loss is 0.0004219088878016919\n",
      "epoch: 9 step: 13, loss is 0.03335198387503624\n",
      "epoch: 9 step: 14, loss is 0.0049333167262375355\n",
      "epoch: 9 step: 15, loss is 0.011792879551649094\n",
      "epoch: 9 step: 16, loss is 0.05815833806991577\n",
      "epoch: 9 step: 17, loss is 0.0027379069942981005\n",
      "epoch: 9 step: 18, loss is 0.043219152837991714\n",
      "epoch: 9 step: 19, loss is 0.011583589017391205\n",
      "epoch: 9 step: 20, loss is 0.0020932152401655912\n",
      "epoch: 9 step: 21, loss is 0.1263965219259262\n",
      "epoch: 9 step: 22, loss is 0.001985271228477359\n",
      "epoch: 9 step: 23, loss is 0.0037993011064827442\n",
      "epoch: 9 step: 24, loss is 0.0005607995553873479\n",
      "epoch: 9 step: 25, loss is 0.07475820183753967\n",
      "epoch: 9 step: 26, loss is 0.04466775059700012\n",
      "epoch: 9 step: 27, loss is 0.006719027180224657\n",
      "epoch: 9 step: 28, loss is 0.0010459223994985223\n",
      "epoch: 9 step: 29, loss is 0.01570221036672592\n",
      "epoch: 9 step: 30, loss is 0.017505060881376266\n",
      "epoch: 9 step: 31, loss is 0.016150929033756256\n",
      "epoch: 9 step: 32, loss is 0.02911488153040409\n",
      "epoch: 9 step: 33, loss is 0.02583896741271019\n",
      "epoch: 9 step: 34, loss is 0.010784388519823551\n",
      "epoch: 9 step: 35, loss is 0.00187544000800699\n",
      "epoch: 9 step: 36, loss is 0.19505628943443298\n",
      "epoch: 9 step: 37, loss is 0.09736953675746918\n",
      "epoch: 9 step: 38, loss is 0.00041817358578555286\n",
      "epoch: 9 step: 39, loss is 0.0021473560482263565\n",
      "epoch: 9 step: 40, loss is 0.001858649542555213\n",
      "epoch: 9 step: 41, loss is 0.0007798339938744903\n",
      "epoch: 9 step: 42, loss is 0.005386461969465017\n",
      "epoch: 9 step: 43, loss is 0.00018252777226734906\n",
      "epoch: 9 step: 44, loss is 0.021660346537828445\n",
      "epoch: 9 step: 45, loss is 0.002102523110806942\n",
      "epoch: 9 step: 46, loss is 0.0034275618381798267\n",
      "epoch: 9 step: 47, loss is 0.0014315529260784388\n",
      "epoch: 9 step: 48, loss is 0.007406981661915779\n",
      "epoch: 9 step: 49, loss is 0.0024383673444390297\n",
      "epoch: 9 step: 50, loss is 0.13237795233726501\n",
      "epoch: 9 step: 51, loss is 0.0011440421221777797\n",
      "epoch: 9 step: 52, loss is 0.0011651628883555532\n",
      "epoch: 9 step: 53, loss is 0.013260145671665668\n",
      "epoch: 9 step: 54, loss is 0.00398824829608202\n",
      "epoch: 9 step: 55, loss is 0.0023331823758780956\n",
      "epoch: 9 step: 56, loss is 0.0016028879908844829\n",
      "epoch: 9 step: 57, loss is 0.01622781902551651\n",
      "epoch: 9 step: 58, loss is 0.01395649928599596\n",
      "epoch: 9 step: 59, loss is 0.02164612151682377\n",
      "epoch: 9 step: 60, loss is 0.057579416781663895\n",
      "epoch: 9 step: 61, loss is 0.002259092405438423\n",
      "epoch: 9 step: 62, loss is 0.03755677118897438\n",
      "epoch: 9 step: 63, loss is 0.05180121213197708\n",
      "epoch: 9 step: 64, loss is 0.07780084013938904\n",
      "epoch: 9 step: 65, loss is 0.002656996715813875\n",
      "epoch: 9 step: 66, loss is 0.01247387658804655\n",
      "epoch: 9 step: 67, loss is 0.0011351683642715216\n",
      "epoch: 9 step: 68, loss is 0.02084236405789852\n",
      "epoch: 9 step: 69, loss is 0.001039179041981697\n",
      "epoch: 9 step: 70, loss is 0.0007112274179235101\n",
      "epoch: 9 step: 71, loss is 0.0014789924025535583\n",
      "epoch: 9 step: 72, loss is 0.005817798897624016\n",
      "epoch: 9 step: 73, loss is 0.0001624384749447927\n",
      "epoch: 9 step: 74, loss is 0.0050883786752820015\n",
      "epoch: 9 step: 75, loss is 0.025523362681269646\n",
      "epoch: 9 step: 76, loss is 0.02027153968811035\n",
      "epoch: 9 step: 77, loss is 0.008075605146586895\n",
      "epoch: 9 step: 78, loss is 0.001780918100848794\n",
      "epoch: 9 step: 79, loss is 0.03712217137217522\n",
      "epoch: 9 step: 80, loss is 0.0009734999621286988\n",
      "epoch: 9 step: 81, loss is 0.0006512475665658712\n",
      "epoch: 9 step: 82, loss is 0.010462338104844093\n",
      "epoch: 9 step: 83, loss is 0.004128236323595047\n",
      "epoch: 9 step: 84, loss is 0.11675956845283508\n",
      "epoch: 9 step: 85, loss is 0.003204707521945238\n",
      "epoch: 9 step: 86, loss is 0.028535986319184303\n",
      "epoch: 9 step: 87, loss is 0.08676765114068985\n",
      "epoch: 9 step: 88, loss is 0.03672906011343002\n",
      "epoch: 9 step: 89, loss is 0.06475035846233368\n",
      "epoch: 9 step: 90, loss is 0.09351900964975357\n",
      "epoch: 9 step: 91, loss is 0.006338020786643028\n",
      "epoch: 9 step: 92, loss is 0.01617530547082424\n",
      "epoch: 9 step: 93, loss is 0.004025694448500872\n",
      "epoch: 9 step: 94, loss is 0.01719694398343563\n",
      "epoch: 9 step: 95, loss is 0.001962719950824976\n",
      "epoch: 9 step: 96, loss is 0.0433880053460598\n",
      "epoch: 9 step: 97, loss is 0.00259255338460207\n",
      "epoch: 9 step: 98, loss is 0.023131590336561203\n",
      "epoch: 9 step: 99, loss is 0.02762017399072647\n",
      "epoch: 9 step: 100, loss is 0.0014917965745553374\n",
      "epoch: 9 step: 101, loss is 0.03018941730260849\n",
      "epoch: 9 step: 102, loss is 0.01627865619957447\n",
      "epoch: 9 step: 103, loss is 0.0005145667237229645\n",
      "epoch: 9 step: 104, loss is 0.00383951747789979\n",
      "epoch: 9 step: 105, loss is 0.04057417809963226\n",
      "epoch: 9 step: 106, loss is 0.0024466447066515684\n",
      "epoch: 9 step: 107, loss is 0.006056108511984348\n",
      "epoch: 9 step: 108, loss is 0.0012428091140463948\n",
      "epoch: 9 step: 109, loss is 0.0035298289731144905\n",
      "epoch: 9 step: 110, loss is 0.02600693330168724\n",
      "epoch: 9 step: 111, loss is 0.008966307155787945\n",
      "epoch: 9 step: 112, loss is 0.05166385695338249\n",
      "epoch: 9 step: 113, loss is 0.00180579605512321\n",
      "epoch: 9 step: 114, loss is 0.0018216876778751612\n",
      "epoch: 9 step: 115, loss is 0.0023484593257308006\n",
      "epoch: 9 step: 116, loss is 0.012590923346579075\n",
      "epoch: 9 step: 117, loss is 0.013964622281491756\n",
      "epoch: 9 step: 118, loss is 0.008310237899422646\n",
      "epoch: 9 step: 119, loss is 0.00016794908151496202\n",
      "epoch: 9 step: 120, loss is 0.0008926511509343982\n",
      "epoch: 9 step: 121, loss is 0.0037806639447808266\n",
      "epoch: 9 step: 122, loss is 0.026132332161068916\n",
      "epoch: 9 step: 123, loss is 0.0013507796684280038\n",
      "epoch: 9 step: 124, loss is 0.0059392717666924\n",
      "epoch: 9 step: 125, loss is 0.003287320025265217\n",
      "epoch: 9 step: 126, loss is 0.004308109637349844\n",
      "epoch: 9 step: 127, loss is 0.01714935526251793\n",
      "epoch: 9 step: 128, loss is 0.025294149294495583\n",
      "epoch: 9 step: 129, loss is 0.0016102716326713562\n",
      "epoch: 9 step: 130, loss is 0.002583598019555211\n",
      "epoch: 9 step: 131, loss is 0.04029514268040657\n",
      "epoch: 9 step: 132, loss is 0.018245400860905647\n",
      "epoch: 9 step: 133, loss is 0.0026522488333284855\n",
      "epoch: 9 step: 134, loss is 0.0014955662190914154\n",
      "epoch: 9 step: 135, loss is 0.011631441302597523\n",
      "epoch: 9 step: 136, loss is 0.001689413096755743\n",
      "epoch: 9 step: 137, loss is 0.003537467448040843\n",
      "epoch: 9 step: 138, loss is 0.01037396676838398\n",
      "epoch: 9 step: 139, loss is 0.005174390971660614\n",
      "epoch: 9 step: 140, loss is 0.00013751383812632412\n",
      "epoch: 9 step: 141, loss is 0.009512932039797306\n",
      "epoch: 9 step: 142, loss is 0.001236487878486514\n",
      "epoch: 9 step: 143, loss is 0.0003926675708498806\n",
      "epoch: 9 step: 144, loss is 0.015145472250878811\n",
      "epoch: 9 step: 145, loss is 0.017528081312775612\n",
      "epoch: 9 step: 146, loss is 0.06381789594888687\n",
      "epoch: 9 step: 147, loss is 0.009909970685839653\n",
      "epoch: 9 step: 148, loss is 0.005665923003107309\n",
      "epoch: 9 step: 149, loss is 0.0025288176257163286\n",
      "epoch: 9 step: 150, loss is 0.12124793976545334\n",
      "epoch: 9 step: 151, loss is 0.13873516023159027\n",
      "epoch: 9 step: 152, loss is 0.007769190240651369\n",
      "epoch: 9 step: 153, loss is 0.05671340227127075\n",
      "epoch: 9 step: 154, loss is 0.005471474956721067\n",
      "epoch: 9 step: 155, loss is 0.010421662591397762\n",
      "epoch: 9 step: 156, loss is 0.0020650143269449472\n",
      "epoch: 9 step: 157, loss is 0.002147267572581768\n",
      "epoch: 9 step: 158, loss is 8.367055124836043e-05\n",
      "epoch: 9 step: 159, loss is 0.011236113496124744\n",
      "epoch: 9 step: 160, loss is 0.0002751699066720903\n",
      "epoch: 9 step: 161, loss is 0.003645980264991522\n",
      "epoch: 9 step: 162, loss is 0.0010589439189061522\n",
      "epoch: 9 step: 163, loss is 0.001390549587085843\n",
      "epoch: 9 step: 164, loss is 0.01842343807220459\n",
      "epoch: 9 step: 165, loss is 0.016594013199210167\n",
      "epoch: 9 step: 166, loss is 0.00022401711612474173\n",
      "epoch: 9 step: 167, loss is 0.033001407980918884\n",
      "epoch: 9 step: 168, loss is 0.000594782701227814\n",
      "epoch: 9 step: 169, loss is 0.00552790705114603\n",
      "epoch: 9 step: 170, loss is 0.0014593646628782153\n",
      "epoch: 9 step: 171, loss is 0.1531190425157547\n",
      "epoch: 9 step: 172, loss is 0.002375693991780281\n",
      "epoch: 9 step: 173, loss is 0.002214950043708086\n",
      "epoch: 9 step: 174, loss is 0.0004019149346277118\n",
      "epoch: 9 step: 175, loss is 0.0013995124027132988\n",
      "epoch: 9 step: 176, loss is 0.007571654859930277\n",
      "epoch: 9 step: 177, loss is 0.0004530925943981856\n",
      "epoch: 9 step: 178, loss is 0.0020641444716602564\n",
      "epoch: 9 step: 179, loss is 0.002188506769016385\n",
      "epoch: 9 step: 180, loss is 0.004532076418399811\n",
      "epoch: 9 step: 181, loss is 0.015373364090919495\n",
      "epoch: 9 step: 182, loss is 0.00025927324895747006\n",
      "epoch: 9 step: 183, loss is 0.004636730998754501\n",
      "epoch: 9 step: 184, loss is 0.0033005515579134226\n",
      "epoch: 9 step: 185, loss is 0.0036386933643370867\n",
      "epoch: 9 step: 186, loss is 0.017469972372055054\n",
      "epoch: 9 step: 187, loss is 0.00039922085124999285\n",
      "epoch: 9 step: 188, loss is 0.051608722656965256\n",
      "epoch: 9 step: 189, loss is 0.025832148268818855\n",
      "epoch: 9 step: 190, loss is 0.0045470488257706165\n",
      "epoch: 9 step: 191, loss is 0.04553144797682762\n",
      "epoch: 9 step: 192, loss is 0.00015177327441051602\n",
      "epoch: 9 step: 193, loss is 0.017174767330288887\n",
      "epoch: 9 step: 194, loss is 0.01492441538721323\n",
      "epoch: 9 step: 195, loss is 0.0077318609692156315\n",
      "epoch: 9 step: 196, loss is 0.009171789512038231\n",
      "epoch: 9 step: 197, loss is 0.01661953143775463\n",
      "epoch: 9 step: 198, loss is 0.015171120874583721\n",
      "epoch: 9 step: 199, loss is 0.0065040201880037785\n",
      "epoch: 9 step: 200, loss is 0.002102694706991315\n",
      "epoch: 9 step: 201, loss is 0.206088587641716\n",
      "epoch: 9 step: 202, loss is 0.26600220799446106\n",
      "epoch: 9 step: 203, loss is 0.008647138252854347\n",
      "epoch: 9 step: 204, loss is 0.0005112572107464075\n",
      "epoch: 9 step: 205, loss is 0.011548526585102081\n",
      "epoch: 9 step: 206, loss is 0.02324502542614937\n",
      "epoch: 9 step: 207, loss is 0.002359810285270214\n",
      "epoch: 9 step: 208, loss is 0.004806218668818474\n",
      "epoch: 9 step: 209, loss is 0.005708401091396809\n",
      "epoch: 9 step: 210, loss is 0.01348364632576704\n",
      "epoch: 9 step: 211, loss is 0.005856385454535484\n",
      "epoch: 9 step: 212, loss is 0.018404526636004448\n",
      "epoch: 9 step: 213, loss is 0.004495731554925442\n",
      "epoch: 9 step: 214, loss is 0.001671746838837862\n",
      "epoch: 9 step: 215, loss is 0.0025522245559841394\n",
      "epoch: 9 step: 216, loss is 0.011253435164690018\n",
      "epoch: 9 step: 217, loss is 0.0406988225877285\n",
      "epoch: 9 step: 218, loss is 0.007172522135078907\n",
      "epoch: 9 step: 219, loss is 0.03810500726103783\n",
      "epoch: 9 step: 220, loss is 0.05515073612332344\n",
      "epoch: 9 step: 221, loss is 0.0003741724358405918\n",
      "epoch: 9 step: 222, loss is 0.005114077124744654\n",
      "epoch: 9 step: 223, loss is 0.024942081421613693\n",
      "epoch: 9 step: 224, loss is 0.006658859085291624\n",
      "epoch: 9 step: 225, loss is 0.013979467563331127\n",
      "epoch: 9 step: 226, loss is 0.0009099012240767479\n",
      "epoch: 9 step: 227, loss is 0.005011910106986761\n",
      "epoch: 9 step: 228, loss is 0.00890145730227232\n",
      "epoch: 9 step: 229, loss is 0.0033704370725899935\n",
      "epoch: 9 step: 230, loss is 0.06883811950683594\n",
      "epoch: 9 step: 231, loss is 0.1327100694179535\n",
      "epoch: 9 step: 232, loss is 0.044478919357061386\n",
      "epoch: 9 step: 233, loss is 0.0010145005071535707\n",
      "epoch: 9 step: 234, loss is 0.009056910872459412\n",
      "epoch: 9 step: 235, loss is 0.0037063383497297764\n",
      "epoch: 9 step: 236, loss is 0.010790654458105564\n",
      "epoch: 9 step: 237, loss is 0.013387693092226982\n",
      "epoch: 9 step: 238, loss is 0.020988326519727707\n",
      "epoch: 9 step: 239, loss is 0.08137943595647812\n",
      "epoch: 9 step: 240, loss is 0.004496230743825436\n",
      "epoch: 9 step: 241, loss is 0.06965961307287216\n",
      "epoch: 9 step: 242, loss is 0.02216706983745098\n",
      "epoch: 9 step: 243, loss is 0.007244353648275137\n",
      "epoch: 9 step: 244, loss is 0.030478928238153458\n",
      "epoch: 9 step: 245, loss is 0.009377948939800262\n",
      "epoch: 9 step: 246, loss is 0.030828768387436867\n",
      "epoch: 9 step: 247, loss is 0.003858314361423254\n",
      "epoch: 9 step: 248, loss is 0.006780877709388733\n",
      "epoch: 9 step: 249, loss is 0.002961336635053158\n",
      "epoch: 9 step: 250, loss is 0.0010900282068178058\n",
      "epoch: 9 step: 251, loss is 0.009302791208028793\n",
      "epoch: 9 step: 252, loss is 0.05827157571911812\n",
      "epoch: 9 step: 253, loss is 0.18216505646705627\n",
      "epoch: 9 step: 254, loss is 0.000630636524874717\n",
      "epoch: 9 step: 255, loss is 0.01475570909678936\n",
      "epoch: 9 step: 256, loss is 0.019930943846702576\n",
      "epoch: 9 step: 257, loss is 0.0004578916123136878\n",
      "epoch: 9 step: 258, loss is 0.00022831688693258911\n",
      "epoch: 9 step: 259, loss is 0.00029382933280430734\n",
      "epoch: 9 step: 260, loss is 0.0019984429236501455\n",
      "epoch: 9 step: 261, loss is 0.03190949186682701\n",
      "epoch: 9 step: 262, loss is 0.00635884515941143\n",
      "epoch: 9 step: 263, loss is 0.106561578810215\n",
      "epoch: 9 step: 264, loss is 0.014808190986514091\n",
      "epoch: 9 step: 265, loss is 0.0008602108573541045\n",
      "epoch: 9 step: 266, loss is 0.0984971821308136\n",
      "epoch: 9 step: 267, loss is 0.09162570536136627\n",
      "epoch: 9 step: 268, loss is 0.0016514130402356386\n",
      "epoch: 9 step: 269, loss is 0.00011907848238479346\n",
      "epoch: 9 step: 270, loss is 0.01120616402477026\n",
      "epoch: 9 step: 271, loss is 0.011226464062929153\n",
      "epoch: 9 step: 272, loss is 0.002285753609612584\n",
      "epoch: 9 step: 273, loss is 0.023300690576434135\n",
      "epoch: 9 step: 274, loss is 0.00415814109146595\n",
      "epoch: 9 step: 275, loss is 0.015980428084731102\n",
      "epoch: 9 step: 276, loss is 0.023076286539435387\n",
      "epoch: 9 step: 277, loss is 0.0010445591760799289\n",
      "epoch: 9 step: 278, loss is 0.0032790827099233866\n",
      "epoch: 9 step: 279, loss is 0.04526614770293236\n",
      "epoch: 9 step: 280, loss is 0.01257677935063839\n",
      "epoch: 9 step: 281, loss is 0.0026655078399926424\n",
      "epoch: 9 step: 282, loss is 0.04932940751314163\n",
      "epoch: 9 step: 283, loss is 0.009193391539156437\n",
      "epoch: 9 step: 284, loss is 0.0021056493278592825\n",
      "epoch: 9 step: 285, loss is 0.0002921100822277367\n",
      "epoch: 9 step: 286, loss is 0.01337103545665741\n",
      "epoch: 9 step: 287, loss is 0.10539589822292328\n",
      "epoch: 9 step: 288, loss is 0.00038735513226129115\n",
      "epoch: 9 step: 289, loss is 0.0005829510046169162\n",
      "epoch: 9 step: 290, loss is 0.016077149659395218\n",
      "epoch: 9 step: 291, loss is 0.015452953986823559\n",
      "epoch: 9 step: 292, loss is 0.042370960116386414\n",
      "epoch: 9 step: 293, loss is 0.01752973534166813\n",
      "epoch: 9 step: 294, loss is 0.0020159967243671417\n",
      "epoch: 9 step: 295, loss is 0.016994228586554527\n",
      "epoch: 9 step: 296, loss is 0.012365728616714478\n",
      "epoch: 9 step: 297, loss is 0.000800368026830256\n",
      "epoch: 9 step: 298, loss is 0.013855639845132828\n",
      "epoch: 9 step: 299, loss is 0.009165022522211075\n",
      "epoch: 9 step: 300, loss is 0.0034533904399722815\n",
      "epoch: 9 step: 301, loss is 0.12535321712493896\n",
      "epoch: 9 step: 302, loss is 0.08361570537090302\n",
      "epoch: 9 step: 303, loss is 0.11447537690401077\n",
      "epoch: 9 step: 304, loss is 0.06979013979434967\n",
      "epoch: 9 step: 305, loss is 0.0028246936853975058\n",
      "epoch: 9 step: 306, loss is 0.004130094312131405\n",
      "epoch: 9 step: 307, loss is 0.0024487399496138096\n",
      "epoch: 9 step: 308, loss is 8.285003423225135e-05\n",
      "epoch: 9 step: 309, loss is 0.018809685483574867\n",
      "epoch: 9 step: 310, loss is 0.007615721318870783\n",
      "epoch: 9 step: 311, loss is 0.009821001440286636\n",
      "epoch: 9 step: 312, loss is 0.0166004728525877\n",
      "epoch: 9 step: 313, loss is 0.01086075697094202\n",
      "epoch: 9 step: 314, loss is 0.02527683973312378\n",
      "epoch: 9 step: 315, loss is 0.0016992592718452215\n",
      "epoch: 9 step: 316, loss is 0.0011862005339935422\n",
      "epoch: 9 step: 317, loss is 0.010835142806172371\n",
      "epoch: 9 step: 318, loss is 0.03928782045841217\n",
      "epoch: 9 step: 319, loss is 0.013951150700449944\n",
      "epoch: 9 step: 320, loss is 0.10212137550115585\n",
      "epoch: 9 step: 321, loss is 0.02118433266878128\n",
      "epoch: 9 step: 322, loss is 0.009439399465918541\n",
      "epoch: 9 step: 323, loss is 0.00037989107659086585\n",
      "epoch: 9 step: 324, loss is 0.0006637877086177468\n",
      "epoch: 9 step: 325, loss is 0.0010538781061768532\n",
      "epoch: 9 step: 326, loss is 0.011986946687102318\n",
      "epoch: 9 step: 327, loss is 0.010445642285048962\n",
      "epoch: 9 step: 328, loss is 0.0027867048047482967\n",
      "epoch: 9 step: 329, loss is 0.0059730289503932\n",
      "epoch: 9 step: 330, loss is 0.005282094702124596\n",
      "epoch: 9 step: 331, loss is 0.08085411787033081\n",
      "epoch: 9 step: 332, loss is 0.00489278556779027\n",
      "epoch: 9 step: 333, loss is 0.021857384592294693\n",
      "epoch: 9 step: 334, loss is 0.0019250157056376338\n",
      "epoch: 9 step: 335, loss is 0.0022117469925433397\n",
      "epoch: 9 step: 336, loss is 0.002733938628807664\n",
      "epoch: 9 step: 337, loss is 0.027690039947628975\n",
      "epoch: 9 step: 338, loss is 0.0004025283851660788\n",
      "epoch: 9 step: 339, loss is 0.0028075017035007477\n",
      "epoch: 9 step: 340, loss is 0.03258100524544716\n",
      "epoch: 9 step: 341, loss is 0.00034146971302106977\n",
      "epoch: 9 step: 342, loss is 0.041012704372406006\n",
      "epoch: 9 step: 343, loss is 0.023934543132781982\n",
      "epoch: 9 step: 344, loss is 0.012904586270451546\n",
      "epoch: 9 step: 345, loss is 0.008118688128888607\n",
      "epoch: 9 step: 346, loss is 0.0003740972315426916\n",
      "epoch: 9 step: 347, loss is 0.031683288514614105\n",
      "epoch: 9 step: 348, loss is 3.577221286832355e-05\n",
      "epoch: 9 step: 349, loss is 0.040409643203020096\n",
      "epoch: 9 step: 350, loss is 0.009161724708974361\n",
      "epoch: 9 step: 351, loss is 0.00023800035705789924\n",
      "epoch: 9 step: 352, loss is 0.00028979333001188934\n",
      "epoch: 9 step: 353, loss is 0.04992114007472992\n",
      "epoch: 9 step: 354, loss is 0.00108586554415524\n",
      "epoch: 9 step: 355, loss is 0.00867999903857708\n",
      "epoch: 9 step: 356, loss is 0.05802641436457634\n",
      "epoch: 9 step: 357, loss is 0.0007658091490156949\n",
      "epoch: 9 step: 358, loss is 0.0018665503012016416\n",
      "epoch: 9 step: 359, loss is 0.09135095030069351\n",
      "epoch: 9 step: 360, loss is 0.002015426754951477\n",
      "epoch: 9 step: 361, loss is 0.001792125403881073\n",
      "epoch: 9 step: 362, loss is 0.0956762433052063\n",
      "epoch: 9 step: 363, loss is 0.0006434689857997\n",
      "epoch: 9 step: 364, loss is 0.00485658785328269\n",
      "epoch: 9 step: 365, loss is 0.0009135412983596325\n",
      "epoch: 9 step: 366, loss is 0.0003663548268377781\n",
      "epoch: 9 step: 367, loss is 0.007968654856085777\n",
      "epoch: 9 step: 368, loss is 0.07625694572925568\n",
      "epoch: 9 step: 369, loss is 0.009287510067224503\n",
      "epoch: 9 step: 370, loss is 0.00021994717826601118\n",
      "epoch: 9 step: 371, loss is 0.026388391852378845\n",
      "epoch: 9 step: 372, loss is 0.04501098394393921\n",
      "epoch: 9 step: 373, loss is 0.008134005591273308\n",
      "epoch: 9 step: 374, loss is 4.6052198740653694e-05\n",
      "epoch: 9 step: 375, loss is 2.8134993044659495e-05\n",
      "epoch: 9 step: 376, loss is 0.02628389000892639\n",
      "epoch: 9 step: 377, loss is 0.029698694124817848\n",
      "epoch: 9 step: 378, loss is 0.00039895219379104674\n",
      "epoch: 9 step: 379, loss is 0.002814291277900338\n",
      "epoch: 9 step: 380, loss is 0.012259907089173794\n",
      "epoch: 9 step: 381, loss is 0.14130239188671112\n",
      "epoch: 9 step: 382, loss is 0.014799085445702076\n",
      "epoch: 9 step: 383, loss is 0.00033175561111420393\n",
      "epoch: 9 step: 384, loss is 0.0020683822222054005\n",
      "epoch: 9 step: 385, loss is 0.010532508604228497\n",
      "epoch: 9 step: 386, loss is 0.0025165711995214224\n",
      "epoch: 9 step: 387, loss is 0.0107268076390028\n",
      "epoch: 9 step: 388, loss is 0.004426090978085995\n",
      "epoch: 9 step: 389, loss is 0.04228166863322258\n",
      "epoch: 9 step: 390, loss is 0.0004969455185346305\n",
      "epoch: 9 step: 391, loss is 0.06961870193481445\n",
      "epoch: 9 step: 392, loss is 0.0031260710675269365\n",
      "epoch: 9 step: 393, loss is 0.0026965499855577946\n",
      "epoch: 9 step: 394, loss is 0.007674969732761383\n",
      "epoch: 9 step: 395, loss is 0.11331355571746826\n",
      "epoch: 9 step: 396, loss is 0.0030610908288508654\n",
      "epoch: 9 step: 397, loss is 0.04600691795349121\n",
      "epoch: 9 step: 398, loss is 0.010322236455976963\n",
      "epoch: 9 step: 399, loss is 0.01078385952860117\n",
      "epoch: 9 step: 400, loss is 0.0005601771990768611\n",
      "epoch: 9 step: 401, loss is 0.007285486441105604\n",
      "epoch: 9 step: 402, loss is 0.025098606944084167\n",
      "epoch: 9 step: 403, loss is 0.08214448392391205\n",
      "epoch: 9 step: 404, loss is 0.019839737564325333\n",
      "epoch: 9 step: 405, loss is 0.0034654198680073023\n",
      "epoch: 9 step: 406, loss is 0.0011929047759622335\n",
      "epoch: 9 step: 407, loss is 0.001468577655032277\n",
      "epoch: 9 step: 408, loss is 0.01667669788002968\n",
      "epoch: 9 step: 409, loss is 0.030197717249393463\n",
      "epoch: 9 step: 410, loss is 0.09476697444915771\n",
      "epoch: 9 step: 411, loss is 0.0807647854089737\n",
      "epoch: 9 step: 412, loss is 0.004187657963484526\n",
      "epoch: 9 step: 413, loss is 0.027591636404395103\n",
      "epoch: 9 step: 414, loss is 7.195555372163653e-05\n",
      "epoch: 9 step: 415, loss is 0.05585155636072159\n",
      "epoch: 9 step: 416, loss is 0.02695586532354355\n",
      "epoch: 9 step: 417, loss is 0.11942268162965775\n",
      "epoch: 9 step: 418, loss is 0.006454350892454386\n",
      "epoch: 9 step: 419, loss is 0.07835850864648819\n",
      "epoch: 9 step: 420, loss is 0.000236998064792715\n",
      "epoch: 9 step: 421, loss is 0.0014048964949324727\n",
      "epoch: 9 step: 422, loss is 0.02034415304660797\n",
      "epoch: 9 step: 423, loss is 0.18675149977207184\n",
      "epoch: 9 step: 424, loss is 0.009403162635862827\n",
      "epoch: 9 step: 425, loss is 0.011764639988541603\n",
      "epoch: 9 step: 426, loss is 0.14768850803375244\n",
      "epoch: 9 step: 427, loss is 0.009461423382163048\n",
      "epoch: 9 step: 428, loss is 0.0402398519217968\n",
      "epoch: 9 step: 429, loss is 0.004686011932790279\n",
      "epoch: 9 step: 430, loss is 0.01643063686788082\n",
      "epoch: 9 step: 431, loss is 0.006437345873564482\n",
      "epoch: 9 step: 432, loss is 0.017718790099024773\n",
      "epoch: 9 step: 433, loss is 0.007550031878054142\n",
      "epoch: 9 step: 434, loss is 0.043675970286130905\n",
      "epoch: 9 step: 435, loss is 0.01747393235564232\n",
      "epoch: 9 step: 436, loss is 0.08763932436704636\n",
      "epoch: 9 step: 437, loss is 0.008496004156768322\n",
      "epoch: 9 step: 438, loss is 0.10327561944723129\n",
      "epoch: 9 step: 439, loss is 0.014178808778524399\n",
      "epoch: 9 step: 440, loss is 0.011310492642223835\n",
      "epoch: 9 step: 441, loss is 0.005989700555801392\n",
      "epoch: 9 step: 442, loss is 0.0005520270206034184\n",
      "epoch: 9 step: 443, loss is 0.02091946452856064\n",
      "epoch: 9 step: 444, loss is 0.0402701161801815\n",
      "epoch: 9 step: 445, loss is 0.0472482331097126\n",
      "epoch: 9 step: 446, loss is 0.08140271157026291\n",
      "epoch: 9 step: 447, loss is 0.008042273111641407\n",
      "epoch: 9 step: 448, loss is 0.05709141865372658\n",
      "epoch: 9 step: 449, loss is 0.0015820616390556097\n",
      "epoch: 9 step: 450, loss is 0.004453609697520733\n",
      "epoch: 9 step: 451, loss is 0.03450646251440048\n",
      "epoch: 9 step: 452, loss is 0.05899491161108017\n",
      "epoch: 9 step: 453, loss is 0.10075436532497406\n",
      "epoch: 9 step: 454, loss is 0.013335909694433212\n",
      "epoch: 9 step: 455, loss is 0.03894132375717163\n",
      "epoch: 9 step: 456, loss is 0.01334588136523962\n",
      "epoch: 9 step: 457, loss is 0.003692393423989415\n",
      "epoch: 9 step: 458, loss is 0.015225945971906185\n",
      "epoch: 9 step: 459, loss is 0.016622241586446762\n",
      "epoch: 9 step: 460, loss is 0.008687594905495644\n",
      "epoch: 9 step: 461, loss is 0.008618420921266079\n",
      "epoch: 9 step: 462, loss is 0.10826389491558075\n",
      "epoch: 9 step: 463, loss is 0.09661208838224411\n",
      "epoch: 9 step: 464, loss is 0.07764774560928345\n",
      "epoch: 9 step: 465, loss is 0.18586187064647675\n",
      "epoch: 9 step: 466, loss is 0.040786392986774445\n",
      "epoch: 9 step: 467, loss is 0.005495815072208643\n",
      "epoch: 9 step: 468, loss is 0.002187969395890832\n",
      "epoch: 9 step: 469, loss is 9.464485628996044e-05\n",
      "epoch: 9 step: 470, loss is 0.01832621917128563\n",
      "epoch: 9 step: 471, loss is 0.0003060390881728381\n",
      "epoch: 9 step: 472, loss is 0.014619509689509869\n",
      "epoch: 9 step: 473, loss is 0.006913331802934408\n",
      "epoch: 9 step: 474, loss is 0.006911227013915777\n",
      "epoch: 9 step: 475, loss is 0.058994460850954056\n",
      "epoch: 9 step: 476, loss is 0.0007971537415869534\n",
      "epoch: 9 step: 477, loss is 0.02763441950082779\n",
      "epoch: 9 step: 478, loss is 0.00023501667601522058\n",
      "epoch: 9 step: 479, loss is 0.0033991136588156223\n",
      "epoch: 9 step: 480, loss is 0.006667247042059898\n",
      "epoch: 9 step: 481, loss is 0.03837721049785614\n",
      "epoch: 9 step: 482, loss is 0.07728225737810135\n",
      "epoch: 9 step: 483, loss is 0.03495754301548004\n",
      "epoch: 9 step: 484, loss is 0.001607371959835291\n",
      "epoch: 9 step: 485, loss is 0.01466215867549181\n",
      "epoch: 9 step: 486, loss is 0.032595645636320114\n",
      "epoch: 9 step: 487, loss is 0.011992458254098892\n",
      "epoch: 9 step: 488, loss is 0.009253961965441704\n",
      "epoch: 9 step: 489, loss is 0.0029215964023023844\n",
      "epoch: 9 step: 490, loss is 0.020784791558980942\n",
      "epoch: 9 step: 491, loss is 0.1187981590628624\n",
      "epoch: 9 step: 492, loss is 0.0004571837780531496\n",
      "epoch: 9 step: 493, loss is 0.0004972394090145826\n",
      "epoch: 9 step: 494, loss is 0.00020977950771339238\n",
      "epoch: 9 step: 495, loss is 0.0020435422193259\n",
      "epoch: 9 step: 496, loss is 0.0009549859096296132\n",
      "epoch: 9 step: 497, loss is 0.0009439390269108117\n",
      "epoch: 9 step: 498, loss is 0.019722843542695045\n",
      "epoch: 9 step: 499, loss is 0.003666570410132408\n",
      "epoch: 9 step: 500, loss is 0.0050033824518322945\n",
      "epoch: 9 step: 501, loss is 0.0062736389227211475\n",
      "epoch: 9 step: 502, loss is 0.020459648221731186\n",
      "epoch: 9 step: 503, loss is 0.003584829857572913\n",
      "epoch: 9 step: 504, loss is 0.016034092754125595\n",
      "epoch: 9 step: 505, loss is 0.0019193018088117242\n",
      "epoch: 9 step: 506, loss is 0.08275392651557922\n",
      "epoch: 9 step: 507, loss is 0.031287483870983124\n",
      "epoch: 9 step: 508, loss is 0.006422166246920824\n",
      "epoch: 9 step: 509, loss is 0.11569315195083618\n",
      "epoch: 9 step: 510, loss is 0.00010198162635788321\n",
      "epoch: 9 step: 511, loss is 0.01394711434841156\n",
      "epoch: 9 step: 512, loss is 0.11011213064193726\n",
      "epoch: 9 step: 513, loss is 0.09435036033391953\n",
      "epoch: 9 step: 514, loss is 0.13505592942237854\n",
      "epoch: 9 step: 515, loss is 0.0016007926315069199\n",
      "epoch: 9 step: 516, loss is 0.0724288821220398\n",
      "epoch: 9 step: 517, loss is 0.02833254635334015\n",
      "epoch: 9 step: 518, loss is 0.0032379592303186655\n",
      "epoch: 9 step: 519, loss is 0.11129803955554962\n",
      "epoch: 9 step: 520, loss is 0.10266532003879547\n",
      "epoch: 9 step: 521, loss is 0.00710429809987545\n",
      "epoch: 9 step: 522, loss is 0.047553159296512604\n",
      "epoch: 9 step: 523, loss is 0.0001893733424367383\n",
      "epoch: 9 step: 524, loss is 0.0013532242737710476\n",
      "epoch: 9 step: 525, loss is 0.004246330354362726\n",
      "epoch: 9 step: 526, loss is 0.0005740277701988816\n",
      "epoch: 9 step: 527, loss is 0.01048225350677967\n",
      "epoch: 9 step: 528, loss is 0.04518718272447586\n",
      "epoch: 9 step: 529, loss is 0.03092103637754917\n",
      "epoch: 9 step: 530, loss is 0.16621160507202148\n",
      "epoch: 9 step: 531, loss is 0.014422714710235596\n",
      "epoch: 9 step: 532, loss is 0.024897057563066483\n",
      "epoch: 9 step: 533, loss is 0.008279443718492985\n",
      "epoch: 9 step: 534, loss is 0.058588236570358276\n",
      "epoch: 9 step: 535, loss is 0.040535107254981995\n",
      "epoch: 9 step: 536, loss is 0.0029832245782017708\n",
      "epoch: 9 step: 537, loss is 0.003894952591508627\n",
      "epoch: 9 step: 538, loss is 0.006408411078155041\n",
      "epoch: 9 step: 539, loss is 0.00012690357107203454\n",
      "epoch: 9 step: 540, loss is 0.005159169435501099\n",
      "epoch: 9 step: 541, loss is 0.0013994794571772218\n",
      "epoch: 9 step: 542, loss is 0.000925934873521328\n",
      "epoch: 9 step: 543, loss is 0.002554341685026884\n",
      "epoch: 9 step: 544, loss is 0.0071189808659255505\n",
      "epoch: 9 step: 545, loss is 0.08803772181272507\n",
      "epoch: 9 step: 546, loss is 0.000862256099935621\n",
      "epoch: 9 step: 547, loss is 0.06597201526165009\n",
      "epoch: 9 step: 548, loss is 0.004766619764268398\n",
      "epoch: 9 step: 549, loss is 0.0010934381280094385\n",
      "epoch: 9 step: 550, loss is 0.0010515962494537234\n",
      "epoch: 9 step: 551, loss is 0.00012461672304198146\n",
      "epoch: 9 step: 552, loss is 0.0002446881262585521\n",
      "epoch: 9 step: 553, loss is 0.019609445706009865\n",
      "epoch: 9 step: 554, loss is 0.0007902647485025227\n",
      "epoch: 9 step: 555, loss is 0.013627208769321442\n",
      "epoch: 9 step: 556, loss is 0.006217157002538443\n",
      "epoch: 9 step: 557, loss is 0.003242606297135353\n",
      "epoch: 9 step: 558, loss is 0.010118105448782444\n",
      "epoch: 9 step: 559, loss is 0.0009301092941313982\n",
      "epoch: 9 step: 560, loss is 0.022082384675741196\n",
      "epoch: 9 step: 561, loss is 0.0030998436268419027\n",
      "epoch: 9 step: 562, loss is 0.0007500244537368417\n",
      "epoch: 9 step: 563, loss is 0.02509734407067299\n",
      "epoch: 9 step: 564, loss is 5.235514254309237e-05\n",
      "epoch: 9 step: 565, loss is 0.06700123846530914\n",
      "epoch: 9 step: 566, loss is 0.08783528208732605\n",
      "epoch: 9 step: 567, loss is 0.001334203639999032\n",
      "epoch: 9 step: 568, loss is 0.0009203559602610767\n",
      "epoch: 9 step: 569, loss is 0.024847708642482758\n",
      "epoch: 9 step: 570, loss is 0.1023329645395279\n",
      "epoch: 9 step: 571, loss is 0.022998828440904617\n",
      "epoch: 9 step: 572, loss is 0.0005482014967128634\n",
      "epoch: 9 step: 573, loss is 0.023673536255955696\n",
      "epoch: 9 step: 574, loss is 0.030992640182375908\n",
      "epoch: 9 step: 575, loss is 0.02449662983417511\n",
      "epoch: 9 step: 576, loss is 0.0009711917373351753\n",
      "epoch: 9 step: 577, loss is 0.37919723987579346\n",
      "epoch: 9 step: 578, loss is 0.019517945125699043\n",
      "epoch: 9 step: 579, loss is 0.03323673829436302\n",
      "epoch: 9 step: 580, loss is 0.00022464001085609198\n",
      "epoch: 9 step: 581, loss is 0.003599019953981042\n",
      "epoch: 9 step: 582, loss is 0.015471206046640873\n",
      "epoch: 9 step: 583, loss is 0.0007511170115321875\n",
      "epoch: 9 step: 584, loss is 0.023332716897130013\n",
      "epoch: 9 step: 585, loss is 0.002612449461594224\n",
      "epoch: 9 step: 586, loss is 0.015287200920283794\n",
      "epoch: 9 step: 587, loss is 0.03936939686536789\n",
      "epoch: 9 step: 588, loss is 0.017646189779043198\n",
      "epoch: 9 step: 589, loss is 0.03685317933559418\n",
      "epoch: 9 step: 590, loss is 0.08330035954713821\n",
      "epoch: 9 step: 591, loss is 0.10254456102848053\n",
      "epoch: 9 step: 592, loss is 0.057722337543964386\n",
      "epoch: 9 step: 593, loss is 0.0015330638270825148\n",
      "epoch: 9 step: 594, loss is 0.0027928161434829235\n",
      "epoch: 9 step: 595, loss is 0.009180109016597271\n",
      "epoch: 9 step: 596, loss is 0.0022726724855601788\n",
      "epoch: 9 step: 597, loss is 0.0011949276085942984\n",
      "epoch: 9 step: 598, loss is 0.03964902460575104\n",
      "epoch: 9 step: 599, loss is 0.015356828458607197\n",
      "epoch: 9 step: 600, loss is 0.08401201665401459\n",
      "epoch: 9 step: 601, loss is 0.010554592125117779\n",
      "epoch: 9 step: 602, loss is 0.0300365649163723\n",
      "epoch: 9 step: 603, loss is 0.015553846955299377\n",
      "epoch: 9 step: 604, loss is 0.010917403735220432\n",
      "epoch: 9 step: 605, loss is 0.0013712890213355422\n",
      "epoch: 9 step: 606, loss is 0.007054487243294716\n",
      "epoch: 9 step: 607, loss is 0.0815807580947876\n",
      "epoch: 9 step: 608, loss is 0.07388236373662949\n",
      "epoch: 9 step: 609, loss is 0.004796803928911686\n",
      "epoch: 9 step: 610, loss is 0.0006175083108246326\n",
      "epoch: 9 step: 611, loss is 0.03928237035870552\n",
      "epoch: 9 step: 612, loss is 0.02148302085697651\n",
      "epoch: 9 step: 613, loss is 0.0023502514231950045\n",
      "epoch: 9 step: 614, loss is 0.001500600134022534\n",
      "epoch: 9 step: 615, loss is 0.050446175038814545\n",
      "epoch: 9 step: 616, loss is 0.022998597472906113\n",
      "epoch: 9 step: 617, loss is 0.0626562312245369\n",
      "epoch: 9 step: 618, loss is 0.030829938128590584\n",
      "epoch: 9 step: 619, loss is 0.018303487449884415\n",
      "epoch: 9 step: 620, loss is 0.0005238064331933856\n",
      "epoch: 9 step: 621, loss is 0.0023000945802778006\n",
      "epoch: 9 step: 622, loss is 0.004482620861381292\n",
      "epoch: 9 step: 623, loss is 0.07182043045759201\n",
      "epoch: 9 step: 624, loss is 0.0038242973387241364\n",
      "epoch: 9 step: 625, loss is 0.08100137114524841\n",
      "epoch: 9 step: 626, loss is 0.06610415875911713\n",
      "epoch: 9 step: 627, loss is 0.044018130749464035\n",
      "epoch: 9 step: 628, loss is 0.016338318586349487\n",
      "epoch: 9 step: 629, loss is 0.0950198769569397\n",
      "epoch: 9 step: 630, loss is 0.0050774686969816685\n",
      "epoch: 9 step: 631, loss is 6.898762512719259e-05\n",
      "epoch: 9 step: 632, loss is 0.0013363807229325175\n",
      "epoch: 9 step: 633, loss is 0.02480124682188034\n",
      "epoch: 9 step: 634, loss is 0.029990751296281815\n",
      "epoch: 9 step: 635, loss is 0.004141219891607761\n",
      "epoch: 9 step: 636, loss is 0.07658808678388596\n",
      "epoch: 9 step: 637, loss is 0.004510330967605114\n",
      "epoch: 9 step: 638, loss is 0.030422938987612724\n",
      "epoch: 9 step: 639, loss is 0.044190049171447754\n",
      "epoch: 9 step: 640, loss is 0.0004337152640800923\n",
      "epoch: 9 step: 641, loss is 0.001980594592168927\n",
      "epoch: 9 step: 642, loss is 0.03740543872117996\n",
      "epoch: 9 step: 643, loss is 0.02706938236951828\n",
      "epoch: 9 step: 644, loss is 0.01350900623947382\n",
      "epoch: 9 step: 645, loss is 0.02064642682671547\n",
      "epoch: 9 step: 646, loss is 0.0005935004446655512\n",
      "epoch: 9 step: 647, loss is 0.12846516072750092\n",
      "epoch: 9 step: 648, loss is 0.001392690115608275\n",
      "epoch: 9 step: 649, loss is 0.11031325161457062\n",
      "epoch: 9 step: 650, loss is 0.00030905133462511003\n",
      "epoch: 9 step: 651, loss is 0.0025299368426203728\n",
      "epoch: 9 step: 652, loss is 0.005843790713697672\n",
      "epoch: 9 step: 653, loss is 0.008536683395504951\n",
      "epoch: 9 step: 654, loss is 0.00033102923771366477\n",
      "epoch: 9 step: 655, loss is 0.06696371734142303\n",
      "epoch: 9 step: 656, loss is 0.0005530161433853209\n",
      "epoch: 9 step: 657, loss is 0.0004531829908955842\n",
      "epoch: 9 step: 658, loss is 0.003205846529453993\n",
      "epoch: 9 step: 659, loss is 0.004206704907119274\n",
      "epoch: 9 step: 660, loss is 0.02336806431412697\n",
      "epoch: 9 step: 661, loss is 0.00017861324886325747\n",
      "epoch: 9 step: 662, loss is 0.0006218117196112871\n",
      "epoch: 9 step: 663, loss is 0.0007569258450530469\n",
      "epoch: 9 step: 664, loss is 0.001554578193463385\n",
      "epoch: 9 step: 665, loss is 0.045595377683639526\n",
      "epoch: 9 step: 666, loss is 0.011970669031143188\n",
      "epoch: 9 step: 667, loss is 0.023704679682850838\n",
      "epoch: 9 step: 668, loss is 0.01559521071612835\n",
      "epoch: 9 step: 669, loss is 0.003166231559589505\n",
      "epoch: 9 step: 670, loss is 0.00036006822483614087\n",
      "epoch: 9 step: 671, loss is 0.00697820819914341\n",
      "epoch: 9 step: 672, loss is 0.005019673146307468\n",
      "epoch: 9 step: 673, loss is 0.029970837756991386\n",
      "epoch: 9 step: 674, loss is 0.019597282633185387\n",
      "epoch: 9 step: 675, loss is 0.010698845610022545\n",
      "epoch: 9 step: 676, loss is 0.06430783867835999\n",
      "epoch: 9 step: 677, loss is 0.06164487078785896\n",
      "epoch: 9 step: 678, loss is 0.027457265183329582\n",
      "epoch: 9 step: 679, loss is 0.0038412464782595634\n",
      "epoch: 9 step: 680, loss is 0.0025109867565333843\n",
      "epoch: 9 step: 681, loss is 0.0035689417272806168\n",
      "epoch: 9 step: 682, loss is 0.012285768054425716\n",
      "epoch: 9 step: 683, loss is 0.0006701020174659789\n",
      "epoch: 9 step: 684, loss is 0.015949049964547157\n",
      "epoch: 9 step: 685, loss is 0.003839727956801653\n",
      "epoch: 9 step: 686, loss is 0.10249660909175873\n",
      "epoch: 9 step: 687, loss is 0.000321593921398744\n",
      "epoch: 9 step: 688, loss is 0.035680774599313736\n",
      "epoch: 9 step: 689, loss is 0.00016243327991105616\n",
      "epoch: 9 step: 690, loss is 0.0026021157391369343\n",
      "epoch: 9 step: 691, loss is 0.002969818189740181\n",
      "epoch: 9 step: 692, loss is 0.007163730449974537\n",
      "epoch: 9 step: 693, loss is 0.002570106415078044\n",
      "epoch: 9 step: 694, loss is 0.008109855465590954\n",
      "epoch: 9 step: 695, loss is 0.02120431326329708\n",
      "epoch: 9 step: 696, loss is 0.0313258022069931\n",
      "epoch: 9 step: 697, loss is 0.06658675521612167\n",
      "epoch: 9 step: 698, loss is 0.022962458431720734\n",
      "epoch: 9 step: 699, loss is 0.00046666362322866917\n",
      "epoch: 9 step: 700, loss is 0.002663047518581152\n",
      "epoch: 9 step: 701, loss is 0.0038809780962765217\n",
      "epoch: 9 step: 702, loss is 0.0014219790464267135\n",
      "epoch: 9 step: 703, loss is 0.03603903576731682\n",
      "epoch: 9 step: 704, loss is 0.0038433787412941456\n",
      "epoch: 9 step: 705, loss is 0.022879553958773613\n",
      "epoch: 9 step: 706, loss is 0.0002098869881592691\n",
      "epoch: 9 step: 707, loss is 0.0005702733178623021\n",
      "epoch: 9 step: 708, loss is 0.0014294213615357876\n",
      "epoch: 9 step: 709, loss is 0.06540737301111221\n",
      "epoch: 9 step: 710, loss is 0.12130066007375717\n",
      "epoch: 9 step: 711, loss is 0.09691152721643448\n",
      "epoch: 9 step: 712, loss is 0.003236425342038274\n",
      "epoch: 9 step: 713, loss is 0.0013510914286598563\n",
      "epoch: 9 step: 714, loss is 0.013160706497728825\n",
      "epoch: 9 step: 715, loss is 0.0013326251646503806\n",
      "epoch: 9 step: 716, loss is 0.06846916675567627\n",
      "epoch: 9 step: 717, loss is 0.026150250807404518\n",
      "epoch: 9 step: 718, loss is 0.013620574027299881\n",
      "epoch: 9 step: 719, loss is 0.0006093126721680164\n",
      "epoch: 9 step: 720, loss is 0.010272141546010971\n",
      "epoch: 9 step: 721, loss is 0.01760510541498661\n",
      "epoch: 9 step: 722, loss is 0.0002464034187141806\n",
      "epoch: 9 step: 723, loss is 0.009957085363566875\n",
      "epoch: 9 step: 724, loss is 0.006256230175495148\n",
      "epoch: 9 step: 725, loss is 0.17716629803180695\n",
      "epoch: 9 step: 726, loss is 0.10920245945453644\n",
      "epoch: 9 step: 727, loss is 0.0008245914359577\n",
      "epoch: 9 step: 728, loss is 0.017247069627046585\n",
      "epoch: 9 step: 729, loss is 0.11280256509780884\n",
      "epoch: 9 step: 730, loss is 0.032122060656547546\n",
      "epoch: 9 step: 731, loss is 0.059649836272001266\n",
      "epoch: 9 step: 732, loss is 0.005918561480939388\n",
      "epoch: 9 step: 733, loss is 0.07661815732717514\n",
      "epoch: 9 step: 734, loss is 0.0006365174194797873\n",
      "epoch: 9 step: 735, loss is 0.008138684555888176\n",
      "epoch: 9 step: 736, loss is 0.003243937622755766\n",
      "epoch: 9 step: 737, loss is 0.016230257228016853\n",
      "epoch: 9 step: 738, loss is 0.0009579917532391846\n",
      "epoch: 9 step: 739, loss is 0.0004977829521521926\n",
      "epoch: 9 step: 740, loss is 0.0006433242815546691\n",
      "epoch: 9 step: 741, loss is 0.02592199109494686\n",
      "epoch: 9 step: 742, loss is 0.008009647950530052\n",
      "epoch: 9 step: 743, loss is 0.1398591548204422\n",
      "epoch: 9 step: 744, loss is 0.05805131420493126\n",
      "epoch: 9 step: 745, loss is 0.0007983238901942968\n",
      "epoch: 9 step: 746, loss is 0.0019521303474903107\n",
      "epoch: 9 step: 747, loss is 0.0010361113818362355\n",
      "epoch: 9 step: 748, loss is 0.05858488008379936\n",
      "epoch: 9 step: 749, loss is 0.07042911648750305\n",
      "epoch: 9 step: 750, loss is 0.19495132565498352\n",
      "epoch: 9 step: 751, loss is 0.01650766283273697\n",
      "epoch: 9 step: 752, loss is 0.013880100101232529\n",
      "epoch: 9 step: 753, loss is 0.06279738992452621\n",
      "epoch: 9 step: 754, loss is 0.016868241131305695\n",
      "epoch: 9 step: 755, loss is 0.0058136507868766785\n",
      "epoch: 9 step: 756, loss is 0.007577524054795504\n",
      "epoch: 9 step: 757, loss is 0.017879126593470573\n",
      "epoch: 9 step: 758, loss is 0.06460465490818024\n",
      "epoch: 9 step: 759, loss is 0.0008802150841802359\n",
      "epoch: 9 step: 760, loss is 0.02944154106080532\n",
      "epoch: 9 step: 761, loss is 0.05252218618988991\n",
      "epoch: 9 step: 762, loss is 0.014128386043012142\n",
      "epoch: 9 step: 763, loss is 0.1755002737045288\n",
      "epoch: 9 step: 764, loss is 0.016222389414906502\n",
      "epoch: 9 step: 765, loss is 0.09730324149131775\n",
      "epoch: 9 step: 766, loss is 0.0377921462059021\n",
      "epoch: 9 step: 767, loss is 0.005783018656075001\n",
      "epoch: 9 step: 768, loss is 0.013276656158268452\n",
      "epoch: 9 step: 769, loss is 0.0007863665232434869\n",
      "epoch: 9 step: 770, loss is 0.004573824815452099\n",
      "epoch: 9 step: 771, loss is 0.09755846112966537\n",
      "epoch: 9 step: 772, loss is 0.006527719087898731\n",
      "epoch: 9 step: 773, loss is 0.00929824635386467\n",
      "epoch: 9 step: 774, loss is 0.025304101407527924\n",
      "epoch: 9 step: 775, loss is 0.06698364019393921\n",
      "epoch: 9 step: 776, loss is 0.0010846818331629038\n",
      "epoch: 9 step: 777, loss is 0.08816999197006226\n",
      "epoch: 9 step: 778, loss is 0.09604275226593018\n",
      "epoch: 9 step: 779, loss is 0.03453062102198601\n",
      "epoch: 9 step: 780, loss is 0.02307737059891224\n",
      "epoch: 9 step: 781, loss is 0.020839380100369453\n",
      "epoch: 9 step: 782, loss is 0.0006938479491509497\n",
      "epoch: 9 step: 783, loss is 0.0032008634880185127\n",
      "epoch: 9 step: 784, loss is 0.0020638215355575085\n",
      "epoch: 9 step: 785, loss is 0.00038880694773979485\n",
      "epoch: 9 step: 786, loss is 0.0523243322968483\n",
      "epoch: 9 step: 787, loss is 0.0001631025952519849\n",
      "epoch: 9 step: 788, loss is 0.020835602656006813\n",
      "epoch: 9 step: 789, loss is 0.017070289701223373\n",
      "epoch: 9 step: 790, loss is 0.00043080138857476413\n",
      "epoch: 9 step: 791, loss is 0.03371519595384598\n",
      "epoch: 9 step: 792, loss is 0.07114766538143158\n",
      "epoch: 9 step: 793, loss is 0.0031201965175569057\n",
      "epoch: 9 step: 794, loss is 0.027548715472221375\n",
      "epoch: 9 step: 795, loss is 0.039363015443086624\n",
      "epoch: 9 step: 796, loss is 0.016556646674871445\n",
      "epoch: 9 step: 797, loss is 0.003565153805539012\n",
      "epoch: 9 step: 798, loss is 0.0032016714103519917\n",
      "epoch: 9 step: 799, loss is 0.03428418189287186\n",
      "epoch: 9 step: 800, loss is 0.00040035255369730294\n",
      "epoch: 9 step: 801, loss is 0.0008145509054884315\n",
      "epoch: 9 step: 802, loss is 0.0018843694124370813\n",
      "epoch: 9 step: 803, loss is 0.07103557884693146\n",
      "epoch: 9 step: 804, loss is 0.03393521532416344\n",
      "epoch: 9 step: 805, loss is 0.007983493618667126\n",
      "epoch: 9 step: 806, loss is 0.0004120257217437029\n",
      "epoch: 9 step: 807, loss is 0.05280417203903198\n",
      "epoch: 9 step: 808, loss is 0.001248232088983059\n",
      "epoch: 9 step: 809, loss is 0.004982256796211004\n",
      "epoch: 9 step: 810, loss is 0.0009315068018622696\n",
      "epoch: 9 step: 811, loss is 0.011852293275296688\n",
      "epoch: 9 step: 812, loss is 0.0003094130370300263\n",
      "epoch: 9 step: 813, loss is 0.0003735573554877192\n",
      "epoch: 9 step: 814, loss is 0.005749869160354137\n",
      "epoch: 9 step: 815, loss is 0.0046348958276212215\n",
      "epoch: 9 step: 816, loss is 0.03512539342045784\n",
      "epoch: 9 step: 817, loss is 0.019352681934833527\n",
      "epoch: 9 step: 818, loss is 0.04936429485678673\n",
      "epoch: 9 step: 819, loss is 0.09638570249080658\n",
      "epoch: 9 step: 820, loss is 0.0023360352497547865\n",
      "epoch: 9 step: 821, loss is 0.0033880160190165043\n",
      "epoch: 9 step: 822, loss is 0.001692779827862978\n",
      "epoch: 9 step: 823, loss is 0.0005360827781260014\n",
      "epoch: 9 step: 824, loss is 0.00026617819094099104\n",
      "epoch: 9 step: 825, loss is 0.06965024024248123\n",
      "epoch: 9 step: 826, loss is 0.02173282764852047\n",
      "epoch: 9 step: 827, loss is 0.07145199924707413\n",
      "epoch: 9 step: 828, loss is 0.011790880002081394\n",
      "epoch: 9 step: 829, loss is 0.001207799301482737\n",
      "epoch: 9 step: 830, loss is 0.0008221540483646095\n",
      "epoch: 9 step: 831, loss is 0.041364748030900955\n",
      "epoch: 9 step: 832, loss is 0.0010389629751443863\n",
      "epoch: 9 step: 833, loss is 0.0018878497648984194\n",
      "epoch: 9 step: 834, loss is 0.026198480278253555\n",
      "epoch: 9 step: 835, loss is 0.0066000064834952354\n",
      "epoch: 9 step: 836, loss is 0.0052209896966814995\n",
      "epoch: 9 step: 837, loss is 0.006977892946451902\n",
      "epoch: 9 step: 838, loss is 0.017100036144256592\n",
      "epoch: 9 step: 839, loss is 0.08565907925367355\n",
      "epoch: 9 step: 840, loss is 0.005519628990441561\n",
      "epoch: 9 step: 841, loss is 0.022224366664886475\n",
      "epoch: 9 step: 842, loss is 0.0017365312669426203\n",
      "epoch: 9 step: 843, loss is 0.019022133201360703\n",
      "epoch: 9 step: 844, loss is 0.012503503821790218\n",
      "epoch: 9 step: 845, loss is 0.0025762778241187334\n",
      "epoch: 9 step: 846, loss is 0.0036026297602802515\n",
      "epoch: 9 step: 847, loss is 0.017001977190375328\n",
      "epoch: 9 step: 848, loss is 0.006318535655736923\n",
      "epoch: 9 step: 849, loss is 0.002996854018419981\n",
      "epoch: 9 step: 850, loss is 0.0012418723199516535\n",
      "epoch: 9 step: 851, loss is 0.017332032322883606\n",
      "epoch: 9 step: 852, loss is 0.1719396859407425\n",
      "epoch: 9 step: 853, loss is 0.009130436927080154\n",
      "epoch: 9 step: 854, loss is 0.009394083172082901\n",
      "epoch: 9 step: 855, loss is 0.05952858179807663\n",
      "epoch: 9 step: 856, loss is 0.10769457370042801\n",
      "epoch: 9 step: 857, loss is 0.01937813311815262\n",
      "epoch: 9 step: 858, loss is 0.020406147465109825\n",
      "epoch: 9 step: 859, loss is 0.0016387662617489696\n",
      "epoch: 9 step: 860, loss is 0.05650993809103966\n",
      "epoch: 9 step: 861, loss is 0.14938803017139435\n",
      "epoch: 9 step: 862, loss is 0.03508324176073074\n",
      "epoch: 9 step: 863, loss is 0.004209648352116346\n",
      "epoch: 9 step: 864, loss is 0.040375206619501114\n",
      "epoch: 9 step: 865, loss is 0.054007600992918015\n",
      "epoch: 9 step: 866, loss is 0.0032578613609075546\n",
      "epoch: 9 step: 867, loss is 0.0011171151418238878\n",
      "epoch: 9 step: 868, loss is 0.007101582828909159\n",
      "epoch: 9 step: 869, loss is 0.032030608505010605\n",
      "epoch: 9 step: 870, loss is 0.0014157663099467754\n",
      "epoch: 9 step: 871, loss is 0.005249825306236744\n",
      "epoch: 9 step: 872, loss is 0.0001360122114419937\n",
      "epoch: 9 step: 873, loss is 0.012071086093783379\n",
      "epoch: 9 step: 874, loss is 0.001256892690435052\n",
      "epoch: 9 step: 875, loss is 0.005923429969698191\n",
      "epoch: 9 step: 876, loss is 0.0016172314062714577\n",
      "epoch: 9 step: 877, loss is 0.0005240725004114211\n",
      "epoch: 9 step: 878, loss is 0.043965894728899\n",
      "epoch: 9 step: 879, loss is 0.0025925219524651766\n",
      "epoch: 9 step: 880, loss is 0.14557380974292755\n",
      "epoch: 9 step: 881, loss is 0.0428970567882061\n",
      "epoch: 9 step: 882, loss is 0.0017224581679329276\n",
      "epoch: 9 step: 883, loss is 0.006540944799780846\n",
      "epoch: 9 step: 884, loss is 0.0003454419784247875\n",
      "epoch: 9 step: 885, loss is 0.025997692719101906\n",
      "epoch: 9 step: 886, loss is 0.010217702016234398\n",
      "epoch: 9 step: 887, loss is 0.033011291176080704\n",
      "epoch: 9 step: 888, loss is 0.05603611469268799\n",
      "epoch: 9 step: 889, loss is 0.00787253025919199\n",
      "epoch: 9 step: 890, loss is 0.007131661754101515\n",
      "epoch: 9 step: 891, loss is 0.1280445158481598\n",
      "epoch: 9 step: 892, loss is 0.01688162051141262\n",
      "epoch: 9 step: 893, loss is 0.013370879925787449\n",
      "epoch: 9 step: 894, loss is 0.12407220900058746\n",
      "epoch: 9 step: 895, loss is 0.0009314921335317194\n",
      "epoch: 9 step: 896, loss is 0.11521986126899719\n",
      "epoch: 9 step: 897, loss is 0.045018553733825684\n",
      "epoch: 9 step: 898, loss is 0.002616423647850752\n",
      "epoch: 9 step: 899, loss is 0.0016865093493834138\n",
      "epoch: 9 step: 900, loss is 0.018001249060034752\n",
      "epoch: 9 step: 901, loss is 0.010233664885163307\n",
      "epoch: 9 step: 902, loss is 0.009324663318693638\n",
      "epoch: 9 step: 903, loss is 0.020691175013780594\n",
      "epoch: 9 step: 904, loss is 0.0011739919427782297\n",
      "epoch: 9 step: 905, loss is 0.06734669953584671\n",
      "epoch: 9 step: 906, loss is 0.01784859225153923\n",
      "epoch: 9 step: 907, loss is 0.011020855978131294\n",
      "epoch: 9 step: 908, loss is 0.0002529582125134766\n",
      "epoch: 9 step: 909, loss is 0.003515171119943261\n",
      "epoch: 9 step: 910, loss is 0.00035339815076440573\n",
      "epoch: 9 step: 911, loss is 0.023306531831622124\n",
      "epoch: 9 step: 912, loss is 0.0001317446876782924\n",
      "epoch: 9 step: 913, loss is 0.012058280408382416\n",
      "epoch: 9 step: 914, loss is 0.0006385791930370033\n",
      "epoch: 9 step: 915, loss is 0.0012359160464257002\n",
      "epoch: 9 step: 916, loss is 0.00912658590823412\n",
      "epoch: 9 step: 917, loss is 0.04983620345592499\n",
      "epoch: 9 step: 918, loss is 0.0006244564428925514\n",
      "epoch: 9 step: 919, loss is 0.008984353393316269\n",
      "epoch: 9 step: 920, loss is 0.336119681596756\n",
      "epoch: 9 step: 921, loss is 0.0037295492365956306\n",
      "epoch: 9 step: 922, loss is 0.00330533180385828\n",
      "epoch: 9 step: 923, loss is 0.008406120352447033\n",
      "epoch: 9 step: 924, loss is 0.00041747416253201663\n",
      "epoch: 9 step: 925, loss is 0.014216205105185509\n",
      "epoch: 9 step: 926, loss is 0.0025840108282864094\n",
      "epoch: 9 step: 927, loss is 0.014103688299655914\n",
      "epoch: 9 step: 928, loss is 0.03157801926136017\n",
      "epoch: 9 step: 929, loss is 0.032442305237054825\n",
      "epoch: 9 step: 930, loss is 0.0005461222026497126\n",
      "epoch: 9 step: 931, loss is 0.002308306284248829\n",
      "epoch: 9 step: 932, loss is 0.000518993241712451\n",
      "epoch: 9 step: 933, loss is 0.05917893722653389\n",
      "epoch: 9 step: 934, loss is 0.04292593523859978\n",
      "epoch: 9 step: 935, loss is 0.04363406077027321\n",
      "epoch: 9 step: 936, loss is 0.002025018911808729\n",
      "epoch: 9 step: 937, loss is 0.008265764452517033\n",
      "epoch: 9 step: 938, loss is 0.0762491226196289\n",
      "epoch: 9 step: 939, loss is 0.025814346969127655\n",
      "epoch: 9 step: 940, loss is 0.010106045752763748\n",
      "epoch: 9 step: 941, loss is 0.029835054650902748\n",
      "epoch: 9 step: 942, loss is 0.00349419005215168\n",
      "epoch: 9 step: 943, loss is 0.013619749806821346\n",
      "epoch: 9 step: 944, loss is 0.08751928061246872\n",
      "epoch: 9 step: 945, loss is 0.008016640320420265\n",
      "epoch: 9 step: 946, loss is 0.0025234143249690533\n",
      "epoch: 9 step: 947, loss is 0.002708018058910966\n",
      "epoch: 9 step: 948, loss is 0.14142842590808868\n",
      "epoch: 9 step: 949, loss is 0.007821694016456604\n",
      "epoch: 9 step: 950, loss is 0.113967664539814\n",
      "epoch: 9 step: 951, loss is 0.0018708318239077926\n",
      "epoch: 9 step: 952, loss is 0.026521261781454086\n",
      "epoch: 9 step: 953, loss is 0.03984742611646652\n",
      "epoch: 9 step: 954, loss is 0.0019676408264786005\n",
      "epoch: 9 step: 955, loss is 0.01230705063790083\n",
      "epoch: 9 step: 956, loss is 0.0061180805787444115\n",
      "epoch: 9 step: 957, loss is 0.0034835843835026026\n",
      "epoch: 9 step: 958, loss is 0.0006566340452991426\n",
      "epoch: 9 step: 959, loss is 0.05704992637038231\n",
      "epoch: 9 step: 960, loss is 0.010654336772859097\n",
      "epoch: 9 step: 961, loss is 0.003258475800976157\n",
      "epoch: 9 step: 962, loss is 0.02959706448018551\n",
      "epoch: 9 step: 963, loss is 0.0016924106748774648\n",
      "epoch: 9 step: 964, loss is 0.002070596907287836\n",
      "epoch: 9 step: 965, loss is 0.0173783041536808\n",
      "epoch: 9 step: 966, loss is 0.006956992670893669\n",
      "epoch: 9 step: 967, loss is 0.01127482857555151\n",
      "epoch: 9 step: 968, loss is 0.012461439706385136\n",
      "epoch: 9 step: 969, loss is 0.004274848848581314\n",
      "epoch: 9 step: 970, loss is 0.07281645387411118\n",
      "epoch: 9 step: 971, loss is 0.0020122546702623367\n",
      "epoch: 9 step: 972, loss is 0.12220577150583267\n",
      "epoch: 9 step: 973, loss is 0.005328190978616476\n",
      "epoch: 9 step: 974, loss is 0.0013030327390879393\n",
      "epoch: 9 step: 975, loss is 0.010769166052341461\n",
      "epoch: 9 step: 976, loss is 0.012727647088468075\n",
      "epoch: 9 step: 977, loss is 0.028194664046168327\n",
      "epoch: 9 step: 978, loss is 0.015633685514330864\n",
      "epoch: 9 step: 979, loss is 0.14666657149791718\n",
      "epoch: 9 step: 980, loss is 0.03149453550577164\n",
      "epoch: 9 step: 981, loss is 0.041157785803079605\n",
      "epoch: 9 step: 982, loss is 0.21251797676086426\n",
      "epoch: 9 step: 983, loss is 0.001173299620859325\n",
      "epoch: 9 step: 984, loss is 0.04608158767223358\n",
      "epoch: 9 step: 985, loss is 0.04773977771401405\n",
      "epoch: 9 step: 986, loss is 0.03989315405488014\n",
      "epoch: 9 step: 987, loss is 0.04455914720892906\n",
      "epoch: 9 step: 988, loss is 0.0004270179197192192\n",
      "epoch: 9 step: 989, loss is 0.012424973770976067\n",
      "epoch: 9 step: 990, loss is 0.12522681057453156\n",
      "epoch: 9 step: 991, loss is 0.01449458859860897\n",
      "epoch: 9 step: 992, loss is 0.02908172644674778\n",
      "epoch: 9 step: 993, loss is 0.00820869393646717\n",
      "epoch: 9 step: 994, loss is 0.000989192514680326\n",
      "epoch: 9 step: 995, loss is 0.003133891150355339\n",
      "epoch: 9 step: 996, loss is 0.05302339419722557\n",
      "epoch: 9 step: 997, loss is 0.000774645188357681\n",
      "epoch: 9 step: 998, loss is 0.0003685142146423459\n",
      "epoch: 9 step: 999, loss is 0.007366251200437546\n",
      "epoch: 9 step: 1000, loss is 0.0009528647060506046\n",
      "epoch: 9 step: 1001, loss is 0.007189556024968624\n",
      "epoch: 9 step: 1002, loss is 0.0761721059679985\n",
      "epoch: 9 step: 1003, loss is 0.12453113496303558\n",
      "epoch: 9 step: 1004, loss is 0.11343176662921906\n",
      "epoch: 9 step: 1005, loss is 0.00433475011959672\n",
      "epoch: 9 step: 1006, loss is 0.0038408427499234676\n",
      "epoch: 9 step: 1007, loss is 9.472878446104005e-05\n",
      "epoch: 9 step: 1008, loss is 0.0002518199326004833\n",
      "epoch: 9 step: 1009, loss is 0.00035002443473786116\n",
      "epoch: 9 step: 1010, loss is 0.0031397705897688866\n",
      "epoch: 9 step: 1011, loss is 0.001693404745310545\n",
      "epoch: 9 step: 1012, loss is 0.001546760555356741\n",
      "epoch: 9 step: 1013, loss is 0.02920091152191162\n",
      "epoch: 9 step: 1014, loss is 0.04525843262672424\n",
      "epoch: 9 step: 1015, loss is 0.020714906975626945\n",
      "epoch: 9 step: 1016, loss is 0.000383622944355011\n",
      "epoch: 9 step: 1017, loss is 0.0024314008187502623\n",
      "epoch: 9 step: 1018, loss is 0.09858772158622742\n",
      "epoch: 9 step: 1019, loss is 0.01065577007830143\n",
      "epoch: 9 step: 1020, loss is 0.05729381740093231\n",
      "epoch: 9 step: 1021, loss is 0.008254350163042545\n",
      "epoch: 9 step: 1022, loss is 0.09111279994249344\n",
      "epoch: 9 step: 1023, loss is 0.0003672331804409623\n",
      "epoch: 9 step: 1024, loss is 0.16658201813697815\n",
      "epoch: 9 step: 1025, loss is 0.009990135207772255\n",
      "epoch: 9 step: 1026, loss is 0.03375713899731636\n",
      "epoch: 9 step: 1027, loss is 0.0008663859334774315\n",
      "epoch: 9 step: 1028, loss is 0.1222689300775528\n",
      "epoch: 9 step: 1029, loss is 0.027910873293876648\n",
      "epoch: 9 step: 1030, loss is 0.0023868242278695107\n",
      "epoch: 9 step: 1031, loss is 0.02873089723289013\n",
      "epoch: 9 step: 1032, loss is 0.002080132719129324\n",
      "epoch: 9 step: 1033, loss is 0.0352512001991272\n",
      "epoch: 9 step: 1034, loss is 0.0020016159396618605\n",
      "epoch: 9 step: 1035, loss is 0.0018028236227110028\n",
      "epoch: 9 step: 1036, loss is 0.0910198986530304\n",
      "epoch: 9 step: 1037, loss is 0.015442553907632828\n",
      "epoch: 9 step: 1038, loss is 0.08147721737623215\n",
      "epoch: 9 step: 1039, loss is 0.011222919449210167\n",
      "epoch: 9 step: 1040, loss is 0.09433683007955551\n",
      "epoch: 9 step: 1041, loss is 0.0038634049706161022\n",
      "epoch: 9 step: 1042, loss is 0.008446373045444489\n",
      "epoch: 9 step: 1043, loss is 0.003384434152394533\n",
      "epoch: 9 step: 1044, loss is 0.022310107946395874\n",
      "epoch: 9 step: 1045, loss is 0.2573987543582916\n",
      "epoch: 9 step: 1046, loss is 0.0016713887453079224\n",
      "epoch: 9 step: 1047, loss is 0.0110591109842062\n",
      "epoch: 9 step: 1048, loss is 0.006906968541443348\n",
      "epoch: 9 step: 1049, loss is 0.03737218305468559\n",
      "epoch: 9 step: 1050, loss is 0.0012678628554567695\n",
      "epoch: 9 step: 1051, loss is 0.05652548372745514\n",
      "epoch: 9 step: 1052, loss is 0.020971857011318207\n",
      "epoch: 9 step: 1053, loss is 0.00777210108935833\n",
      "epoch: 9 step: 1054, loss is 0.004471665248274803\n",
      "epoch: 9 step: 1055, loss is 0.006962700746953487\n",
      "epoch: 9 step: 1056, loss is 0.04048197343945503\n",
      "epoch: 9 step: 1057, loss is 0.028884531930088997\n",
      "epoch: 9 step: 1058, loss is 0.0007706995820626616\n",
      "epoch: 9 step: 1059, loss is 0.04736337065696716\n",
      "epoch: 9 step: 1060, loss is 0.001506802742369473\n",
      "epoch: 9 step: 1061, loss is 0.07768313586711884\n",
      "epoch: 9 step: 1062, loss is 0.002515515312552452\n",
      "epoch: 9 step: 1063, loss is 0.002170027233660221\n",
      "epoch: 9 step: 1064, loss is 0.011491655372083187\n",
      "epoch: 9 step: 1065, loss is 0.001045810291543603\n",
      "epoch: 9 step: 1066, loss is 0.03949521481990814\n",
      "epoch: 9 step: 1067, loss is 0.012536821886897087\n",
      "epoch: 9 step: 1068, loss is 0.07984927296638489\n",
      "epoch: 9 step: 1069, loss is 0.007263790350407362\n",
      "epoch: 9 step: 1070, loss is 0.0023959316313266754\n",
      "epoch: 9 step: 1071, loss is 0.0014343709917739034\n",
      "epoch: 9 step: 1072, loss is 0.07340045273303986\n",
      "epoch: 9 step: 1073, loss is 0.003325634403154254\n",
      "epoch: 9 step: 1074, loss is 0.021581394597887993\n",
      "epoch: 9 step: 1075, loss is 0.006661980412900448\n",
      "epoch: 9 step: 1076, loss is 0.034847527742385864\n",
      "epoch: 9 step: 1077, loss is 0.01934124529361725\n",
      "epoch: 9 step: 1078, loss is 0.07693242281675339\n",
      "epoch: 9 step: 1079, loss is 0.3284893035888672\n",
      "epoch: 9 step: 1080, loss is 0.04513879492878914\n",
      "epoch: 9 step: 1081, loss is 0.009033694863319397\n",
      "epoch: 9 step: 1082, loss is 0.0011109467595815659\n",
      "epoch: 9 step: 1083, loss is 0.4242580235004425\n",
      "epoch: 9 step: 1084, loss is 0.15103459358215332\n",
      "epoch: 9 step: 1085, loss is 0.10042580962181091\n",
      "epoch: 9 step: 1086, loss is 0.020985344424843788\n",
      "epoch: 9 step: 1087, loss is 0.013957826420664787\n",
      "epoch: 9 step: 1088, loss is 0.006648741662502289\n",
      "epoch: 9 step: 1089, loss is 0.004905446898192167\n",
      "epoch: 9 step: 1090, loss is 0.07817430794239044\n",
      "epoch: 9 step: 1091, loss is 0.16832590103149414\n",
      "epoch: 9 step: 1092, loss is 0.00035167185706086457\n",
      "epoch: 9 step: 1093, loss is 0.12474996596574783\n",
      "epoch: 9 step: 1094, loss is 0.001341038616374135\n",
      "epoch: 9 step: 1095, loss is 0.009543590247631073\n",
      "epoch: 9 step: 1096, loss is 0.0750294178724289\n",
      "epoch: 9 step: 1097, loss is 0.0006302774418145418\n",
      "epoch: 9 step: 1098, loss is 0.04265577718615532\n",
      "epoch: 9 step: 1099, loss is 0.04817632585763931\n",
      "epoch: 9 step: 1100, loss is 0.015863412991166115\n",
      "epoch: 9 step: 1101, loss is 0.002042075153440237\n",
      "epoch: 9 step: 1102, loss is 0.09861201792955399\n",
      "epoch: 9 step: 1103, loss is 0.011880534701049328\n",
      "epoch: 9 step: 1104, loss is 0.02086864598095417\n",
      "epoch: 9 step: 1105, loss is 0.24908645451068878\n",
      "epoch: 9 step: 1106, loss is 0.02827376499772072\n",
      "epoch: 9 step: 1107, loss is 0.005077647045254707\n",
      "epoch: 9 step: 1108, loss is 0.027114281430840492\n",
      "epoch: 9 step: 1109, loss is 0.009307913482189178\n",
      "epoch: 9 step: 1110, loss is 0.1783159375190735\n",
      "epoch: 9 step: 1111, loss is 0.1289299726486206\n",
      "epoch: 9 step: 1112, loss is 0.012115735560655594\n",
      "epoch: 9 step: 1113, loss is 0.003822008380666375\n",
      "epoch: 9 step: 1114, loss is 0.09493494778871536\n",
      "epoch: 9 step: 1115, loss is 0.005907540675252676\n",
      "epoch: 9 step: 1116, loss is 0.0008246979559771717\n",
      "epoch: 9 step: 1117, loss is 0.0018583830678835511\n",
      "epoch: 9 step: 1118, loss is 0.1945093870162964\n",
      "epoch: 9 step: 1119, loss is 0.2995647192001343\n",
      "epoch: 9 step: 1120, loss is 0.07227128744125366\n",
      "epoch: 9 step: 1121, loss is 0.007391782011836767\n",
      "epoch: 9 step: 1122, loss is 0.03385940194129944\n",
      "epoch: 9 step: 1123, loss is 0.10061824321746826\n",
      "epoch: 9 step: 1124, loss is 0.013411246240139008\n",
      "epoch: 9 step: 1125, loss is 0.10545556992292404\n",
      "epoch: 9 step: 1126, loss is 0.010730276815593243\n",
      "epoch: 9 step: 1127, loss is 0.010451563633978367\n",
      "epoch: 9 step: 1128, loss is 0.0015877550467848778\n",
      "epoch: 9 step: 1129, loss is 0.010810988023877144\n",
      "epoch: 9 step: 1130, loss is 0.016917075961828232\n",
      "epoch: 9 step: 1131, loss is 0.10557439178228378\n",
      "epoch: 9 step: 1132, loss is 0.17022685706615448\n",
      "epoch: 9 step: 1133, loss is 0.0005560826975852251\n",
      "epoch: 9 step: 1134, loss is 0.008071796968579292\n",
      "epoch: 9 step: 1135, loss is 0.010477369651198387\n",
      "epoch: 9 step: 1136, loss is 0.008004022762179375\n",
      "epoch: 9 step: 1137, loss is 0.12421834468841553\n",
      "epoch: 9 step: 1138, loss is 0.05252600088715553\n",
      "epoch: 9 step: 1139, loss is 0.00751239899545908\n",
      "epoch: 9 step: 1140, loss is 0.032164834439754486\n",
      "epoch: 9 step: 1141, loss is 0.001258191536180675\n",
      "epoch: 9 step: 1142, loss is 0.03620746731758118\n",
      "epoch: 9 step: 1143, loss is 0.0011082170531153679\n",
      "epoch: 9 step: 1144, loss is 0.001578137744218111\n",
      "epoch: 9 step: 1145, loss is 0.21865810453891754\n",
      "epoch: 9 step: 1146, loss is 0.006196006201207638\n",
      "epoch: 9 step: 1147, loss is 0.011055168695747852\n",
      "epoch: 9 step: 1148, loss is 0.05681638419628143\n",
      "epoch: 9 step: 1149, loss is 0.0042691780254244804\n",
      "epoch: 9 step: 1150, loss is 0.12587207555770874\n",
      "epoch: 9 step: 1151, loss is 0.001650744117796421\n",
      "epoch: 9 step: 1152, loss is 0.0022988058626651764\n",
      "epoch: 9 step: 1153, loss is 0.003962972667068243\n",
      "epoch: 9 step: 1154, loss is 0.003873276524245739\n",
      "epoch: 9 step: 1155, loss is 0.06697331368923187\n",
      "epoch: 9 step: 1156, loss is 0.2747103273868561\n",
      "epoch: 9 step: 1157, loss is 0.007223344407975674\n",
      "epoch: 9 step: 1158, loss is 5.684439747710712e-05\n",
      "epoch: 9 step: 1159, loss is 0.008606882765889168\n",
      "epoch: 9 step: 1160, loss is 0.02866460010409355\n",
      "epoch: 9 step: 1161, loss is 0.12975426018238068\n",
      "epoch: 9 step: 1162, loss is 0.16087667644023895\n",
      "epoch: 9 step: 1163, loss is 0.008488187566399574\n",
      "epoch: 9 step: 1164, loss is 0.023995475843548775\n",
      "epoch: 9 step: 1165, loss is 0.03353871777653694\n",
      "epoch: 9 step: 1166, loss is 0.010452553629875183\n",
      "epoch: 9 step: 1167, loss is 0.004317198880016804\n",
      "epoch: 9 step: 1168, loss is 0.0007420062320306897\n",
      "epoch: 9 step: 1169, loss is 0.00019395585695747286\n",
      "epoch: 9 step: 1170, loss is 0.04059481620788574\n",
      "epoch: 9 step: 1171, loss is 0.0007543727988377213\n",
      "epoch: 9 step: 1172, loss is 0.017316654324531555\n",
      "epoch: 9 step: 1173, loss is 0.028846517205238342\n",
      "epoch: 9 step: 1174, loss is 0.0548817440867424\n",
      "epoch: 9 step: 1175, loss is 0.0007667087484151125\n",
      "epoch: 9 step: 1176, loss is 0.018942292779684067\n",
      "epoch: 9 step: 1177, loss is 0.10677780956029892\n",
      "epoch: 9 step: 1178, loss is 0.012280485592782497\n",
      "epoch: 9 step: 1179, loss is 0.006330134812742472\n",
      "epoch: 9 step: 1180, loss is 0.011151690967381\n",
      "epoch: 9 step: 1181, loss is 0.0058174398727715015\n",
      "epoch: 9 step: 1182, loss is 0.04438401386141777\n",
      "epoch: 9 step: 1183, loss is 0.028218736872076988\n",
      "epoch: 9 step: 1184, loss is 0.01546403020620346\n",
      "epoch: 9 step: 1185, loss is 0.16892372071743011\n",
      "epoch: 9 step: 1186, loss is 0.014813407324254513\n",
      "epoch: 9 step: 1187, loss is 0.038602039217948914\n",
      "epoch: 9 step: 1188, loss is 0.03530176728963852\n",
      "epoch: 9 step: 1189, loss is 0.05252436548471451\n",
      "epoch: 9 step: 1190, loss is 0.05386320874094963\n",
      "epoch: 9 step: 1191, loss is 0.002370410831645131\n",
      "epoch: 9 step: 1192, loss is 0.007681812159717083\n",
      "epoch: 9 step: 1193, loss is 0.0021847004536539316\n",
      "epoch: 9 step: 1194, loss is 0.04199640825390816\n",
      "epoch: 9 step: 1195, loss is 0.04683433473110199\n",
      "epoch: 9 step: 1196, loss is 0.00926140882074833\n",
      "epoch: 9 step: 1197, loss is 0.018832476809620857\n",
      "epoch: 9 step: 1198, loss is 0.09012056142091751\n",
      "epoch: 9 step: 1199, loss is 0.009265026077628136\n",
      "epoch: 9 step: 1200, loss is 0.020149217918515205\n",
      "epoch: 9 step: 1201, loss is 0.0008354239398613572\n",
      "epoch: 9 step: 1202, loss is 0.05014387145638466\n",
      "epoch: 9 step: 1203, loss is 0.002072704955935478\n",
      "epoch: 9 step: 1204, loss is 0.0038130474276840687\n",
      "epoch: 9 step: 1205, loss is 0.02127116359770298\n",
      "epoch: 9 step: 1206, loss is 0.019444124773144722\n",
      "epoch: 9 step: 1207, loss is 0.005912179592996836\n",
      "epoch: 9 step: 1208, loss is 0.0012154141440987587\n",
      "epoch: 9 step: 1209, loss is 0.0013244790025055408\n",
      "epoch: 9 step: 1210, loss is 0.025579091161489487\n",
      "epoch: 9 step: 1211, loss is 0.1535198837518692\n",
      "epoch: 9 step: 1212, loss is 0.003151267534121871\n",
      "epoch: 9 step: 1213, loss is 0.07592655718326569\n",
      "epoch: 9 step: 1214, loss is 0.030438896268606186\n",
      "epoch: 9 step: 1215, loss is 0.003995542414486408\n",
      "epoch: 9 step: 1216, loss is 0.0022374268155544996\n",
      "epoch: 9 step: 1217, loss is 0.1020636036992073\n",
      "epoch: 9 step: 1218, loss is 0.010194385424256325\n",
      "epoch: 9 step: 1219, loss is 0.0931922122836113\n",
      "epoch: 9 step: 1220, loss is 0.052246563136577606\n",
      "epoch: 9 step: 1221, loss is 0.018337035551667213\n",
      "epoch: 9 step: 1222, loss is 0.0022517552133649588\n",
      "epoch: 9 step: 1223, loss is 0.0006564997020177543\n",
      "epoch: 9 step: 1224, loss is 0.01995481178164482\n",
      "epoch: 9 step: 1225, loss is 0.028859781101346016\n",
      "epoch: 9 step: 1226, loss is 0.014055624604225159\n",
      "epoch: 9 step: 1227, loss is 0.10040757805109024\n",
      "epoch: 9 step: 1228, loss is 0.056589264422655106\n",
      "epoch: 9 step: 1229, loss is 0.015160169452428818\n",
      "epoch: 9 step: 1230, loss is 0.000193412575754337\n",
      "epoch: 9 step: 1231, loss is 0.03602131828665733\n",
      "epoch: 9 step: 1232, loss is 0.02093847468495369\n",
      "epoch: 9 step: 1233, loss is 0.0371108241379261\n",
      "epoch: 9 step: 1234, loss is 0.036833420395851135\n",
      "epoch: 9 step: 1235, loss is 0.001468712231144309\n",
      "epoch: 9 step: 1236, loss is 0.0007041365606710315\n",
      "epoch: 9 step: 1237, loss is 0.012350094504654408\n",
      "epoch: 9 step: 1238, loss is 0.13908584415912628\n",
      "epoch: 9 step: 1239, loss is 0.16465060412883759\n",
      "epoch: 9 step: 1240, loss is 0.001307669677771628\n",
      "epoch: 9 step: 1241, loss is 0.007474375888705254\n",
      "epoch: 9 step: 1242, loss is 0.014197883196175098\n",
      "epoch: 9 step: 1243, loss is 0.0009287695866078138\n",
      "epoch: 9 step: 1244, loss is 0.11107862740755081\n",
      "epoch: 9 step: 1245, loss is 0.0012544038472697139\n",
      "epoch: 9 step: 1246, loss is 0.026776092126965523\n",
      "epoch: 9 step: 1247, loss is 0.029363946989178658\n",
      "epoch: 9 step: 1248, loss is 0.03814343735575676\n",
      "epoch: 9 step: 1249, loss is 0.021962370723485947\n",
      "epoch: 9 step: 1250, loss is 0.016070768237113953\n",
      "epoch: 9 step: 1251, loss is 0.0003500250168144703\n",
      "epoch: 9 step: 1252, loss is 0.0013017298188060522\n",
      "epoch: 9 step: 1253, loss is 0.013123842887580395\n",
      "epoch: 9 step: 1254, loss is 0.0001769952941685915\n",
      "epoch: 9 step: 1255, loss is 0.07976747304201126\n",
      "epoch: 9 step: 1256, loss is 0.009388796985149384\n",
      "epoch: 9 step: 1257, loss is 0.0013160828966647387\n",
      "epoch: 9 step: 1258, loss is 0.0002387996792094782\n",
      "epoch: 9 step: 1259, loss is 0.08654943108558655\n",
      "epoch: 9 step: 1260, loss is 0.0038755363784730434\n",
      "epoch: 9 step: 1261, loss is 0.01885656639933586\n",
      "epoch: 9 step: 1262, loss is 0.0036983005702495575\n",
      "epoch: 9 step: 1263, loss is 0.013323447667062283\n",
      "epoch: 9 step: 1264, loss is 0.04390733689069748\n",
      "epoch: 9 step: 1265, loss is 0.04659838229417801\n",
      "epoch: 9 step: 1266, loss is 0.13555768132209778\n",
      "epoch: 9 step: 1267, loss is 0.016223859041929245\n",
      "epoch: 9 step: 1268, loss is 0.04563780128955841\n",
      "epoch: 9 step: 1269, loss is 0.014800592325627804\n",
      "epoch: 9 step: 1270, loss is 0.008373865857720375\n",
      "epoch: 9 step: 1271, loss is 0.07770023494958878\n",
      "epoch: 9 step: 1272, loss is 0.0018014467786997557\n",
      "epoch: 9 step: 1273, loss is 0.0010772091336548328\n",
      "epoch: 9 step: 1274, loss is 0.00872836820781231\n",
      "epoch: 9 step: 1275, loss is 0.019182296469807625\n",
      "epoch: 9 step: 1276, loss is 0.003435499034821987\n",
      "epoch: 9 step: 1277, loss is 0.14884965121746063\n",
      "epoch: 9 step: 1278, loss is 0.0023343763314187527\n",
      "epoch: 9 step: 1279, loss is 0.005684021860361099\n",
      "epoch: 9 step: 1280, loss is 0.0005844084662385285\n",
      "epoch: 9 step: 1281, loss is 0.032277584075927734\n",
      "epoch: 9 step: 1282, loss is 0.009797362610697746\n",
      "epoch: 9 step: 1283, loss is 0.0885908380150795\n",
      "epoch: 9 step: 1284, loss is 0.10061058402061462\n",
      "epoch: 9 step: 1285, loss is 4.613543569575995e-05\n",
      "epoch: 9 step: 1286, loss is 0.009195615537464619\n",
      "epoch: 9 step: 1287, loss is 0.0010459315963089466\n",
      "epoch: 9 step: 1288, loss is 0.06398623436689377\n",
      "epoch: 9 step: 1289, loss is 0.08681020885705948\n",
      "epoch: 9 step: 1290, loss is 0.011016803793609142\n",
      "epoch: 9 step: 1291, loss is 0.08189089596271515\n",
      "epoch: 9 step: 1292, loss is 0.012992833741009235\n",
      "epoch: 9 step: 1293, loss is 0.06308792531490326\n",
      "epoch: 9 step: 1294, loss is 0.01879177987575531\n",
      "epoch: 9 step: 1295, loss is 0.000747841433621943\n",
      "epoch: 9 step: 1296, loss is 0.0030409477185457945\n",
      "epoch: 9 step: 1297, loss is 0.17597293853759766\n",
      "epoch: 9 step: 1298, loss is 0.0018242046935483813\n",
      "epoch: 9 step: 1299, loss is 0.10767395794391632\n",
      "epoch: 9 step: 1300, loss is 0.13052675127983093\n",
      "epoch: 9 step: 1301, loss is 0.0003404917661100626\n",
      "epoch: 9 step: 1302, loss is 0.018519945442676544\n",
      "epoch: 9 step: 1303, loss is 0.0018667388940230012\n",
      "epoch: 9 step: 1304, loss is 0.0018797003431245685\n",
      "epoch: 9 step: 1305, loss is 0.031901247799396515\n",
      "epoch: 9 step: 1306, loss is 0.09066697955131531\n",
      "epoch: 9 step: 1307, loss is 0.027653023600578308\n",
      "epoch: 9 step: 1308, loss is 0.06879853457212448\n",
      "epoch: 9 step: 1309, loss is 0.041101012378931046\n",
      "epoch: 9 step: 1310, loss is 0.02720252051949501\n",
      "epoch: 9 step: 1311, loss is 0.13768790662288666\n",
      "epoch: 9 step: 1312, loss is 0.0016075188759714365\n",
      "epoch: 9 step: 1313, loss is 0.02750353515148163\n",
      "epoch: 9 step: 1314, loss is 0.11514824628829956\n",
      "epoch: 9 step: 1315, loss is 0.1280905306339264\n",
      "epoch: 9 step: 1316, loss is 0.0001122603498515673\n",
      "epoch: 9 step: 1317, loss is 0.006755242124199867\n",
      "epoch: 9 step: 1318, loss is 0.00399845652282238\n",
      "epoch: 9 step: 1319, loss is 0.0004981954698450863\n",
      "epoch: 9 step: 1320, loss is 0.14312858879566193\n",
      "epoch: 9 step: 1321, loss is 0.05982457101345062\n",
      "epoch: 9 step: 1322, loss is 0.0055003114975988865\n",
      "epoch: 9 step: 1323, loss is 0.051396749913692474\n",
      "epoch: 9 step: 1324, loss is 0.0016918365145102143\n",
      "epoch: 9 step: 1325, loss is 0.10480812191963196\n",
      "epoch: 9 step: 1326, loss is 0.01793241687119007\n",
      "epoch: 9 step: 1327, loss is 0.07252530008554459\n",
      "epoch: 9 step: 1328, loss is 0.001953566912561655\n",
      "epoch: 9 step: 1329, loss is 0.2113768607378006\n",
      "epoch: 9 step: 1330, loss is 0.0383404903113842\n",
      "epoch: 9 step: 1331, loss is 0.0379929356276989\n",
      "epoch: 9 step: 1332, loss is 0.013634506613016129\n",
      "epoch: 9 step: 1333, loss is 0.0015945527702569962\n",
      "epoch: 9 step: 1334, loss is 0.15407945215702057\n",
      "epoch: 9 step: 1335, loss is 0.000642401457298547\n",
      "epoch: 9 step: 1336, loss is 0.027005773037672043\n",
      "epoch: 9 step: 1337, loss is 0.001546841231174767\n",
      "epoch: 9 step: 1338, loss is 0.0016376377316191792\n",
      "epoch: 9 step: 1339, loss is 0.030140720307826996\n",
      "epoch: 9 step: 1340, loss is 0.037045713514089584\n",
      "epoch: 9 step: 1341, loss is 0.0039777630008757114\n",
      "epoch: 9 step: 1342, loss is 0.020168103277683258\n",
      "epoch: 9 step: 1343, loss is 0.004002824425697327\n",
      "epoch: 9 step: 1344, loss is 0.11032469570636749\n",
      "epoch: 9 step: 1345, loss is 0.0318497009575367\n",
      "epoch: 9 step: 1346, loss is 0.0013305624015629292\n",
      "epoch: 9 step: 1347, loss is 0.17625488340854645\n",
      "epoch: 9 step: 1348, loss is 0.049035049974918365\n",
      "epoch: 9 step: 1349, loss is 0.00327332504093647\n",
      "epoch: 9 step: 1350, loss is 0.21207565069198608\n",
      "epoch: 9 step: 1351, loss is 0.004341369029134512\n",
      "epoch: 9 step: 1352, loss is 0.0013670891057699919\n",
      "epoch: 9 step: 1353, loss is 0.06238435208797455\n",
      "epoch: 9 step: 1354, loss is 0.07131949812173843\n",
      "epoch: 9 step: 1355, loss is 0.054948899894952774\n",
      "epoch: 9 step: 1356, loss is 0.054338451474905014\n",
      "epoch: 9 step: 1357, loss is 0.009478417225182056\n",
      "epoch: 9 step: 1358, loss is 0.22312979400157928\n",
      "epoch: 9 step: 1359, loss is 0.013480599038302898\n",
      "epoch: 9 step: 1360, loss is 0.01768764853477478\n",
      "epoch: 9 step: 1361, loss is 0.049341246485710144\n",
      "epoch: 9 step: 1362, loss is 0.00031601745286025107\n",
      "epoch: 9 step: 1363, loss is 0.05180220305919647\n",
      "epoch: 9 step: 1364, loss is 0.008942774496972561\n",
      "epoch: 9 step: 1365, loss is 0.003587910672649741\n",
      "epoch: 9 step: 1366, loss is 0.005189553368836641\n",
      "epoch: 9 step: 1367, loss is 0.0016943763475865126\n",
      "epoch: 9 step: 1368, loss is 0.046420514583587646\n",
      "epoch: 9 step: 1369, loss is 0.019125375896692276\n",
      "epoch: 9 step: 1370, loss is 0.0012577325105667114\n",
      "epoch: 9 step: 1371, loss is 0.048509787768125534\n",
      "epoch: 9 step: 1372, loss is 0.02466997690498829\n",
      "epoch: 9 step: 1373, loss is 0.0002742692013271153\n",
      "epoch: 9 step: 1374, loss is 0.005351501051336527\n",
      "epoch: 9 step: 1375, loss is 0.006918727420270443\n",
      "epoch: 9 step: 1376, loss is 0.0004233091603964567\n",
      "epoch: 9 step: 1377, loss is 7.567710417788476e-05\n",
      "epoch: 9 step: 1378, loss is 0.01840360276401043\n",
      "epoch: 9 step: 1379, loss is 0.0011520315892994404\n",
      "epoch: 9 step: 1380, loss is 0.0009793515782803297\n",
      "epoch: 9 step: 1381, loss is 0.005321387201547623\n",
      "epoch: 9 step: 1382, loss is 0.0058830976486206055\n",
      "epoch: 9 step: 1383, loss is 0.01816522143781185\n",
      "epoch: 9 step: 1384, loss is 0.0019504789961501956\n",
      "epoch: 9 step: 1385, loss is 0.04686443880200386\n",
      "epoch: 9 step: 1386, loss is 0.00036135330446995795\n",
      "epoch: 9 step: 1387, loss is 0.00022466426889877766\n",
      "epoch: 9 step: 1388, loss is 0.0010249423794448376\n",
      "epoch: 9 step: 1389, loss is 0.022390216588974\n",
      "epoch: 9 step: 1390, loss is 0.00037340595736168325\n",
      "epoch: 9 step: 1391, loss is 0.0001314417750108987\n",
      "epoch: 9 step: 1392, loss is 0.017278706654906273\n",
      "epoch: 9 step: 1393, loss is 0.08682885766029358\n",
      "epoch: 9 step: 1394, loss is 0.013315427117049694\n",
      "epoch: 9 step: 1395, loss is 0.006438869051635265\n",
      "epoch: 9 step: 1396, loss is 0.0004440726770553738\n",
      "epoch: 9 step: 1397, loss is 0.002104497281834483\n",
      "epoch: 9 step: 1398, loss is 0.0009126802906394005\n",
      "epoch: 9 step: 1399, loss is 0.03099602647125721\n",
      "epoch: 9 step: 1400, loss is 0.043866246938705444\n",
      "epoch: 9 step: 1401, loss is 0.08558574318885803\n",
      "epoch: 9 step: 1402, loss is 0.05032658949494362\n",
      "epoch: 9 step: 1403, loss is 0.09323630481958389\n",
      "epoch: 9 step: 1404, loss is 0.003590230830013752\n",
      "epoch: 9 step: 1405, loss is 0.08116106688976288\n",
      "epoch: 9 step: 1406, loss is 0.007722494192421436\n",
      "epoch: 9 step: 1407, loss is 0.009432355873286724\n",
      "epoch: 9 step: 1408, loss is 0.017910443246364594\n",
      "epoch: 9 step: 1409, loss is 0.005412832368165255\n",
      "epoch: 9 step: 1410, loss is 0.029004113748669624\n",
      "epoch: 9 step: 1411, loss is 0.06333507597446442\n",
      "epoch: 9 step: 1412, loss is 0.00293365609832108\n",
      "epoch: 9 step: 1413, loss is 0.017958158627152443\n",
      "epoch: 9 step: 1414, loss is 0.005374033469706774\n",
      "epoch: 9 step: 1415, loss is 0.09030874073505402\n",
      "epoch: 9 step: 1416, loss is 0.010970463044941425\n",
      "epoch: 9 step: 1417, loss is 0.0007946707773953676\n",
      "epoch: 9 step: 1418, loss is 0.16733935475349426\n",
      "epoch: 9 step: 1419, loss is 0.02807500772178173\n",
      "epoch: 9 step: 1420, loss is 0.002833403181284666\n",
      "epoch: 9 step: 1421, loss is 0.042931850999593735\n",
      "epoch: 9 step: 1422, loss is 0.08222714066505432\n",
      "epoch: 9 step: 1423, loss is 0.004639758728444576\n",
      "epoch: 9 step: 1424, loss is 0.004214477259665728\n",
      "epoch: 9 step: 1425, loss is 0.00911659561097622\n",
      "epoch: 9 step: 1426, loss is 0.11885624378919601\n",
      "epoch: 9 step: 1427, loss is 0.21949900686740875\n",
      "epoch: 9 step: 1428, loss is 0.0038033309392631054\n",
      "epoch: 9 step: 1429, loss is 0.007729702163487673\n",
      "epoch: 9 step: 1430, loss is 0.001419591368176043\n",
      "epoch: 9 step: 1431, loss is 0.016467474400997162\n",
      "epoch: 9 step: 1432, loss is 0.03467574715614319\n",
      "epoch: 9 step: 1433, loss is 0.005094171967357397\n",
      "epoch: 9 step: 1434, loss is 0.0009972862899303436\n",
      "epoch: 9 step: 1435, loss is 0.009617089293897152\n",
      "epoch: 9 step: 1436, loss is 0.06281306594610214\n",
      "epoch: 9 step: 1437, loss is 0.04640587791800499\n",
      "epoch: 9 step: 1438, loss is 0.002741649281233549\n",
      "epoch: 9 step: 1439, loss is 0.021753815934062004\n",
      "epoch: 9 step: 1440, loss is 0.024612270295619965\n",
      "epoch: 9 step: 1441, loss is 0.005852867849171162\n",
      "epoch: 9 step: 1442, loss is 0.005946725141257048\n",
      "epoch: 9 step: 1443, loss is 0.007291330955922604\n",
      "epoch: 9 step: 1444, loss is 0.0006162750069051981\n",
      "epoch: 9 step: 1445, loss is 0.032180432230234146\n",
      "epoch: 9 step: 1446, loss is 0.008081666193902493\n",
      "epoch: 9 step: 1447, loss is 0.01589697040617466\n",
      "epoch: 9 step: 1448, loss is 0.015801431611180305\n",
      "epoch: 9 step: 1449, loss is 0.018283525481820107\n",
      "epoch: 9 step: 1450, loss is 0.024044545367360115\n",
      "epoch: 9 step: 1451, loss is 0.009448480792343616\n",
      "epoch: 9 step: 1452, loss is 0.07590027153491974\n",
      "epoch: 9 step: 1453, loss is 0.03228381648659706\n",
      "epoch: 9 step: 1454, loss is 0.023144608363509178\n",
      "epoch: 9 step: 1455, loss is 0.09264808893203735\n",
      "epoch: 9 step: 1456, loss is 0.03809559717774391\n",
      "epoch: 9 step: 1457, loss is 0.003039710223674774\n",
      "epoch: 9 step: 1458, loss is 0.1411840170621872\n",
      "epoch: 9 step: 1459, loss is 0.011673282831907272\n",
      "epoch: 9 step: 1460, loss is 0.004185100086033344\n",
      "epoch: 9 step: 1461, loss is 0.004797116853296757\n",
      "epoch: 9 step: 1462, loss is 0.002356387674808502\n",
      "epoch: 9 step: 1463, loss is 0.0019129374995827675\n",
      "epoch: 9 step: 1464, loss is 0.00024291394220199436\n",
      "epoch: 9 step: 1465, loss is 0.07516012340784073\n",
      "epoch: 9 step: 1466, loss is 0.003876742674037814\n",
      "epoch: 9 step: 1467, loss is 0.005196970421820879\n",
      "epoch: 9 step: 1468, loss is 0.0017027952708303928\n",
      "epoch: 9 step: 1469, loss is 0.008871148340404034\n",
      "epoch: 9 step: 1470, loss is 0.003219490870833397\n",
      "epoch: 9 step: 1471, loss is 0.004156072624027729\n",
      "epoch: 9 step: 1472, loss is 0.000381291814846918\n",
      "epoch: 9 step: 1473, loss is 0.0293793473392725\n",
      "epoch: 9 step: 1474, loss is 0.01683122105896473\n",
      "epoch: 9 step: 1475, loss is 0.04614141210913658\n",
      "epoch: 9 step: 1476, loss is 0.06894546002149582\n",
      "epoch: 9 step: 1477, loss is 0.0004638776299543679\n",
      "epoch: 9 step: 1478, loss is 0.0011945399455726147\n",
      "epoch: 9 step: 1479, loss is 5.78468716412317e-05\n",
      "epoch: 9 step: 1480, loss is 0.02789473719894886\n",
      "epoch: 9 step: 1481, loss is 0.002079692203551531\n",
      "epoch: 9 step: 1482, loss is 0.000272268895059824\n",
      "epoch: 9 step: 1483, loss is 0.003434457117691636\n",
      "epoch: 9 step: 1484, loss is 0.02888576313853264\n",
      "epoch: 9 step: 1485, loss is 0.008984250016510487\n",
      "epoch: 9 step: 1486, loss is 0.04208468273282051\n",
      "epoch: 9 step: 1487, loss is 0.0011656159767881036\n",
      "epoch: 9 step: 1488, loss is 0.002014511963352561\n",
      "epoch: 9 step: 1489, loss is 0.0034753880463540554\n",
      "epoch: 9 step: 1490, loss is 0.018768342211842537\n",
      "epoch: 9 step: 1491, loss is 0.006702392362058163\n",
      "epoch: 9 step: 1492, loss is 0.019112378358840942\n",
      "epoch: 9 step: 1493, loss is 0.005582163576036692\n",
      "epoch: 9 step: 1494, loss is 0.009719738736748695\n",
      "epoch: 9 step: 1495, loss is 0.002098820637911558\n",
      "epoch: 9 step: 1496, loss is 0.017587732523679733\n",
      "epoch: 9 step: 1497, loss is 0.052936192601919174\n",
      "epoch: 9 step: 1498, loss is 0.13335180282592773\n",
      "epoch: 9 step: 1499, loss is 0.006841435097157955\n",
      "epoch: 9 step: 1500, loss is 0.05492422357201576\n",
      "epoch: 9 step: 1501, loss is 0.0070662591606378555\n",
      "epoch: 9 step: 1502, loss is 0.0012733187759295106\n",
      "epoch: 9 step: 1503, loss is 0.0007014003931544721\n",
      "epoch: 9 step: 1504, loss is 0.11302220821380615\n",
      "epoch: 9 step: 1505, loss is 0.09602948278188705\n",
      "epoch: 9 step: 1506, loss is 0.0008306563249789178\n",
      "epoch: 9 step: 1507, loss is 0.002621181309223175\n",
      "epoch: 9 step: 1508, loss is 0.01727665401995182\n",
      "epoch: 9 step: 1509, loss is 0.017507702112197876\n",
      "epoch: 9 step: 1510, loss is 0.002032174728810787\n",
      "epoch: 9 step: 1511, loss is 0.004551063757389784\n",
      "epoch: 9 step: 1512, loss is 0.03305021673440933\n",
      "epoch: 9 step: 1513, loss is 0.05932336673140526\n",
      "epoch: 9 step: 1514, loss is 0.0052672033198177814\n",
      "epoch: 9 step: 1515, loss is 0.004505457356572151\n",
      "epoch: 9 step: 1516, loss is 0.0034901329781860113\n",
      "epoch: 9 step: 1517, loss is 0.012958850711584091\n",
      "epoch: 9 step: 1518, loss is 0.00949813611805439\n",
      "epoch: 9 step: 1519, loss is 0.0446760468184948\n",
      "epoch: 9 step: 1520, loss is 0.3038574159145355\n",
      "epoch: 9 step: 1521, loss is 0.0039525385946035385\n",
      "epoch: 9 step: 1522, loss is 0.0032062993850558996\n",
      "epoch: 9 step: 1523, loss is 0.017494376748800278\n",
      "epoch: 9 step: 1524, loss is 0.010291016660630703\n",
      "epoch: 9 step: 1525, loss is 0.0003971869882661849\n",
      "epoch: 9 step: 1526, loss is 0.004803739488124847\n",
      "epoch: 9 step: 1527, loss is 0.09362229704856873\n",
      "epoch: 9 step: 1528, loss is 8.337059989571571e-05\n",
      "epoch: 9 step: 1529, loss is 0.054018303751945496\n",
      "epoch: 9 step: 1530, loss is 0.0024563686456531286\n",
      "epoch: 9 step: 1531, loss is 0.10980536043643951\n",
      "epoch: 9 step: 1532, loss is 0.040185678750276566\n",
      "epoch: 9 step: 1533, loss is 0.00417554285377264\n",
      "epoch: 9 step: 1534, loss is 0.002101984340697527\n",
      "epoch: 9 step: 1535, loss is 0.019341155886650085\n",
      "epoch: 9 step: 1536, loss is 0.021479928866028786\n",
      "epoch: 9 step: 1537, loss is 0.0007103921379894018\n",
      "epoch: 9 step: 1538, loss is 0.004615524783730507\n",
      "epoch: 9 step: 1539, loss is 0.09557125717401505\n",
      "epoch: 9 step: 1540, loss is 0.03214229643344879\n",
      "epoch: 9 step: 1541, loss is 0.008149076253175735\n",
      "epoch: 9 step: 1542, loss is 0.18205362558364868\n",
      "epoch: 9 step: 1543, loss is 0.00045863931882195175\n",
      "epoch: 9 step: 1544, loss is 0.014349423348903656\n",
      "epoch: 9 step: 1545, loss is 0.13536809384822845\n",
      "epoch: 9 step: 1546, loss is 0.008800444193184376\n",
      "epoch: 9 step: 1547, loss is 0.002279244363307953\n",
      "epoch: 9 step: 1548, loss is 0.038551099598407745\n",
      "epoch: 9 step: 1549, loss is 0.11540233343839645\n",
      "epoch: 9 step: 1550, loss is 0.03416308015584946\n",
      "epoch: 9 step: 1551, loss is 0.03146953508257866\n",
      "epoch: 9 step: 1552, loss is 0.04454822838306427\n",
      "epoch: 9 step: 1553, loss is 0.10963607579469681\n",
      "epoch: 9 step: 1554, loss is 0.016812987625598907\n",
      "epoch: 9 step: 1555, loss is 0.008420308120548725\n",
      "epoch: 9 step: 1556, loss is 0.0012734256451949477\n",
      "epoch: 9 step: 1557, loss is 0.023329878225922585\n",
      "epoch: 9 step: 1558, loss is 0.15596246719360352\n",
      "epoch: 9 step: 1559, loss is 0.000519332941621542\n",
      "epoch: 9 step: 1560, loss is 0.022163784131407738\n",
      "epoch: 9 step: 1561, loss is 0.005230753682553768\n",
      "epoch: 9 step: 1562, loss is 0.04007119685411453\n",
      "epoch: 9 step: 1563, loss is 0.002417637500911951\n",
      "epoch: 9 step: 1564, loss is 0.0008102273568511009\n",
      "epoch: 9 step: 1565, loss is 0.04362165182828903\n",
      "epoch: 9 step: 1566, loss is 0.017612729221582413\n",
      "epoch: 9 step: 1567, loss is 0.06392942368984222\n",
      "epoch: 9 step: 1568, loss is 0.0164539385586977\n",
      "epoch: 9 step: 1569, loss is 0.09134474396705627\n",
      "epoch: 9 step: 1570, loss is 0.20179390907287598\n",
      "epoch: 9 step: 1571, loss is 0.007022298406809568\n",
      "epoch: 9 step: 1572, loss is 0.0037570081185549498\n",
      "epoch: 9 step: 1573, loss is 0.040420737117528915\n",
      "epoch: 9 step: 1574, loss is 0.04307049885392189\n",
      "epoch: 9 step: 1575, loss is 0.0012192833237349987\n",
      "epoch: 9 step: 1576, loss is 0.0013290175702422857\n",
      "epoch: 9 step: 1577, loss is 0.010642139241099358\n",
      "epoch: 9 step: 1578, loss is 0.053412288427352905\n",
      "epoch: 9 step: 1579, loss is 0.023359088227152824\n",
      "epoch: 9 step: 1580, loss is 0.10654422640800476\n",
      "epoch: 9 step: 1581, loss is 0.19270052015781403\n",
      "epoch: 9 step: 1582, loss is 0.011902793310582638\n",
      "epoch: 9 step: 1583, loss is 0.010838953778147697\n",
      "epoch: 9 step: 1584, loss is 0.017394304275512695\n",
      "epoch: 9 step: 1585, loss is 0.0022358486894518137\n",
      "epoch: 9 step: 1586, loss is 0.019798152148723602\n",
      "epoch: 9 step: 1587, loss is 0.030813295394182205\n",
      "epoch: 9 step: 1588, loss is 0.00023593756486661732\n",
      "epoch: 9 step: 1589, loss is 0.04212332144379616\n",
      "epoch: 9 step: 1590, loss is 0.011373135261237621\n",
      "epoch: 9 step: 1591, loss is 0.032950595021247864\n",
      "epoch: 9 step: 1592, loss is 0.011341805569827557\n",
      "epoch: 9 step: 1593, loss is 0.0024351798929274082\n",
      "epoch: 9 step: 1594, loss is 0.004868605174124241\n",
      "epoch: 9 step: 1595, loss is 0.0043795849196612835\n",
      "epoch: 9 step: 1596, loss is 0.016916727647185326\n",
      "epoch: 9 step: 1597, loss is 0.014505187049508095\n",
      "epoch: 9 step: 1598, loss is 0.04474832862615585\n",
      "epoch: 9 step: 1599, loss is 0.027051739394664764\n",
      "epoch: 9 step: 1600, loss is 0.012221152894198895\n",
      "epoch: 9 step: 1601, loss is 0.017835095524787903\n",
      "epoch: 9 step: 1602, loss is 0.02804393880069256\n",
      "epoch: 9 step: 1603, loss is 0.0037923105992376804\n",
      "epoch: 9 step: 1604, loss is 0.0016900197369977832\n",
      "epoch: 9 step: 1605, loss is 0.11264979094266891\n",
      "epoch: 9 step: 1606, loss is 0.004718023352324963\n",
      "epoch: 9 step: 1607, loss is 0.05526183173060417\n",
      "epoch: 9 step: 1608, loss is 0.07320981472730637\n",
      "epoch: 9 step: 1609, loss is 0.005014658905565739\n",
      "epoch: 9 step: 1610, loss is 0.007851414382457733\n",
      "epoch: 9 step: 1611, loss is 0.0097593292593956\n",
      "epoch: 9 step: 1612, loss is 0.023795535787940025\n",
      "epoch: 9 step: 1613, loss is 0.0062268516048789024\n",
      "epoch: 9 step: 1614, loss is 0.05949370190501213\n",
      "epoch: 9 step: 1615, loss is 0.000551560427993536\n",
      "epoch: 9 step: 1616, loss is 0.007884104736149311\n",
      "epoch: 9 step: 1617, loss is 0.0010688059264793992\n",
      "epoch: 9 step: 1618, loss is 0.004330765455961227\n",
      "epoch: 9 step: 1619, loss is 0.009845642372965813\n",
      "epoch: 9 step: 1620, loss is 0.013005483895540237\n",
      "epoch: 9 step: 1621, loss is 0.0006468534120358527\n",
      "epoch: 9 step: 1622, loss is 0.00017544100410304964\n",
      "epoch: 9 step: 1623, loss is 0.0017409786814823747\n",
      "epoch: 9 step: 1624, loss is 0.11545068025588989\n",
      "epoch: 9 step: 1625, loss is 0.0007182693807408214\n",
      "epoch: 9 step: 1626, loss is 0.001443055341951549\n",
      "epoch: 9 step: 1627, loss is 0.019335631281137466\n",
      "epoch: 9 step: 1628, loss is 0.00030742574017494917\n",
      "epoch: 9 step: 1629, loss is 0.0514485165476799\n",
      "epoch: 9 step: 1630, loss is 0.07217510789632797\n",
      "epoch: 9 step: 1631, loss is 0.009334024973213673\n",
      "epoch: 9 step: 1632, loss is 0.005610222462564707\n",
      "epoch: 9 step: 1633, loss is 0.00012994062853977084\n",
      "epoch: 9 step: 1634, loss is 0.028411727398633957\n",
      "epoch: 9 step: 1635, loss is 0.0015874238451942801\n",
      "epoch: 9 step: 1636, loss is 0.08532778918743134\n",
      "epoch: 9 step: 1637, loss is 0.024772875010967255\n",
      "epoch: 9 step: 1638, loss is 0.09374383091926575\n",
      "epoch: 9 step: 1639, loss is 0.005575303453952074\n",
      "epoch: 9 step: 1640, loss is 0.08251126855611801\n",
      "epoch: 9 step: 1641, loss is 0.000506970623973757\n",
      "epoch: 9 step: 1642, loss is 0.003229566151276231\n",
      "epoch: 9 step: 1643, loss is 0.01303420402109623\n",
      "epoch: 9 step: 1644, loss is 0.07866398990154266\n",
      "epoch: 9 step: 1645, loss is 0.04487272724509239\n",
      "epoch: 9 step: 1646, loss is 0.06054745241999626\n",
      "epoch: 9 step: 1647, loss is 0.02274223044514656\n",
      "epoch: 9 step: 1648, loss is 0.012642864137887955\n",
      "epoch: 9 step: 1649, loss is 0.003201326122507453\n",
      "epoch: 9 step: 1650, loss is 0.002429099753499031\n",
      "epoch: 9 step: 1651, loss is 0.03170998394489288\n",
      "epoch: 9 step: 1652, loss is 0.00020643946481868625\n",
      "epoch: 9 step: 1653, loss is 0.015957171097397804\n",
      "epoch: 9 step: 1654, loss is 0.001107789110392332\n",
      "epoch: 9 step: 1655, loss is 0.00031411778763867915\n",
      "epoch: 9 step: 1656, loss is 0.015575655736029148\n",
      "epoch: 9 step: 1657, loss is 0.009668068028986454\n",
      "epoch: 9 step: 1658, loss is 0.007717154920101166\n",
      "epoch: 9 step: 1659, loss is 0.024786561727523804\n",
      "epoch: 9 step: 1660, loss is 0.020840240642428398\n",
      "epoch: 9 step: 1661, loss is 0.006889595650136471\n",
      "epoch: 9 step: 1662, loss is 0.0028169292490929365\n",
      "epoch: 9 step: 1663, loss is 0.034357406198978424\n",
      "epoch: 9 step: 1664, loss is 0.0014408293645828962\n",
      "epoch: 9 step: 1665, loss is 0.22919493913650513\n",
      "epoch: 9 step: 1666, loss is 0.0013736231485381722\n",
      "epoch: 9 step: 1667, loss is 0.08991814404726028\n",
      "epoch: 9 step: 1668, loss is 0.08005809783935547\n",
      "epoch: 9 step: 1669, loss is 0.045106835663318634\n",
      "epoch: 9 step: 1670, loss is 0.004082460887730122\n",
      "epoch: 9 step: 1671, loss is 0.04295693710446358\n",
      "epoch: 9 step: 1672, loss is 0.05681092292070389\n",
      "epoch: 9 step: 1673, loss is 0.012752660550177097\n",
      "epoch: 9 step: 1674, loss is 0.0047165872529149055\n",
      "epoch: 9 step: 1675, loss is 0.0004760999290738255\n",
      "epoch: 9 step: 1676, loss is 0.01131045538932085\n",
      "epoch: 9 step: 1677, loss is 0.24649463593959808\n",
      "epoch: 9 step: 1678, loss is 0.015854761004447937\n",
      "epoch: 9 step: 1679, loss is 0.02247491478919983\n",
      "epoch: 9 step: 1680, loss is 0.0192419420927763\n",
      "epoch: 9 step: 1681, loss is 0.0007961872615851462\n",
      "epoch: 9 step: 1682, loss is 0.031640343368053436\n",
      "epoch: 9 step: 1683, loss is 0.03313051164150238\n",
      "epoch: 9 step: 1684, loss is 0.010737382806837559\n",
      "epoch: 9 step: 1685, loss is 0.0020544349681586027\n",
      "epoch: 9 step: 1686, loss is 0.00815621204674244\n",
      "epoch: 9 step: 1687, loss is 0.18653729557991028\n",
      "epoch: 9 step: 1688, loss is 0.009760896675288677\n",
      "epoch: 9 step: 1689, loss is 0.08765612542629242\n",
      "epoch: 9 step: 1690, loss is 0.05172831565141678\n",
      "epoch: 9 step: 1691, loss is 0.008767828345298767\n",
      "epoch: 9 step: 1692, loss is 0.3616998791694641\n",
      "epoch: 9 step: 1693, loss is 0.0008577911648899317\n",
      "epoch: 9 step: 1694, loss is 0.0011496020015329123\n",
      "epoch: 9 step: 1695, loss is 0.0006395999807864428\n",
      "epoch: 9 step: 1696, loss is 0.01064896397292614\n",
      "epoch: 9 step: 1697, loss is 0.06714680790901184\n",
      "epoch: 9 step: 1698, loss is 0.004129764623939991\n",
      "epoch: 9 step: 1699, loss is 0.008685383014380932\n",
      "epoch: 9 step: 1700, loss is 0.0008480945834890008\n",
      "epoch: 9 step: 1701, loss is 0.10043373703956604\n",
      "epoch: 9 step: 1702, loss is 0.015200366266071796\n",
      "epoch: 9 step: 1703, loss is 0.0007561060483567417\n",
      "epoch: 9 step: 1704, loss is 0.0037771076895296574\n",
      "epoch: 9 step: 1705, loss is 0.06454937905073166\n",
      "epoch: 9 step: 1706, loss is 0.003618659684434533\n",
      "epoch: 9 step: 1707, loss is 0.021542057394981384\n",
      "epoch: 9 step: 1708, loss is 0.17537523806095123\n",
      "epoch: 9 step: 1709, loss is 0.02411586046218872\n",
      "epoch: 9 step: 1710, loss is 0.0023764397483319044\n",
      "epoch: 9 step: 1711, loss is 0.014833127148449421\n",
      "epoch: 9 step: 1712, loss is 0.0009975368157029152\n",
      "epoch: 9 step: 1713, loss is 0.00034348858753219247\n",
      "epoch: 9 step: 1714, loss is 0.0057078758254647255\n",
      "epoch: 9 step: 1715, loss is 0.03505812957882881\n",
      "epoch: 9 step: 1716, loss is 0.0021143704652786255\n",
      "epoch: 9 step: 1717, loss is 0.01318296231329441\n",
      "epoch: 9 step: 1718, loss is 0.006132072769105434\n",
      "epoch: 9 step: 1719, loss is 0.000702582998201251\n",
      "epoch: 9 step: 1720, loss is 3.990985351265408e-05\n",
      "epoch: 9 step: 1721, loss is 0.012694115750491619\n",
      "epoch: 9 step: 1722, loss is 0.03453388065099716\n",
      "epoch: 9 step: 1723, loss is 0.17626184225082397\n",
      "epoch: 9 step: 1724, loss is 0.0009154335130006075\n",
      "epoch: 9 step: 1725, loss is 0.014866993762552738\n",
      "epoch: 9 step: 1726, loss is 0.0023240805603563786\n",
      "epoch: 9 step: 1727, loss is 0.03858410567045212\n",
      "epoch: 9 step: 1728, loss is 0.006418619304895401\n",
      "epoch: 9 step: 1729, loss is 0.00026801603962667286\n",
      "epoch: 9 step: 1730, loss is 0.0006145385559648275\n",
      "epoch: 9 step: 1731, loss is 0.014792249538004398\n",
      "epoch: 9 step: 1732, loss is 0.003598256967961788\n",
      "epoch: 9 step: 1733, loss is 0.00140001077670604\n",
      "epoch: 9 step: 1734, loss is 0.001126167713664472\n",
      "epoch: 9 step: 1735, loss is 0.034661952406167984\n",
      "epoch: 9 step: 1736, loss is 0.012825626879930496\n",
      "epoch: 9 step: 1737, loss is 0.034394096583127975\n",
      "epoch: 9 step: 1738, loss is 0.0052450960502028465\n",
      "epoch: 9 step: 1739, loss is 0.0995306447148323\n",
      "epoch: 9 step: 1740, loss is 0.01580001972615719\n",
      "epoch: 9 step: 1741, loss is 0.024043703451752663\n",
      "epoch: 9 step: 1742, loss is 0.0169764906167984\n",
      "epoch: 9 step: 1743, loss is 0.06614276021718979\n",
      "epoch: 9 step: 1744, loss is 0.031798917800188065\n",
      "epoch: 9 step: 1745, loss is 0.0014348153490573168\n",
      "epoch: 9 step: 1746, loss is 0.0015793910715728998\n",
      "epoch: 9 step: 1747, loss is 0.0008294006111100316\n",
      "epoch: 9 step: 1748, loss is 0.003849790431559086\n",
      "epoch: 9 step: 1749, loss is 0.0012967680813744664\n",
      "epoch: 9 step: 1750, loss is 0.011092949658632278\n",
      "epoch: 9 step: 1751, loss is 0.006129282992333174\n",
      "epoch: 9 step: 1752, loss is 0.05997662991285324\n",
      "epoch: 9 step: 1753, loss is 0.0076834820210933685\n",
      "epoch: 9 step: 1754, loss is 0.0012930084485560656\n",
      "epoch: 9 step: 1755, loss is 0.009190378710627556\n",
      "epoch: 9 step: 1756, loss is 0.004572275094687939\n",
      "epoch: 9 step: 1757, loss is 0.10706878453493118\n",
      "epoch: 9 step: 1758, loss is 0.0012970343232154846\n",
      "epoch: 9 step: 1759, loss is 0.004130824469029903\n",
      "epoch: 9 step: 1760, loss is 0.0018256205366924405\n",
      "epoch: 9 step: 1761, loss is 0.006720001809298992\n",
      "epoch: 9 step: 1762, loss is 0.0001345999480690807\n",
      "epoch: 9 step: 1763, loss is 0.003651653416454792\n",
      "epoch: 9 step: 1764, loss is 0.008746322244405746\n",
      "epoch: 9 step: 1765, loss is 0.061414461582899094\n",
      "epoch: 9 step: 1766, loss is 0.10828735679388046\n",
      "epoch: 9 step: 1767, loss is 0.015005107969045639\n",
      "epoch: 9 step: 1768, loss is 0.004671608563512564\n",
      "epoch: 9 step: 1769, loss is 0.016663458198308945\n",
      "epoch: 9 step: 1770, loss is 0.0003349597391206771\n",
      "epoch: 9 step: 1771, loss is 0.008330144919455051\n",
      "epoch: 9 step: 1772, loss is 0.03775883838534355\n",
      "epoch: 9 step: 1773, loss is 0.015276813879609108\n",
      "epoch: 9 step: 1774, loss is 0.04338221997022629\n",
      "epoch: 9 step: 1775, loss is 0.0007237475365400314\n",
      "epoch: 9 step: 1776, loss is 0.014181987382471561\n",
      "epoch: 9 step: 1777, loss is 0.06891297549009323\n",
      "epoch: 9 step: 1778, loss is 0.10573501139879227\n",
      "epoch: 9 step: 1779, loss is 0.0006427338812500238\n",
      "epoch: 9 step: 1780, loss is 0.0041600437834858894\n",
      "epoch: 9 step: 1781, loss is 0.009188574738800526\n",
      "epoch: 9 step: 1782, loss is 0.04220246896147728\n",
      "epoch: 9 step: 1783, loss is 0.03966040164232254\n",
      "epoch: 9 step: 1784, loss is 0.0493643656373024\n",
      "epoch: 9 step: 1785, loss is 0.01230534166097641\n",
      "epoch: 9 step: 1786, loss is 0.003925652243196964\n",
      "epoch: 9 step: 1787, loss is 0.049357425421476364\n",
      "epoch: 9 step: 1788, loss is 0.09923513233661652\n",
      "epoch: 9 step: 1789, loss is 0.020231161266565323\n",
      "epoch: 9 step: 1790, loss is 0.00869324803352356\n",
      "epoch: 9 step: 1791, loss is 0.10213706642389297\n",
      "epoch: 9 step: 1792, loss is 5.318250259733759e-05\n",
      "epoch: 9 step: 1793, loss is 0.22106336057186127\n",
      "epoch: 9 step: 1794, loss is 0.0022586463019251823\n",
      "epoch: 9 step: 1795, loss is 0.0007283631130121648\n",
      "epoch: 9 step: 1796, loss is 0.04690592736005783\n",
      "epoch: 9 step: 1797, loss is 0.01163538545370102\n",
      "epoch: 9 step: 1798, loss is 0.006905956193804741\n",
      "epoch: 9 step: 1799, loss is 0.006042162887752056\n",
      "epoch: 9 step: 1800, loss is 0.024591192603111267\n",
      "epoch: 9 step: 1801, loss is 0.007942630909383297\n",
      "epoch: 9 step: 1802, loss is 0.002044323366135359\n",
      "epoch: 9 step: 1803, loss is 0.02459307201206684\n",
      "epoch: 9 step: 1804, loss is 0.05474579334259033\n",
      "epoch: 9 step: 1805, loss is 0.000514065963216126\n",
      "epoch: 9 step: 1806, loss is 0.008334255777299404\n",
      "epoch: 9 step: 1807, loss is 0.004810412880033255\n",
      "epoch: 9 step: 1808, loss is 0.19889101386070251\n",
      "epoch: 9 step: 1809, loss is 0.013619111850857735\n",
      "epoch: 9 step: 1810, loss is 0.000728355604223907\n",
      "epoch: 9 step: 1811, loss is 0.022368017584085464\n",
      "epoch: 9 step: 1812, loss is 0.007029470521956682\n",
      "epoch: 9 step: 1813, loss is 0.0014539374969899654\n",
      "epoch: 9 step: 1814, loss is 0.11728576570749283\n",
      "epoch: 9 step: 1815, loss is 0.0004828399105463177\n",
      "epoch: 9 step: 1816, loss is 0.01680854707956314\n",
      "epoch: 9 step: 1817, loss is 0.0028245397843420506\n",
      "epoch: 9 step: 1818, loss is 0.017369944602251053\n",
      "epoch: 9 step: 1819, loss is 0.07070966809988022\n",
      "epoch: 9 step: 1820, loss is 0.17080651223659515\n",
      "epoch: 9 step: 1821, loss is 0.005155113525688648\n",
      "epoch: 9 step: 1822, loss is 0.013644244521856308\n",
      "epoch: 9 step: 1823, loss is 0.0025631559547036886\n",
      "epoch: 9 step: 1824, loss is 0.001177097437903285\n",
      "epoch: 9 step: 1825, loss is 0.007291931193321943\n",
      "epoch: 9 step: 1826, loss is 0.016738122329115868\n",
      "epoch: 9 step: 1827, loss is 0.0010229158215224743\n",
      "epoch: 9 step: 1828, loss is 0.003796258708462119\n",
      "epoch: 9 step: 1829, loss is 0.03100675530731678\n",
      "epoch: 9 step: 1830, loss is 0.00036699656629934907\n",
      "epoch: 9 step: 1831, loss is 0.009579870849847794\n",
      "epoch: 9 step: 1832, loss is 0.017312614247202873\n",
      "epoch: 9 step: 1833, loss is 0.11013852059841156\n",
      "epoch: 9 step: 1834, loss is 0.28081926703453064\n",
      "epoch: 9 step: 1835, loss is 0.03957006707787514\n",
      "epoch: 9 step: 1836, loss is 0.0016529140993952751\n",
      "epoch: 9 step: 1837, loss is 0.03899310156702995\n",
      "epoch: 9 step: 1838, loss is 0.0035625763703137636\n",
      "epoch: 9 step: 1839, loss is 0.0022036025766283274\n",
      "epoch: 9 step: 1840, loss is 0.029225682839751244\n",
      "epoch: 9 step: 1841, loss is 0.004157050978392363\n",
      "epoch: 9 step: 1842, loss is 0.00100484408903867\n",
      "epoch: 9 step: 1843, loss is 0.03566909581422806\n",
      "epoch: 9 step: 1844, loss is 0.006838061846792698\n",
      "epoch: 9 step: 1845, loss is 0.04169924557209015\n",
      "epoch: 9 step: 1846, loss is 0.011695883236825466\n",
      "epoch: 9 step: 1847, loss is 0.0004156204522587359\n",
      "epoch: 9 step: 1848, loss is 0.0005503723514266312\n",
      "epoch: 9 step: 1849, loss is 0.0029941087123006582\n",
      "epoch: 9 step: 1850, loss is 0.007726335432380438\n",
      "epoch: 9 step: 1851, loss is 0.03360946848988533\n",
      "epoch: 9 step: 1852, loss is 0.009604288265109062\n",
      "epoch: 9 step: 1853, loss is 0.0032665394246578217\n",
      "epoch: 9 step: 1854, loss is 0.1479605883359909\n",
      "epoch: 9 step: 1855, loss is 0.0019577385392040014\n",
      "epoch: 9 step: 1856, loss is 0.04098111391067505\n",
      "epoch: 9 step: 1857, loss is 0.010044286027550697\n",
      "epoch: 9 step: 1858, loss is 0.0009999594185501337\n",
      "epoch: 9 step: 1859, loss is 0.07665878534317017\n",
      "epoch: 9 step: 1860, loss is 0.021858563646674156\n",
      "epoch: 9 step: 1861, loss is 0.017598889768123627\n",
      "epoch: 9 step: 1862, loss is 0.11491373926401138\n",
      "epoch: 9 step: 1863, loss is 0.15399768948554993\n",
      "epoch: 9 step: 1864, loss is 0.0011537362588569522\n",
      "epoch: 9 step: 1865, loss is 0.00978192687034607\n",
      "epoch: 9 step: 1866, loss is 0.0070275356993079185\n",
      "epoch: 9 step: 1867, loss is 0.029937047511339188\n",
      "epoch: 9 step: 1868, loss is 0.02197379805147648\n",
      "epoch: 9 step: 1869, loss is 0.12851490080356598\n",
      "epoch: 9 step: 1870, loss is 0.01248905248939991\n",
      "epoch: 9 step: 1871, loss is 0.006153327412903309\n",
      "epoch: 9 step: 1872, loss is 0.0010144213447347283\n",
      "epoch: 9 step: 1873, loss is 0.07820436358451843\n",
      "epoch: 9 step: 1874, loss is 0.00943919736891985\n",
      "epoch: 9 step: 1875, loss is 0.05942676588892937\n",
      "epoch: 10 step: 1, loss is 0.11806822568178177\n",
      "epoch: 10 step: 2, loss is 0.001250355620868504\n",
      "epoch: 10 step: 3, loss is 0.0013946245890110731\n",
      "epoch: 10 step: 4, loss is 0.0014708585804328322\n",
      "epoch: 10 step: 5, loss is 0.06522446125745773\n",
      "epoch: 10 step: 6, loss is 0.002892413642257452\n",
      "epoch: 10 step: 7, loss is 0.0017929056193679571\n",
      "epoch: 10 step: 8, loss is 0.001450450741685927\n",
      "epoch: 10 step: 9, loss is 0.000511143880430609\n",
      "epoch: 10 step: 10, loss is 0.0012251930311322212\n",
      "epoch: 10 step: 11, loss is 0.0031547474209219217\n",
      "epoch: 10 step: 12, loss is 0.029666906222701073\n",
      "epoch: 10 step: 13, loss is 0.0193624347448349\n",
      "epoch: 10 step: 14, loss is 0.038009557873010635\n",
      "epoch: 10 step: 15, loss is 0.0018413470825180411\n",
      "epoch: 10 step: 16, loss is 0.003953888081014156\n",
      "epoch: 10 step: 17, loss is 0.0007380627212114632\n",
      "epoch: 10 step: 18, loss is 0.00025761916185729206\n",
      "epoch: 10 step: 19, loss is 0.025435391813516617\n",
      "epoch: 10 step: 20, loss is 0.00374152441509068\n",
      "epoch: 10 step: 21, loss is 0.0001993040059460327\n",
      "epoch: 10 step: 22, loss is 0.0028070600237697363\n",
      "epoch: 10 step: 23, loss is 0.004156502895057201\n",
      "epoch: 10 step: 24, loss is 0.0004640071128960699\n",
      "epoch: 10 step: 25, loss is 0.0108254449442029\n",
      "epoch: 10 step: 26, loss is 0.009857643395662308\n",
      "epoch: 10 step: 27, loss is 0.00785213802009821\n",
      "epoch: 10 step: 28, loss is 0.003498472273349762\n",
      "epoch: 10 step: 29, loss is 0.14268819987773895\n",
      "epoch: 10 step: 30, loss is 0.1763850599527359\n",
      "epoch: 10 step: 31, loss is 0.0004891693242825568\n",
      "epoch: 10 step: 32, loss is 0.002625576453283429\n",
      "epoch: 10 step: 33, loss is 0.02974272519350052\n",
      "epoch: 10 step: 34, loss is 0.0008519159164279699\n",
      "epoch: 10 step: 35, loss is 0.21167996525764465\n",
      "epoch: 10 step: 36, loss is 0.05434799566864967\n",
      "epoch: 10 step: 37, loss is 0.005211631301790476\n",
      "epoch: 10 step: 38, loss is 0.0007571501191705465\n",
      "epoch: 10 step: 39, loss is 0.0023333202116191387\n",
      "epoch: 10 step: 40, loss is 0.004036037717014551\n",
      "epoch: 10 step: 41, loss is 0.01683248206973076\n",
      "epoch: 10 step: 42, loss is 0.005764647386968136\n",
      "epoch: 10 step: 43, loss is 0.0958225280046463\n",
      "epoch: 10 step: 44, loss is 0.006526590790599585\n",
      "epoch: 10 step: 45, loss is 0.06214451789855957\n",
      "epoch: 10 step: 46, loss is 0.011061017401516438\n",
      "epoch: 10 step: 47, loss is 0.013784816488623619\n",
      "epoch: 10 step: 48, loss is 0.0022975082974880934\n",
      "epoch: 10 step: 49, loss is 0.004496367648243904\n",
      "epoch: 10 step: 50, loss is 0.023492000997066498\n",
      "epoch: 10 step: 51, loss is 0.0011915323557332158\n",
      "epoch: 10 step: 52, loss is 0.06103845685720444\n",
      "epoch: 10 step: 53, loss is 0.07441907376050949\n",
      "epoch: 10 step: 54, loss is 0.05442121997475624\n",
      "epoch: 10 step: 55, loss is 0.0007540310616604984\n",
      "epoch: 10 step: 56, loss is 0.03362211585044861\n",
      "epoch: 10 step: 57, loss is 0.023036910220980644\n",
      "epoch: 10 step: 58, loss is 0.010725704953074455\n",
      "epoch: 10 step: 59, loss is 0.2406417429447174\n",
      "epoch: 10 step: 60, loss is 0.06968460232019424\n",
      "epoch: 10 step: 61, loss is 0.0017437241040170193\n",
      "epoch: 10 step: 62, loss is 0.023837460204958916\n",
      "epoch: 10 step: 63, loss is 0.03772293031215668\n",
      "epoch: 10 step: 64, loss is 0.1669449508190155\n",
      "epoch: 10 step: 65, loss is 0.012309999205172062\n",
      "epoch: 10 step: 66, loss is 0.00455118715763092\n",
      "epoch: 10 step: 67, loss is 0.007926762104034424\n",
      "epoch: 10 step: 68, loss is 0.09689991176128387\n",
      "epoch: 10 step: 69, loss is 0.004158538766205311\n",
      "epoch: 10 step: 70, loss is 0.004607132636010647\n",
      "epoch: 10 step: 71, loss is 0.04556075111031532\n",
      "epoch: 10 step: 72, loss is 0.0017316188896074891\n",
      "epoch: 10 step: 73, loss is 0.03392883390188217\n",
      "epoch: 10 step: 74, loss is 0.029579099267721176\n",
      "epoch: 10 step: 75, loss is 0.03828345611691475\n",
      "epoch: 10 step: 76, loss is 0.007719940505921841\n",
      "epoch: 10 step: 77, loss is 0.02617335133254528\n",
      "epoch: 10 step: 78, loss is 0.0006410449859686196\n",
      "epoch: 10 step: 79, loss is 0.010025396943092346\n",
      "epoch: 10 step: 80, loss is 0.019620506092905998\n",
      "epoch: 10 step: 81, loss is 0.013188447803258896\n",
      "epoch: 10 step: 82, loss is 0.018146101385354996\n",
      "epoch: 10 step: 83, loss is 0.0785001665353775\n",
      "epoch: 10 step: 84, loss is 0.006557947024703026\n",
      "epoch: 10 step: 85, loss is 0.0004093894676771015\n",
      "epoch: 10 step: 86, loss is 0.00018062753952108324\n",
      "epoch: 10 step: 87, loss is 0.002129156840965152\n",
      "epoch: 10 step: 88, loss is 0.01593821309506893\n",
      "epoch: 10 step: 89, loss is 0.0018295330228284001\n",
      "epoch: 10 step: 90, loss is 0.08175137639045715\n",
      "epoch: 10 step: 91, loss is 0.0017635314725339413\n",
      "epoch: 10 step: 92, loss is 0.0011259351158514619\n",
      "epoch: 10 step: 93, loss is 0.0013922560028731823\n",
      "epoch: 10 step: 94, loss is 0.024002689868211746\n",
      "epoch: 10 step: 95, loss is 0.0023489443119615316\n",
      "epoch: 10 step: 96, loss is 0.00035824926453642547\n",
      "epoch: 10 step: 97, loss is 0.0010538501664996147\n",
      "epoch: 10 step: 98, loss is 0.0015469864010810852\n",
      "epoch: 10 step: 99, loss is 0.019208082929253578\n",
      "epoch: 10 step: 100, loss is 0.004081175662577152\n",
      "epoch: 10 step: 101, loss is 0.002560528228059411\n",
      "epoch: 10 step: 102, loss is 0.00029304332565516233\n",
      "epoch: 10 step: 103, loss is 0.0018962249159812927\n",
      "epoch: 10 step: 104, loss is 0.0007490585558116436\n",
      "epoch: 10 step: 105, loss is 0.013199473731219769\n",
      "epoch: 10 step: 106, loss is 0.0420403927564621\n",
      "epoch: 10 step: 107, loss is 0.02297387830913067\n",
      "epoch: 10 step: 108, loss is 0.00014980493870098144\n",
      "epoch: 10 step: 109, loss is 0.01817782036960125\n",
      "epoch: 10 step: 110, loss is 0.0006786144222132862\n",
      "epoch: 10 step: 111, loss is 0.08125168085098267\n",
      "epoch: 10 step: 112, loss is 0.07749078422784805\n",
      "epoch: 10 step: 113, loss is 0.00985047873109579\n",
      "epoch: 10 step: 114, loss is 0.03179488703608513\n",
      "epoch: 10 step: 115, loss is 0.003950407262891531\n",
      "epoch: 10 step: 116, loss is 0.001037999871186912\n",
      "epoch: 10 step: 117, loss is 0.03208232298493385\n",
      "epoch: 10 step: 118, loss is 0.02421661838889122\n",
      "epoch: 10 step: 119, loss is 0.004839146975427866\n",
      "epoch: 10 step: 120, loss is 0.0008300275076180696\n",
      "epoch: 10 step: 121, loss is 0.0005831201560795307\n",
      "epoch: 10 step: 122, loss is 0.014406504109501839\n",
      "epoch: 10 step: 123, loss is 0.0066193221136927605\n",
      "epoch: 10 step: 124, loss is 0.0016881326446309686\n",
      "epoch: 10 step: 125, loss is 0.0006218142225407064\n",
      "epoch: 10 step: 126, loss is 0.008865784853696823\n",
      "epoch: 10 step: 127, loss is 0.00688554672524333\n",
      "epoch: 10 step: 128, loss is 0.034542180597782135\n",
      "epoch: 10 step: 129, loss is 0.002222952898591757\n",
      "epoch: 10 step: 130, loss is 0.007722622714936733\n",
      "epoch: 10 step: 131, loss is 0.05398798733949661\n",
      "epoch: 10 step: 132, loss is 0.018707383424043655\n",
      "epoch: 10 step: 133, loss is 0.07619312405586243\n",
      "epoch: 10 step: 134, loss is 0.0013059930643066764\n",
      "epoch: 10 step: 135, loss is 0.00043726369040086865\n",
      "epoch: 10 step: 136, loss is 0.004240505397319794\n",
      "epoch: 10 step: 137, loss is 0.0021776987705379725\n",
      "epoch: 10 step: 138, loss is 0.0033634849824011326\n",
      "epoch: 10 step: 139, loss is 0.0019220564281567931\n",
      "epoch: 10 step: 140, loss is 0.06598430126905441\n",
      "epoch: 10 step: 141, loss is 0.10122796148061752\n",
      "epoch: 10 step: 142, loss is 0.0424734391272068\n",
      "epoch: 10 step: 143, loss is 0.0005479553365148604\n",
      "epoch: 10 step: 144, loss is 0.006326571572571993\n",
      "epoch: 10 step: 145, loss is 0.07397890836000443\n",
      "epoch: 10 step: 146, loss is 0.0008816352928988636\n",
      "epoch: 10 step: 147, loss is 0.0022734184749424458\n",
      "epoch: 10 step: 148, loss is 0.004333009477704763\n",
      "epoch: 10 step: 149, loss is 0.05850132182240486\n",
      "epoch: 10 step: 150, loss is 0.0018434696830809116\n",
      "epoch: 10 step: 151, loss is 0.06738961488008499\n",
      "epoch: 10 step: 152, loss is 0.08977746963500977\n",
      "epoch: 10 step: 153, loss is 0.06783237308263779\n",
      "epoch: 10 step: 154, loss is 0.011663037352263927\n",
      "epoch: 10 step: 155, loss is 0.0010652233613654971\n",
      "epoch: 10 step: 156, loss is 0.028083406388759613\n",
      "epoch: 10 step: 157, loss is 0.004578195512294769\n",
      "epoch: 10 step: 158, loss is 0.07123660296201706\n",
      "epoch: 10 step: 159, loss is 0.07202672213315964\n",
      "epoch: 10 step: 160, loss is 0.0022158476058393717\n",
      "epoch: 10 step: 161, loss is 0.0816529169678688\n",
      "epoch: 10 step: 162, loss is 0.0025588413700461388\n",
      "epoch: 10 step: 163, loss is 0.03540858253836632\n",
      "epoch: 10 step: 164, loss is 0.0035253686364740133\n",
      "epoch: 10 step: 165, loss is 0.0003710888558998704\n",
      "epoch: 10 step: 166, loss is 0.006915015168488026\n",
      "epoch: 10 step: 167, loss is 0.012344651855528355\n",
      "epoch: 10 step: 168, loss is 0.00042008823947981\n",
      "epoch: 10 step: 169, loss is 0.0011790205026045442\n",
      "epoch: 10 step: 170, loss is 0.11010219156742096\n",
      "epoch: 10 step: 171, loss is 0.028537267819046974\n",
      "epoch: 10 step: 172, loss is 0.01452611479908228\n",
      "epoch: 10 step: 173, loss is 0.0018264143727719784\n",
      "epoch: 10 step: 174, loss is 0.016461340710520744\n",
      "epoch: 10 step: 175, loss is 0.07562995702028275\n",
      "epoch: 10 step: 176, loss is 0.006853622384369373\n",
      "epoch: 10 step: 177, loss is 0.004213123582303524\n",
      "epoch: 10 step: 178, loss is 0.000936293276026845\n",
      "epoch: 10 step: 179, loss is 0.0009931764798238873\n",
      "epoch: 10 step: 180, loss is 0.041419170796871185\n",
      "epoch: 10 step: 181, loss is 0.033909738063812256\n",
      "epoch: 10 step: 182, loss is 0.011899671517312527\n",
      "epoch: 10 step: 183, loss is 0.16224101185798645\n",
      "epoch: 10 step: 184, loss is 0.03400618955492973\n",
      "epoch: 10 step: 185, loss is 0.070353664457798\n",
      "epoch: 10 step: 186, loss is 0.0031848610378801823\n",
      "epoch: 10 step: 187, loss is 0.11777523159980774\n",
      "epoch: 10 step: 188, loss is 0.0012381921987980604\n",
      "epoch: 10 step: 189, loss is 0.03210797905921936\n",
      "epoch: 10 step: 190, loss is 0.0008180484874173999\n",
      "epoch: 10 step: 191, loss is 0.003388455603271723\n",
      "epoch: 10 step: 192, loss is 0.018182802945375443\n",
      "epoch: 10 step: 193, loss is 0.16735559701919556\n",
      "epoch: 10 step: 194, loss is 0.0005571111687459052\n",
      "epoch: 10 step: 195, loss is 0.0002591400407254696\n",
      "epoch: 10 step: 196, loss is 0.02722674235701561\n",
      "epoch: 10 step: 197, loss is 0.014145636931061745\n",
      "epoch: 10 step: 198, loss is 0.12496873736381531\n",
      "epoch: 10 step: 199, loss is 0.03377486392855644\n",
      "epoch: 10 step: 200, loss is 0.0680016502737999\n",
      "epoch: 10 step: 201, loss is 0.004226370714604855\n",
      "epoch: 10 step: 202, loss is 0.02907780557870865\n",
      "epoch: 10 step: 203, loss is 0.024119151756167412\n",
      "epoch: 10 step: 204, loss is 0.00029527340666390955\n",
      "epoch: 10 step: 205, loss is 0.0007155279745347798\n",
      "epoch: 10 step: 206, loss is 0.04549061134457588\n",
      "epoch: 10 step: 207, loss is 0.04045282304286957\n",
      "epoch: 10 step: 208, loss is 0.0004360429011285305\n",
      "epoch: 10 step: 209, loss is 0.034791719168424606\n",
      "epoch: 10 step: 210, loss is 0.004639426711946726\n",
      "epoch: 10 step: 211, loss is 0.00021049822680652142\n",
      "epoch: 10 step: 212, loss is 0.13772627711296082\n",
      "epoch: 10 step: 213, loss is 0.07764799147844315\n",
      "epoch: 10 step: 214, loss is 0.002239818451926112\n",
      "epoch: 10 step: 215, loss is 0.0009480395237915218\n",
      "epoch: 10 step: 216, loss is 0.08445923775434494\n",
      "epoch: 10 step: 217, loss is 0.0006917496211826801\n",
      "epoch: 10 step: 218, loss is 0.00017937627853825688\n",
      "epoch: 10 step: 219, loss is 0.01708684116601944\n",
      "epoch: 10 step: 220, loss is 0.004196648020297289\n",
      "epoch: 10 step: 221, loss is 0.011136282235383987\n",
      "epoch: 10 step: 222, loss is 0.14067156612873077\n",
      "epoch: 10 step: 223, loss is 0.00017581185966264457\n",
      "epoch: 10 step: 224, loss is 0.021321840584278107\n",
      "epoch: 10 step: 225, loss is 0.0032393985893577337\n",
      "epoch: 10 step: 226, loss is 0.012771667912602425\n",
      "epoch: 10 step: 227, loss is 0.00013454866711981595\n",
      "epoch: 10 step: 228, loss is 0.007966051809489727\n",
      "epoch: 10 step: 229, loss is 0.02113819494843483\n",
      "epoch: 10 step: 230, loss is 0.06626776605844498\n",
      "epoch: 10 step: 231, loss is 0.044026486575603485\n",
      "epoch: 10 step: 232, loss is 0.0030736750923097134\n",
      "epoch: 10 step: 233, loss is 0.012674588710069656\n",
      "epoch: 10 step: 234, loss is 0.0015959247248247266\n",
      "epoch: 10 step: 235, loss is 0.04021404683589935\n",
      "epoch: 10 step: 236, loss is 0.005699316039681435\n",
      "epoch: 10 step: 237, loss is 0.0042331931181252\n",
      "epoch: 10 step: 238, loss is 0.00037771908682771027\n",
      "epoch: 10 step: 239, loss is 0.0007455861195921898\n",
      "epoch: 10 step: 240, loss is 0.03335738554596901\n",
      "epoch: 10 step: 241, loss is 0.02548390068113804\n",
      "epoch: 10 step: 242, loss is 0.0017324943328276277\n",
      "epoch: 10 step: 243, loss is 0.008852807804942131\n",
      "epoch: 10 step: 244, loss is 0.0012142069172114134\n",
      "epoch: 10 step: 245, loss is 0.045657191425561905\n",
      "epoch: 10 step: 246, loss is 0.0003665773547254503\n",
      "epoch: 10 step: 247, loss is 0.00964740477502346\n",
      "epoch: 10 step: 248, loss is 0.026946721598505974\n",
      "epoch: 10 step: 249, loss is 0.01137832086533308\n",
      "epoch: 10 step: 250, loss is 0.029127294197678566\n",
      "epoch: 10 step: 251, loss is 0.0047400938346982\n",
      "epoch: 10 step: 252, loss is 0.007236925885081291\n",
      "epoch: 10 step: 253, loss is 0.02369781583547592\n",
      "epoch: 10 step: 254, loss is 0.039818085730075836\n",
      "epoch: 10 step: 255, loss is 0.0064970930106937885\n",
      "epoch: 10 step: 256, loss is 0.005681694485247135\n",
      "epoch: 10 step: 257, loss is 0.0011723787756636739\n",
      "epoch: 10 step: 258, loss is 0.02757689356803894\n",
      "epoch: 10 step: 259, loss is 0.002898703096434474\n",
      "epoch: 10 step: 260, loss is 0.007543774321675301\n",
      "epoch: 10 step: 261, loss is 0.06858981400728226\n",
      "epoch: 10 step: 262, loss is 0.019365599378943443\n",
      "epoch: 10 step: 263, loss is 0.003535524709150195\n",
      "epoch: 10 step: 264, loss is 0.0017138454131782055\n",
      "epoch: 10 step: 265, loss is 0.014206442050635815\n",
      "epoch: 10 step: 266, loss is 0.0019267729949206114\n",
      "epoch: 10 step: 267, loss is 0.09859409928321838\n",
      "epoch: 10 step: 268, loss is 0.04959843307733536\n",
      "epoch: 10 step: 269, loss is 0.0025463858619332314\n",
      "epoch: 10 step: 270, loss is 0.0014768396504223347\n",
      "epoch: 10 step: 271, loss is 0.0030594589188694954\n",
      "epoch: 10 step: 272, loss is 0.0029151311609894037\n",
      "epoch: 10 step: 273, loss is 0.04723425582051277\n",
      "epoch: 10 step: 274, loss is 0.0034941607154905796\n",
      "epoch: 10 step: 275, loss is 0.01176482904702425\n",
      "epoch: 10 step: 276, loss is 0.08493960648775101\n",
      "epoch: 10 step: 277, loss is 0.00108009681571275\n",
      "epoch: 10 step: 278, loss is 0.015772690996527672\n",
      "epoch: 10 step: 279, loss is 0.08792904764413834\n",
      "epoch: 10 step: 280, loss is 0.0395173616707325\n",
      "epoch: 10 step: 281, loss is 0.02063474804162979\n",
      "epoch: 10 step: 282, loss is 0.0006095233256928623\n",
      "epoch: 10 step: 283, loss is 0.005073959473520517\n",
      "epoch: 10 step: 284, loss is 0.024824678897857666\n",
      "epoch: 10 step: 285, loss is 0.025428367778658867\n",
      "epoch: 10 step: 286, loss is 0.03971432149410248\n",
      "epoch: 10 step: 287, loss is 0.015555710531771183\n",
      "epoch: 10 step: 288, loss is 0.00017029806622304022\n",
      "epoch: 10 step: 289, loss is 0.0013366325292736292\n",
      "epoch: 10 step: 290, loss is 0.0062448750250041485\n",
      "epoch: 10 step: 291, loss is 0.0005717476597055793\n",
      "epoch: 10 step: 292, loss is 0.018866052851080894\n",
      "epoch: 10 step: 293, loss is 0.028509467840194702\n",
      "epoch: 10 step: 294, loss is 0.001266375882551074\n",
      "epoch: 10 step: 295, loss is 0.03220105916261673\n",
      "epoch: 10 step: 296, loss is 0.094545878469944\n",
      "epoch: 10 step: 297, loss is 0.24221768975257874\n",
      "epoch: 10 step: 298, loss is 0.0007914667367003858\n",
      "epoch: 10 step: 299, loss is 0.10432402789592743\n",
      "epoch: 10 step: 300, loss is 0.010946116410195827\n",
      "epoch: 10 step: 301, loss is 0.0002540273417253047\n",
      "epoch: 10 step: 302, loss is 0.008171494118869305\n",
      "epoch: 10 step: 303, loss is 0.00792184378951788\n",
      "epoch: 10 step: 304, loss is 0.007229154463857412\n",
      "epoch: 10 step: 305, loss is 0.0002822333190124482\n",
      "epoch: 10 step: 306, loss is 0.022668426856398582\n",
      "epoch: 10 step: 307, loss is 0.005293371621519327\n",
      "epoch: 10 step: 308, loss is 0.0005651492392644286\n",
      "epoch: 10 step: 309, loss is 0.00014274225395638496\n",
      "epoch: 10 step: 310, loss is 0.0013215962098911405\n",
      "epoch: 10 step: 311, loss is 0.007701185531914234\n",
      "epoch: 10 step: 312, loss is 0.08813268691301346\n",
      "epoch: 10 step: 313, loss is 0.005916151218116283\n",
      "epoch: 10 step: 314, loss is 0.038973331451416016\n",
      "epoch: 10 step: 315, loss is 0.004675532691180706\n",
      "epoch: 10 step: 316, loss is 0.0024495581164956093\n",
      "epoch: 10 step: 317, loss is 0.06288113445043564\n",
      "epoch: 10 step: 318, loss is 0.005364549346268177\n",
      "epoch: 10 step: 319, loss is 0.00043014338007196784\n",
      "epoch: 10 step: 320, loss is 0.006742654833942652\n",
      "epoch: 10 step: 321, loss is 0.026792144402861595\n",
      "epoch: 10 step: 322, loss is 0.011975442990660667\n",
      "epoch: 10 step: 323, loss is 0.007193317171186209\n",
      "epoch: 10 step: 324, loss is 0.0015929995570331812\n",
      "epoch: 10 step: 325, loss is 0.0013648649910464883\n",
      "epoch: 10 step: 326, loss is 0.0008561286958865821\n",
      "epoch: 10 step: 327, loss is 0.0056016412563622\n",
      "epoch: 10 step: 328, loss is 0.0021004462614655495\n",
      "epoch: 10 step: 329, loss is 0.045039013028144836\n",
      "epoch: 10 step: 330, loss is 0.024746857583522797\n",
      "epoch: 10 step: 331, loss is 0.034063857048749924\n",
      "epoch: 10 step: 332, loss is 0.018427329137921333\n",
      "epoch: 10 step: 333, loss is 0.00010176833893638104\n",
      "epoch: 10 step: 334, loss is 0.010792037472128868\n",
      "epoch: 10 step: 335, loss is 0.0014866729034110904\n",
      "epoch: 10 step: 336, loss is 0.020013893023133278\n",
      "epoch: 10 step: 337, loss is 0.00500255823135376\n",
      "epoch: 10 step: 338, loss is 0.009489577263593674\n",
      "epoch: 10 step: 339, loss is 0.19895395636558533\n",
      "epoch: 10 step: 340, loss is 0.14597855508327484\n",
      "epoch: 10 step: 341, loss is 0.011763338930904865\n",
      "epoch: 10 step: 342, loss is 0.028743835166096687\n",
      "epoch: 10 step: 343, loss is 0.0045169442892074585\n",
      "epoch: 10 step: 344, loss is 0.0012319848174229264\n",
      "epoch: 10 step: 345, loss is 0.07160612940788269\n",
      "epoch: 10 step: 346, loss is 0.03448576107621193\n",
      "epoch: 10 step: 347, loss is 0.013851817697286606\n",
      "epoch: 10 step: 348, loss is 0.0060618286952376366\n",
      "epoch: 10 step: 349, loss is 0.060168616473674774\n",
      "epoch: 10 step: 350, loss is 0.000306770351016894\n",
      "epoch: 10 step: 351, loss is 0.0004557518695946783\n",
      "epoch: 10 step: 352, loss is 0.0045857783406972885\n",
      "epoch: 10 step: 353, loss is 0.0005792867159470916\n",
      "epoch: 10 step: 354, loss is 0.01863115467131138\n",
      "epoch: 10 step: 355, loss is 0.0036497232504189014\n",
      "epoch: 10 step: 356, loss is 0.04546467214822769\n",
      "epoch: 10 step: 357, loss is 0.01977907121181488\n",
      "epoch: 10 step: 358, loss is 0.003021709155291319\n",
      "epoch: 10 step: 359, loss is 0.01832643151283264\n",
      "epoch: 10 step: 360, loss is 0.0320117250084877\n",
      "epoch: 10 step: 361, loss is 0.02865506149828434\n",
      "epoch: 10 step: 362, loss is 0.001690430217422545\n",
      "epoch: 10 step: 363, loss is 0.16811677813529968\n",
      "epoch: 10 step: 364, loss is 0.03608906641602516\n",
      "epoch: 10 step: 365, loss is 0.03556397184729576\n",
      "epoch: 10 step: 366, loss is 0.0023153992369771004\n",
      "epoch: 10 step: 367, loss is 0.004191046115010977\n",
      "epoch: 10 step: 368, loss is 0.0029646779876202345\n",
      "epoch: 10 step: 369, loss is 0.007922702468931675\n",
      "epoch: 10 step: 370, loss is 0.07453176379203796\n",
      "epoch: 10 step: 371, loss is 0.10900703072547913\n",
      "epoch: 10 step: 372, loss is 0.03329871594905853\n",
      "epoch: 10 step: 373, loss is 0.011786464601755142\n",
      "epoch: 10 step: 374, loss is 0.022596996277570724\n",
      "epoch: 10 step: 375, loss is 0.08504147082567215\n",
      "epoch: 10 step: 376, loss is 0.13740266859531403\n",
      "epoch: 10 step: 377, loss is 0.015662679448723793\n",
      "epoch: 10 step: 378, loss is 0.009635576978325844\n",
      "epoch: 10 step: 379, loss is 0.0036619501188397408\n",
      "epoch: 10 step: 380, loss is 0.005913823843002319\n",
      "epoch: 10 step: 381, loss is 0.0011924676364287734\n",
      "epoch: 10 step: 382, loss is 0.0027931409422308207\n",
      "epoch: 10 step: 383, loss is 0.07267139852046967\n",
      "epoch: 10 step: 384, loss is 0.0003479479346424341\n",
      "epoch: 10 step: 385, loss is 0.004648116417229176\n",
      "epoch: 10 step: 386, loss is 0.05021148547530174\n",
      "epoch: 10 step: 387, loss is 0.0019650959875434637\n",
      "epoch: 10 step: 388, loss is 0.02573893964290619\n",
      "epoch: 10 step: 389, loss is 0.01588379219174385\n",
      "epoch: 10 step: 390, loss is 0.013510264456272125\n",
      "epoch: 10 step: 391, loss is 0.0002611068484839052\n",
      "epoch: 10 step: 392, loss is 0.0005907200393266976\n",
      "epoch: 10 step: 393, loss is 0.0010506974067538977\n",
      "epoch: 10 step: 394, loss is 0.11115197092294693\n",
      "epoch: 10 step: 395, loss is 0.0011572210351005197\n",
      "epoch: 10 step: 396, loss is 0.041235845535993576\n",
      "epoch: 10 step: 397, loss is 0.0027372550684958696\n",
      "epoch: 10 step: 398, loss is 0.0037906912621110678\n",
      "epoch: 10 step: 399, loss is 0.005456562153995037\n",
      "epoch: 10 step: 400, loss is 0.01399116963148117\n",
      "epoch: 10 step: 401, loss is 0.003004743717610836\n",
      "epoch: 10 step: 402, loss is 0.0039549279026687145\n",
      "epoch: 10 step: 403, loss is 0.00010925105743808672\n",
      "epoch: 10 step: 404, loss is 0.006478872615844011\n",
      "epoch: 10 step: 405, loss is 0.038403015583753586\n",
      "epoch: 10 step: 406, loss is 0.28790873289108276\n",
      "epoch: 10 step: 407, loss is 0.01294730231165886\n",
      "epoch: 10 step: 408, loss is 0.0007074453169479966\n",
      "epoch: 10 step: 409, loss is 0.0033720340579748154\n",
      "epoch: 10 step: 410, loss is 0.19923701882362366\n",
      "epoch: 10 step: 411, loss is 0.09309812635183334\n",
      "epoch: 10 step: 412, loss is 0.10682785511016846\n",
      "epoch: 10 step: 413, loss is 0.023606272414326668\n",
      "epoch: 10 step: 414, loss is 0.01756010204553604\n",
      "epoch: 10 step: 415, loss is 0.010498754680156708\n",
      "epoch: 10 step: 416, loss is 0.020490452647209167\n",
      "epoch: 10 step: 417, loss is 0.1430004984140396\n",
      "epoch: 10 step: 418, loss is 0.05590003728866577\n",
      "epoch: 10 step: 419, loss is 0.23679327964782715\n",
      "epoch: 10 step: 420, loss is 0.0969146117568016\n",
      "epoch: 10 step: 421, loss is 0.005967995617538691\n",
      "epoch: 10 step: 422, loss is 0.00014323185314424336\n",
      "epoch: 10 step: 423, loss is 0.059986092150211334\n",
      "epoch: 10 step: 424, loss is 0.03316636011004448\n",
      "epoch: 10 step: 425, loss is 0.007778635714203119\n",
      "epoch: 10 step: 426, loss is 0.010395742952823639\n",
      "epoch: 10 step: 427, loss is 0.0065487646497786045\n",
      "epoch: 10 step: 428, loss is 9.633837180444971e-05\n",
      "epoch: 10 step: 429, loss is 0.016192607581615448\n",
      "epoch: 10 step: 430, loss is 0.0008793619344942272\n",
      "epoch: 10 step: 431, loss is 0.014187444932758808\n",
      "epoch: 10 step: 432, loss is 0.00226572691462934\n",
      "epoch: 10 step: 433, loss is 0.07062382996082306\n",
      "epoch: 10 step: 434, loss is 0.06202220916748047\n",
      "epoch: 10 step: 435, loss is 0.05804823338985443\n",
      "epoch: 10 step: 436, loss is 0.0007763832109048963\n",
      "epoch: 10 step: 437, loss is 0.006427087355405092\n",
      "epoch: 10 step: 438, loss is 0.009351692162454128\n",
      "epoch: 10 step: 439, loss is 0.004562286660075188\n",
      "epoch: 10 step: 440, loss is 0.14801372587680817\n",
      "epoch: 10 step: 441, loss is 0.13541844487190247\n",
      "epoch: 10 step: 442, loss is 0.002202569739893079\n",
      "epoch: 10 step: 443, loss is 0.0849853977560997\n",
      "epoch: 10 step: 444, loss is 0.007247976493090391\n",
      "epoch: 10 step: 445, loss is 0.05089230835437775\n",
      "epoch: 10 step: 446, loss is 0.0015994557179510593\n",
      "epoch: 10 step: 447, loss is 0.02520117163658142\n",
      "epoch: 10 step: 448, loss is 0.01199765782803297\n",
      "epoch: 10 step: 449, loss is 0.09936908632516861\n",
      "epoch: 10 step: 450, loss is 0.0023354743607342243\n",
      "epoch: 10 step: 451, loss is 0.000799581001047045\n",
      "epoch: 10 step: 452, loss is 0.027734430506825447\n",
      "epoch: 10 step: 453, loss is 0.011354302987456322\n",
      "epoch: 10 step: 454, loss is 0.001852549728937447\n",
      "epoch: 10 step: 455, loss is 0.004551662597805262\n",
      "epoch: 10 step: 456, loss is 0.0984957367181778\n",
      "epoch: 10 step: 457, loss is 0.10354667156934738\n",
      "epoch: 10 step: 458, loss is 0.06631315499544144\n",
      "epoch: 10 step: 459, loss is 0.08859709650278091\n",
      "epoch: 10 step: 460, loss is 0.0001064736134139821\n",
      "epoch: 10 step: 461, loss is 0.010942621156573296\n",
      "epoch: 10 step: 462, loss is 0.0014495961368083954\n",
      "epoch: 10 step: 463, loss is 0.011224828660488129\n",
      "epoch: 10 step: 464, loss is 0.16034920513629913\n",
      "epoch: 10 step: 465, loss is 0.011720634065568447\n",
      "epoch: 10 step: 466, loss is 0.00016191827307920903\n",
      "epoch: 10 step: 467, loss is 0.024990329518914223\n",
      "epoch: 10 step: 468, loss is 0.0815223902463913\n",
      "epoch: 10 step: 469, loss is 0.11251199245452881\n",
      "epoch: 10 step: 470, loss is 0.3063783645629883\n",
      "epoch: 10 step: 471, loss is 0.004735929425805807\n",
      "epoch: 10 step: 472, loss is 0.08699920773506165\n",
      "epoch: 10 step: 473, loss is 0.08651569485664368\n",
      "epoch: 10 step: 474, loss is 0.0017613702220842242\n",
      "epoch: 10 step: 475, loss is 0.012807508930563927\n",
      "epoch: 10 step: 476, loss is 0.041682954877614975\n",
      "epoch: 10 step: 477, loss is 0.012868002988398075\n",
      "epoch: 10 step: 478, loss is 0.0020957619417458773\n",
      "epoch: 10 step: 479, loss is 0.0468858927488327\n",
      "epoch: 10 step: 480, loss is 0.03594031557440758\n",
      "epoch: 10 step: 481, loss is 0.008149228058755398\n",
      "epoch: 10 step: 482, loss is 0.07719762623310089\n",
      "epoch: 10 step: 483, loss is 0.00350305181927979\n",
      "epoch: 10 step: 484, loss is 0.003193092532455921\n",
      "epoch: 10 step: 485, loss is 0.000715585716534406\n",
      "epoch: 10 step: 486, loss is 0.0001158306622528471\n",
      "epoch: 10 step: 487, loss is 0.044278260320425034\n",
      "epoch: 10 step: 488, loss is 0.03815224766731262\n",
      "epoch: 10 step: 489, loss is 0.0005068585742264986\n",
      "epoch: 10 step: 490, loss is 9.264265827368945e-05\n",
      "epoch: 10 step: 491, loss is 0.003837739583104849\n",
      "epoch: 10 step: 492, loss is 0.03509492427110672\n",
      "epoch: 10 step: 493, loss is 0.009650150313973427\n",
      "epoch: 10 step: 494, loss is 0.01842811517417431\n",
      "epoch: 10 step: 495, loss is 0.0015268962597474456\n",
      "epoch: 10 step: 496, loss is 0.006169721484184265\n",
      "epoch: 10 step: 497, loss is 0.0057935346849262714\n",
      "epoch: 10 step: 498, loss is 0.024494856595993042\n",
      "epoch: 10 step: 499, loss is 0.024923590943217278\n",
      "epoch: 10 step: 500, loss is 0.008117911405861378\n",
      "epoch: 10 step: 501, loss is 0.004474724642932415\n",
      "epoch: 10 step: 502, loss is 0.14897803962230682\n",
      "epoch: 10 step: 503, loss is 0.012284877710044384\n",
      "epoch: 10 step: 504, loss is 0.003677584230899811\n",
      "epoch: 10 step: 505, loss is 0.03911324217915535\n",
      "epoch: 10 step: 506, loss is 0.021445738151669502\n",
      "epoch: 10 step: 507, loss is 0.001788245514035225\n",
      "epoch: 10 step: 508, loss is 0.06661278009414673\n",
      "epoch: 10 step: 509, loss is 0.0005631732055917382\n",
      "epoch: 10 step: 510, loss is 0.006072389427572489\n",
      "epoch: 10 step: 511, loss is 0.436011403799057\n",
      "epoch: 10 step: 512, loss is 0.0024358078371733427\n",
      "epoch: 10 step: 513, loss is 0.0006604497320950031\n",
      "epoch: 10 step: 514, loss is 0.03811841085553169\n",
      "epoch: 10 step: 515, loss is 0.0055833980441093445\n",
      "epoch: 10 step: 516, loss is 0.00015768771118018776\n",
      "epoch: 10 step: 517, loss is 0.0008476601215079427\n",
      "epoch: 10 step: 518, loss is 0.003936365246772766\n",
      "epoch: 10 step: 519, loss is 0.03211132436990738\n",
      "epoch: 10 step: 520, loss is 0.03225740045309067\n",
      "epoch: 10 step: 521, loss is 0.004505642224103212\n",
      "epoch: 10 step: 522, loss is 0.026631586253643036\n",
      "epoch: 10 step: 523, loss is 0.026628047227859497\n",
      "epoch: 10 step: 524, loss is 0.10399500280618668\n",
      "epoch: 10 step: 525, loss is 0.027168454602360725\n",
      "epoch: 10 step: 526, loss is 0.0035265192855149508\n",
      "epoch: 10 step: 527, loss is 0.0008016731590032578\n",
      "epoch: 10 step: 528, loss is 0.20335324108600616\n",
      "epoch: 10 step: 529, loss is 0.0016695655649527907\n",
      "epoch: 10 step: 530, loss is 0.08914586901664734\n",
      "epoch: 10 step: 531, loss is 0.09237450361251831\n",
      "epoch: 10 step: 532, loss is 0.07888613641262054\n",
      "epoch: 10 step: 533, loss is 0.005238744430243969\n",
      "epoch: 10 step: 534, loss is 0.012270172126591206\n",
      "epoch: 10 step: 535, loss is 0.000846383161842823\n",
      "epoch: 10 step: 536, loss is 0.08068329840898514\n",
      "epoch: 10 step: 537, loss is 0.05135555937886238\n",
      "epoch: 10 step: 538, loss is 0.003420666791498661\n",
      "epoch: 10 step: 539, loss is 0.003213700605556369\n",
      "epoch: 10 step: 540, loss is 0.10908679664134979\n",
      "epoch: 10 step: 541, loss is 0.002743145450949669\n",
      "epoch: 10 step: 542, loss is 0.016354050487279892\n",
      "epoch: 10 step: 543, loss is 0.018665995448827744\n",
      "epoch: 10 step: 544, loss is 0.07903638482093811\n",
      "epoch: 10 step: 545, loss is 0.0006428073393180966\n",
      "epoch: 10 step: 546, loss is 0.002285996451973915\n",
      "epoch: 10 step: 547, loss is 0.0935027152299881\n",
      "epoch: 10 step: 548, loss is 0.0064559997990727425\n",
      "epoch: 10 step: 549, loss is 0.057220980525016785\n",
      "epoch: 10 step: 550, loss is 0.06307132542133331\n",
      "epoch: 10 step: 551, loss is 0.026396362110972404\n",
      "epoch: 10 step: 552, loss is 0.021796876564621925\n",
      "epoch: 10 step: 553, loss is 0.053043875843286514\n",
      "epoch: 10 step: 554, loss is 0.002356592332944274\n",
      "epoch: 10 step: 555, loss is 8.939264807850122e-05\n",
      "epoch: 10 step: 556, loss is 0.0012063294416293502\n",
      "epoch: 10 step: 557, loss is 0.11167507618665695\n",
      "epoch: 10 step: 558, loss is 0.2735332250595093\n",
      "epoch: 10 step: 559, loss is 0.0013250326737761497\n",
      "epoch: 10 step: 560, loss is 0.0002526487223803997\n",
      "epoch: 10 step: 561, loss is 0.0017509828321635723\n",
      "epoch: 10 step: 562, loss is 0.003725598566234112\n",
      "epoch: 10 step: 563, loss is 0.08315254002809525\n",
      "epoch: 10 step: 564, loss is 0.09313476085662842\n",
      "epoch: 10 step: 565, loss is 0.0056505524553358555\n",
      "epoch: 10 step: 566, loss is 0.03232221677899361\n",
      "epoch: 10 step: 567, loss is 0.005501770880073309\n",
      "epoch: 10 step: 568, loss is 0.05785584822297096\n",
      "epoch: 10 step: 569, loss is 0.000510326586663723\n",
      "epoch: 10 step: 570, loss is 0.002848289906978607\n",
      "epoch: 10 step: 571, loss is 0.002445547841489315\n",
      "epoch: 10 step: 572, loss is 0.0021183642093092203\n",
      "epoch: 10 step: 573, loss is 0.011895895004272461\n",
      "epoch: 10 step: 574, loss is 0.0046372790820896626\n",
      "epoch: 10 step: 575, loss is 0.02188359573483467\n",
      "epoch: 10 step: 576, loss is 0.0183878056704998\n",
      "epoch: 10 step: 577, loss is 0.04236047714948654\n",
      "epoch: 10 step: 578, loss is 0.0021085261832922697\n",
      "epoch: 10 step: 579, loss is 0.00026826298562809825\n",
      "epoch: 10 step: 580, loss is 0.0061690546572208405\n",
      "epoch: 10 step: 581, loss is 0.0003358464455232024\n",
      "epoch: 10 step: 582, loss is 0.005902356933802366\n",
      "epoch: 10 step: 583, loss is 0.003227519802749157\n",
      "epoch: 10 step: 584, loss is 0.12388212978839874\n",
      "epoch: 10 step: 585, loss is 0.007708644028753042\n",
      "epoch: 10 step: 586, loss is 0.056622229516506195\n",
      "epoch: 10 step: 587, loss is 0.046464886516332626\n",
      "epoch: 10 step: 588, loss is 0.07613839209079742\n",
      "epoch: 10 step: 589, loss is 0.002364178653806448\n",
      "epoch: 10 step: 590, loss is 0.07166176289319992\n",
      "epoch: 10 step: 591, loss is 0.0006409381749108434\n",
      "epoch: 10 step: 592, loss is 0.1660565286874771\n",
      "epoch: 10 step: 593, loss is 0.0007739795255474746\n",
      "epoch: 10 step: 594, loss is 0.0008155048126354814\n",
      "epoch: 10 step: 595, loss is 0.018299981951713562\n",
      "epoch: 10 step: 596, loss is 0.002237119944766164\n",
      "epoch: 10 step: 597, loss is 0.0062871831469237804\n",
      "epoch: 10 step: 598, loss is 0.06722107529640198\n",
      "epoch: 10 step: 599, loss is 0.029900122433900833\n",
      "epoch: 10 step: 600, loss is 0.0014728063251823187\n",
      "epoch: 10 step: 601, loss is 0.0010483243968337774\n",
      "epoch: 10 step: 602, loss is 0.2975090742111206\n",
      "epoch: 10 step: 603, loss is 0.01890982687473297\n",
      "epoch: 10 step: 604, loss is 0.16075819730758667\n",
      "epoch: 10 step: 605, loss is 0.004784206859767437\n",
      "epoch: 10 step: 606, loss is 0.012744829058647156\n",
      "epoch: 10 step: 607, loss is 0.0005833915201947093\n",
      "epoch: 10 step: 608, loss is 0.005842472892254591\n",
      "epoch: 10 step: 609, loss is 0.022385450080037117\n",
      "epoch: 10 step: 610, loss is 0.008929258212447166\n",
      "epoch: 10 step: 611, loss is 0.0006553307757712901\n",
      "epoch: 10 step: 612, loss is 0.020890668034553528\n",
      "epoch: 10 step: 613, loss is 0.025717994198203087\n",
      "epoch: 10 step: 614, loss is 0.0032911179587244987\n",
      "epoch: 10 step: 615, loss is 0.008702771738171577\n",
      "epoch: 10 step: 616, loss is 0.005256357602775097\n",
      "epoch: 10 step: 617, loss is 0.011025691404938698\n",
      "epoch: 10 step: 618, loss is 0.13192839920520782\n",
      "epoch: 10 step: 619, loss is 0.018578853458166122\n",
      "epoch: 10 step: 620, loss is 0.01631612330675125\n",
      "epoch: 10 step: 621, loss is 0.006026661954820156\n",
      "epoch: 10 step: 622, loss is 0.0023704939521849155\n",
      "epoch: 10 step: 623, loss is 0.004745643585920334\n",
      "epoch: 10 step: 624, loss is 0.010888774879276752\n",
      "epoch: 10 step: 625, loss is 0.014886392280459404\n",
      "epoch: 10 step: 626, loss is 0.00027528515784069896\n",
      "epoch: 10 step: 627, loss is 0.0033857563976198435\n",
      "epoch: 10 step: 628, loss is 0.003963544964790344\n",
      "epoch: 10 step: 629, loss is 0.007233226206153631\n",
      "epoch: 10 step: 630, loss is 0.002355399075895548\n",
      "epoch: 10 step: 631, loss is 0.010755413211882114\n",
      "epoch: 10 step: 632, loss is 0.15519283711910248\n",
      "epoch: 10 step: 633, loss is 0.00948595441877842\n",
      "epoch: 10 step: 634, loss is 0.0202361810952425\n",
      "epoch: 10 step: 635, loss is 0.000400279852328822\n",
      "epoch: 10 step: 636, loss is 7.619730604346842e-05\n",
      "epoch: 10 step: 637, loss is 0.07343877106904984\n",
      "epoch: 10 step: 638, loss is 0.016208026558160782\n",
      "epoch: 10 step: 639, loss is 0.004671434871852398\n",
      "epoch: 10 step: 640, loss is 0.020636511966586113\n",
      "epoch: 10 step: 641, loss is 0.012624368071556091\n",
      "epoch: 10 step: 642, loss is 0.004457161296159029\n",
      "epoch: 10 step: 643, loss is 0.023612191900610924\n",
      "epoch: 10 step: 644, loss is 0.020495863631367683\n",
      "epoch: 10 step: 645, loss is 0.014576560817658901\n",
      "epoch: 10 step: 646, loss is 0.0003498552250675857\n",
      "epoch: 10 step: 647, loss is 0.0018199071055278182\n",
      "epoch: 10 step: 648, loss is 0.014755737967789173\n",
      "epoch: 10 step: 649, loss is 0.10005506873130798\n",
      "epoch: 10 step: 650, loss is 0.0009668925777077675\n",
      "epoch: 10 step: 651, loss is 0.0013417996233329177\n",
      "epoch: 10 step: 652, loss is 0.0022336721885949373\n",
      "epoch: 10 step: 653, loss is 0.0016482225619256496\n",
      "epoch: 10 step: 654, loss is 0.0034658967051655054\n",
      "epoch: 10 step: 655, loss is 0.001930785016156733\n",
      "epoch: 10 step: 656, loss is 0.005579712800681591\n",
      "epoch: 10 step: 657, loss is 0.004256610292941332\n",
      "epoch: 10 step: 658, loss is 0.001429394236765802\n",
      "epoch: 10 step: 659, loss is 0.0005398845532909036\n",
      "epoch: 10 step: 660, loss is 0.00040321715641766787\n",
      "epoch: 10 step: 661, loss is 0.09034421294927597\n",
      "epoch: 10 step: 662, loss is 0.0070732999593019485\n",
      "epoch: 10 step: 663, loss is 0.0018661455251276493\n",
      "epoch: 10 step: 664, loss is 0.0673307329416275\n",
      "epoch: 10 step: 665, loss is 0.0026433460880070925\n",
      "epoch: 10 step: 666, loss is 0.17712919414043427\n",
      "epoch: 10 step: 667, loss is 0.002438409486785531\n",
      "epoch: 10 step: 668, loss is 8.70664807735011e-05\n",
      "epoch: 10 step: 669, loss is 0.10158462822437286\n",
      "epoch: 10 step: 670, loss is 0.06456495076417923\n",
      "epoch: 10 step: 671, loss is 0.003145352704450488\n",
      "epoch: 10 step: 672, loss is 0.00046675215708091855\n",
      "epoch: 10 step: 673, loss is 0.00034625091939233243\n",
      "epoch: 10 step: 674, loss is 0.011746685951948166\n",
      "epoch: 10 step: 675, loss is 0.006096880882978439\n",
      "epoch: 10 step: 676, loss is 0.06508743017911911\n",
      "epoch: 10 step: 677, loss is 0.0016409188974648714\n",
      "epoch: 10 step: 678, loss is 0.003717281622812152\n",
      "epoch: 10 step: 679, loss is 0.00029742621700279415\n",
      "epoch: 10 step: 680, loss is 0.0005602099699899554\n",
      "epoch: 10 step: 681, loss is 0.0035591500345617533\n",
      "epoch: 10 step: 682, loss is 0.029119564220309258\n",
      "epoch: 10 step: 683, loss is 0.0021693946328014135\n",
      "epoch: 10 step: 684, loss is 0.003145320573821664\n",
      "epoch: 10 step: 685, loss is 0.031734347343444824\n",
      "epoch: 10 step: 686, loss is 0.005901193246245384\n",
      "epoch: 10 step: 687, loss is 0.0002519480185583234\n",
      "epoch: 10 step: 688, loss is 0.007337492890655994\n",
      "epoch: 10 step: 689, loss is 0.0804176777601242\n",
      "epoch: 10 step: 690, loss is 0.0002497730602044612\n",
      "epoch: 10 step: 691, loss is 0.002865971066057682\n",
      "epoch: 10 step: 692, loss is 0.012250781990587711\n",
      "epoch: 10 step: 693, loss is 0.00038530846359208226\n",
      "epoch: 10 step: 694, loss is 0.009059094823896885\n",
      "epoch: 10 step: 695, loss is 0.015022559091448784\n",
      "epoch: 10 step: 696, loss is 0.037404149770736694\n",
      "epoch: 10 step: 697, loss is 0.0007403924246318638\n",
      "epoch: 10 step: 698, loss is 0.02264377288520336\n",
      "epoch: 10 step: 699, loss is 0.024068232625722885\n",
      "epoch: 10 step: 700, loss is 0.1218142956495285\n",
      "epoch: 10 step: 701, loss is 0.11676313728094101\n",
      "epoch: 10 step: 702, loss is 0.0065007819794118404\n",
      "epoch: 10 step: 703, loss is 0.0008447740110568702\n",
      "epoch: 10 step: 704, loss is 0.12032140791416168\n",
      "epoch: 10 step: 705, loss is 0.08291761577129364\n",
      "epoch: 10 step: 706, loss is 0.001694964594207704\n",
      "epoch: 10 step: 707, loss is 0.1787591576576233\n",
      "epoch: 10 step: 708, loss is 0.0009374847868457437\n",
      "epoch: 10 step: 709, loss is 0.003407666925340891\n",
      "epoch: 10 step: 710, loss is 0.012155875563621521\n",
      "epoch: 10 step: 711, loss is 0.007053518667817116\n",
      "epoch: 10 step: 712, loss is 0.07874990254640579\n",
      "epoch: 10 step: 713, loss is 0.07111824303865433\n",
      "epoch: 10 step: 714, loss is 0.0004593502962961793\n",
      "epoch: 10 step: 715, loss is 0.008908357471227646\n",
      "epoch: 10 step: 716, loss is 0.028103282675147057\n",
      "epoch: 10 step: 717, loss is 0.05169960856437683\n",
      "epoch: 10 step: 718, loss is 0.0004918958875350654\n",
      "epoch: 10 step: 719, loss is 0.01587677374482155\n",
      "epoch: 10 step: 720, loss is 0.07689279317855835\n",
      "epoch: 10 step: 721, loss is 0.0002452373446431011\n",
      "epoch: 10 step: 722, loss is 0.02766953408718109\n",
      "epoch: 10 step: 723, loss is 0.005115418694913387\n",
      "epoch: 10 step: 724, loss is 0.055910829454660416\n",
      "epoch: 10 step: 725, loss is 0.00614972086623311\n",
      "epoch: 10 step: 726, loss is 0.1737121343612671\n",
      "epoch: 10 step: 727, loss is 0.16573487222194672\n",
      "epoch: 10 step: 728, loss is 0.016917839646339417\n",
      "epoch: 10 step: 729, loss is 0.003031550208106637\n",
      "epoch: 10 step: 730, loss is 0.001717724371701479\n",
      "epoch: 10 step: 731, loss is 0.18575139343738556\n",
      "epoch: 10 step: 732, loss is 0.04228636622428894\n",
      "epoch: 10 step: 733, loss is 0.004515618085861206\n",
      "epoch: 10 step: 734, loss is 0.002011998323723674\n",
      "epoch: 10 step: 735, loss is 0.15787608921527863\n",
      "epoch: 10 step: 736, loss is 0.03881561756134033\n",
      "epoch: 10 step: 737, loss is 0.014346439391374588\n",
      "epoch: 10 step: 738, loss is 0.007994165644049644\n",
      "epoch: 10 step: 739, loss is 0.1988282948732376\n",
      "epoch: 10 step: 740, loss is 0.0640658512711525\n",
      "epoch: 10 step: 741, loss is 0.0032720286399126053\n",
      "epoch: 10 step: 742, loss is 0.09068791568279266\n",
      "epoch: 10 step: 743, loss is 0.0552520826458931\n",
      "epoch: 10 step: 744, loss is 0.0068263402208685875\n",
      "epoch: 10 step: 745, loss is 0.003228740533813834\n",
      "epoch: 10 step: 746, loss is 0.09584552049636841\n",
      "epoch: 10 step: 747, loss is 0.017654092982411385\n",
      "epoch: 10 step: 748, loss is 0.0007091766456142068\n",
      "epoch: 10 step: 749, loss is 0.006855472922325134\n",
      "epoch: 10 step: 750, loss is 0.004452459514141083\n",
      "epoch: 10 step: 751, loss is 0.019707582890987396\n",
      "epoch: 10 step: 752, loss is 0.09617394208908081\n",
      "epoch: 10 step: 753, loss is 0.006878707557916641\n",
      "epoch: 10 step: 754, loss is 0.10543341189622879\n",
      "epoch: 10 step: 755, loss is 0.04482767730951309\n",
      "epoch: 10 step: 756, loss is 0.09425973892211914\n",
      "epoch: 10 step: 757, loss is 0.01902282051742077\n",
      "epoch: 10 step: 758, loss is 0.012249378487467766\n",
      "epoch: 10 step: 759, loss is 0.007793496362864971\n",
      "epoch: 10 step: 760, loss is 0.004435484763234854\n",
      "epoch: 10 step: 761, loss is 0.0040274071507155895\n",
      "epoch: 10 step: 762, loss is 0.02922414243221283\n",
      "epoch: 10 step: 763, loss is 0.020459992811083794\n",
      "epoch: 10 step: 764, loss is 0.003970366436988115\n",
      "epoch: 10 step: 765, loss is 0.020768091082572937\n",
      "epoch: 10 step: 766, loss is 0.03256618231534958\n",
      "epoch: 10 step: 767, loss is 0.01696201041340828\n",
      "epoch: 10 step: 768, loss is 0.0007261570426635444\n",
      "epoch: 10 step: 769, loss is 0.0017774727893993258\n",
      "epoch: 10 step: 770, loss is 0.009309419430792332\n",
      "epoch: 10 step: 771, loss is 0.00373678095638752\n",
      "epoch: 10 step: 772, loss is 0.012813755311071873\n",
      "epoch: 10 step: 773, loss is 0.007121472619473934\n",
      "epoch: 10 step: 774, loss is 0.0014123546425253153\n",
      "epoch: 10 step: 775, loss is 0.018579721450805664\n",
      "epoch: 10 step: 776, loss is 0.013283818028867245\n",
      "epoch: 10 step: 777, loss is 0.00017836132610682398\n",
      "epoch: 10 step: 778, loss is 0.17894108593463898\n",
      "epoch: 10 step: 779, loss is 0.08810511976480484\n",
      "epoch: 10 step: 780, loss is 0.0010425625368952751\n",
      "epoch: 10 step: 781, loss is 0.03614458441734314\n",
      "epoch: 10 step: 782, loss is 0.0015390574699267745\n",
      "epoch: 10 step: 783, loss is 0.009769509546458721\n",
      "epoch: 10 step: 784, loss is 0.002822338603436947\n",
      "epoch: 10 step: 785, loss is 0.14595745503902435\n",
      "epoch: 10 step: 786, loss is 0.012563098222017288\n",
      "epoch: 10 step: 787, loss is 0.0040840874426066875\n",
      "epoch: 10 step: 788, loss is 0.04702358692884445\n",
      "epoch: 10 step: 789, loss is 0.007549299858510494\n",
      "epoch: 10 step: 790, loss is 0.03551684692502022\n",
      "epoch: 10 step: 791, loss is 0.011271242052316666\n",
      "epoch: 10 step: 792, loss is 0.011957183480262756\n",
      "epoch: 10 step: 793, loss is 0.06979813426733017\n",
      "epoch: 10 step: 794, loss is 0.07977497577667236\n",
      "epoch: 10 step: 795, loss is 0.09033642709255219\n",
      "epoch: 10 step: 796, loss is 0.0007099264184944332\n",
      "epoch: 10 step: 797, loss is 0.05073842033743858\n",
      "epoch: 10 step: 798, loss is 0.0013942740624770522\n",
      "epoch: 10 step: 799, loss is 0.10895880311727524\n",
      "epoch: 10 step: 800, loss is 0.10310594737529755\n",
      "epoch: 10 step: 801, loss is 0.003497442463412881\n",
      "epoch: 10 step: 802, loss is 0.04879770055413246\n",
      "epoch: 10 step: 803, loss is 0.027456283569335938\n",
      "epoch: 10 step: 804, loss is 0.06311282515525818\n",
      "epoch: 10 step: 805, loss is 0.004761178977787495\n",
      "epoch: 10 step: 806, loss is 0.00828638207167387\n",
      "epoch: 10 step: 807, loss is 0.03616965189576149\n",
      "epoch: 10 step: 808, loss is 0.006910049822181463\n",
      "epoch: 10 step: 809, loss is 0.01784006878733635\n",
      "epoch: 10 step: 810, loss is 0.003885287092998624\n",
      "epoch: 10 step: 811, loss is 0.010370302014052868\n",
      "epoch: 10 step: 812, loss is 0.22606541216373444\n",
      "epoch: 10 step: 813, loss is 0.004890936426818371\n",
      "epoch: 10 step: 814, loss is 0.0024889602791517973\n",
      "epoch: 10 step: 815, loss is 0.16175968945026398\n",
      "epoch: 10 step: 816, loss is 0.09103281050920486\n",
      "epoch: 10 step: 817, loss is 0.0932299867272377\n",
      "epoch: 10 step: 818, loss is 0.004102430306375027\n",
      "epoch: 10 step: 819, loss is 0.001857559196650982\n",
      "epoch: 10 step: 820, loss is 0.003083749208599329\n",
      "epoch: 10 step: 821, loss is 0.02565525658428669\n",
      "epoch: 10 step: 822, loss is 0.1970268040895462\n",
      "epoch: 10 step: 823, loss is 0.0010910247219726443\n",
      "epoch: 10 step: 824, loss is 0.026714840903878212\n",
      "epoch: 10 step: 825, loss is 0.001975183840841055\n",
      "epoch: 10 step: 826, loss is 0.0022434485144913197\n",
      "epoch: 10 step: 827, loss is 0.009120861068367958\n",
      "epoch: 10 step: 828, loss is 0.0009789252653717995\n",
      "epoch: 10 step: 829, loss is 0.13126203417778015\n",
      "epoch: 10 step: 830, loss is 0.013290595263242722\n",
      "epoch: 10 step: 831, loss is 0.01465796958655119\n",
      "epoch: 10 step: 832, loss is 0.005440372973680496\n",
      "epoch: 10 step: 833, loss is 0.011309841647744179\n",
      "epoch: 10 step: 834, loss is 0.033590905368328094\n",
      "epoch: 10 step: 835, loss is 0.0006588983815163374\n",
      "epoch: 10 step: 836, loss is 0.007072657346725464\n",
      "epoch: 10 step: 837, loss is 0.003020116128027439\n",
      "epoch: 10 step: 838, loss is 0.001960642635822296\n",
      "epoch: 10 step: 839, loss is 0.009484309703111649\n",
      "epoch: 10 step: 840, loss is 0.002634181873872876\n",
      "epoch: 10 step: 841, loss is 0.0110021336004138\n",
      "epoch: 10 step: 842, loss is 0.001458680839277804\n",
      "epoch: 10 step: 843, loss is 0.007195058278739452\n",
      "epoch: 10 step: 844, loss is 0.0007668726611882448\n",
      "epoch: 10 step: 845, loss is 0.021130574867129326\n",
      "epoch: 10 step: 846, loss is 0.025325052440166473\n",
      "epoch: 10 step: 847, loss is 0.0010711693903431296\n",
      "epoch: 10 step: 848, loss is 0.04885438084602356\n",
      "epoch: 10 step: 849, loss is 0.0032779600005596876\n",
      "epoch: 10 step: 850, loss is 0.009740137495100498\n",
      "epoch: 10 step: 851, loss is 0.00037440762389451265\n",
      "epoch: 10 step: 852, loss is 0.054368842393159866\n",
      "epoch: 10 step: 853, loss is 0.041430287063121796\n",
      "epoch: 10 step: 854, loss is 0.0026143663562834263\n",
      "epoch: 10 step: 855, loss is 0.0003972583799622953\n",
      "epoch: 10 step: 856, loss is 0.0019557978957891464\n",
      "epoch: 10 step: 857, loss is 0.021761441603302956\n",
      "epoch: 10 step: 858, loss is 0.0002815702755469829\n",
      "epoch: 10 step: 859, loss is 0.04525533691048622\n",
      "epoch: 10 step: 860, loss is 0.016647687181830406\n",
      "epoch: 10 step: 861, loss is 0.009154981933534145\n",
      "epoch: 10 step: 862, loss is 0.008640688844025135\n",
      "epoch: 10 step: 863, loss is 0.026704663410782814\n",
      "epoch: 10 step: 864, loss is 0.00784085039049387\n",
      "epoch: 10 step: 865, loss is 5.471003532875329e-05\n",
      "epoch: 10 step: 866, loss is 0.004049885086715221\n",
      "epoch: 10 step: 867, loss is 9.109410166274756e-05\n",
      "epoch: 10 step: 868, loss is 0.05966443940997124\n",
      "epoch: 10 step: 869, loss is 0.002069350564852357\n",
      "epoch: 10 step: 870, loss is 0.026573747396469116\n",
      "epoch: 10 step: 871, loss is 0.014109542593359947\n",
      "epoch: 10 step: 872, loss is 0.02725202776491642\n",
      "epoch: 10 step: 873, loss is 0.0005645363125950098\n",
      "epoch: 10 step: 874, loss is 0.006955381017178297\n",
      "epoch: 10 step: 875, loss is 0.014032253995537758\n",
      "epoch: 10 step: 876, loss is 0.002442907774820924\n",
      "epoch: 10 step: 877, loss is 0.01333865337073803\n",
      "epoch: 10 step: 878, loss is 0.001938183675520122\n",
      "epoch: 10 step: 879, loss is 0.003319033421576023\n",
      "epoch: 10 step: 880, loss is 0.12585438787937164\n",
      "epoch: 10 step: 881, loss is 0.010851404629647732\n",
      "epoch: 10 step: 882, loss is 0.03397827595472336\n",
      "epoch: 10 step: 883, loss is 0.0011315393494442105\n",
      "epoch: 10 step: 884, loss is 0.0010664132423698902\n",
      "epoch: 10 step: 885, loss is 0.002275786828249693\n",
      "epoch: 10 step: 886, loss is 0.06233379244804382\n",
      "epoch: 10 step: 887, loss is 0.002329461043700576\n",
      "epoch: 10 step: 888, loss is 0.00023478901130147278\n",
      "epoch: 10 step: 889, loss is 0.01630684733390808\n",
      "epoch: 10 step: 890, loss is 0.0003922651521861553\n",
      "epoch: 10 step: 891, loss is 0.0007447897223755717\n",
      "epoch: 10 step: 892, loss is 0.04214402660727501\n",
      "epoch: 10 step: 893, loss is 0.011166633106768131\n",
      "epoch: 10 step: 894, loss is 0.0016417661681771278\n",
      "epoch: 10 step: 895, loss is 0.0024319596122950315\n",
      "epoch: 10 step: 896, loss is 0.005948485340923071\n",
      "epoch: 10 step: 897, loss is 0.041196059435606\n",
      "epoch: 10 step: 898, loss is 0.01371551863849163\n",
      "epoch: 10 step: 899, loss is 0.0023746355436742306\n",
      "epoch: 10 step: 900, loss is 0.14730502665042877\n",
      "epoch: 10 step: 901, loss is 0.0002035849029198289\n",
      "epoch: 10 step: 902, loss is 0.04675058647990227\n",
      "epoch: 10 step: 903, loss is 0.035398226231336594\n",
      "epoch: 10 step: 904, loss is 5.5595406593056396e-05\n",
      "epoch: 10 step: 905, loss is 0.00403225002810359\n",
      "epoch: 10 step: 906, loss is 0.0008200237643904984\n",
      "epoch: 10 step: 907, loss is 0.028505774214863777\n",
      "epoch: 10 step: 908, loss is 0.004209619015455246\n",
      "epoch: 10 step: 909, loss is 0.0032809264957904816\n",
      "epoch: 10 step: 910, loss is 0.01051110215485096\n",
      "epoch: 10 step: 911, loss is 0.0029533023480325937\n",
      "epoch: 10 step: 912, loss is 0.03773749992251396\n",
      "epoch: 10 step: 913, loss is 0.031344298273324966\n",
      "epoch: 10 step: 914, loss is 0.0018832982750609517\n",
      "epoch: 10 step: 915, loss is 0.0950527936220169\n",
      "epoch: 10 step: 916, loss is 0.0694098174571991\n",
      "epoch: 10 step: 917, loss is 0.016117386519908905\n",
      "epoch: 10 step: 918, loss is 0.0013990107690915465\n",
      "epoch: 10 step: 919, loss is 0.10475034266710281\n",
      "epoch: 10 step: 920, loss is 0.0027437726967036724\n",
      "epoch: 10 step: 921, loss is 0.0699460506439209\n",
      "epoch: 10 step: 922, loss is 0.0006155123701319098\n",
      "epoch: 10 step: 923, loss is 0.00025571277365088463\n",
      "epoch: 10 step: 924, loss is 0.007310624700039625\n",
      "epoch: 10 step: 925, loss is 0.0003604462544899434\n",
      "epoch: 10 step: 926, loss is 0.14892978966236115\n",
      "epoch: 10 step: 927, loss is 0.0014958711108192801\n",
      "epoch: 10 step: 928, loss is 0.030769193544983864\n",
      "epoch: 10 step: 929, loss is 0.038943711668252945\n",
      "epoch: 10 step: 930, loss is 0.0011233108816668391\n",
      "epoch: 10 step: 931, loss is 0.029765421524643898\n",
      "epoch: 10 step: 932, loss is 0.016106756404042244\n",
      "epoch: 10 step: 933, loss is 0.005992461461573839\n",
      "epoch: 10 step: 934, loss is 0.0030103072058409452\n",
      "epoch: 10 step: 935, loss is 0.007148010190576315\n",
      "epoch: 10 step: 936, loss is 0.023447733372449875\n",
      "epoch: 10 step: 937, loss is 3.119372922810726e-05\n",
      "epoch: 10 step: 938, loss is 0.02353430539369583\n",
      "epoch: 10 step: 939, loss is 0.05551706254482269\n",
      "epoch: 10 step: 940, loss is 0.06311587244272232\n",
      "epoch: 10 step: 941, loss is 0.0031154919415712357\n",
      "epoch: 10 step: 942, loss is 0.0009298856602981687\n",
      "epoch: 10 step: 943, loss is 0.0013489807024598122\n",
      "epoch: 10 step: 944, loss is 0.0016006060177460313\n",
      "epoch: 10 step: 945, loss is 0.0008425359847024083\n",
      "epoch: 10 step: 946, loss is 0.006218920461833477\n",
      "epoch: 10 step: 947, loss is 0.04017792269587517\n",
      "epoch: 10 step: 948, loss is 0.006664139684289694\n",
      "epoch: 10 step: 949, loss is 0.07710351794958115\n",
      "epoch: 10 step: 950, loss is 0.0035321167670190334\n",
      "epoch: 10 step: 951, loss is 0.014587042853236198\n",
      "epoch: 10 step: 952, loss is 0.004704916849732399\n",
      "epoch: 10 step: 953, loss is 0.054940834641456604\n",
      "epoch: 10 step: 954, loss is 0.001245311344973743\n",
      "epoch: 10 step: 955, loss is 0.00046221347292885184\n",
      "epoch: 10 step: 956, loss is 0.020459219813346863\n",
      "epoch: 10 step: 957, loss is 0.09941861778497696\n",
      "epoch: 10 step: 958, loss is 0.03172130137681961\n",
      "epoch: 10 step: 959, loss is 0.2685352861881256\n",
      "epoch: 10 step: 960, loss is 0.0022906814701855183\n",
      "epoch: 10 step: 961, loss is 0.001479072030633688\n",
      "epoch: 10 step: 962, loss is 0.0011827114503830671\n",
      "epoch: 10 step: 963, loss is 0.02236264944076538\n",
      "epoch: 10 step: 964, loss is 0.0038520905654877424\n",
      "epoch: 10 step: 965, loss is 0.0024054415989667177\n",
      "epoch: 10 step: 966, loss is 0.003031957894563675\n",
      "epoch: 10 step: 967, loss is 9.147785021923482e-05\n",
      "epoch: 10 step: 968, loss is 0.0029194827657192945\n",
      "epoch: 10 step: 969, loss is 0.0040593077428638935\n",
      "epoch: 10 step: 970, loss is 0.0006062475149519742\n",
      "epoch: 10 step: 971, loss is 0.046797461807727814\n",
      "epoch: 10 step: 972, loss is 0.009182899259030819\n",
      "epoch: 10 step: 973, loss is 0.04168938845396042\n",
      "epoch: 10 step: 974, loss is 0.0038932831957936287\n",
      "epoch: 10 step: 975, loss is 0.014275838620960712\n",
      "epoch: 10 step: 976, loss is 0.01383215468376875\n",
      "epoch: 10 step: 977, loss is 0.000524262199178338\n",
      "epoch: 10 step: 978, loss is 0.0007481315406039357\n",
      "epoch: 10 step: 979, loss is 0.019018668681383133\n",
      "epoch: 10 step: 980, loss is 0.06181848794221878\n",
      "epoch: 10 step: 981, loss is 0.014703758992254734\n",
      "epoch: 10 step: 982, loss is 0.09799715131521225\n",
      "epoch: 10 step: 983, loss is 0.00214622775092721\n",
      "epoch: 10 step: 984, loss is 0.04484989121556282\n",
      "epoch: 10 step: 985, loss is 0.0010595410130918026\n",
      "epoch: 10 step: 986, loss is 0.0015075969276949763\n",
      "epoch: 10 step: 987, loss is 0.1401723325252533\n",
      "epoch: 10 step: 988, loss is 0.0015288428403437138\n",
      "epoch: 10 step: 989, loss is 0.0032163928262889385\n",
      "epoch: 10 step: 990, loss is 0.011279929429292679\n",
      "epoch: 10 step: 991, loss is 0.10250551998615265\n",
      "epoch: 10 step: 992, loss is 0.014566040597856045\n",
      "epoch: 10 step: 993, loss is 3.494025440886617e-05\n",
      "epoch: 10 step: 994, loss is 0.004175427369773388\n",
      "epoch: 10 step: 995, loss is 0.028435777872800827\n",
      "epoch: 10 step: 996, loss is 0.011102507822215557\n",
      "epoch: 10 step: 997, loss is 0.050106827169656754\n",
      "epoch: 10 step: 998, loss is 0.023300953209400177\n",
      "epoch: 10 step: 999, loss is 0.00624060956761241\n",
      "epoch: 10 step: 1000, loss is 0.03520975634455681\n",
      "epoch: 10 step: 1001, loss is 0.007256853394210339\n",
      "epoch: 10 step: 1002, loss is 0.009523620828986168\n",
      "epoch: 10 step: 1003, loss is 0.002645296510308981\n",
      "epoch: 10 step: 1004, loss is 0.07304010540246964\n",
      "epoch: 10 step: 1005, loss is 0.007948470301926136\n",
      "epoch: 10 step: 1006, loss is 0.06770384311676025\n",
      "epoch: 10 step: 1007, loss is 2.509049591026269e-05\n",
      "epoch: 10 step: 1008, loss is 0.00018928734061773866\n",
      "epoch: 10 step: 1009, loss is 0.09950268268585205\n",
      "epoch: 10 step: 1010, loss is 0.0011846732813864946\n",
      "epoch: 10 step: 1011, loss is 0.05515131726861\n",
      "epoch: 10 step: 1012, loss is 0.029534608125686646\n",
      "epoch: 10 step: 1013, loss is 0.00996498391032219\n",
      "epoch: 10 step: 1014, loss is 0.01432214118540287\n",
      "epoch: 10 step: 1015, loss is 0.004666313994675875\n",
      "epoch: 10 step: 1016, loss is 0.03348928689956665\n",
      "epoch: 10 step: 1017, loss is 0.00024605606449767947\n",
      "epoch: 10 step: 1018, loss is 0.0009504447807557881\n",
      "epoch: 10 step: 1019, loss is 0.027168190106749535\n",
      "epoch: 10 step: 1020, loss is 0.02155790664255619\n",
      "epoch: 10 step: 1021, loss is 0.002052906434983015\n",
      "epoch: 10 step: 1022, loss is 0.1011475920677185\n",
      "epoch: 10 step: 1023, loss is 7.146113784983754e-05\n",
      "epoch: 10 step: 1024, loss is 0.008180323988199234\n",
      "epoch: 10 step: 1025, loss is 0.00022367577184922993\n",
      "epoch: 10 step: 1026, loss is 0.14203618466854095\n",
      "epoch: 10 step: 1027, loss is 0.17670851945877075\n",
      "epoch: 10 step: 1028, loss is 0.10966464132070541\n",
      "epoch: 10 step: 1029, loss is 0.17224572598934174\n",
      "epoch: 10 step: 1030, loss is 0.0012066654162481427\n",
      "epoch: 10 step: 1031, loss is 0.001690566074103117\n",
      "epoch: 10 step: 1032, loss is 0.008324564434587955\n",
      "epoch: 10 step: 1033, loss is 0.021586433053016663\n",
      "epoch: 10 step: 1034, loss is 0.0022676538210362196\n",
      "epoch: 10 step: 1035, loss is 0.0636298805475235\n",
      "epoch: 10 step: 1036, loss is 0.030931202694773674\n",
      "epoch: 10 step: 1037, loss is 0.008802788332104683\n",
      "epoch: 10 step: 1038, loss is 0.022789686918258667\n",
      "epoch: 10 step: 1039, loss is 0.0014937426894903183\n",
      "epoch: 10 step: 1040, loss is 0.00034535181475803256\n",
      "epoch: 10 step: 1041, loss is 0.00016052888531703502\n",
      "epoch: 10 step: 1042, loss is 0.010252208448946476\n",
      "epoch: 10 step: 1043, loss is 0.12453318387269974\n",
      "epoch: 10 step: 1044, loss is 0.020913217216730118\n",
      "epoch: 10 step: 1045, loss is 0.05489138513803482\n",
      "epoch: 10 step: 1046, loss is 0.006104073021560907\n",
      "epoch: 10 step: 1047, loss is 0.0018342692637816072\n",
      "epoch: 10 step: 1048, loss is 0.04007701575756073\n",
      "epoch: 10 step: 1049, loss is 0.0026644859462976456\n",
      "epoch: 10 step: 1050, loss is 0.017620662227272987\n",
      "epoch: 10 step: 1051, loss is 0.006974673829972744\n",
      "epoch: 10 step: 1052, loss is 0.02620912715792656\n",
      "epoch: 10 step: 1053, loss is 0.00590133061632514\n",
      "epoch: 10 step: 1054, loss is 0.03295917436480522\n",
      "epoch: 10 step: 1055, loss is 0.009540221653878689\n",
      "epoch: 10 step: 1056, loss is 0.07426239550113678\n",
      "epoch: 10 step: 1057, loss is 0.04419013112783432\n",
      "epoch: 10 step: 1058, loss is 0.030470792204141617\n",
      "epoch: 10 step: 1059, loss is 0.0010698115220293403\n",
      "epoch: 10 step: 1060, loss is 0.08299565315246582\n",
      "epoch: 10 step: 1061, loss is 0.028840351849794388\n",
      "epoch: 10 step: 1062, loss is 0.001914334250614047\n",
      "epoch: 10 step: 1063, loss is 0.04328983649611473\n",
      "epoch: 10 step: 1064, loss is 0.193808451294899\n",
      "epoch: 10 step: 1065, loss is 0.014716580510139465\n",
      "epoch: 10 step: 1066, loss is 0.0013246049638837576\n",
      "epoch: 10 step: 1067, loss is 0.07240957766771317\n",
      "epoch: 10 step: 1068, loss is 0.003880694042891264\n",
      "epoch: 10 step: 1069, loss is 0.011530481278896332\n",
      "epoch: 10 step: 1070, loss is 0.02684270776808262\n",
      "epoch: 10 step: 1071, loss is 0.0013547212583944201\n",
      "epoch: 10 step: 1072, loss is 0.022252297028899193\n",
      "epoch: 10 step: 1073, loss is 0.006656007841229439\n",
      "epoch: 10 step: 1074, loss is 0.03467603027820587\n",
      "epoch: 10 step: 1075, loss is 0.05893808603286743\n",
      "epoch: 10 step: 1076, loss is 0.029904983937740326\n",
      "epoch: 10 step: 1077, loss is 0.04856506362557411\n",
      "epoch: 10 step: 1078, loss is 0.002444211160764098\n",
      "epoch: 10 step: 1079, loss is 0.0033299943897873163\n",
      "epoch: 10 step: 1080, loss is 0.004637671168893576\n",
      "epoch: 10 step: 1081, loss is 0.013770469464361668\n",
      "epoch: 10 step: 1082, loss is 0.004184736870229244\n",
      "epoch: 10 step: 1083, loss is 0.08183876425027847\n",
      "epoch: 10 step: 1084, loss is 0.0022237307857722044\n",
      "epoch: 10 step: 1085, loss is 0.0035120949614793062\n",
      "epoch: 10 step: 1086, loss is 0.0009320643730461597\n",
      "epoch: 10 step: 1087, loss is 0.05141105502843857\n",
      "epoch: 10 step: 1088, loss is 0.0005271332338452339\n",
      "epoch: 10 step: 1089, loss is 0.030973196029663086\n",
      "epoch: 10 step: 1090, loss is 0.09881928563117981\n",
      "epoch: 10 step: 1091, loss is 0.009086532518267632\n",
      "epoch: 10 step: 1092, loss is 0.0008688099915161729\n",
      "epoch: 10 step: 1093, loss is 0.11926698684692383\n",
      "epoch: 10 step: 1094, loss is 0.005190265830606222\n",
      "epoch: 10 step: 1095, loss is 0.0010271189967170358\n",
      "epoch: 10 step: 1096, loss is 0.013821501284837723\n",
      "epoch: 10 step: 1097, loss is 0.0061683859676122665\n",
      "epoch: 10 step: 1098, loss is 0.03358376398682594\n",
      "epoch: 10 step: 1099, loss is 0.0099773108959198\n",
      "epoch: 10 step: 1100, loss is 0.0009840561542659998\n",
      "epoch: 10 step: 1101, loss is 0.05483654886484146\n",
      "epoch: 10 step: 1102, loss is 0.06932461261749268\n",
      "epoch: 10 step: 1103, loss is 0.007332575041800737\n",
      "epoch: 10 step: 1104, loss is 0.005316858645528555\n",
      "epoch: 10 step: 1105, loss is 0.051464762538671494\n",
      "epoch: 10 step: 1106, loss is 0.03622181713581085\n",
      "epoch: 10 step: 1107, loss is 0.012668226845562458\n",
      "epoch: 10 step: 1108, loss is 0.11373472958803177\n",
      "epoch: 10 step: 1109, loss is 0.00019640327082015574\n",
      "epoch: 10 step: 1110, loss is 0.005214622709900141\n",
      "epoch: 10 step: 1111, loss is 0.0009147267555817962\n",
      "epoch: 10 step: 1112, loss is 0.0010174886556342244\n",
      "epoch: 10 step: 1113, loss is 0.0007681194110773504\n",
      "epoch: 10 step: 1114, loss is 0.006327909883111715\n",
      "epoch: 10 step: 1115, loss is 0.0014399223728105426\n",
      "epoch: 10 step: 1116, loss is 0.00421086186543107\n",
      "epoch: 10 step: 1117, loss is 0.016554690897464752\n",
      "epoch: 10 step: 1118, loss is 0.0039163134060800076\n",
      "epoch: 10 step: 1119, loss is 0.0241779126226902\n",
      "epoch: 10 step: 1120, loss is 0.007162644527852535\n",
      "epoch: 10 step: 1121, loss is 0.005590449552983046\n",
      "epoch: 10 step: 1122, loss is 0.027329374104738235\n",
      "epoch: 10 step: 1123, loss is 0.00012599122419487685\n",
      "epoch: 10 step: 1124, loss is 0.16954335570335388\n",
      "epoch: 10 step: 1125, loss is 0.012543518096208572\n",
      "epoch: 10 step: 1126, loss is 0.1100219190120697\n",
      "epoch: 10 step: 1127, loss is 0.014639155939221382\n",
      "epoch: 10 step: 1128, loss is 0.010706226341426373\n",
      "epoch: 10 step: 1129, loss is 0.017679067328572273\n",
      "epoch: 10 step: 1130, loss is 0.014767404645681381\n",
      "epoch: 10 step: 1131, loss is 0.01659315451979637\n",
      "epoch: 10 step: 1132, loss is 0.061203863471746445\n",
      "epoch: 10 step: 1133, loss is 0.013654464855790138\n",
      "epoch: 10 step: 1134, loss is 0.2214237004518509\n",
      "epoch: 10 step: 1135, loss is 0.07429586350917816\n",
      "epoch: 10 step: 1136, loss is 0.009557598270475864\n",
      "epoch: 10 step: 1137, loss is 0.004113300237804651\n",
      "epoch: 10 step: 1138, loss is 0.04467696696519852\n",
      "epoch: 10 step: 1139, loss is 0.001686369301751256\n",
      "epoch: 10 step: 1140, loss is 0.01882956363260746\n",
      "epoch: 10 step: 1141, loss is 0.0030449938494712114\n",
      "epoch: 10 step: 1142, loss is 0.0023490884341299534\n",
      "epoch: 10 step: 1143, loss is 0.0217299722135067\n",
      "epoch: 10 step: 1144, loss is 0.0505005419254303\n",
      "epoch: 10 step: 1145, loss is 0.014957987703382969\n",
      "epoch: 10 step: 1146, loss is 0.05779100954532623\n",
      "epoch: 10 step: 1147, loss is 0.06856367737054825\n",
      "epoch: 10 step: 1148, loss is 0.008225567638874054\n",
      "epoch: 10 step: 1149, loss is 0.03570139780640602\n",
      "epoch: 10 step: 1150, loss is 0.06002359464764595\n",
      "epoch: 10 step: 1151, loss is 0.127298966050148\n",
      "epoch: 10 step: 1152, loss is 0.006150263361632824\n",
      "epoch: 10 step: 1153, loss is 0.06362662464380264\n",
      "epoch: 10 step: 1154, loss is 0.25308239459991455\n",
      "epoch: 10 step: 1155, loss is 0.01482334267348051\n",
      "epoch: 10 step: 1156, loss is 2.3804714146535844e-05\n",
      "epoch: 10 step: 1157, loss is 0.0012088584480807185\n",
      "epoch: 10 step: 1158, loss is 0.007450069300830364\n",
      "epoch: 10 step: 1159, loss is 0.07309935241937637\n",
      "epoch: 10 step: 1160, loss is 0.023789694532752037\n",
      "epoch: 10 step: 1161, loss is 0.0032726938370615244\n",
      "epoch: 10 step: 1162, loss is 0.007978925481438637\n",
      "epoch: 10 step: 1163, loss is 0.0034301530104130507\n",
      "epoch: 10 step: 1164, loss is 0.011558333411812782\n",
      "epoch: 10 step: 1165, loss is 0.06603634357452393\n",
      "epoch: 10 step: 1166, loss is 0.05336634814739227\n",
      "epoch: 10 step: 1167, loss is 0.033834125846624374\n",
      "epoch: 10 step: 1168, loss is 0.00027906516334041953\n",
      "epoch: 10 step: 1169, loss is 0.00035346756340004504\n",
      "epoch: 10 step: 1170, loss is 0.005258231423795223\n",
      "epoch: 10 step: 1171, loss is 0.019892625510692596\n",
      "epoch: 10 step: 1172, loss is 0.0022526199463754892\n",
      "epoch: 10 step: 1173, loss is 0.0014098412357270718\n",
      "epoch: 10 step: 1174, loss is 0.0023886647541075945\n",
      "epoch: 10 step: 1175, loss is 0.08503054082393646\n",
      "epoch: 10 step: 1176, loss is 0.16407065093517303\n",
      "epoch: 10 step: 1177, loss is 0.007406375836580992\n",
      "epoch: 10 step: 1178, loss is 0.1528102457523346\n",
      "epoch: 10 step: 1179, loss is 0.02166573889553547\n",
      "epoch: 10 step: 1180, loss is 0.027567002922296524\n",
      "epoch: 10 step: 1181, loss is 0.019290583208203316\n",
      "epoch: 10 step: 1182, loss is 0.005348265171051025\n",
      "epoch: 10 step: 1183, loss is 0.004242999479174614\n",
      "epoch: 10 step: 1184, loss is 0.014073128812015057\n",
      "epoch: 10 step: 1185, loss is 0.030524050816893578\n",
      "epoch: 10 step: 1186, loss is 0.005289373453706503\n",
      "epoch: 10 step: 1187, loss is 0.020896952599287033\n",
      "epoch: 10 step: 1188, loss is 0.09154901653528214\n",
      "epoch: 10 step: 1189, loss is 0.06148914620280266\n",
      "epoch: 10 step: 1190, loss is 0.0009155107545666397\n",
      "epoch: 10 step: 1191, loss is 0.00024127353390213102\n",
      "epoch: 10 step: 1192, loss is 0.12195897102355957\n",
      "epoch: 10 step: 1193, loss is 0.011807181872427464\n",
      "epoch: 10 step: 1194, loss is 0.0005206306232139468\n",
      "epoch: 10 step: 1195, loss is 0.008525356650352478\n",
      "epoch: 10 step: 1196, loss is 0.007709919009357691\n",
      "epoch: 10 step: 1197, loss is 0.007816639728844166\n",
      "epoch: 10 step: 1198, loss is 0.018370112404227257\n",
      "epoch: 10 step: 1199, loss is 0.010136853903532028\n",
      "epoch: 10 step: 1200, loss is 0.02272617816925049\n",
      "epoch: 10 step: 1201, loss is 0.029500262811779976\n",
      "epoch: 10 step: 1202, loss is 0.01467860396951437\n",
      "epoch: 10 step: 1203, loss is 0.009964595548808575\n",
      "epoch: 10 step: 1204, loss is 0.0027608764357864857\n",
      "epoch: 10 step: 1205, loss is 0.01460967306047678\n",
      "epoch: 10 step: 1206, loss is 0.0007003076607361436\n",
      "epoch: 10 step: 1207, loss is 0.0024132668040692806\n",
      "epoch: 10 step: 1208, loss is 0.00012020802387269214\n",
      "epoch: 10 step: 1209, loss is 0.07489187270402908\n",
      "epoch: 10 step: 1210, loss is 8.151131623890251e-05\n",
      "epoch: 10 step: 1211, loss is 0.004593780729919672\n",
      "epoch: 10 step: 1212, loss is 0.00834022369235754\n",
      "epoch: 10 step: 1213, loss is 0.0011191616067662835\n",
      "epoch: 10 step: 1214, loss is 0.0003726570284925401\n",
      "epoch: 10 step: 1215, loss is 0.0033796459902077913\n",
      "epoch: 10 step: 1216, loss is 0.00753055140376091\n",
      "epoch: 10 step: 1217, loss is 0.0005154823302291334\n",
      "epoch: 10 step: 1218, loss is 0.012092969380319118\n",
      "epoch: 10 step: 1219, loss is 0.02671775221824646\n",
      "epoch: 10 step: 1220, loss is 0.0014628780772909522\n",
      "epoch: 10 step: 1221, loss is 0.03826150670647621\n",
      "epoch: 10 step: 1222, loss is 0.002008030191063881\n",
      "epoch: 10 step: 1223, loss is 0.005305630154907703\n",
      "epoch: 10 step: 1224, loss is 0.0011191748781129718\n",
      "epoch: 10 step: 1225, loss is 0.0004071448347531259\n",
      "epoch: 10 step: 1226, loss is 6.105396460043266e-05\n",
      "epoch: 10 step: 1227, loss is 0.0009763440466485918\n",
      "epoch: 10 step: 1228, loss is 0.0075331819243729115\n",
      "epoch: 10 step: 1229, loss is 0.03124615177512169\n",
      "epoch: 10 step: 1230, loss is 0.06201272830367088\n",
      "epoch: 10 step: 1231, loss is 0.016568398103117943\n",
      "epoch: 10 step: 1232, loss is 0.08381539583206177\n",
      "epoch: 10 step: 1233, loss is 0.001431830576620996\n",
      "epoch: 10 step: 1234, loss is 0.01273505948483944\n",
      "epoch: 10 step: 1235, loss is 0.0032350586261600256\n",
      "epoch: 10 step: 1236, loss is 0.0014617366250604391\n",
      "epoch: 10 step: 1237, loss is 0.0014968705363571644\n",
      "epoch: 10 step: 1238, loss is 0.03542518988251686\n",
      "epoch: 10 step: 1239, loss is 0.0020525804720818996\n",
      "epoch: 10 step: 1240, loss is 0.0003687260905280709\n",
      "epoch: 10 step: 1241, loss is 0.005862923339009285\n",
      "epoch: 10 step: 1242, loss is 0.00043418785207904875\n",
      "epoch: 10 step: 1243, loss is 0.17162483930587769\n",
      "epoch: 10 step: 1244, loss is 0.024223757907748222\n",
      "epoch: 10 step: 1245, loss is 0.00031948162359185517\n",
      "epoch: 10 step: 1246, loss is 0.12523649632930756\n",
      "epoch: 10 step: 1247, loss is 0.12951073050498962\n",
      "epoch: 10 step: 1248, loss is 0.001606180565431714\n",
      "epoch: 10 step: 1249, loss is 0.00039683596696704626\n",
      "epoch: 10 step: 1250, loss is 0.08463054150342941\n",
      "epoch: 10 step: 1251, loss is 0.1835184246301651\n",
      "epoch: 10 step: 1252, loss is 0.010029512457549572\n",
      "epoch: 10 step: 1253, loss is 0.001005382975563407\n",
      "epoch: 10 step: 1254, loss is 0.00043977907625958323\n",
      "epoch: 10 step: 1255, loss is 0.0005185508052818477\n",
      "epoch: 10 step: 1256, loss is 0.018300088122487068\n",
      "epoch: 10 step: 1257, loss is 0.01425404753535986\n",
      "epoch: 10 step: 1258, loss is 0.1380237340927124\n",
      "epoch: 10 step: 1259, loss is 0.019273467361927032\n",
      "epoch: 10 step: 1260, loss is 0.00030221641645766795\n",
      "epoch: 10 step: 1261, loss is 0.012077451683580875\n",
      "epoch: 10 step: 1262, loss is 0.10907965898513794\n",
      "epoch: 10 step: 1263, loss is 0.0004587308503687382\n",
      "epoch: 10 step: 1264, loss is 0.0012680479558184743\n",
      "epoch: 10 step: 1265, loss is 0.01767718978226185\n",
      "epoch: 10 step: 1266, loss is 0.004442570731043816\n",
      "epoch: 10 step: 1267, loss is 0.009933672845363617\n",
      "epoch: 10 step: 1268, loss is 0.07484818249940872\n",
      "epoch: 10 step: 1269, loss is 0.007466203533113003\n",
      "epoch: 10 step: 1270, loss is 0.006148518994450569\n",
      "epoch: 10 step: 1271, loss is 0.070036381483078\n",
      "epoch: 10 step: 1272, loss is 0.1200171485543251\n",
      "epoch: 10 step: 1273, loss is 0.11079823225736618\n",
      "epoch: 10 step: 1274, loss is 0.009012808091938496\n",
      "epoch: 10 step: 1275, loss is 0.002724322024732828\n",
      "epoch: 10 step: 1276, loss is 0.03428728133440018\n",
      "epoch: 10 step: 1277, loss is 0.0005671432008966804\n",
      "epoch: 10 step: 1278, loss is 0.006408922839909792\n",
      "epoch: 10 step: 1279, loss is 0.18245084583759308\n",
      "epoch: 10 step: 1280, loss is 0.0023577900137752295\n",
      "epoch: 10 step: 1281, loss is 0.003893192857503891\n",
      "epoch: 10 step: 1282, loss is 0.3208925724029541\n",
      "epoch: 10 step: 1283, loss is 0.011151408776640892\n",
      "epoch: 10 step: 1284, loss is 0.0011508942116051912\n",
      "epoch: 10 step: 1285, loss is 0.0023062347900122404\n",
      "epoch: 10 step: 1286, loss is 0.003257719101384282\n",
      "epoch: 10 step: 1287, loss is 0.02535506710410118\n",
      "epoch: 10 step: 1288, loss is 0.03363155946135521\n",
      "epoch: 10 step: 1289, loss is 0.07524868100881577\n",
      "epoch: 10 step: 1290, loss is 0.05549831688404083\n",
      "epoch: 10 step: 1291, loss is 0.00024008612672332674\n",
      "epoch: 10 step: 1292, loss is 0.07610645890235901\n",
      "epoch: 10 step: 1293, loss is 0.036342814564704895\n",
      "epoch: 10 step: 1294, loss is 0.002279424574226141\n",
      "epoch: 10 step: 1295, loss is 0.0012781708501279354\n",
      "epoch: 10 step: 1296, loss is 0.0034685672726482153\n",
      "epoch: 10 step: 1297, loss is 0.10693744570016861\n",
      "epoch: 10 step: 1298, loss is 0.23514264822006226\n",
      "epoch: 10 step: 1299, loss is 0.004139230120927095\n",
      "epoch: 10 step: 1300, loss is 0.06073480099439621\n",
      "epoch: 10 step: 1301, loss is 0.0019037938909605145\n",
      "epoch: 10 step: 1302, loss is 0.2936803996562958\n",
      "epoch: 10 step: 1303, loss is 0.000725825026165694\n",
      "epoch: 10 step: 1304, loss is 0.1289258748292923\n",
      "epoch: 10 step: 1305, loss is 0.024835238233208656\n",
      "epoch: 10 step: 1306, loss is 0.004995445720851421\n",
      "epoch: 10 step: 1307, loss is 0.0012693603057414293\n",
      "epoch: 10 step: 1308, loss is 0.1256573498249054\n",
      "epoch: 10 step: 1309, loss is 0.17600701749324799\n",
      "epoch: 10 step: 1310, loss is 0.047148942947387695\n",
      "epoch: 10 step: 1311, loss is 0.0010009512770920992\n",
      "epoch: 10 step: 1312, loss is 0.050120364874601364\n",
      "epoch: 10 step: 1313, loss is 0.018184857442975044\n",
      "epoch: 10 step: 1314, loss is 0.004722086247056723\n",
      "epoch: 10 step: 1315, loss is 0.17987112700939178\n",
      "epoch: 10 step: 1316, loss is 0.04890621826052666\n",
      "epoch: 10 step: 1317, loss is 0.03992068022489548\n",
      "epoch: 10 step: 1318, loss is 0.0012631203280761838\n",
      "epoch: 10 step: 1319, loss is 0.013393634930253029\n",
      "epoch: 10 step: 1320, loss is 0.019035324454307556\n",
      "epoch: 10 step: 1321, loss is 0.0006790453917346895\n",
      "epoch: 10 step: 1322, loss is 0.003982034046202898\n",
      "epoch: 10 step: 1323, loss is 0.0015093163819983602\n",
      "epoch: 10 step: 1324, loss is 0.001309216022491455\n",
      "epoch: 10 step: 1325, loss is 0.011738539673388004\n",
      "epoch: 10 step: 1326, loss is 0.027155689895153046\n",
      "epoch: 10 step: 1327, loss is 0.0024982597678899765\n",
      "epoch: 10 step: 1328, loss is 0.021038983017206192\n",
      "epoch: 10 step: 1329, loss is 0.01066366396844387\n",
      "epoch: 10 step: 1330, loss is 0.48794025182724\n",
      "epoch: 10 step: 1331, loss is 0.14852873980998993\n",
      "epoch: 10 step: 1332, loss is 0.0031851613894104958\n",
      "epoch: 10 step: 1333, loss is 0.1632053703069687\n",
      "epoch: 10 step: 1334, loss is 0.0007263608858920634\n",
      "epoch: 10 step: 1335, loss is 0.2156965583562851\n",
      "epoch: 10 step: 1336, loss is 0.0026410208083689213\n",
      "epoch: 10 step: 1337, loss is 0.17892171442508698\n",
      "epoch: 10 step: 1338, loss is 0.0007233657524921\n",
      "epoch: 10 step: 1339, loss is 0.11568215489387512\n",
      "epoch: 10 step: 1340, loss is 0.006555890664458275\n",
      "epoch: 10 step: 1341, loss is 0.0056999446824193\n",
      "epoch: 10 step: 1342, loss is 0.03415750712156296\n",
      "epoch: 10 step: 1343, loss is 0.004872290883213282\n",
      "epoch: 10 step: 1344, loss is 0.022352997213602066\n",
      "epoch: 10 step: 1345, loss is 0.007941124960780144\n",
      "epoch: 10 step: 1346, loss is 0.020646870136260986\n",
      "epoch: 10 step: 1347, loss is 0.004743637982755899\n",
      "epoch: 10 step: 1348, loss is 0.0041807107627391815\n",
      "epoch: 10 step: 1349, loss is 0.0056867715902626514\n",
      "epoch: 10 step: 1350, loss is 0.20557089149951935\n",
      "epoch: 10 step: 1351, loss is 0.038208670914173126\n",
      "epoch: 10 step: 1352, loss is 0.007642551325261593\n",
      "epoch: 10 step: 1353, loss is 0.03525061905384064\n",
      "epoch: 10 step: 1354, loss is 0.018075907602906227\n",
      "epoch: 10 step: 1355, loss is 0.0046656252816319466\n",
      "epoch: 10 step: 1356, loss is 0.007778141647577286\n",
      "epoch: 10 step: 1357, loss is 0.04166681319475174\n",
      "epoch: 10 step: 1358, loss is 0.009927473030984402\n",
      "epoch: 10 step: 1359, loss is 0.053673580288887024\n",
      "epoch: 10 step: 1360, loss is 0.0011110255727544427\n",
      "epoch: 10 step: 1361, loss is 0.0009487581555731595\n",
      "epoch: 10 step: 1362, loss is 0.03454507887363434\n",
      "epoch: 10 step: 1363, loss is 0.000949953799135983\n",
      "epoch: 10 step: 1364, loss is 0.04906831309199333\n",
      "epoch: 10 step: 1365, loss is 0.004128089640289545\n",
      "epoch: 10 step: 1366, loss is 0.0007721356232650578\n",
      "epoch: 10 step: 1367, loss is 0.00497988099232316\n",
      "epoch: 10 step: 1368, loss is 0.006504723336547613\n",
      "epoch: 10 step: 1369, loss is 0.05397487059235573\n",
      "epoch: 10 step: 1370, loss is 0.008687605150043964\n",
      "epoch: 10 step: 1371, loss is 0.0003337813832331449\n",
      "epoch: 10 step: 1372, loss is 0.004422897472977638\n",
      "epoch: 10 step: 1373, loss is 0.03148648515343666\n",
      "epoch: 10 step: 1374, loss is 0.029440462589263916\n",
      "epoch: 10 step: 1375, loss is 0.014755859039723873\n",
      "epoch: 10 step: 1376, loss is 0.028995556756854057\n",
      "epoch: 10 step: 1377, loss is 0.01581484079360962\n",
      "epoch: 10 step: 1378, loss is 0.00026681245071813464\n",
      "epoch: 10 step: 1379, loss is 0.00031167478300631046\n",
      "epoch: 10 step: 1380, loss is 0.09369470924139023\n",
      "epoch: 10 step: 1381, loss is 0.004595011007040739\n",
      "epoch: 10 step: 1382, loss is 0.0031875474378466606\n",
      "epoch: 10 step: 1383, loss is 0.0006489520310424268\n",
      "epoch: 10 step: 1384, loss is 0.0012872713850811124\n",
      "epoch: 10 step: 1385, loss is 0.03580616042017937\n",
      "epoch: 10 step: 1386, loss is 0.17181731760501862\n",
      "epoch: 10 step: 1387, loss is 0.018035316839814186\n",
      "epoch: 10 step: 1388, loss is 0.0025873370468616486\n",
      "epoch: 10 step: 1389, loss is 0.010448169894516468\n",
      "epoch: 10 step: 1390, loss is 0.001112288678996265\n",
      "epoch: 10 step: 1391, loss is 0.01275345217436552\n",
      "epoch: 10 step: 1392, loss is 0.018746444955468178\n",
      "epoch: 10 step: 1393, loss is 0.056440602988004684\n",
      "epoch: 10 step: 1394, loss is 0.01859435625374317\n",
      "epoch: 10 step: 1395, loss is 0.05550406128168106\n",
      "epoch: 10 step: 1396, loss is 0.006407265551388264\n",
      "epoch: 10 step: 1397, loss is 0.024413181468844414\n",
      "epoch: 10 step: 1398, loss is 0.0034104103688150644\n",
      "epoch: 10 step: 1399, loss is 0.0023488239385187626\n",
      "epoch: 10 step: 1400, loss is 0.0005521599086932838\n",
      "epoch: 10 step: 1401, loss is 0.008571670390665531\n",
      "epoch: 10 step: 1402, loss is 0.1748967170715332\n",
      "epoch: 10 step: 1403, loss is 0.016488146036863327\n",
      "epoch: 10 step: 1404, loss is 0.0008101302664726973\n",
      "epoch: 10 step: 1405, loss is 0.004008785355836153\n",
      "epoch: 10 step: 1406, loss is 0.021002426743507385\n",
      "epoch: 10 step: 1407, loss is 0.1228460818529129\n",
      "epoch: 10 step: 1408, loss is 0.009799312800168991\n",
      "epoch: 10 step: 1409, loss is 0.18610841035842896\n",
      "epoch: 10 step: 1410, loss is 0.0002808622666634619\n",
      "epoch: 10 step: 1411, loss is 0.043306540697813034\n",
      "epoch: 10 step: 1412, loss is 0.03423914685845375\n",
      "epoch: 10 step: 1413, loss is 0.1832987368106842\n",
      "epoch: 10 step: 1414, loss is 0.008643068373203278\n",
      "epoch: 10 step: 1415, loss is 0.0063954791985452175\n",
      "epoch: 10 step: 1416, loss is 0.09139327704906464\n",
      "epoch: 10 step: 1417, loss is 0.004830090329051018\n",
      "epoch: 10 step: 1418, loss is 0.025857236236333847\n",
      "epoch: 10 step: 1419, loss is 0.001134492107667029\n",
      "epoch: 10 step: 1420, loss is 0.004154536873102188\n",
      "epoch: 10 step: 1421, loss is 0.008091982454061508\n",
      "epoch: 10 step: 1422, loss is 0.0002384931722190231\n",
      "epoch: 10 step: 1423, loss is 0.03117053583264351\n",
      "epoch: 10 step: 1424, loss is 0.023313604295253754\n",
      "epoch: 10 step: 1425, loss is 0.0010385960340499878\n",
      "epoch: 10 step: 1426, loss is 0.027684753760695457\n",
      "epoch: 10 step: 1427, loss is 0.15226039290428162\n",
      "epoch: 10 step: 1428, loss is 0.0011102142743766308\n",
      "epoch: 10 step: 1429, loss is 0.13157233595848083\n",
      "epoch: 10 step: 1430, loss is 0.03316451981663704\n",
      "epoch: 10 step: 1431, loss is 0.08790718764066696\n",
      "epoch: 10 step: 1432, loss is 0.00783808808773756\n",
      "epoch: 10 step: 1433, loss is 0.0007971579325385392\n",
      "epoch: 10 step: 1434, loss is 0.0051963613368570805\n",
      "epoch: 10 step: 1435, loss is 0.14520351588726044\n",
      "epoch: 10 step: 1436, loss is 0.0035665908362716436\n",
      "epoch: 10 step: 1437, loss is 0.04972466453909874\n",
      "epoch: 10 step: 1438, loss is 0.04005415365099907\n",
      "epoch: 10 step: 1439, loss is 0.06735681742429733\n",
      "epoch: 10 step: 1440, loss is 0.04358918219804764\n",
      "epoch: 10 step: 1441, loss is 0.0008809959981590509\n",
      "epoch: 10 step: 1442, loss is 0.00019337433332111686\n",
      "epoch: 10 step: 1443, loss is 0.004229758400470018\n",
      "epoch: 10 step: 1444, loss is 0.013662060722708702\n",
      "epoch: 10 step: 1445, loss is 0.08575188368558884\n",
      "epoch: 10 step: 1446, loss is 0.03390679135918617\n",
      "epoch: 10 step: 1447, loss is 0.011822603642940521\n",
      "epoch: 10 step: 1448, loss is 0.00043701118556782603\n",
      "epoch: 10 step: 1449, loss is 0.016885483637452126\n",
      "epoch: 10 step: 1450, loss is 0.051866304129362106\n",
      "epoch: 10 step: 1451, loss is 0.0008042049594223499\n",
      "epoch: 10 step: 1452, loss is 0.0023368687834590673\n",
      "epoch: 10 step: 1453, loss is 0.02422100119292736\n",
      "epoch: 10 step: 1454, loss is 0.003183159278705716\n",
      "epoch: 10 step: 1455, loss is 0.04674645513296127\n",
      "epoch: 10 step: 1456, loss is 0.056304097175598145\n",
      "epoch: 10 step: 1457, loss is 0.07713765650987625\n",
      "epoch: 10 step: 1458, loss is 0.002021829131990671\n",
      "epoch: 10 step: 1459, loss is 0.03864693269133568\n",
      "epoch: 10 step: 1460, loss is 0.00784411933273077\n",
      "epoch: 10 step: 1461, loss is 0.0012566667282953858\n",
      "epoch: 10 step: 1462, loss is 0.005229777190834284\n",
      "epoch: 10 step: 1463, loss is 0.008487912826240063\n",
      "epoch: 10 step: 1464, loss is 0.14809665083885193\n",
      "epoch: 10 step: 1465, loss is 0.06491494923830032\n",
      "epoch: 10 step: 1466, loss is 0.007270431146025658\n",
      "epoch: 10 step: 1467, loss is 0.011768116615712643\n",
      "epoch: 10 step: 1468, loss is 0.0043503036722540855\n",
      "epoch: 10 step: 1469, loss is 0.10585762560367584\n",
      "epoch: 10 step: 1470, loss is 0.24472209811210632\n",
      "epoch: 10 step: 1471, loss is 0.00043736284715123475\n",
      "epoch: 10 step: 1472, loss is 0.0018799345707520843\n",
      "epoch: 10 step: 1473, loss is 0.01753110997378826\n",
      "epoch: 10 step: 1474, loss is 0.07154074311256409\n",
      "epoch: 10 step: 1475, loss is 0.00027064437745139003\n",
      "epoch: 10 step: 1476, loss is 0.001777364988811314\n",
      "epoch: 10 step: 1477, loss is 0.06310278177261353\n",
      "epoch: 10 step: 1478, loss is 0.004170698579400778\n",
      "epoch: 10 step: 1479, loss is 0.00035060581285506487\n",
      "epoch: 10 step: 1480, loss is 0.006140467710793018\n",
      "epoch: 10 step: 1481, loss is 0.0006792439962737262\n",
      "epoch: 10 step: 1482, loss is 0.06706760078668594\n",
      "epoch: 10 step: 1483, loss is 0.0050658672116696835\n",
      "epoch: 10 step: 1484, loss is 0.00071080302586779\n",
      "epoch: 10 step: 1485, loss is 0.001085593132302165\n",
      "epoch: 10 step: 1486, loss is 0.01219686958938837\n",
      "epoch: 10 step: 1487, loss is 0.0009996019070968032\n",
      "epoch: 10 step: 1488, loss is 4.319334038882516e-05\n",
      "epoch: 10 step: 1489, loss is 0.0010484537342563272\n",
      "epoch: 10 step: 1490, loss is 0.0001419833133695647\n",
      "epoch: 10 step: 1491, loss is 0.0019093790324404836\n",
      "epoch: 10 step: 1492, loss is 0.004134327173233032\n",
      "epoch: 10 step: 1493, loss is 0.09167949855327606\n",
      "epoch: 10 step: 1494, loss is 0.01931324228644371\n",
      "epoch: 10 step: 1495, loss is 0.09513590484857559\n",
      "epoch: 10 step: 1496, loss is 0.0008931593620218337\n",
      "epoch: 10 step: 1497, loss is 0.0009736704523675144\n",
      "epoch: 10 step: 1498, loss is 0.0006190668791532516\n",
      "epoch: 10 step: 1499, loss is 0.0017601312138140202\n",
      "epoch: 10 step: 1500, loss is 0.12561801075935364\n",
      "epoch: 10 step: 1501, loss is 0.046242568641901016\n",
      "epoch: 10 step: 1502, loss is 0.0003835036768577993\n",
      "epoch: 10 step: 1503, loss is 6.991391273913905e-05\n",
      "epoch: 10 step: 1504, loss is 0.022204171866178513\n",
      "epoch: 10 step: 1505, loss is 0.017912987619638443\n",
      "epoch: 10 step: 1506, loss is 0.012631106190383434\n",
      "epoch: 10 step: 1507, loss is 0.024545002728700638\n",
      "epoch: 10 step: 1508, loss is 0.024575116112828255\n",
      "epoch: 10 step: 1509, loss is 0.020342472940683365\n",
      "epoch: 10 step: 1510, loss is 0.009842388331890106\n",
      "epoch: 10 step: 1511, loss is 0.0008887667208909988\n",
      "epoch: 10 step: 1512, loss is 0.10223037749528885\n",
      "epoch: 10 step: 1513, loss is 0.009799657389521599\n",
      "epoch: 10 step: 1514, loss is 0.0036534490063786507\n",
      "epoch: 10 step: 1515, loss is 0.06791692227125168\n",
      "epoch: 10 step: 1516, loss is 0.10304292291402817\n",
      "epoch: 10 step: 1517, loss is 0.01578862965106964\n",
      "epoch: 10 step: 1518, loss is 0.07925155013799667\n",
      "epoch: 10 step: 1519, loss is 0.003169764531776309\n",
      "epoch: 10 step: 1520, loss is 0.05388608202338219\n",
      "epoch: 10 step: 1521, loss is 0.0019185106502845883\n",
      "epoch: 10 step: 1522, loss is 0.001797245116904378\n",
      "epoch: 10 step: 1523, loss is 0.13658712804317474\n",
      "epoch: 10 step: 1524, loss is 0.02573045901954174\n",
      "epoch: 10 step: 1525, loss is 0.0006854336243122816\n",
      "epoch: 10 step: 1526, loss is 0.02353564277291298\n",
      "epoch: 10 step: 1527, loss is 0.0039240033365786076\n",
      "epoch: 10 step: 1528, loss is 0.001967370044440031\n",
      "epoch: 10 step: 1529, loss is 0.0007062407676130533\n",
      "epoch: 10 step: 1530, loss is 0.02683001011610031\n",
      "epoch: 10 step: 1531, loss is 0.0006401898572221398\n",
      "epoch: 10 step: 1532, loss is 0.0012354656355455518\n",
      "epoch: 10 step: 1533, loss is 0.037808824330568314\n",
      "epoch: 10 step: 1534, loss is 0.07692883908748627\n",
      "epoch: 10 step: 1535, loss is 0.12144237756729126\n",
      "epoch: 10 step: 1536, loss is 0.0018799116369336843\n",
      "epoch: 10 step: 1537, loss is 0.07326860725879669\n",
      "epoch: 10 step: 1538, loss is 0.0006888706120662391\n",
      "epoch: 10 step: 1539, loss is 0.0036920215934515\n",
      "epoch: 10 step: 1540, loss is 0.0015805732691660523\n",
      "epoch: 10 step: 1541, loss is 0.008866549469530582\n",
      "epoch: 10 step: 1542, loss is 0.00020102952839806676\n",
      "epoch: 10 step: 1543, loss is 0.13864290714263916\n",
      "epoch: 10 step: 1544, loss is 0.028770679607987404\n",
      "epoch: 10 step: 1545, loss is 0.0063828155398368835\n",
      "epoch: 10 step: 1546, loss is 0.04788073152303696\n",
      "epoch: 10 step: 1547, loss is 0.017760660499334335\n",
      "epoch: 10 step: 1548, loss is 0.032937899231910706\n",
      "epoch: 10 step: 1549, loss is 0.0030451060738414526\n",
      "epoch: 10 step: 1550, loss is 0.0017952462658286095\n",
      "epoch: 10 step: 1551, loss is 0.002230964368209243\n",
      "epoch: 10 step: 1552, loss is 0.0003132879501208663\n",
      "epoch: 10 step: 1553, loss is 0.010618038475513458\n",
      "epoch: 10 step: 1554, loss is 0.00027521714218892157\n",
      "epoch: 10 step: 1555, loss is 0.12782852351665497\n",
      "epoch: 10 step: 1556, loss is 0.0114473607391119\n",
      "epoch: 10 step: 1557, loss is 0.02215360291302204\n",
      "epoch: 10 step: 1558, loss is 0.006280325353145599\n",
      "epoch: 10 step: 1559, loss is 0.0009511653915978968\n",
      "epoch: 10 step: 1560, loss is 0.00597446458414197\n",
      "epoch: 10 step: 1561, loss is 0.003219613106921315\n",
      "epoch: 10 step: 1562, loss is 0.0056889052502810955\n",
      "epoch: 10 step: 1563, loss is 6.032801320543513e-05\n",
      "epoch: 10 step: 1564, loss is 0.006885220762342215\n",
      "epoch: 10 step: 1565, loss is 0.0050988635048270226\n",
      "epoch: 10 step: 1566, loss is 0.007473655976355076\n",
      "epoch: 10 step: 1567, loss is 0.01710987463593483\n",
      "epoch: 10 step: 1568, loss is 0.02914792113006115\n",
      "epoch: 10 step: 1569, loss is 0.04439172521233559\n",
      "epoch: 10 step: 1570, loss is 0.0006551908445544541\n",
      "epoch: 10 step: 1571, loss is 0.000278268737019971\n",
      "epoch: 10 step: 1572, loss is 0.005560251418501139\n",
      "epoch: 10 step: 1573, loss is 0.00013297796249389648\n",
      "epoch: 10 step: 1574, loss is 0.00044515987974591553\n",
      "epoch: 10 step: 1575, loss is 0.07658272236585617\n",
      "epoch: 10 step: 1576, loss is 0.0003453536774031818\n",
      "epoch: 10 step: 1577, loss is 0.008338534273207188\n",
      "epoch: 10 step: 1578, loss is 0.0026162085123360157\n",
      "epoch: 10 step: 1579, loss is 0.005883814301341772\n",
      "epoch: 10 step: 1580, loss is 0.002120905090123415\n",
      "epoch: 10 step: 1581, loss is 0.00015321988030336797\n",
      "epoch: 10 step: 1582, loss is 0.001156938960775733\n",
      "epoch: 10 step: 1583, loss is 0.002447778359055519\n",
      "epoch: 10 step: 1584, loss is 0.010025990195572376\n",
      "epoch: 10 step: 1585, loss is 0.0004375676508061588\n",
      "epoch: 10 step: 1586, loss is 0.0030925970058888197\n",
      "epoch: 10 step: 1587, loss is 3.07726368191652e-05\n",
      "epoch: 10 step: 1588, loss is 0.047474924474954605\n",
      "epoch: 10 step: 1589, loss is 0.00035342108458280563\n",
      "epoch: 10 step: 1590, loss is 0.05164692550897598\n",
      "epoch: 10 step: 1591, loss is 0.006810660939663649\n",
      "epoch: 10 step: 1592, loss is 0.05824211239814758\n",
      "epoch: 10 step: 1593, loss is 0.021972376853227615\n",
      "epoch: 10 step: 1594, loss is 0.008300707675516605\n",
      "epoch: 10 step: 1595, loss is 0.0010411880211904645\n",
      "epoch: 10 step: 1596, loss is 0.0372919887304306\n",
      "epoch: 10 step: 1597, loss is 0.0002050737093668431\n",
      "epoch: 10 step: 1598, loss is 0.008356806822121143\n",
      "epoch: 10 step: 1599, loss is 0.018038000911474228\n",
      "epoch: 10 step: 1600, loss is 0.0031451566610485315\n",
      "epoch: 10 step: 1601, loss is 0.0016008465318009257\n",
      "epoch: 10 step: 1602, loss is 0.020592741668224335\n",
      "epoch: 10 step: 1603, loss is 0.0006921900785528123\n",
      "epoch: 10 step: 1604, loss is 0.012807313352823257\n",
      "epoch: 10 step: 1605, loss is 0.04157644510269165\n",
      "epoch: 10 step: 1606, loss is 0.011344661004841328\n",
      "epoch: 10 step: 1607, loss is 0.026216929778456688\n",
      "epoch: 10 step: 1608, loss is 0.006925343535840511\n",
      "epoch: 10 step: 1609, loss is 0.07610057294368744\n",
      "epoch: 10 step: 1610, loss is 0.0003049714141525328\n",
      "epoch: 10 step: 1611, loss is 0.00033682299545034766\n",
      "epoch: 10 step: 1612, loss is 0.003221616381779313\n",
      "epoch: 10 step: 1613, loss is 0.003342241048812866\n",
      "epoch: 10 step: 1614, loss is 0.01355497632175684\n",
      "epoch: 10 step: 1615, loss is 0.0027285791002213955\n",
      "epoch: 10 step: 1616, loss is 0.014157426543533802\n",
      "epoch: 10 step: 1617, loss is 0.027124179527163506\n",
      "epoch: 10 step: 1618, loss is 0.0008158331620506942\n",
      "epoch: 10 step: 1619, loss is 0.12430182844400406\n",
      "epoch: 10 step: 1620, loss is 0.0038692369125783443\n",
      "epoch: 10 step: 1621, loss is 0.0026234828401356936\n",
      "epoch: 10 step: 1622, loss is 0.021406106650829315\n",
      "epoch: 10 step: 1623, loss is 0.019624734297394753\n",
      "epoch: 10 step: 1624, loss is 0.1945628970861435\n",
      "epoch: 10 step: 1625, loss is 0.05117284134030342\n",
      "epoch: 10 step: 1626, loss is 0.009560458362102509\n",
      "epoch: 10 step: 1627, loss is 0.009529641829431057\n",
      "epoch: 10 step: 1628, loss is 0.0020281102042645216\n",
      "epoch: 10 step: 1629, loss is 0.0878191590309143\n",
      "epoch: 10 step: 1630, loss is 0.0038531189784407616\n",
      "epoch: 10 step: 1631, loss is 0.0004677676479332149\n",
      "epoch: 10 step: 1632, loss is 0.008559845387935638\n",
      "epoch: 10 step: 1633, loss is 0.01005555223673582\n",
      "epoch: 10 step: 1634, loss is 0.011495751328766346\n",
      "epoch: 10 step: 1635, loss is 0.0005594664253294468\n",
      "epoch: 10 step: 1636, loss is 0.007329542189836502\n",
      "epoch: 10 step: 1637, loss is 0.18473926186561584\n",
      "epoch: 10 step: 1638, loss is 0.013247844763100147\n",
      "epoch: 10 step: 1639, loss is 0.0014130867784842849\n",
      "epoch: 10 step: 1640, loss is 0.0012918295105919242\n",
      "epoch: 10 step: 1641, loss is 0.009076284244656563\n",
      "epoch: 10 step: 1642, loss is 0.01206347905099392\n",
      "epoch: 10 step: 1643, loss is 0.0019387465436011553\n",
      "epoch: 10 step: 1644, loss is 0.10847203433513641\n",
      "epoch: 10 step: 1645, loss is 0.000702611927408725\n",
      "epoch: 10 step: 1646, loss is 0.023277489468455315\n",
      "epoch: 10 step: 1647, loss is 0.00887942686676979\n",
      "epoch: 10 step: 1648, loss is 0.0006831936771050096\n",
      "epoch: 10 step: 1649, loss is 0.06917089968919754\n",
      "epoch: 10 step: 1650, loss is 0.0002914339420385659\n",
      "epoch: 10 step: 1651, loss is 0.0067083886824548244\n",
      "epoch: 10 step: 1652, loss is 0.0001287543127546087\n",
      "epoch: 10 step: 1653, loss is 0.004307514987885952\n",
      "epoch: 10 step: 1654, loss is 0.0901675820350647\n",
      "epoch: 10 step: 1655, loss is 0.053739745169878006\n",
      "epoch: 10 step: 1656, loss is 0.003017983166500926\n",
      "epoch: 10 step: 1657, loss is 0.199550062417984\n",
      "epoch: 10 step: 1658, loss is 0.05054409056901932\n",
      "epoch: 10 step: 1659, loss is 0.018466318026185036\n",
      "epoch: 10 step: 1660, loss is 0.004750540945678949\n",
      "epoch: 10 step: 1661, loss is 0.03274238854646683\n",
      "epoch: 10 step: 1662, loss is 0.023115966469049454\n",
      "epoch: 10 step: 1663, loss is 0.11340656131505966\n",
      "epoch: 10 step: 1664, loss is 0.0040373411029577255\n",
      "epoch: 10 step: 1665, loss is 0.00102635205257684\n",
      "epoch: 10 step: 1666, loss is 0.015192692168056965\n",
      "epoch: 10 step: 1667, loss is 0.26412472128868103\n",
      "epoch: 10 step: 1668, loss is 2.0668996512540616e-05\n",
      "epoch: 10 step: 1669, loss is 0.00015135294233914465\n",
      "epoch: 10 step: 1670, loss is 0.00047047415864653885\n",
      "epoch: 10 step: 1671, loss is 0.002103253500536084\n",
      "epoch: 10 step: 1672, loss is 0.06253767758607864\n",
      "epoch: 10 step: 1673, loss is 0.03546956554055214\n",
      "epoch: 10 step: 1674, loss is 0.0006597989704459906\n",
      "epoch: 10 step: 1675, loss is 0.0022637799847871065\n",
      "epoch: 10 step: 1676, loss is 0.007097336463630199\n",
      "epoch: 10 step: 1677, loss is 0.19696438312530518\n",
      "epoch: 10 step: 1678, loss is 0.00014002781244926155\n",
      "epoch: 10 step: 1679, loss is 0.07006381452083588\n",
      "epoch: 10 step: 1680, loss is 0.03497704863548279\n",
      "epoch: 10 step: 1681, loss is 0.003143677953630686\n",
      "epoch: 10 step: 1682, loss is 0.0051408801227808\n",
      "epoch: 10 step: 1683, loss is 0.002922286279499531\n",
      "epoch: 10 step: 1684, loss is 0.026728352531790733\n",
      "epoch: 10 step: 1685, loss is 0.0018761949613690376\n",
      "epoch: 10 step: 1686, loss is 0.00014021609968040138\n",
      "epoch: 10 step: 1687, loss is 0.028343254700303078\n",
      "epoch: 10 step: 1688, loss is 0.0820535197854042\n",
      "epoch: 10 step: 1689, loss is 0.004650943446904421\n",
      "epoch: 10 step: 1690, loss is 0.00477971788495779\n",
      "epoch: 10 step: 1691, loss is 0.03159938380122185\n",
      "epoch: 10 step: 1692, loss is 0.0031428076326847076\n",
      "epoch: 10 step: 1693, loss is 0.002870046766474843\n",
      "epoch: 10 step: 1694, loss is 0.03185011446475983\n",
      "epoch: 10 step: 1695, loss is 0.01644095778465271\n",
      "epoch: 10 step: 1696, loss is 0.1170339360833168\n",
      "epoch: 10 step: 1697, loss is 0.0010822287295013666\n",
      "epoch: 10 step: 1698, loss is 0.006893853656947613\n",
      "epoch: 10 step: 1699, loss is 0.1141979843378067\n",
      "epoch: 10 step: 1700, loss is 0.05737852305173874\n",
      "epoch: 10 step: 1701, loss is 0.0034586521796882153\n",
      "epoch: 10 step: 1702, loss is 0.0011993638472631574\n",
      "epoch: 10 step: 1703, loss is 0.05972336605191231\n",
      "epoch: 10 step: 1704, loss is 0.001826662803068757\n",
      "epoch: 10 step: 1705, loss is 0.0018998471787199378\n",
      "epoch: 10 step: 1706, loss is 0.0003943825722672045\n",
      "epoch: 10 step: 1707, loss is 0.0011703737545758486\n",
      "epoch: 10 step: 1708, loss is 0.00020612857770174742\n",
      "epoch: 10 step: 1709, loss is 0.0005316655733622611\n",
      "epoch: 10 step: 1710, loss is 0.011442972347140312\n",
      "epoch: 10 step: 1711, loss is 0.0013518489431589842\n",
      "epoch: 10 step: 1712, loss is 0.001077479450032115\n",
      "epoch: 10 step: 1713, loss is 0.2320898324251175\n",
      "epoch: 10 step: 1714, loss is 0.0007281151483766735\n",
      "epoch: 10 step: 1715, loss is 0.12403471022844315\n",
      "epoch: 10 step: 1716, loss is 0.0004956789198331535\n",
      "epoch: 10 step: 1717, loss is 0.0201213750988245\n",
      "epoch: 10 step: 1718, loss is 0.0006043866742402315\n",
      "epoch: 10 step: 1719, loss is 0.002883316483348608\n",
      "epoch: 10 step: 1720, loss is 0.16493935883045197\n",
      "epoch: 10 step: 1721, loss is 0.009388194419443607\n",
      "epoch: 10 step: 1722, loss is 0.0002762723306659609\n",
      "epoch: 10 step: 1723, loss is 0.12338113784790039\n",
      "epoch: 10 step: 1724, loss is 0.004146844148635864\n",
      "epoch: 10 step: 1725, loss is 0.0002577319974079728\n",
      "epoch: 10 step: 1726, loss is 0.013609453104436398\n",
      "epoch: 10 step: 1727, loss is 0.06996152549982071\n",
      "epoch: 10 step: 1728, loss is 0.0013450166443362832\n",
      "epoch: 10 step: 1729, loss is 0.03915594890713692\n",
      "epoch: 10 step: 1730, loss is 0.12570255994796753\n",
      "epoch: 10 step: 1731, loss is 0.10435257852077484\n",
      "epoch: 10 step: 1732, loss is 0.05958960950374603\n",
      "epoch: 10 step: 1733, loss is 0.005560858640819788\n",
      "epoch: 10 step: 1734, loss is 0.00606524758040905\n",
      "epoch: 10 step: 1735, loss is 0.015817886218428612\n",
      "epoch: 10 step: 1736, loss is 0.018360456451773643\n",
      "epoch: 10 step: 1737, loss is 0.0022004670463502407\n",
      "epoch: 10 step: 1738, loss is 0.015921199694275856\n",
      "epoch: 10 step: 1739, loss is 0.1097053736448288\n",
      "epoch: 10 step: 1740, loss is 0.0010958820348605514\n",
      "epoch: 10 step: 1741, loss is 0.14445215463638306\n",
      "epoch: 10 step: 1742, loss is 0.003073267638683319\n",
      "epoch: 10 step: 1743, loss is 0.03305535763502121\n",
      "epoch: 10 step: 1744, loss is 0.0012550466926768422\n",
      "epoch: 10 step: 1745, loss is 0.0012672810116782784\n",
      "epoch: 10 step: 1746, loss is 0.002032472752034664\n",
      "epoch: 10 step: 1747, loss is 0.0046670688316226006\n",
      "epoch: 10 step: 1748, loss is 0.02301463857293129\n",
      "epoch: 10 step: 1749, loss is 0.004036876372992992\n",
      "epoch: 10 step: 1750, loss is 0.003105700947344303\n",
      "epoch: 10 step: 1751, loss is 0.006045636720955372\n",
      "epoch: 10 step: 1752, loss is 0.00130881043151021\n",
      "epoch: 10 step: 1753, loss is 0.00297488528303802\n",
      "epoch: 10 step: 1754, loss is 0.037440910935401917\n",
      "epoch: 10 step: 1755, loss is 0.0012715770862996578\n",
      "epoch: 10 step: 1756, loss is 0.024289743974804878\n",
      "epoch: 10 step: 1757, loss is 0.006524356547743082\n",
      "epoch: 10 step: 1758, loss is 0.08009908348321915\n",
      "epoch: 10 step: 1759, loss is 0.010996505618095398\n",
      "epoch: 10 step: 1760, loss is 0.05838698148727417\n",
      "epoch: 10 step: 1761, loss is 0.02129623480141163\n",
      "epoch: 10 step: 1762, loss is 0.0004720210563391447\n",
      "epoch: 10 step: 1763, loss is 5.7471996115054935e-05\n",
      "epoch: 10 step: 1764, loss is 0.0011078831739723682\n",
      "epoch: 10 step: 1765, loss is 0.014026325196027756\n",
      "epoch: 10 step: 1766, loss is 0.0006394736701622605\n",
      "epoch: 10 step: 1767, loss is 0.0630214586853981\n",
      "epoch: 10 step: 1768, loss is 0.00019863882334902883\n",
      "epoch: 10 step: 1769, loss is 0.002072099829092622\n",
      "epoch: 10 step: 1770, loss is 0.03156493231654167\n",
      "epoch: 10 step: 1771, loss is 0.0019001469481736422\n",
      "epoch: 10 step: 1772, loss is 0.0006584846414625645\n",
      "epoch: 10 step: 1773, loss is 0.06342200189828873\n",
      "epoch: 10 step: 1774, loss is 0.00044643590808846056\n",
      "epoch: 10 step: 1775, loss is 0.009121074341237545\n",
      "epoch: 10 step: 1776, loss is 0.00045354824396781623\n",
      "epoch: 10 step: 1777, loss is 0.048756182193756104\n",
      "epoch: 10 step: 1778, loss is 0.020130516961216927\n",
      "epoch: 10 step: 1779, loss is 0.016202274709939957\n",
      "epoch: 10 step: 1780, loss is 0.017267044633626938\n",
      "epoch: 10 step: 1781, loss is 0.01849144510924816\n",
      "epoch: 10 step: 1782, loss is 0.21416857838630676\n",
      "epoch: 10 step: 1783, loss is 0.0003663358511403203\n",
      "epoch: 10 step: 1784, loss is 0.002809931756928563\n",
      "epoch: 10 step: 1785, loss is 0.08006175607442856\n",
      "epoch: 10 step: 1786, loss is 0.07959304004907608\n",
      "epoch: 10 step: 1787, loss is 0.0008507873280905187\n",
      "epoch: 10 step: 1788, loss is 0.024478252977132797\n",
      "epoch: 10 step: 1789, loss is 0.0003321115509606898\n",
      "epoch: 10 step: 1790, loss is 0.03642613813281059\n",
      "epoch: 10 step: 1791, loss is 0.0005185243207961321\n",
      "epoch: 10 step: 1792, loss is 0.08132006227970123\n",
      "epoch: 10 step: 1793, loss is 0.0422140397131443\n",
      "epoch: 10 step: 1794, loss is 0.10981494933366776\n",
      "epoch: 10 step: 1795, loss is 0.1112985908985138\n",
      "epoch: 10 step: 1796, loss is 0.004352715332061052\n",
      "epoch: 10 step: 1797, loss is 0.00804058276116848\n",
      "epoch: 10 step: 1798, loss is 0.00032605911837890744\n",
      "epoch: 10 step: 1799, loss is 0.0621839314699173\n",
      "epoch: 10 step: 1800, loss is 0.05787933990359306\n",
      "epoch: 10 step: 1801, loss is 0.00390856247395277\n",
      "epoch: 10 step: 1802, loss is 0.031767141073942184\n",
      "epoch: 10 step: 1803, loss is 0.0005974047817289829\n",
      "epoch: 10 step: 1804, loss is 0.0022426245268434286\n",
      "epoch: 10 step: 1805, loss is 0.011065765284001827\n",
      "epoch: 10 step: 1806, loss is 0.09528869390487671\n",
      "epoch: 10 step: 1807, loss is 0.00190918508451432\n",
      "epoch: 10 step: 1808, loss is 0.022358201444149017\n",
      "epoch: 10 step: 1809, loss is 0.0008339557098224759\n",
      "epoch: 10 step: 1810, loss is 0.12134120613336563\n",
      "epoch: 10 step: 1811, loss is 0.0610966719686985\n",
      "epoch: 10 step: 1812, loss is 0.02320198342204094\n",
      "epoch: 10 step: 1813, loss is 0.008522914722561836\n",
      "epoch: 10 step: 1814, loss is 0.008310993202030659\n",
      "epoch: 10 step: 1815, loss is 0.0009964772034436464\n",
      "epoch: 10 step: 1816, loss is 0.001514527015388012\n",
      "epoch: 10 step: 1817, loss is 0.000672383583150804\n",
      "epoch: 10 step: 1818, loss is 0.012336148880422115\n",
      "epoch: 10 step: 1819, loss is 0.00484886160120368\n",
      "epoch: 10 step: 1820, loss is 0.25893324613571167\n",
      "epoch: 10 step: 1821, loss is 0.0003200013015884906\n",
      "epoch: 10 step: 1822, loss is 7.931241270853207e-05\n",
      "epoch: 10 step: 1823, loss is 0.030346563085913658\n",
      "epoch: 10 step: 1824, loss is 0.003401990979909897\n",
      "epoch: 10 step: 1825, loss is 0.32115837931632996\n",
      "epoch: 10 step: 1826, loss is 0.0980677455663681\n",
      "epoch: 10 step: 1827, loss is 0.061434563249349594\n",
      "epoch: 10 step: 1828, loss is 0.004370375070720911\n",
      "epoch: 10 step: 1829, loss is 0.0004331675299908966\n",
      "epoch: 10 step: 1830, loss is 0.02621776983141899\n",
      "epoch: 10 step: 1831, loss is 0.011099305003881454\n",
      "epoch: 10 step: 1832, loss is 0.03192104399204254\n",
      "epoch: 10 step: 1833, loss is 0.009520627558231354\n",
      "epoch: 10 step: 1834, loss is 0.10556813329458237\n",
      "epoch: 10 step: 1835, loss is 0.009971660561859608\n",
      "epoch: 10 step: 1836, loss is 0.0008188362699002028\n",
      "epoch: 10 step: 1837, loss is 0.002075975062325597\n",
      "epoch: 10 step: 1838, loss is 0.04391007870435715\n",
      "epoch: 10 step: 1839, loss is 0.004048474133014679\n",
      "epoch: 10 step: 1840, loss is 0.008343175053596497\n",
      "epoch: 10 step: 1841, loss is 0.09080830961465836\n",
      "epoch: 10 step: 1842, loss is 0.06084462255239487\n",
      "epoch: 10 step: 1843, loss is 0.0006869198987260461\n",
      "epoch: 10 step: 1844, loss is 0.0042729503475129604\n",
      "epoch: 10 step: 1845, loss is 0.0259800273925066\n",
      "epoch: 10 step: 1846, loss is 0.004747490398585796\n",
      "epoch: 10 step: 1847, loss is 0.06276503950357437\n",
      "epoch: 10 step: 1848, loss is 6.149683758849278e-05\n",
      "epoch: 10 step: 1849, loss is 0.0006308052106760442\n",
      "epoch: 10 step: 1850, loss is 0.12499044835567474\n",
      "epoch: 10 step: 1851, loss is 0.0034803985618054867\n",
      "epoch: 10 step: 1852, loss is 0.0010007196106016636\n",
      "epoch: 10 step: 1853, loss is 0.05065430328249931\n",
      "epoch: 10 step: 1854, loss is 0.0007125385454855859\n",
      "epoch: 10 step: 1855, loss is 0.003959464840590954\n",
      "epoch: 10 step: 1856, loss is 0.02268686145544052\n",
      "epoch: 10 step: 1857, loss is 0.0017925896681845188\n",
      "epoch: 10 step: 1858, loss is 0.055991243571043015\n",
      "epoch: 10 step: 1859, loss is 0.031702250242233276\n",
      "epoch: 10 step: 1860, loss is 0.018935875967144966\n",
      "epoch: 10 step: 1861, loss is 0.06259261071681976\n",
      "epoch: 10 step: 1862, loss is 0.0888746827840805\n",
      "epoch: 10 step: 1863, loss is 0.020227115601301193\n",
      "epoch: 10 step: 1864, loss is 0.0020068008452653885\n",
      "epoch: 10 step: 1865, loss is 0.04597335308790207\n",
      "epoch: 10 step: 1866, loss is 0.019080881029367447\n",
      "epoch: 10 step: 1867, loss is 0.06576462835073471\n",
      "epoch: 10 step: 1868, loss is 0.021636953577399254\n",
      "epoch: 10 step: 1869, loss is 0.014407088048756123\n",
      "epoch: 10 step: 1870, loss is 0.01009941566735506\n",
      "epoch: 10 step: 1871, loss is 0.020924948155879974\n",
      "epoch: 10 step: 1872, loss is 0.012145396322011948\n",
      "epoch: 10 step: 1873, loss is 0.0004627480811905116\n",
      "epoch: 10 step: 1874, loss is 0.02395828627049923\n",
      "epoch: 10 step: 1875, loss is 0.0013913611182942986\n"
     ]
    }
   ],
   "source": [
    "# 实现训练部分代码，并打印训练过程中的loss值，[建议]可视化查看loss值的变化\n",
    "loss_monitor = ms.LossMonitor()\n",
    "model.train(epoch=10, train_dataset=train_dataset,callbacks=loss_monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a45123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.save_checkpoint(net, \"net.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c1fa409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Accuracy:{'Accuracy': 0.9782652243589743} ==============\n"
     ]
    }
   ],
   "source": [
    "## test\n",
    "def test_net(network, model,path):\n",
    "    \"\"\"Define the evaluation method.\"\"\"\n",
    "    # 加载已保存的模型\n",
    "    param_dict = ms.load_checkpoint(path)\n",
    "    # load parameter to the network\n",
    "    ms.load_param_into_net(network, param_dict)\n",
    "    # evaluation\n",
    "    acc = model.eval(test_dataset, dataset_sink_mode=False)\n",
    "    print(\"============== Accuracy:{} ==============\".format(acc))\n",
    "\n",
    "# 修改为你的checkpoint路径\n",
    "test_net(net, model,r\"D:\\machine learning\\Lab4-LeNet(1)\\net.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5cb9523a3ce612da594b1721626e95f90edbb30067f86e1c6d953bc531f0cdd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
